[{"body":" This page provides a brief introduction to chip verification, including concepts used in examples such as DUT (Design Under Test) and RM (Reference Model).\nThe chip verification process needs to align with the actual situation of the company or team. There is no absolute standard that meets all requirements and must be referenced.\nWhat is Chip Verification? The chip design-to-production process involves three main stages: chip design, chip manufacturing, and chip packaging/testing. Chip design is further divided into front-end and back-end design. Front-end design, also known as logic design, aims to achieve the desired circuit logic functionality. Back-end design, or physical design, focuses on optimizing layout and routing to reduce chip area, lower power consumption, and increase frequency. Chip verification is a critical step in the chip design process. Its goal is to ensure that the designed chip meets the specified requirements in terms of functionality, performance, and power consumption. The verification process typically includes functional verification, timing verification, and power verification, using methods and tools such as simulation, formal verification, hardware acceleration, and prototyping. For this tutorial, chip verification refers only to the verification of the front-end design to ensure that the circuit logic meets the specified requirements (“Does this proposed design do what is intended?”), commonly known as functional verification. This does not include back-end design aspects like power and frequency.\nFor chip products, design errors that make it to production can be extremely costly to fix, as it might require recalling products and remanufacturing chips, incurring significant financial and time costs. Here are some classic examples of failures due to inadequate chip verification: Intel Pentium FDIV Bug：In 1994, Intel’s Pentium processor was found to have a severe division error known as the FDIV bug. This error was due to incorrect entries in a lookup table within the chip’s floating-point unit. Although it rarely affected most applications, it caused incorrect results in specific calculations. Intel had to recall a large number of processors, leading to significant financial losses.\nAriane 5 Rocket Failure：Though not a chip example, this highlights the importance of hardware verification. In 1996, the European Space Agency’s Ariane 5 rocket exploded shortly after launch due to an overflow when converting a 64-bit floating-point number to a 16-bit integer in the navigation system, causing the system to crash. This error went undetected during design and led to the rocket’s failure.\nAMD Barcelona Bug：In 2007, AMD’s Barcelona processor had a severe Translation Lookaside Buffer (TLB) error that could cause system crashes or reboots. AMD had to mitigate this by lowering the processor’s frequency and releasing BIOS updates, which negatively impacted their reputation and financial status.\nThese cases emphasize the importance of chip verification. Errors detected and fixed during the design phase can prevent these costly failures. Insufficient verification continues to cause issues today, such as a new entrant in the ASIC chip market rushing a 55nm chip without proper verification, leading to three failed tape-outs and approximately $500,000 in losses per failure.\nChip Verification Process The coupling relationship between chip design and verification is shown in the diagram above. Both design and verification have the same input: the specification document. Based on this document, both design and verification teams independently code according to their understanding and requirements. The design team needs to ensure that the RTL code is “synthesizable,” considering circuit characteristics, while the verification team mainly focuses on whether the functionality meets the requirements, with fewer coding constraints. After both teams complete module development, a sanity test is conducted to check if the functionality matches. If there are discrepancies, collaborative debugging is done to identify and fix issues before retesting. Due to the high coupling between chip design and verification, some companies directly couple their design and verification teams, assigning verification teams to each design submodule. The coupling process in the diagram is coarse-grained, with specific chips (e.g., SoC, DDR) and companies having their cooperation models.\nIn the above comparison test, the module produced by the design team is usually called DUT (Design Under Test), while the model developed by the verification team is called RM (Reference Model). The verification process includes: writing a verification plan, creating a verification platform, organizing functional points, constructing test cases, running and debugging, collecting bugs/coverage, regression testing, and writing test reports.\nVerification Plan： The verification plan describes how verification will be carried out and how verification quality will be ensured to meet functional verification requirements. It typically includes verification goals, strategies, environment, items, process, risk mitigation, resources, schedule, results, and reports. Verification goals specify the functions or performance metrics to be verified, directly extracted from the chip specification. Verification strategy outlines the methods to be used, such as simulation, formal verification, FPGA acceleration, etc., and how to organize the verification tasks. The verification environment details the specific testing environment, including verification tools and versions. The verification item library lists specific items to be verified and expected results. Verification plans can be general or specific to sub-tasks.\nPlatform Setup： The verification platform is the execution environment for specific verification tasks. Similar verification tasks can use the same platform. Setting up the platform is a key step, including choosing verification tools (e.g., software simulation, formal verification, hardware acceleration), configuring the environment (e.g., server, FPGA), creating the test environment, and basic test cases. Initial basic test cases are often called “smoke tests.” Subsequent test codes are based on this platform, so it must be reusable. The platform includes the test framework, the code being tested, and basic signal stimuli.\nOrganizing Functional Points： This involves listing the DUT’s basic functions based on the specification manual and detailing how to test each function. Functional points are prioritized based on importance, risk, and complexity. They also need to be tracked for status, with updates synchronized to the plan if changes occur.\nTest Cases These are conditions or variables used to determine if the DUT meets specific requirements and operates correctly. Each case includes test conditions, input data, expected results, actual results, and test outcomes. Running test cases and comparing expected vs. actual results help verify the system or application’s correct implementation of functions or requirements. Test cases are crucial tools for verifying chip design against specifications.\nCoding Implementation： This is the execution of test cases, including generating test data, selecting the test framework, programming language, and writing the reference model. This phase requires a deep understanding of functional points and test cases. Misunderstandings can lead to the DUT being undrivable or undetected bugs.\nCollecting Bugs/Coverage： The goal of verification is to find design bugs early, so collected bugs need unique identifiers, severity ratings, and status tracking with design engineers. Discovering bugs is ideal, but since not every test finds bugs, coverage is another metric to evaluate verification thoroughness. Sufficient verification is indicated when coverage (e.g., code coverage \u003e90%) exceeds a threshold.\nRegression Testing： As verification and design are iterative, regression tests ensure the modified DUT still functions correctly after bug fixes. This catches new errors or reactivates old ones due to changes. Regression tests can be comprehensive or selective, covering all functions or specific parts.\nTest Report： This summarizes the entire verification process, providing a comprehensive view of the testing activities, including objectives, executed test cases, discovered issues, coverage, and efficiency.\nLevels of Chip Verification Chip verification typically includes four levels based on the object size: UT, BT, IT, and ST.\nUnit Testing（UT）： The lowest verification level, focusing on single modules or components to ensure their functionality is correct.\nBlock Testing (BT) ： Modules often have tight coupling, making isolated UT testing complex. BT merges several coupled modules into one DUT block for testing.\nIntegration Testing (IT) ： Builds on UT by combining multiple modules or components to verify their collaborative functionality, usually testing subsystem functionality.\nSystem Testing (ST) ： Also called Top verification, ST combines all modules or components into a complete system to verify overall functionality and performance requirements.\nIn theory, these levels follow a bottom-up order, each building on the previous level. However, practical verification activities depend on the scale, expertise, and functional needs of the enterprise, so not all levels are always involved. At each level, relevant test cases are written, tests run, and results analyzed to ensure the chip design’s correctness and quality.\nChip Verification Metrics Verification metrics typically include functional correctness, test coverage, defect density, verification efficiency, and verification cost. Functional correctness is the fundamental metric, ensuring the chip executes its designed functions correctly. This is validated through functional test cases, including normal and robustness tests. Test coverage indicates the extent to which test cases cover design functionality, with higher coverage implying higher verification quality. Coverage can be further divided into code coverage, functional coverage, condition coverage, etc. Defect density measures the number of defects found in a given design scale or code volume, with lower density indicating higher design quality. Verification efficiency measures the amount of verification work completed within a given time and resource frame, with higher efficiency indicating higher productivity. Verification cost encompasses all resources required for verification, including manpower, equipment, and time, with lower costs indicating higher cost-effectiveness.\nFunctional correctness is the absolute benchmark for verification. However, in practice, it is often impossible to determine if the test plan is comprehensive and if all test spaces have been adequately covered. Therefore, a quantifiable metric is needed to guide whether verification is sufficient and when it can be concluded. This metric is commonly referred to as “test coverage.” Test coverage typically includes code coverage (lines, functions, branches) and functional coverage.\nCode Line Coverage： This indicates how many lines of the DUT design code were executed during testing.\nFunction Coverage： This indicates how many functions of the DUT design code were executed during testing.\nBranch Coverage： This indicates how many branches (if-else) of the DUT design code were executed during testing.\nFunctional Coverage： This indicates how many predefined functions were triggered during testing.\nHigh code coverage can improve the quality and reliability of verification but does not guarantee complete correctness since it cannot cover all input and state combinations. Therefore, in addition to pursuing high code coverage, other testing methods and metrics, such as functional testing, performance testing, and defect density, should be combined.\nChip Verification Management Chip verification management is a comprehensive process that encompasses all activities in the chip verification process, including the development of verification strategies, the setup of the verification environment, the writing and execution of test cases, the collection and analysis of results, and the tracking and resolution of issues and defects. The goal of chip verification management is to ensure that the chip design meets all functional and performance requirements, as well as specifications and standards.\nIn chip verification management, the first step is to formulate a detailed verification strategy, including objectives, scope, methods, and schedules. Then, a suitable verification environment must be set up, including hardware, software tools, and test data. Next, a series of test cases covering all functional and performance points must be written and executed, with results collected and analyzed to identify problems and defects. Finally, these issues and defects need to be tracked and fixed until all test cases pass.\nChip verification management is a complex process requiring a variety of skills and knowledge, including chip design, testing methods, and project management. It requires close collaboration with other activities, such as chip design, production, and sales, to ensure the quality and performance of the chip. The effectiveness of chip verification management directly impacts the success of the chip and the company’s competitiveness. Therefore, chip verification management is a crucial part of the chip development process.\nThe chip verification management process can be based on a “project management platform” and a “bug management platform,” with platform-based management typically being significantly more efficient than manual management.\nCurrent State of Chip Verification Currently, chip verification is typically completed within chip design companies. This process is not only technically complex but also entails significant costs. Given the close relationship between acceptance and design, chip verification inevitably involves the source code of the chip design. However, chip design companies usually consider the source code as a trade secret, necessitating internal personnel to perform the verification, making outsourcing difficult.\nThe importance of chip verification lies in ensuring that the designed chip operates reliably under various conditions. Verification is not only for meeting technical specifications but also for addressing the growing complexity and emerging technology demands. As the semiconductor industry evolves, the workload of chip verification has been continuously increasing, especially for complex chips, where verification work has exceeded design work, accounting for more than 70%. This means that in terms of engineer personnel ratio, verification engineers are usually twice the number of design engineers (e.g., in a team of three thousand at Zeku, there are about one thousand design engineers and two thousand verification engineers. Similar or higher ratios apply to other large chip design companies).\nDue to the specificity of verification work, which requires access to the chip design source code, it significantly limits the possibility of outsourcing chip verification. The source code is considered the company’s core trade secret, involving technical details and innovations, thus making it legally and securely unfeasible to share with external parties. Consequently, internal personnel must shoulder the verification work, increasing the internal workload and costs.\nGiven the current situation, the demand for chip verification engineers continues to grow. They need a solid technical background, familiarity with various verification tools and methods, and keen insight into emerging technologies. Due to the complexity of verification work, verification teams typically need a large scale, contrasting sharply with the design team size.\nTo meet this challenge, the industry may need to continuously explore innovative verification methods and tools to improve efficiency and reduce costs.\nSummary: Complex Chip Verification Costs High Verification Workload： For complex chips, verification work accounts for over 70% of the entire chip design work.\nHigh Labor Costs： The number of verification engineers is twice that of design engineers, with complex tasks requiring thousands of engineers.\nInternal Verification： To ensure trade secrets (chip design code) are not leaked, chip design companies can only hire a large number of verification engineers to perform verification work internally.\nCrowdsourcing Chip Verification In contrast to hardware, the software field has already made testing outsourcing (subcontracting) a norm to reduce testing costs. This business is highly mature, with a market size in the billions of yuan, advancing towards the trillion-yuan scale. From the content perspective, software testing and hardware verification share significant similarities (different targets with the same system objective). Is it feasible to subcontract hardware verification in the same way as software?\nCrowdsourcing chip verification faces many challenges, such as: Small Number of Practitioners： Compared to the software field, the number of hardware developers is several orders of magnitude smaller. For instance, according to GitHub statistics (https://madnight.github.io/githut/#/pull_requests/2023/2), traditional software programming languages (Python, Java, C++, Go) account for nearly 50%, whereas hardware description languages like Verilog account for only 0.076%, reflecting the disparity in developer numbers.\nCommercial Verification Tools： The verification tools used in enterprises (simulators, formal verification, data analysis) are almost all commercial tools, which are nearly invisible to ordinary people and difficult to self-learn.\nLack of Open Learning Materials： Chip verification involves accessing the chip design source code, which is typically regarded as the company’s trade secrets and proprietary technology. Chip design companies may be unwilling to disclose detailed verification processes and techniques, limiting the availability of learning materials.\nFeasibility Analysis Although the chip verification field has been relatively closed, from a technical perspective, adopting a subcontracting approach for verification is a feasible option due to several factors:\nFirstly, with the gradual increase of open-source chip projects, the source code involved in verification has become more open and transparent. These open-source projects do not have concerns about trade secrets in their design and verification process, providing more possibilities for learning and research. Even if some projects involve trade secrets, encryption and other methods can be used to hide design codes, addressing trade secret issues to a certain extent and making verification easier to achieve.\nSecondly, many fundamental verification tools have emerged in the chip verification field, such as Verilator and SystemC. These tools provide robust support for verification engineers, helping them perform verification work more efficiently. These tools alleviate some of the complexity and difficulty of the verification process, providing a more feasible technical foundation for adopting subcontracted verification methods.\nIn the open-source software field, some successful cases can be referenced. For example, the Linux kernel verification process adopts a subcontracting approach, with different developers and teams responsible for verifying different modules, ultimately forming a complete system. Similarly, in the machine learning field, the ImageNet project adopted a crowdsourced annotation strategy, completing large-scale image annotation tasks through crowdsourcing. These cases provide successful experiences for the chip verification field, proving the potential of subcontracted verification to improve efficiency and reduce costs.\nTherefore, despite the chip verification field being relatively closed compared to other technical fields, technological advances and the increase of open-source projects offer new possibilities for adopting subcontracted verification. By drawing on successful experiences from other fields and utilizing existing verification tools, we can promote the application of more open and efficient verification methods in chip verification, further advancing the industry. This openness and flexibility in technology will provide more choices for verification engineers, promoting innovative and diverse development in the chip verification field.\nTechnical Route To overcome challenges and engage more people in chip verification, this project continuously attempts the following technical directions:\nProvide Multi-language Verification Tools： Traditional chip verification is based on the System Verilog programming language, which has a small user base. To allow other software development/testing professionals to participate in chip verification, this project provides multi-language verification conversion tools Picker, enabling verifiers to use familiar programming languages (e.g., C++, Python, Java, Go) with open-source verification tools.\nProvide Verification Learning Materials： The scarcity of chip verification learning materials is mainly due to the improbability of commercial companies disclosing internal data. Therefore, this project will continuously update learning materials, allowing verifiers to learn the necessary skills online for free.\nProvide Real Chip Verification Cases： To make the learning materials more practical, this project uses the “Xiangshan Kunming Lake (an industrial-grade high-performance RISC-V processor) IP core” as a basis, continuously updating verification cases by extracting modules from it.\nOrganize Chip Design Subcontracted Verification： Applying what is learned is the goal of every learner. Therefore, this project periodically organizes subcontracted chip design verification, allowing everyone (whether you are a university student, verification expert, software developer, tester, or high school student) to participate in real chip design work.\nThe goal of this project is to achieve the following vision: “Open the black box of traditional verification modes, allowing anyone interested to participate in chip verification anytime, anywhere, using their preferred programming language.”\n","categories":"","description":"Basic concepts of chip verification\n","excerpt":"Basic concepts of chip verification\n","ref":"/mlvp/en/docs/basic/ic_verify/","tags":"","title":"Chip Verification"},{"body":"Verification Report Chinese version:\nhttps://github.com/XS-MLVP/Example-NutShellCache/blob/master/nutshell_cache_report_demo.pdf\nEnglish verision:\nTBD\nVerification Environment \u0026 Test Case Code https://github.com/XS-MLVP/Example-NutShellCache\n","categories":["Example Projects","Tutorials"],"description":"Verification of Nutshell Cache using Python.","excerpt":"Verification of Nutshell Cache using Python.","ref":"/mlvp/en/docs/advance_case/nutshellcache/","tags":["examples","docs"],"title":"Complete Verification of Nutshell Cache"},{"body":" As chip designs grow in complexity, verification effort and time increase dramatically, while LLM capabilities have surged. UCAgent is an LLM-driven automation agent for hardware unit-test verification, aiming to reduce repetitive verification work via staged workflow and tool orchestration. This document covers Introduction, Installation, Usage, Workflow, and Advanced.\nIntroduction Background Verification time already accounts for 50–60% of chip development; design engineers spend ~49% of their time on verification, yet first-silicon success rate in 2024 was only ~14%. With the rise of LLMs and coding agents, reframing “hardware verification” as a “software testing problem” enables high automation. What is UCAgent An LLM-driven AI agent for unit-test (UT) of chip designs, centered on a staged workflow + tool orchestration to semi/fully automate requirement understanding, test generation, execution, and report. Collaboration-first: user-led with LLM as assistant. Built on Picker \u0026 Toffee; DUTs are tested as Python packages; integrates with OpenHands/Copilot/Claude Code/Gemini-CLI/Qwen Code via MCP. Capabilities and Goals Semi/fully automated: generate/refine tests and docs, run cases, and summarize reports. Completeness: functional coverage, line coverage, and doc consistency. Integrable: standard CLI, TUI; MCP server for external code agents. Goal: reduce repetitive human effort in verification. Installation System Requirements Python: 3.11+ OS: Linux / macOS API: OpenAI-compatible API Memory: 4GB+ recommended Dependency: picker (export Verilog DUT to a Python package) Methods Method 1: Clone and install\ngit clone https://github.com/XS-MLVP/UCAgent.git cd UCAgent pip3 install . Method 2: pip install\npip3 install git+https://git@github.com/XS-MLVP/UCAgent@main ucagent --help # verify installation Usage Quick Start Install UCAgent via pip\npip3 install git+https://git@github.com/XS-MLVP/UCAgent@main Prepare DUT\nCreate directory {workspace}/Adder where {workspace} is where ucagent runs.\nmkdir -p Adder RTL: use the adder from Quick Start: https://open-verify.cc/mlvp/docs/quick-start/eg-adder/ and put it at Adder/Adder.v.\nInject a bug: change output width to 63-bit (demonstrate width error).\nChange line with output [WIDTH-1:0] sum, to output [WIDTH-2:0] sum, (e.g. line 9). Current Verilog:\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-2:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule Export RTL to Python Module picker can package the RTL verification module into a shared library and provide Python APIs to drive the circuit. See Env Usage - Picker and picker docs\nFrom {workspace} run: picker export Adder/Adder.v --rw 1 --sname Adder --tdir output/ -c -w output/Adder/Adder.fst Write README Document adder description, verification goals, bug analysis, etc. in Adder/README.md and copy it to output/Adder/README.md. Install Qwen Code CLI Install globally via npm: sudo npm install -g @qwen-code/qwen-code (requires nodejs). More ways: Qwen Code Deployment Configure Qwen Code CLI Edit ~/.qwen/settings.json: { \"mcpServers\": { \"unitytest\": { \"httpUrl\": \"http://localhost:5000/mcp\", \"timeout\": 10000 } } } Start MCP Server In {workspace}: ucagent output/ Adder -s -hm --tui --mcp-server-no-file-tools --no-embed-tools Seeing the following UI means success: Start Qwen Code In UCAgent/output run qwen to start Qwen Code; you should see \u003eQWEN. Start verification In the console, enter the prompt and approve Qwen Code tool/command/file permission requests via j/k: Please get your role and basic guidance via RoleInfo, then complete the task. Use ReadTextFile to read files. Operate only within the current working directory; do not go outside it.\nSometimes Qwen Code pauses. You can confirm via the server TUI whether tasks are finished.\nIf Mission shows stage still at 13, continue execution.\nIf paused mid-way, simply type “continue” to proceed.\nResults All results are under output:\n. ├── Adder # packaged Python DUT ├── Guide_Doc # various template / specification files ├── uc_test_report # toffee-test report (index.html etc.) └── unity_test # generated verification docs and test cases └── tests # test case source and support files Guide_Doc: these files are “specification / example / template” style reference documents. On startup they are copied from vagent/lang/zh/doc/Guide_Doc into the workspace Guide_Doc/ (here with output as workspace it is output/Guide_Doc/). They are not executed directly. They serve humans and AI as paradigms and norms for writing unity_test documentation and tests, and are read by semantic-retrieval tools during initialization.\ndut_functions_and_checks.md\nPurpose: defines the organization and naming norms for Function Groups FG-, Function Points FC-, and Check Points CK-*. Must cover all function points; at least one check per function point.\nFinal artifact: unity_test/{DUT}_functions_and_checks.md (e.g. Adder_functions_and_checks.md). dut_fixture.md\nPurpose: explains how to write the DUT Fixture / Env (interfaces, timing, reset, stimulus, monitor, check, hooks, etc.), giving standard form and required items.\nArtifact: unity_test/DutFixture and EnvFixture related implementation / docs. dut_api_instruction.md\nPurpose: DUT API design \u0026 documentation spec (naming, parameters, returns, constraints, boundary conditions, error handling, examples).\nArtifact: unity_test/{DUT}_api.md or the API implementation + tests (e.g. Adder_api.py). dut_function_coverage_def.md\nPurpose: functional coverage definition method; how to derive coverage items (covergroup / coverpoint / bin) from FG/FC/CK, and organization / naming rules.\nArtifact: coverage definition file and generated coverage data, plus related explanatory doc (e.g. Adder_function_coverage_def.py). dut_line_coverage.md\nPurpose: line coverage collection and analysis method; how to enable, count, interpret missed lines, and locate redundant or missing tests.\nArtifact: line coverage data file and analysis notes (unity_test/{DUT}_line_coverage_analysis.md, e.g. Adder_line_coverage_analysis.md). dut_test_template.md\nPurpose: skeleton / template for test cases; minimal viable structure and writing paradigm (Arrange-Act-Assert, setup/teardown, marks/selectors, etc.).\nArtifact: baseline structural reference for concrete test files under tests/. dut_test_case.md\nPurpose: single test case authoring spec (naming, input space, boundary / exceptional cases, reproducibility, assertion quality, logging, marks).\nArtifact: quality baseline and fill requirements for tests/test_xxx.py::test_yyy cases. dut_test_program.md\nPurpose: test plan / test orchestration (regression sets, layered / staged execution, marks \u0026 selection, timeout control, ordering, dependencies).\nArtifact: regression configuration, commands / scripts, staged execution strategy docs. dut_test_summary.md\nPurpose: structure of stage / final summary (pass rate, coverage, main issues, fix status, risks / remaining problems, next plans).\nArtifact: unity_test/{DUT}_test_summary.md (e.g. Adder_test_summary.md) or report page (output/uc_test_report). dut_bug_analysis.md\nPurpose: Bug recording \u0026 analysis spec (reproduction steps, root cause, impact scope, fix suggestion, verification status, tags \u0026 tracking).\nArtifact: unity_test/{DUT}_bug_analysis.md (e.g. Adder_bug_analysis.md). uc_test_report: generated by toffee-test (index.html).\nContains line coverage, functional coverage, test case pass status, function point marks, etc.\nunity_test/tests: verification code directory:\nAdder.ignore\nRole: line coverage ignore list. Supports ignoring entire files or code segments via start-end line ranges.\nUsed by: Adder_api.py through set_line_coverage(request, get_coverage_data_path(request, new_path=False), ignore=current_path_file(\"Adder.ignore\")).\nRelation to Guide_Doc: references dut_line_coverage.md (explains enabling / counting / analyzing line coverage, and meaning / scenarios for ignore rules). Adder_api.py\nRole: test common base: concentrates DUT construction, coverage wiring \u0026 sampling, pytest base fixtures and sample API.\nIncludes: create_dut(request): instantiate DUT, set coverage file, optional waveform, bind StepRis sampling. AdderEnv: encapsulates pins and common operations (Step). api_Adder_add: exposed test API completing parameter validation, signal assignment, stepping, result read. pytest fixtures: dut (module scope, coverage sampling / collection for toffee_test), env (function scope, fresh environment per test).\nRelation to Guide_Doc: dut_fixture.md: organization of fixtures / environment, Step / StepRis usage and responsibility boundaries. dut_api_instruction.md: API design (naming, parameter constraints, returns, examples, exceptions) and doc spec. dut_function_coverage_def.md: how functional coverage groups are wired to DUT and sampled in StepRis. dut_line_coverage.md: setting line coverage file, ignore list, and reporting data to toffee_test. Adder_function_coverage_def.py\nRole: functional coverage definition: declares FG/FC/CK and watchpoint conditions.\nDefines coverage groups: FG-API, FG-ARITHMETIC, FG-BIT-WIDTH. Under each group defines FC- and CK-_ conditions (e.g. CK-BASIC / CK-CARRY-IN / CK-OVERFLOW etc.). get_coverage_groups(dut): initialize and return group list for binding \u0026 sampling in Adder_api.py.\nRelation to Guide_Doc: dut_function_coverage_def.md: organization / naming of groups / points, expression of watch_point. dut_functions_and_checks.md: source naming system \u0026 mapping; test mark_function coverage must align. test_Adder_api_basic.py\nRole: API-level basic function tests: typical inputs, carry, zero, overflow, boundary, etc.\nUses from Adder_api import * to get fixtures (dut/env) and API.\nIn each test: env.dut.fc_cover[“FG-…”].mark_function(“FC-…”, \u003ctest_fn\u003e, [“CK-…”]) to mark functional coverage hits.\nRelation to Guide_Doc: dut_test_case.md: single-test structure (goal / flow / expectation), naming \u0026 assertion norms, reproducibility, marks \u0026 logs. dut_functions_and_checks.md: correct referencing \u0026 marking of FG/FC/CK. dut_test_template.md: docstring \u0026 structure paradigm. test_Adder_functional.py\nRole: functional behavior tests (scenario / function-item angle), more comprehensive coverage than API basics.\nAlso uses mark_function with FG/FC/CK tags.\nRelation to Guide_Doc: dut_test_case.md: writing norms \u0026 assertion requirements for functional tests. dut_functions_and_checks.md: coverage marking norms \u0026 completeness. dut_test_template.md: organizational paradigm. test_example.py\nRole: blank example (scaffold) for minimal template when adding new test files.\nRelation to Guide_Doc: dut_test_template.md: template for structure, imports, marking method when creating new tests. unity_test/*.md: verification-related docs:\nAdder_basic_info.md\nPurpose: DUT overview \u0026 interface description (function, ports, types, coarse-grained function classification).\nReference: Guide_Doc/dut_functions_and_checks.md (interface / function classification wording), Guide_Doc/dut_fixture.md (describe I/O \u0026 Step from verification view). Adder_verification_needs_and_plan.md\nPurpose: verification needs \u0026 plan (goals, risk points, test item planning, methodology).\nReference: Guide_Doc/dut_test_program.md (orchestration \u0026 selection strategy), Guide_Doc/dut_test_case.md (test quality requirements), Guide_Doc/dut_functions_and_checks.md (mapping from needs to FG/FC/CK). Adder_functions_and_checks.md\nPurpose: source list of FG/FC/CK; test marking \u0026 functional coverage definitions must match.\nReference: Guide_Doc/dut_functions_and_checks.md (structure / naming), Guide_Doc/dut_function_coverage_def.md (materialization as coverage implementation). Adder_line_coverage_analysis.md\nPurpose: line coverage conclusions \u0026 analysis: explain ignore list, missed lines, supplement suggestions.\nReference: Guide_Doc/dut_line_coverage.md; plus tests directory Adder.ignore. Adder_bug_analysis.md\nPurpose: defect analysis report: CK/TC correspondence, confidence, root cause, fix suggestions, regression method.\nReference: Guide_Doc/dut_bug_analysis.md (structure / elements), Guide_Doc/dut_functions_and_checks.md (naming consistency). Adder_test_summary.md\nPurpose: stage / final test summary (execution stats, coverage status, defect distribution, suggestions, conclusions).\nReference: Guide_Doc/dut_test_summary.md, echoes Guide_Doc/dut_test_program.md. Process Summary What to do:\nPackage the DUT (e.g. Adder) as a testable Python module Start UCAgent (optionally with MCP Server) to let the code agent collaborate and advance verification by stages According to Guide_Doc norms generate / refine unity_test docs and tests, driving by functional + line coverage Discover and analyze defects; produce reports and conclusions What was done:\nUsed picker to export RTL as Python package (output/Adder/), prepared minimal README \u0026 file list Started ucagent (with --mcp-server / --mcp-server-no-file-tools), collaborated under TUI / MCP Under Guide_Doc constraints, generated / completed: Function \u0026 check list: unity_test/Adder_functions_and_checks.md (FG/FC/CK) Fixture / environment \u0026 API: tests/Adder_api.py (create_dut, AdderEnv, api_Adder_*) Functional coverage definition: tests/Adder_function_coverage_def.py (bind StepRis sampling) Line coverage config \u0026 ignore: tests/Adder.ignore, analysis unity_test/Adder_line_coverage_analysis.md Test case implementation: tests/test_*.py (mark_function with FG/FC/CK) Defect analysis \u0026 summary: unity_test/Adder_bug_analysis.md, unity_test/Adder_test_summary.md Advanced via tool orchestration: RunTestCases / Check / StdCheck / KillCheck / Complete / GoToStage Write permissions restricted to unity_test/ and tests (add_un_write_path / del_un_write_path) Achieved effects:\nSemi/fully automated generation of compliant docs and a regression-capable test set (supports full and targeted regression) Functional and line coverage data complete; missed points can be located and supplemented Defect root cause, fix suggestions, and verification method are evidence-based; structured report formed (uc_test_report/index.html) Supports MCP integration and TUI collaboration; process can pause / inspect / patch; easy iteration \u0026 reuse Typical operation track (when stuck):\nCheck → StdCheck(lines=-1) → KillCheck → fix → Check → Complete ","categories":["Tutorial"],"description":"Overview and installation.","excerpt":"Overview and installation.","ref":"/mlvp/en/docs/ucagent/introduce/","tags":["docs"],"title":"Introduction"},{"body":"MCP Integration (Recommended) Integrate Code Agent Collaborate with external CLI via MCP. This mode works with all LLM clients that support MCP-Server invocation, such as Cherry Studio, Claude Code, Gemini-CLI, VS Code Copilot, Qwen Code, etc. Daily usage is to use make directly; for detailed commands see Quick Start, or check the root Makefile.\nPrepare RTL and the corresponding SPEC docs under examples/{dut}. {dut} is the module name; if it is Adder, the directory is examples/Adder.\nPackage RTL, place docs, and start MCP server: make mcp_{dut} (e.g., make mcp_Adder).\nConfigure your MCP client:\n{ \"mcpServers\": { \"unitytest\": { \"httpUrl\": \"http://localhost:5000/mcp\", \"timeout\": 10000 } } } Start the client: for Qwen Code, run qwen under UCAgent/output, then input the prompt.\nprompt:\nPlease get your role and basic guidance via RoleInfo, then complete the task. Use ReadTextFile to read files. Operate only within the current working directory; do not go outside it.\n","categories":["Tutorial"],"description":"How to use UCAgent via MCP integration mode.","excerpt":"How to use UCAgent via MCP integration mode.","ref":"/mlvp/en/docs/ucagent/usage/mcp/","tags":["docs"],"title":"MCP Integration Mode (Recommended)"},{"body":"MCP 集成（推荐）集成Code Agent 基于 MCP 的外部编程 CLI 协作方式。该模式能与所有支持 MCP-Server 调用的 LLM 客户端进行协同验证，例如：Cherry Studio、Claude Code、 Gemini-CLI、VS Code Copilot、Qwen-Code等。 平常使用是直接使用make命令的，要看详细命令可参考快速开始，也可以直接查看项目根目录的Makefile文件。\n准备RTL和对应的SPEC文档放入examples/{dut}文件夹。{dut}是模块的名称，比如Adder，如果是Adder，目录则为examples/Adder。\n打包RTL，将文档放入工作目录并且启动 MCP server：make mcp_{dut}，{dut}为对应的模块。此处如果使用的Adder，则命令为make mcp_Adder\n在支持 MCP client 的应用中配置 JSON：\n{ \"mcpServers\": { \"unitytest\": { \"httpUrl\": \"http://localhost:5000/mcp\", \"timeout\": 10000 } } } 启动应用：此处使用的Qwen Code，在UCAgent/output启动qwen,然后输入提示词。\n输入提示词：\n请通过工具RoleInfo获取你的角色信息和基本指导，然后完成任务。工具ReadTextFile读取文件。你需要在当前工作目录进行文件操作，不要超出该目录。\n","categories":["教程"],"description":"如何使用MCP集成模式来使用UCAgent。","excerpt":"如何使用MCP集成模式来使用UCAgent。","ref":"/mlvp/docs/ucagent/usage/mcp/","tags":["docs"],"title":"MCP集成模式（推荐）"},{"body":" This page will briefly introduce what verification is, as well as concepts used in the examples, such as DUT (Design Under Test) and RM (Reference Model).\nChip Verification Chip verification is a crucial step to ensure the correctness and reliability of chip designs, including functional verification, formal verification, and physical verification. This material only covers functional verification, focusing on simulation-based chip functional verification. The processes and methods of chip functional verification have many similarities with software testing, such as unit testing, system testing, black-box testing, and white-box testing. They also share similar metrics, such as functional coverage and code coverage. In essence, apart from the different tools and programming languages used, their goals and processes are almost identical. Thus, software test engineers should be able to perform chip verification without considering the tools and programming languages. However, in practice, software testing and chip verification are two completely separate fields, primarily due to the different verification tools and languages, making it difficult for software test engineers to crossover. In chip verification, hardware description languages (e.g., Verilog or SystemVerilog) and specialized commercial tools for circuit simulation are commonly used. Hardware description languages differ from high-level software programming languages like C++/Python, featuring a unique “clock” characteristic, which poses a high learning curve for software engineers.\nTo bridge the gap between chip verification and traditional software testing, allowing more people to participate in chip verification, this project provides the following content:\nMulti-language verification tools (Picker), allowing users to use their preferred programming language for chip verification. Verification framework (MLVP), enabling functional verification without worrying about the clock.\nIntroduction to basic circuits and verification knowledge, helping software enthusiasts understand circuit characteristics more easily.\nBasic learning materials for fundamental verification knowledge.\nReal high-performance chip verification cases, allowing enthusiasts to participate in verification work remotely.\nBasic Terms DUT: Design Under Test, usually referring to the designed RTL code.\nRM: Reference Model, a standard error-free model corresponding to the unit under test.\nRTL: Register Transfer Level, typically referring to the Verilog or VHDL code corresponding to the chip design.\nCoverage: The percentage of the test range relative to the entire requirement range. In chip verification, this typically includes line coverage, function coverage, and functional coverage.\nDV: Design Verification, referring to the collaboration of design and verification.\nDifferential Testing (difftest): Selecting two (or more) functionally identical units under test, submitting the same test cases that meet the unit’s requirements to observe whether there are differences in the execution results.\nTool Introduction The core tool used in this material is Picker（https://github.com/XS-MLVP/picker）. Its purpose is to automatically provide high-level programming language interfaces (Python/C++) for RTL-written design modules. Based on this tool, verification personnel with a software development (testing) background can perform chip verification without learning hardware description languages like Verilog/VHDL.\nSystem Requirements Recommended operating system: Ubuntu 22.04 LTS\nIn the development and research of system architecture, Linux is the most commonly used platform, mainly because Linux has a rich set of software and tool resources. Due to its open-source nature, important tools and software (such as Verilator) can be easily developed for Linux. In this course, multi-language verification tools like Picker and Swig can run stably on Linux. ","categories":["Sample Projects","Tutorials"],"description":"How to use the open verification platform to participate in hardware verification.","excerpt":"How to use the open verification platform to participate in hardware …","ref":"/mlvp/en/docs/quick-start/","tags":["examples","docs"],"title":"Quick Start"},{"body":"Installing the Picker Tool from Source Installing Dependencies cmake ( \u003e=3.11 )\ngcc ( Supports C++20, at least GCC version 10, recommended 11 or higher )\npython3 ( \u003e=3.8 )\nverilator ( ==4.218 )\nverible-verilog-format ( \u003e=0.0-3428-gcfcbb82b )\nswig ( \u003e=4.2.0 , for multi-language support )\nPlease ensure that the tools like verible-verilog-format have been added to the environment variable $PATH, so they can be called directly from the command line.\nSource Code Download git clone https://github.com/XS-MLVP/picker.git --depth=1 cd picker make init Build and Install cd picker make # You can enable support for other languages by # using `make BUILD_XSPCOMM_SWIG=python,java,scala,golang`. # Each language requires its own development environment, # which needs to be configured separately, such as `javac` for Java. sudo -E make install The default installation path is /usr/local, with binary files placed in /usr/local/bin and template files in /usr/local/share/picker. If you need to change the installation directory, you can pass arguments to cmake by specifying ARGS, for example: make ARGS=\"-DCMAKE_INSTALL_PREFIX=your_install_dir\" The installation will automatically install the xspcomm base library (https://github.com/XS-MLVP/xcomm), which is used to encapsulate the basic types of RTL modules, located at /usr/local/lib/libxspcomm.so. You may need to manually set the link directory parameters (-L) during compilation.\nIf support for languages such as Java is enabled, the corresponding xspcomm multi-language packages will also be installed.\npicker can also be compiled into a wheel file and installed via pip\nTo package picker into a wheel installation package, use the following command:\nmake wheel # or BUILD_XSPCOMM_SWIG=python,java,scala,golang make wheel After compilation, the wheel file will be located in the dist directory. You can then install it via pip, for example:\npip install dist/xspcomm-0.0.1-cp311-cp311-linux_x86_64.whl pip install dist/picker-0.0.1-cp311-cp311-linux_x86_64.whl After installation, execute the picker command to except the flow output:\nXDut Generate. Convert DUT(*.v/*.sv) to C++ DUT libs. Usage: ./build/bin/picker [OPTIONS] [SUBCOMMAND] Options: -h,--help Print this help message and exit -v,--version Print version --show_default_template_path Print default template path --show_xcom_lib_location_cpp Print xspcomm lib and include location --show_xcom_lib_location_java Print xspcomm-java.jar location --show_xcom_lib_location_scala Print xspcomm-scala.jar location --show_xcom_lib_location_python Print python module xspcomm location --show_xcom_lib_location_golang Print golang module xspcomm location --check check install location and supproted languages Subcommands: export Export RTL Projects Sources as Software libraries such as C++/Python pack Pack UVM transaction as a UVM agent and Python class Installation Test picker currently has two subcommands: export and pack.\nThe export subcommand is used to convert RTL designs into “libraries” corresponding to other high-level programming languages, which can be driven through software.\n$picker export –help\nExport RTL Projects Sources as Software libraries such as C++/Python Usage: picker export [OPTIONS] file... Positionals: file TEXT ... REQUIRED DUT .v/.sv source file, contain the top module Options: -h,--help Print this help message and exit --fs,--filelist TEXT ... DUT .v/.sv source files, contain the top module, split by comma. Or use '*.txt' file with one RTL file path per line to specify the file list --sim TEXT [verilator] vcs or verilator as simulator, default is verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python] Build example project, default is python, choose cpp, java or python --sdir,--source_dir TEXT [/home/yaozhicheng/workspace/picker/template] Template Files Dir, default is ${picker_install_path}/../picker/template --tdir,--target_dir TEXT [./picker_out] Codegen render files to target dir, default is ./picker_out --sname,--source_module_name TEXT ... Pick the module in DUT .v file, default is the last module in the -f marked file --tname,--target_module_name TEXT Set the module name and file name of target DUT, default is the same as source. For example, -T top, will generate UTtop.cpp and UTtop.hpp with UTtop class --internal TEXT Exported internal signal config file, default is empty, means no internal pin -F,--frequency TEXT [100MHz] Set the frequency of the **only VCS** DUT, default is 100MHz, use Hz, KHz, MHz, GHz as unit -w,--wave_file_name TEXT Wave file name, emtpy mean don't dump wave -c,--coverage Enable coverage, default is not selected as OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1] Copy xspcomm lib to generated DUT dir, default is true -V,--vflag TEXT User defined simulator compile args, passthrough. Eg: '-v -x-assign=fast -Wall --trace' || '-C vcs -cc -f filelist.f' -C,--cflag TEXT User defined gcc/clang compile command, passthrough. Eg:'-O3 -std=c++17 -I./include' --verbose Verbose mode -e,--example Build example project, default is OFF --autobuild BOOLEAN [1] Auto build the generated project, default is true Static Multi-Module Support:\nWhen generating the wrapper for dut_top.sv/v, picker allows specifying multiple module names and their corresponding quantities using the --sname parameter. For example, if there are modules A and B in the design files a.v and b.v respectively, and you need 2 instances of A and 3 instances of B in the generated DUT, and the combined module name is C (if not specified, the default name will be A_B). This can be achieved using the following command:\npicker path/a.v,path/b.v --sname A,2,B,3 --tname C Environment Variables:\nDUMPVARS_OPTION: Sets the option parameter for $dumpvars. For example, DUMPVARS_OPTION=\"+mda\" picker .... enables array waveform support in VCS. SIMULATOR_FLAGS: Parameters passed to the backend simulator. Refer to the documentation of the specific backend simulator for details. CFLAGS: Sets the -cflags parameter for the backend simulator. The pack subcommand is used to convert UVM sequence_item into other languages and then communicate through TLM (currently supports Python, other languages are under development).\n$picker pack –help\nPack uvm transaction as a uvm agent and python class Usage: picker pack [OPTIONS] file... Positionals: file TEXT ... REQUIRED Sv source file, contain the transaction define Options: -h,--help Print this help message and exit -e,--example Generate example project based on transaction, default is OFF -c,--force Force delete folder when the code has already generated by picker -r,--rename TEXT ... Rename transaction name in picker generate code Test Examples After picker compilation, execute the following commands in the picker directory to test the examples:\nbash example/Adder/release-verilator.sh --lang cpp bash example/Adder/release-verilator.sh --lang python # Default enable cpp and python # for other languages support：make BUILD_XSPCOMM_SWIG=python,java,scala,golang bash example/Adder/release-verilator.sh --lang java bash example/Adder/release-verilator.sh --lang scala bash example/Adder/release-verilator.sh --lang golang bash example/RandomGenerator/release-verilator.sh --lang cpp bash example/RandomGenerator/release-verilator.sh --lang python bash example/RandomGenerator/release-verilator.sh --lang java More Documents For guidance on chip verification with picker, please refer to: https://open-verify.cc/mlvp/en/docs/\n","categories":["Tutorials"],"description":"Install the necessary dependencies, **download, build, and install**  the required tools.","excerpt":"Install the necessary dependencies, **download, build, and install** …","ref":"/mlvp/en/docs/quick-start/installer/","tags":["docs"],"title":"Setting Up the Verification Environment"},{"body":"To meet the requirements of an open verification environment, we have developed the Picker tool, which is used to convert RTL designs into multi-language interfaces for verification. We will use the environment generated by the Picker tool as the basic verification environment. Next, we will introduce the Picker tool and its basic usage.\nIntroduction to Picker Picker is an auxiliary tool for chip verification with two main functions:\nPackaging RTL Design Verification Modules: Picker can package RTL design verification modules (.v/.scala/.sv) into dynamic libraries and provide programming interfaces in various high-level languages (currently supporting C++, Python, Java, Scala, Golang) to drive the circuit.\nAutomatic UVM-TLM Code Generation: Picker can automate TLM code encapsulation based on the UVM sequence_item provided by the user, providing a communication interface between UVM and other high-level languages such as Python.\nThis tool allows users to perform chip unit testing based on existing software testing frameworks such as pytest, junit, TestNG, go test, etc. Advantages of Verification Using Picker:\nNo RTL Design Leakage: After conversion by Picker, the original design files (.v) are transformed into binary files (.so). Verification can still be performed without the original design files, and the verifier cannot access the RTL source code.\nReduced Compilation Time: When the DUT (Design Under Test) is stable, it only needs to be compiled once (packaged into a .so file).\nWide User Base: With support for multiple programming interfaces, it caters to developers of various languages.\nUtilization of a Rich Software Ecosystem: Supports ecosystems such as Python3, Java, Golang, etc.\nAutomated UVM Transaction Encapsulation: Enables communication between UVM and Python through automated UVM transaction encapsulation.\nRTL Simulators Currently Supported by Picker:\nVerilator\nSynopsys VCS Working Principle of Picker The main function of Picker is to convert Verilog code into C++ or Python code. For example, using a processor developed with Chisel: first, it is converted into Verilog code through Chisel’s built-in tools, and then Picker provides high-level programming language interfaces.\nPython Module Generation Process of Module Generation Picker exports Python modules based on C++.\nPicker is a code generation tool. It first generates project files and then uses make to compile them into binary files.\nPicker first uses a simulator to compile the RTL code into a C++ class and then compiles it into a dynamic library (see the C++ steps for details).\nUsing the Swig tool, Picker then exports the dynamic library as a Python module based on the C++ header file definitions generated in the previous step.\nFinally, the generated module is exported to a directory, with other intermediate files being either cleaned up or retained as needed.\nSwig is a tool used to export C/C++ code to other high-level languages. It parses C++ header files and generates corresponding intermediate code. For detailed information on the generation process, please refer to the Swig official documentation . For information on how Picker generates C++ classes, please refer to C++ .\nThe generated module can be imported and used by other Python programs, with a file structure similar to that of standard Python modules. Using the Python Module The --language python or --lang python parameter specifies the generation of the Python base library.\nThe --example, -e parameter generates an executable file containing an example project.\nThe --verbose, -v parameter preserves intermediate files generated during project creation.\nUsing the Tool to Generate Python’s DUT Class Using the simple adder example from Case One:\nPicker automatically generates a base class in Python, referred to as the DUT class. For the adder example, the user needs to write test cases, importing the Python module generated in the previous section and calling its methods to operate on the hardware module. The directory structure is as follows: picker_out_adder |-- UT_Adder # Project generated by Picker tool | |-- Adder.fst.hier | |-- _UT_Adder.so | |-- __init__.py | |-- libDPIAdder.a | |-- libUTAdder.so | `-- libUT_Adder.py `-- example.py # User-written code The DUTAdder class has a total of eight methods, as shown below: class DUTAdder: def InitClock(name: str) # Initialize clock, with the clock pin name as a parameter, e.g., clk def Step(i: int = 1) # Advance the circuit by i cycles def StepRis(callback: Callable, args=None, args=(), kwargs={}) # Set rising edge callback function def StepFal(callback: Callable, args=None, args=(), kwargs={}) # Set falling edge callback function def SetWaveform(filename) # Set waveform file def SetCoverage(filename) # Set code coverage file def RefreshComb() # Advance combinational circuit def Finish() # Destroy the circuit Pins corresponding to the DUT, such as reset and clock, are represented as member variables in the DUTAdder class. As shown below, pin values can be read and written via the value attribute. from Adder import * dut = DUTAdder() dut.a.value = 1 # Assign value to the pin by setting the .value attribute dut.a[12] = 1 # Assign value to the 12th bit of the input pin a x = dut.a.value # Read the value of pin a y = dut.a[12] # Read the 12th bit of pin a General Flow for Driving DUT Create DUT and Set Pin Modes: By default, pins are assigned values on the rising edge of the next cycle. For combinational logic, you need to set the assignment mode to immediate assignment.\nInitialize the Clock: This binds the clock pin to the internal xclock of the DUT. Combinational logic does not require a clock and can be ignored.\nReset the Circuit: Most sequential circuits need to be reset.\nWrite Data to DUT Input Pins: Use the pin.Set(x) interface or pin.value = x for assignment.\nDrive the Circuit: Use Step for sequential circuits and RefreshComb for combinational circuits.\nObtain and Check Outputs of DUT Pins: For example, compare the results with a reference model using assertions.\nComplete Verification and Destroy DUT: Calling Finish() will write waveform, coverage, and other information to files.\nThe corresponding pseudocode is as follows:\nfrom DUT import * # 1 Create dut = DUT() # 2 Initialize dut.SetWaveform(\"test.fst\") dut.InitClock(\"clock\") # 3 Reset dut.reset = 1 dut.Step(1) dut.reset = 0 dut.Step(1) # 4 Input Data dut.input_pin1.value = 0x123123 dut.input_pin3.value = \"0b1011\" # 5 Drive the Circuit dut.Step(1) # 6 Get Results x = dut.output_pin.value print(\"result:\", x) # 7 Destroy dut.Finish() Other Data Types In general, most DUT verification tasks can be accomplished using the interfaces provided by the DUT class. However, for special cases, additional interfaces are needed, such as custom clocks, asynchronous operations, advancing combinational circuits and writing waveforms, and modifying pin properties. In the DUT class generated by Picker, in addition to XData type pin member variables , there are also XClock type xclock and XPort type xport .\nclass DUTAdder(object): xport: XPort # Member variable xport for managing all pins in the DUT xclock: XClock # Member variable xclock for managing the clock # DUT Pins a: XData b: XData cin: XData cout: XData XData Class Data in DUT pins usually have an uncertain bit width and can be in one of four states: 0, 1, Z, and X. Picker provides XData to represent pin data in the circuit. Main Methods class XData: # Split XData, for example, create a separate XData for bits 7-10 of a 32-bit XData # name: Name, start: Start bit, width: Bit width, e.g., auto sub = a.SubDataRef(\"sub_pin\", 0, 4) def SubDataRef(name, start, width): XData def GetWriteMode(): WriteMode # Get the write mode of XData: Imme (immediate), Rise (rising edge), Fall (falling edge) def SetWriteMode(mode: WriteMode) # Set the write mode of XData, e.g., a.SetWriteMode(WriteMode::Imme) def DataValid(): bool # Check if the data is valid (returns false if value contains X or Z states, otherwise true) def W(): int # Get the bit width of XData (0 indicates XData is of Verilog's logic type, otherwise it's the width of Vec type) def U(): int # Get the unsigned value of XData (e.g., x = a.value) def S(): int # Get the signed value of XData def String(): str # Convert XData to a hexadecimal string, e.g., \"0x123ff\", if ? appears, it means X or Z state in the corresponding 4 bits def Equal(xdata): bool # Compare two XData instances for equality def Set(value) # Assign value to XData, value can be XData, string, int, bytes, etc. def GetBytes(): bytes # Get the value of XData in bytes format def Connect(xdata): bool # Connect two XData instances; only In and Out types can be connected. When Out data changes, In type XData will be automatically updated. def IsInIO(): bool # Check if XData is of In type, which can be read and written def IsOutIO(): bool # Check if XData is of Out type, which is read-only def IsBiIO(): bool # Check if XData is of Bi type, which can be read and written def IsImmWrite(): bool # Check if XData is in Imm write mode def IsRiseWrite(): bool # Check if XData is in Rise write mode def IsFallWrite(): bool # Check if XData is in Fall write mode def AsImmWrite() # Change XData's write mode to Imm def AsRiseWrite() # Change XData's write mode to Rise def AsFallWrite() # Change XData's write mode to Fall def AsBiIO() # Change XData to Bi type def AsInIO() # Change XData to In type def AsOutIO() # Change XData to Out type def FlipIOType() # Invert the IO type of XData, e.g., In to Out or Out to In def Invert() # Invert the data in XData def At(index): PinBind # Get the pin at index, e.g., x = a.At(12).Get() or a.At(12).Set(1) def AsBinaryString() # Convert XData's data to a binary string, e.g., \"1001011\" To simplify assignment operations, XData has overloaded property assignment for Set(value) and U() methods, allowing assignments and retrievals with pin.value = x and x = pin.value.\n# Access with .value # a is of XData type a.value = 12345 # Decimal assignment a.value = 0b11011 # Binary assignment a.value = 0o12345 # Octal assignment a.value = 0x12345 # Hexadecimal assignment a.value = -1 # Assign all bits to 1, a.value = x is equivalent to a.Set(x) a[31] = 0 # Assign value to bit 31 a.value = \"x\" # Assign high impedance state a.value = \"z\" # Assign unknown state x = a.value # Retrieve value, equivalent to x = a.U() XPort Class Directly operating on XData is clear and intuitive when dealing with a few pins. However, managing multiple XData instances can be cumbersome. XPort is a wrapper around XData that allows centralized management of multiple XData instances. It also provides methods for convenient batch management. Initialization and Adding Pins port = XPort(\"p\") # Create an XPort instance with prefix p Main Methods\nclass XPort: def XPort(prefix = \"\") # Create a port with prefix prefix, e.g., p = XPort(\"tile_link_\") def PortCount(): int # Get the number of pins in the port (i.e., number of bound XData instances) def Add(pin_name, XData) # Add a pin, e.g., p.Add(\"reset\", dut.reset) def Del(pin_name) # Delete a pin def Connect(xport2) # Connect two ports def NewSubPort(std::string subprefix): XPort # Create a sub-port with all pins starting with subprefix def Get(key, raw_key = False): XData # Get XData def SetZero() # Set all XData in the port to 0 XClock Class XClock is a wrapper for the circuit clock used to drive the circuit. In traditional simulation tools (e.g., Verilator), you need to manually assign values to clk and update the state using functions like step_eval. Our tool provides methods to bind the clock directly to XClock, allowing the Step() method to simultaneously update the clk and circuit state. Initialization and Adding Pins # Initialization clk = XClock(stepfunc) # Parameter stepfunc is the circuit advancement method provided by DUT backend, e.g., Verilator's step_eval Main Methods\nclass XClock: def Add(xdata) # Bind Clock with xdata, e.g., clock.Add(dut.clk) def Add(xport) # Bind Clock with XData def RefreshComb() # Advance circuit state without advancing time or dumping waveform def RefreshCombT() # Advance circuit state (advance time and dump waveform) def Step(int s = 1) # Advance the circuit by s clock cycles, DUT.Step = DUT.xclock.Step def StepRis(func, args=(), kwargs={}) ","categories":["Tutorial"],"description":"Basic usage of the verification tool.","excerpt":"Basic usage of the verification tool.","ref":"/mlvp/en/docs/env_usage/picker_usage/","tags":["docs"],"title":"Tool Introduction"},{"body":"Generated Library Files Picker can specify the target language for conversion using the --lang parameter (supported values: cpp, python, java, lua, scala, golang). Since different programming languages use different types of “libraries,” the generated files will vary. For example, Java produces a JAR package, while Python generates a directory. Exporting a library for a specific language with picker requires xcomm support. You can check support status with picker --check:\n$ picker --check [OK ] Version: 0.9.0-feat_performance_improve-b7001a6-2025-04-11-dirty [OK ] Exec path: /usr/local/share/lib/python3.11/site-packages/picker/bin/picker [OK ] Template path: /usr/local/share/lib/python3.11/site-packages/picker/share/picker/template [OK ] Support Cpp (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/lib' success) [OK ] Support Golang (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/golang' success) [OK ] Support Java (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/java/xspcomm-java.jar' success) [OK ] Support Lua (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/lua/luaxspcomm.so' success) [OK ] Support Python (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/python' success) [OK ] Support Scala (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/scala/xspcomm-scala.jar' success) A status of “success” means supported, while “fail” means not supported.\nC++ For C++, picker generates a shared object (.so) dynamic library and the corresponding header files. For example:\nUT_Adder/ ├── UT_Adder.cpp # DUT source file ├── UT_Adder.hpp # DUT header file ├── UT_Adder_dpi.hpp # DPI header file ├── dut_base.hpp # DUT base header file ├── libDPIAdder.a # DPI static library └── libUTAdder.so # DUT dynamic library When using, set the LD path, then #include UT_Adder.hpp in your test code.\nPython For Python, picker generates a directory (Python modules are represented as directories):\nUT_Adder/ ├── _UT_Adder.so ├── __init__.py ├── libUTAdder.so └── libUT_Adder.py After setting the PYTHONPATH, you can import UT_Adder in your test code.\nJava/Scala For Java and Scala (JVM-based languages), picker generates the corresponding JAR packages.\nUT_Adder/ ├── UT_Adder-scala.jar └── UT_Adder-java.jar Go For Go, picker generates a directory (similar to Python):\nUT_Adder/ └── golang └── src └── UT_Adder ├── UT_Adder.go ├── UT_Adder.so ├── UT_Adder_Wrapper.go ├── go.mod └── libUTAdder.so After setting the GOPATH, you can directly import the package.\nVerification Interfaces For DUT verification interfaces, refer to: https://github.com/XS-MLVP/picker/blob/master/doc/API.zh.md\nFor xspcomm library interfaces, refer to: https://github.com/XS-MLVP/xcomm/blob/master/docs/APIs.cn.md\n","categories":["Tutorial"],"description":"Verification interfaces supported by DUT files and all programming languages","excerpt":"Verification interfaces supported by DUT files and all programming …","ref":"/mlvp/en/docs/multi-lang/interface/","tags":["docs"],"title":"Verification Interfaces"},{"body":"源码安装Picker工具 依赖安装 cmake ( \u003e=3.11 ) gcc ( 支持c++20,至少为gcc版本10, 建议11及以上 ) python3 ( \u003e=3.8 ) verilator ( \u003e=4.218 ) verible-verilog-format ( \u003e=0.0-3428-gcfcbb82b ) swig ( \u003e=4.2.0, 用于多语言支持 ) 请注意，请确保verible-verilog-format等工具的路径已经添加到环境变量$PATH中，可以直接命令行调用。\n下载源码 git clone https://github.com/XS-MLVP/picker.git --depth=1 cd picker make init 构建并安装 cd picker make # 可通过 make BUILD_XSPCOMM_SWIG=python,java,scala,golang 开启其他语言支持。 # 各语言需要自己的开发环境，需要自行配置，例如javac等 sudo -E make install 默认的安装的目标路径是 /usr/local， 二进制文件被置于 /usr/local/bin，模板文件被置于 /usr/local/share/picker。 如果需要修改安装目录，可以通过指定ARGS给cmake传递参数，例如make ARGS=\"-DCMAKE_INSTALL_PREFIX=your_instal_dir\" 安装时会自动安装 xspcomm基础库（https://github.com/XS-MLVP/xcomm），该基础库是用于封装 RTL 模块的基础类型，位于 /usr/local/lib/libxspcomm.so。 可能需要手动设置编译时的链接目录参数(-L) 如果开启了Java等语言支持，还会安装 xspcomm 对应的多语言软件包。\npicker也可以编译为wheel文件，通过pip安装\n通过以下命令把picker打包成wheel安装包：\nmake wheel # or BUILD_XSPCOMM_SWIG=python,java,scala,golang make wheel 编译完成后，wheel文件位于dist目录，然后通过pip安装，例如：\npip install dist/xspcomm-0.0.1-cp311-cp311-linux_x86_64.whl pip install dist/picker-0.0.1-cp311-cp311-linux_x86_64.whl 安装完成后，执行picker命令可以得到以下输出:\nXDut Generate. Convert DUT(*.v/*.sv) to C++ DUT libs. Usage: ./build/bin/picker [OPTIONS] [SUBCOMMAND] Options: -h,--help Print this help message and exit -v,--version Print version --show_default_template_path Print default template path --show_xcom_lib_location_cpp Print xspcomm lib and include location --show_xcom_lib_location_java Print xspcomm-java.jar location --show_xcom_lib_location_scala Print xspcomm-scala.jar location --show_xcom_lib_location_python Print python module xspcomm location --show_xcom_lib_location_golang Print golang module xspcomm location --check check install location and supproted languages Subcommands: export Export RTL Projects Sources as Software libraries such as C++/Python pack Pack UVM transaction as a UVM agent and Python class 安装测试 当前picker有export和pack两个子命令。\nexport 子命令用于将RTL设计转换成其他高级编程语言对应的“库”，可以通过软件的方式进行驱动。\npicker export --help Export RTL Projects Sources as Software libraries such as C++/Python Usage: picker export [OPTIONS] file... Positionals: file TEXT ... REQUIRED DUT .v/.sv source file, contain the top module Options: -h,--help Print this help message and exit --fs,--filelist TEXT ... DUT .v/.sv source files, contain the top module, split by comma. Or use '*.txt' file with one RTL file path per line to specify the file list --sim TEXT [verilator] vcs or verilator as simulator, default is verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python] Build example project, default is python, choose cpp, java or python --sdir,--source_dir TEXT Template Files Dir, default is ${picker_install_path}/../picker/template --sname,--source_module_name TEXT ... Pick the module in DUT .v file, default is the last module in the -f marked file --tname,--target_module_name TEXT Set the module name and file name of target DUT, default is the same as source. For example, -T top, will generate UTtop.cpp and UTtop.hpp with UTtop class --tdir,--target_dir TEXT Target directory to store all the results. If it ends with '/' or is empty, the directory name will be the same as the target module name --internal TEXT Exported internal signal config file, default is empty, means no internal pin -F,--frequency TEXT [100MHz] Set the frequency of the **only VCS** DUT, default is 100MHz, use Hz, KHz, MHz, GHz as unit -w,--wave_file_name TEXT Wave file name, emtpy mean don't dump wave -c,--coverage Enable coverage, default is not selected as OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1] Copy xspcomm lib to generated DUT dir, default is true -V,--vflag TEXT User defined simulator compile args, passthrough. Eg: '-v -x-assign=fast -Wall --trace' || '-C vcs -cc -f filelist.f' -C,--cflag TEXT User defined gcc/clang compile command, passthrough. Eg:'-O3 -std=c++17 -I./include' --verbose Verbose mode -e,--example Build example project, default is OFF --autobuild BOOLEAN [1] Auto build the generated project, default is true pack子命令用于将UVM中的 sequence_item 转换为其他语言，然后通过TLM进行通信（目前支持Python，其他语言在开发中）\npicker pack --help Pack uvm transaction as a uvm agent and python class Usage: picker pack [OPTIONS] file... Positionals: file TEXT ... REQUIRED Sv source file, contain the transaction define Options: -h,--help Print this help message and exit -e,--example Generate example project based on transaction, default is OFF -c,--force Force delete folder when the code has already generated by picker -r,--rename TEXT ... Rename transaction name in picker generate code 参数解释 export: file TEXT ... REQUIRED：必须。位置参数，DUT.v/.sv 源文件，包含顶层模块 -h,--help: 可选。打印此帮助信息并退出 --fs,--filelist TEXT ...: 可选。DUT .v/.sv 源文件，包含顶层模块，逗号分隔。或使用 ‘*.txt’ 文件，每行指定一个 RTL 文件路径来指定文件列表 --sim TEXT [verilator]: 可选。使用 vcs 或 verilator 作为模拟器，默认是 verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python]: 可选。构建示例项目，默认是 python，可选择 cpp、java 或 python --sdir,--source_dir TEXT: 可选。模板文件目录，默认是 ${picker_install_path}/../picker/template --sname,--source_module_name TEXT ...: 可选。在 DUT .v 文件中选择模块，默认是 -f 标记的文件中的最后一个模块 --tname,--target_module_name TEXT: 可选。设置目标 DUT 的模块名和文件名，默认与源相同。例如，-T top 将生成 UTtop.cpp 和 UTtop.hpp，并包含 UTtop 类 --tdir,--target_dir TEXT: 可选。代码生成渲染文件的目标目录，默认为DUT的模块名。如果该参数以’/‘结尾，则在该参数指定的目录中创建以DUT模块名的子目录。 --internal TEXT: 可选。导出的内部信号配置文件，默认为空，表示没有内部引脚 -F,--frequency TEXT [100MHz]: 可选。设置 仅 VCS DUT 的频率，默认是 100MHz，可以使用 Hz、KHz、MHz、GHz 作为单位 -w,--wave_file_name TEXT: 可选。波形文件名，空表示不导出波形 -c,--coverage: 可选。启用覆盖率，默认不选择为 OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1]: 可选。将 xspcomm 库复制到生成的 DUT 目录，默认是 true -V,--vflag TEXT: 可选。用户定义的模拟器编译参数，透传。例如：’-v -x-assign=fast -Wall –trace’ 或 ‘-C vcs -cc -f filelist.f’ -C,--cflag TEXT: 可选。用户定义的 gcc/clang 编译命令，透传。例如：’-O3 -std=c++17 -I./include’ --verbose: 可选。详细模式 -e,--example: 可选。构建示例项目，默认是 OFF --autobuild BOOLEAN [1]: 可选。自动构建生成的项目，默认是 true 静态多模块支持：\npicker在生成dut_top.sv/v的封装时，可以通过--sname参数指定多个模块名称和对应的数量。例如在a.v和b.v设计文件中分别有模块A和B，需要DUT中有2个A，3个B，生成的模块名称为C（若不指定，默认名称为A_B），则可执行如下命令：\npicker path/a.v,path/b.v --sname A,2,B,3 --tname C 环境变量：\nDUMPVARS_OPTION: 设置$dumpvars的option参数。例如DUMPVARS_OPTION=\"+mda\" picker .... 开启vcs中数组波形的支持。 SIMULATOR_FLAGS: 传递给后端仿真器的参数。具体可参考所使用的后端仿真器文档。 CFLAGS: 设置后端仿真器的-cflags参数。 pack: file: 必需。待解析的UVM transaction文件 --example, -e: 可选。根据UVM的transaction生成示例项目。 --force， -c: 可选。若已存在picker根据当前transaction解析出的文件，通过该命令可强制删除该文件，并重新生成 --rename, -r: 可选。配置生成文件以及生成的agent的名称，默认为transaction名。 测试Examples 编译完成后，在picker目录执行以下命令，进行测试：\nbash example/Adder/release-verilator.sh --lang cpp bash example/Adder/release-verilator.sh --lang python # 默认仅开启 cpp 和 Python 支持 # 支持其他语言编译命令为：make BUILD_XSPCOMM_SWIG=python,java,scala,golang bash example/Adder/release-verilator.sh --lang java bash example/Adder/release-verilator.sh --lang scala bash example/Adder/release-verilator.sh --lang golang bash example/RandomGenerator/release-verilator.sh --lang cpp bash example/RandomGenerator/release-verilator.sh --lang python bash example/RandomGenerator/release-verilator.sh --lang java 参考材料 如何基于picker进行芯片验证，可参考：https://open-verify.cc/mlvp/docs/\n","categories":["教程"],"description":"安装相关依赖，**下载、构建并安装**对应的工具。","excerpt":"安装相关依赖，**下载、构建并安装**对应的工具。","ref":"/mlvp/docs/quick-start/installer/","tags":["docs"],"title":"搭建验证环境"},{"body":" 为满足开放验证的环境要求，我们开发了 Picker 工具，用于将 RTL 设计转换为多语言接口，并在此基础上进行验证，我们将会使用 Picker 工具生成的环境作为基础的验证环境。接下来我们将介绍 Picker 工具，及其基础的使用方法。\nPicker 简介 picker 是一个芯片验证辅助工具，具有两个主要功能：\n打包RTL设计验证模块： picker 可以将 RTL 设计验证模块（.v/.scala/.sv）打包成动态库，并提供多种高级语言（目前支持 C++、Python、Java、Scala、Golang）的编程接口来驱动电路。 UVM-TLM代码自动生成： picker 能够基于用户提供的 UVM sequence_item 进行自动化的 TLM 代码封装，提供 UVM 与其他高级语言（如 Python）的通信接口。 该工具允许用户基于现有的软件测试框架，例如 pytest、junit、TestNG、go test 等，进行芯片单元测试。 基于 Picker 进行验证的优点:\n不泄露 RTL 设计：经过 Picker 转换后，原始的设计文件（.v）被转化成了二进制文件（.so），脱离原始设计文件后，依旧可进行验证，且验证者无法获取 RTL 源代码。 减少编译时间：当 DUT（设计待测）稳定时，只需要编译一次（打包成 .so 文件）。 用户范围广：提供的编程接口多，覆盖不同语言的开发者。 使用丰富的软件生态：支持 Python3、Java、Golang 等生态系统。 自动化的 UVM 事务封装：通过自动化封装 UVM 事务，实现 UVM 和 Python 的通信。 Picker 目前支持的 RTL 仿真器：\nVerilator Synopsys VCS Picker的工作原理\nPicker的主要功能就是将Verilog代码转换为C++或者Python代码，以Chisel开发的处理器为例:先通过Chisel自带的工具将其转换为Verilog代码，再通Picker提供高级编程语言接口。\nPython 模块生成 生成模块的过程 Picker 导出 Python Module 的方式是基于 C++ 的。\nPicker 是 代码生成(codegen)工具，它会先生成项目文件，再利用 make 编译出二进制文件。 Picker 首先会利用仿真器将 RTL 代码编译为 C++ Class，并编译为动态库。（见C++步骤详情） 再基于 Swig 工具，利用上一步生成的 C++ 的头文件定义，将动态库导出为 Python Module。 最终将生成的模块导出到目录，并按照需求清理或保留其他中间文件。 Swig 是一个用于将 C/C++ 导出为其他高级语言的工具。该工具会解析 C++ 头文件，并生成对应的中间代码。 如果希望详细了解生成过程，请参阅 Swig 官方文档。 如果希望知道 Picker 如何生成 C++ Class，请参阅 C++。\n该这个模块和标准的 Python 模块一样，可以被其他 Python 程序导入并调用，文件结构也与普通 Python 模块无异。 Python 模块使用 参数 --language python 或 --lang python 用于指定生成Python基础库。 参数 --example, -e 用于生成包含示例项目的可执行文件。 参数 --verbose, -v 用于保留生成项目时的中间文件。 使用工具生成Python的DUT类 以案例一中的简单加法器为例：\nPicker会自动生成Python的一个基础类，我们称之为DUT类，以前加法器为例，用户需要编写测试用例，即导入上一章节生成的 Python Module，并调用其中的方法，以实现对硬件模块的操作。 目录结构为： picker_out_adder ├── Adder # Picker 工具生成的项目 │ ├── _UT_Adder.so │ ├── __init__.py │ ├── libUTAdder.so │ ├── libUT_Adder.py │ └── signals.json └── example.py # 用户需要编写的代码 在DUT对应的DUTAdder类中共有8个方法(位于Adder/init.py文件)，具体如下： class DUTAdder: def InitClock(name: str) # 初始化时钟，参数时钟引脚对应的名称，例如clk def Step(i:int = 1) # 推进电路i个周期 def StepRis(callback: Callable, args=None, args=(), kwargs={}) # 设置上升沿回调函数 def StepFal(callback: Callable, args=None, args=(), kwargs={}) # 设置下降沿回调函数 def SetWaveform(filename) # 设置波形文件 def SetCoverage(filename) # 设置代码覆盖率文件 def RefreshComb() # 推进组合电路 def Finish() # 销毁电路 DUT对应的引脚，例如reset，clock等在DUTAdder类中以成员变量的形式呈现。如下所示，可以通过value进行引脚的读取和写入。 from Adder import * dut = DUTAdder() dut.a.value = 1 # 通过给引脚的.value属性赋值完成对引脚的赋值 dut.a[12] = 1 # 对引脚输入a的第12bit进行赋值 x = dut.a.value # 读取引脚a的值 y = dut.a[12] # 读取引脚a的第12bit的值 驱动DUT的一般流程 创建DUT，设置引脚模式。默认情况下，引脚是在一下个周期的上升沿进行赋值，如果是组合逻辑，需要设置赋值模式为立即赋值。 初始化时钟。其目的是将时钟引脚与DUT内置的xclock进行绑定。组合逻辑没有时钟可以忽略。 reset电路。大部分时序电路都需要reset。 给DUT输入引脚写入数据。通过“pin.Set(x)”接口，或者pin.vaulue=x进行赋值。 驱动电路。时序电路用Step，组合电路用RefreshComb。 获取DUT各个引脚的输出进行检测。例如和参考模型进行的结果进行assert对比。 完成验证，销毁DUT。调用Finish()时，会把波形，覆盖率等写入到文件。 对应伪代码如下：\n# Python DUT 的名字可通过 --tdir 指定 from DUT import * # 1 创建 dut = DUT() # 2 初始化 dut.SetWaveform(\"test.fst\") dut.InitClock(\"clock\") # 3 reset dut.reset = 1 dut.Step(1) dut.reset = 0 dut.Step(1) # 4 输入数据 dut.input_pin1.value = 0x123123 dut.input_pin3.value = \"0b1011\" # 5 驱动电路 dut.Step(1) # 6 得到结果 x = dut.output_pin.value print(\"result:\", x) # 7 销毁 dut.Finish() 其他数据类型 一般情况下，通过上述DUT类自带的接口就能完成绝大部分DUT的验证，但一些特殊情况需要其他对应的接口，例如自定义时钟、异步操作、推进组合电路并写入波形、修改引脚属性等。\n在picker生成的DUT类中，除了XData类型的引脚成员变量外，还有XClock类型的xclock和XPort类型的xport。\nclass DUTAdder(object): xport: XPort # 成员变量 xport，用于管理DUT中的所有引脚 xclock: XClock # 成员变量 xclock，用于管理时钟 # DUT 引脚 a: XData b: XData cin: XData cout: XData XData 类 DUT引脚中的数据通常位宽不确定，且有四种状态：0、1、Z和X。为此picker提供了XData进行电路引脚数据表示。 主要方法\nclass XData: #拆分XData，例如把一个32位XData中的第7-10位创建成为一个独立XData # name：名称，start：开始位，width：位宽，例如auto sub = a.SubDataRef(\"sub_pin\", 0, 4) def SubDataRef(name, start, width): XData def GetWriteMode():WriteMode #获取XData的写模式，写模式有三种：Imme立即写，Rise上升沿写，Fall下降沿写 def SetWriteMode(mode:WriteMode) #设置XData的写模式 eg: a.SetWriteMode(WriteMode::Imme) def DataValid():bool #检测数据是否有效（Value中含有X或者Z态返回false否者true） def W():int #获取XData的位宽（如果为0，表示XData为verilog中的logic类型，否则为Vec类型的位宽） def U():int #获取XData的值（无符号，同 x = a.value） def S():int #获取XData的值（有符号类型） def String():str #将XData转位16进制的字符串类型，eg: \"0x123ff\"，如果出现?，表现对应的4bit中有x或z态 def Equal(xdata):bool #判断2个XData是否相等 def Set(value) #对XData进行赋值，value类型可以为：XData, string, int, bytes等 def GetBytes(): bytes #以bytes格式获取XData中的数 def Connect(xdata):bool #连接2个XData，只有In和Out类型的可以连接，当Out数据发生变化时，In类型的XData会自动写入 def IsInIO():bool #判断XData是否为In类型，改类型可读可写 def IsOutIO():bool #判断XData是否为Out类型，改类型只可读 def IsBiIO():bool #判断XData是否为Bi类型，改类型可读可写 def IsImmWrite(): bool #判断XData是否为Imm写入模式 def IsRiseWrite(): bool #判断XData是否为Rise写入模式 def IsFallWrite(): bool #判断XData是否为Fall写入模式 def AsImmWrite() #更改XData的写模式为Imm def AsRiseWrite() #更改XData的写模式为Rise def AsFallWrite() #更改XData的写模式为Fall def AsBiIO() #更改XData为Bi类型 def AsInIO() #更改XData为In类型 def AsOutIO() #更改XData为Out类型 def FlipIOType() #将XData的IO类型进行取反，例如In变为Out或者Out变为In def Invert() #将XData中的数据进行取反 def At(index): PinBind #获取第index, eg: x = a.At(12).Get() or a.At(12).Set(1) def AsBinaryString() #将XData的数据变为二进制字符串，eg: \"1001011\" 为了简化赋值操作，XData 对 Set(value) 和 U() 方法进行了属性赋值重载，可以通过pin.value=x 和 x=pin.value进行赋值和取值。\n# 使用.value可以进行访问 # a 为XData类型 a.value = 12345 # 十进制赋值 a.value = 0b11011 # 二进制赋值 a.value = 0o12345 # 八进制赋值 a.value = 0x12345 # 十六进制赋值 a.value = -1 # 所有bit赋值1, a.value = x 与 a.Set(x) 等价 a[31] = 0 # 对第31位进行赋值 a.value = \"x\" # 赋值高阻态 a.value = \"z\" # 赋值不定态 x = a.value # 获取值，与 x = a.U() 等价 XPort 类 在处理少数几个XData引脚时，直接操作XData是比较清晰和直观的。但是，当涉及到多个XData时，进行批量管理就不太方便了。XPort是对XData的一种封装，它允许我们对多个XData进行集中操作。我们还提供了一些方法来方便地进行批量管理。 初始化与添加引脚\nport = XPort(\"p\") #创建前缀为p的XPort实例 主要方法\nclass XPort: def XPort(prefix = \"\") #创建前缀为prefix的port， eg：p = XPort(\"tile_link_\") def PortCount(): int #获取端口中的Pin数量（即绑定的XData个数） def Add(pin_name, XData) #添加Pin, eg：p.Add(\"reset\", dut.reset) def Del(pin_name) #删除Pin def Connect(xport2) #链接2个Port def NewSubPort(std::string subprefix): XPort # 创建子Port，以subprefix开头的所有Pin构成子Port def Get(key, raw_key = False): XData # 获取XData def SetZero() #设置Port中的所有XData为0 XClock 类 XClock是电路时钟的封装，用于驱动电路。在传统仿真工具（例如Verilator）中，需要手动为clk赋值，并通过step_eval函数更新状态。但在我们的工具中，我们提供了相应的方法，可以将时钟直接绑定到XClock上。只需使用我们的Step()方法，就可以同时更新clk和电路状态。 初始化与添加引脚\n# 初始化 clk = XClock(stepfunc) #参数stepfunc为DUT后端提供的电路推进方法，例如verilaor的step_eval等 主要方法\nclass XClock: def Add(xdata) #将Clock和时钟进行绑定， eg：clock.Add(dut.clk) def Add(xport) #将Clock和XData进行绑定 def RefreshComb() #推进电路状态，不推进时间，不dump波形 def RefreshCombT() #推进电路状态（推进时间，dump波形） def Step(int s = 1) #推进电路s个时钟周期， DUT.Step = DUT.xclock.Step def StepRis(func, args=(), kwargs={}) #设置上升沿回调函数，DUT.StepRis = DUT.xclock.StepRis def StepFal(func, args=(), kwargs={}) #设置下降沿回调函数，DUT.StepFal = DUT.xclock.StepFal # 异步方法 async AStep(cycle: int) #异步推进cycle个时钟， eg：await dut.AStep(5) async ACondition(condition) #异步等待conditon()为true async ANext() #异步推进一个时钟周期，等同AStep(1) async RunStep(cycle: int) #持续推进时钟cycle个时钟，用于最外层 ","categories":["教程"],"description":"验证工具的基本使用。","excerpt":"验证工具的基本使用。","ref":"/mlvp/docs/env_usage/picker_usage/","tags":["docs"],"title":"工具介绍"},{"body":" 随着芯片设计的愈发复杂，其验证难度和耗时也成倍增长，而近年来大语言模型的能力突飞猛进。于是我们推出了 UCAgent——一个基于大语言模型的自动化硬件验证 AI 代理，专注于芯片设计的单元测试(Unit Test)验证工作。 接下来我将从介绍、安装、使用、工作流、高级这五个方面来说明 UCAgent。\n介绍 背景 芯片验证时间已经占据了芯片开发时间的 50-60%，并且设计工程师也将 49%的时间投入了硬件验证工作，但是 2024 年首次流片成功率仅有 14%。 随着 LLM 与编程类 Agent 兴起，将“硬件验证”抽象为“软件测试问题”可实现高比例自动化。 UCAgent 是什么 面向芯片设计单元测试(Unit Test)的 AI Agent，基于 LLM 驱动，围绕“阶段化工作流 + 工具编排”自动/半自动完成需求理解、测试生成、执行与报告产出。 以用户为主导，LLM 为助理的协作式交互 Agent 以 Picker \u0026 Toffee 为基础，DUT 以 Python 包形式被测试；可与 OpenHands/Copilot/Claude Code/Gemini-CLI/Qwen Code/ 等通过 MCP 协议深度协作。 能力与目标 自动/半自动：生成/完善测试代码与文档、运行用例、汇总报告 完整：功能覆盖率、代码行覆盖率与文档一致性 可集成：支持标准 CLI、TUI；提供 MCP server 接口便于外部 Code Agent 接入 目标：有效减少用户在验证过程中的重复工作 安装 系统要求 Python 版本： 3.11+ 操作系统：Linux / macOS API 需求：可访问 OpenAI 兼容 API 内存：建议 4GB+ 依赖：picker（将 Verilog DUT 导出为 Python 包） 安装方式 方式一：克隆仓库并安装依赖\ngit clone https://github.com/XS-MLVP/UCAgent.git cd UCAgent pip3 install . 方式二（pip 安装）\npip3 install git+https://git@github.com/XS-MLVP/UCAgent@main ucagent --help # 确认安装成功 使用 快速开始 pip 安装 UCAgent\npip3 install git+https://git@github.com/XS-MLVP/UCAgent@main 准备 DUT（待测模块）\n创建目录：在{工作区}目录下创建Adder目录。({工作区}是指当前运行ucagent命令的地方，其他的的目录都以{工作区}为根目录)\nmkdir -p Adder RTL：使用快速开始-简单加法器的加法器,将其代码放入Adder/Adder.v\n注入 bug：将输出和位宽修改为 63 位（用于演示位宽错误导致的缺陷）。\n将Adder.v第九行由output [WIDTH-1:0] sum,改为output [WIDTH-2:0] sum,，vim Adder/Adder.v。目前的 verilog 代码为：\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-2:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule 将 RTL 导出为 Python Module picker 可以将 RTL 设计验证模块打包成动态库，并提供 Python 的编程接口来驱动电路。参照基础工具-工具介绍和picker 文档\n直接在{工作区}目录下执行命令picker export Adder/Adder.v --rw 1 --sname Adder --tdir output/ -c -w output/Adder/Adder.fst 编写 README 将加法器的说明、验证目标、bug 分析和其他都写在Adder文件夹的README.md文件中，同时将这个文件向output/Adder文件夹复制一份。\n将内容写入 readme 中，vim Adder/README.md，将下面内容复制到README.md中。\n复制文件，cp Adder/README.md output/Adder/README.md。\nAdder/README.md内容可以是如下：\n### Adder 64 位加法器 输入 a, b, cin 输出 sum，cout 实现 sum = a + b + cin cin 是进位输入 cout 是进位输出 ### 验证目标 只要验证加法相关的功能，其他验证，例如波形、接口等，不需要出现 ### bug 分析 在 bug 分析时，请参考源码：examples/MyAdder/Adder.v ### 其他 所有的文档和注释都用中文编写 安装 Qwen Code CLI 直接使用npm全局安装sudo npm install -g @qwen-code/qwen-code。（需要本地有nodejs 环境） 其他安装方式请参考：Qwen Code 执行与部署 配置 Qwen Code CLI 修改~/.qwen/settings.json 配置文件，vim ~/.qwen/settings.json，示例 Qwen 配置文件如下： { \"mcpServers\": { \"unitytest\": { \"httpUrl\": \"http://localhost:5000/mcp\", \"timeout\": 10000 } } } 启动 MCP Server 在{工作区}目录下： ucagent output/ Adder -s -hm --tui --mcp-server-no-file-tools --no-embed-tools 见到如下图则表示启动成功： 启动 Qwen Code 在UCAgent/output目录输入qwen启动 Qwen Code，看见 \u003eQWEN 图就表示启动成功。 开始验证 在框内输入提示词并且同意 Qwen Code 的使用工具、命令和读写文件请求。（通过j/k控制上/下）提示词如下： 请通过工具 RoleInfo 获取你的角色信息和基本指导，然后完成任务。请使用工具 ReadTextFile 读取文件。你需要在当前工作目录进行文件操作，不要超出该目录。\n有时候 Qwen Code 停止了，但是我们不确定是否完成了任务，此时可以通过查看 server 的 tui 界面来确认。\n此时 Mission 部分显示阶段还在 13，所以我们还要让 Qwen Code 继续执行任务。\n中途停止了，但是任务没有完成，可以通过在输入框里输入“继续”来继续。\n结果分析 最终的结果都在output文件夹中，其中的内容如下：\n. ├── Adder # 打包好的python DUT ├── Guide_Doc # 各种模板文件 ├── uc_test_report # 跑完的测试报告，包含可以直接网页运行的index.html └── unity_test # 各种生成的文档和测试用例文件 └── tests # 测试用例及其依赖 Guide_Doc：这些文件是“规范/示例/模板型”的参考文档，启动时会从vagent/lang/zh/doc/Guide_Doc复制到工作区的 Guide_Doc/（当前以 output 作为 workspace 时即 output/Guide_Doc/）。它们不会被直接执行，供人和 AI 作为编写 unity_test 文档与测试的范式与规范，并被语义检索工具读取，在 UCAgent 初始化时复制过来。\ndut_functions_and_checks.md\n用途：定义功能分组 FG-、功能点 FC-、检测点 CK-* 的组织方式与写法规范，要求覆盖所有功能点，每个功能点至少一个检测点。\n最终要产出的对应物：unity_test/{DUT}_functions_and_checks.md（如 Adder_functions_and_checks.md）。\ndut_fixture.md\n用途：说明如何编写 DUT Fixture/Env（包含接口、时序、复位、激励、监视、检查、钩子等），给出标准写法和必备项。\n对应物：unity_test/DutFixture 与 EnvFixture 相关实现/文档。\ndut_api_instruction.md\n用途：DUT API 设计与文档规范（接口命名、参数、返回、约束、边界条件、错误处理、示例）。\n对应物：unity_test/{DUT}_api.md 或 API 实现+测试（如 Adder_api.py）。\ndut_function_coverage_def.md\n用途：功能覆盖（Functional Coverage）定义方法，如何从 FG/FC/CK 推导覆盖项、covergroup/coverpoint/bin 的组织与命名。\n对应物：coverage 定义文件与生成的覆盖数据、以及相关说明文档，如Adder_function_coverage_def.py。\ndut_line_coverage.md\n用途：行覆盖采集与分析方法，如何启用、统计、解读未命中行、定位冗余或缺失测试。\n对应物：行覆盖数据文件与分析笔记（unity_test/{DUT}_line_coverage_analysis.md，如 Adder_line_coverage_analysis.md）。\ndut_test_template.md\n用途：测试用例的骨架/模板，给出最小可行的结构与编写范式（Arrange-Act-Assert、前后置、标记/选择器等）。\n对应物：tests/ 下各具体测试文件的基本结构参考。\ndut_test_case.md\n用途：单个测试用例的撰写规范（命名、输入空间、边界/异常、可重复性、断言质量、日志、标记）。\n对应物：tests/ 中具体 test_xxx.py::test_yyy 的质量基准与填写要求。\ndut_test_program.md\n用途：测试计划/测试编排（回归集合、分层/分阶段执行、标记与选择、超时控制、先后顺序、依赖关系）。\n对应物：回归集配置、命令/脚本、阶段化执行策略文档。\ndut_test_summary.md\n用途：测试阶段性/最终总结的结构（通过率、覆盖率、主要问题、修复状态、风险/残留问题、下一步计划）。\n对应物：unity_test/{DUT}_test_summary.md（如Adder_test_summary.md） 或报告页面（output/uc_test_report）。\ndut_bug_analysis.md\n用途：Bug 记录与分析规范（复现步骤、根因分析、影响范围、修复建议、验证状态、标签与追踪）。\n对应物：unity_test/{DUT}_bug_analysis.md（如Adder_bug_analysis.md）。\nuc_test_report：由 toffee-test 生成的 index.html 报告，可直接使用浏览器打开。\n这个报告包含了 Line Coverage 行覆盖率，Functional Coverage 功能覆盖率，测试用例的通过情况，功能点标记具体情况等内容。 unity_test/tests：验证代码文件夹\nAdder.ignore\n作用：行覆盖率忽略清单。支持忽略整个文件，或以“起止行段”形式忽略代码段。\n被谁使用：Adder_api.py 的 set_line_coverage(request, get_coverage_data_path(request, new_path=False), ignore=current_path_file(\"Adder.ignore\"))。\n与 Guide_Doc 的关系： 对应参考：dut_line_coverage.md（说明如何启用/统计/分析行覆盖，以及忽略规则的意义和使用场景）。\nAdder_api.py\n作用：测试公共基座，集中放 DUT 构造、覆盖率接线与采样、pytest 基础夹具（fixtures）和示例 API。\ncreate_dut(request): 实例化 DUT、设置覆盖率文件、可选波形、绑定 StepRis 采样。 AdderEnv: 封装引脚与常用操作（Step）。 api_Adder_add: 对外暴露的测试 API，完成参数校验、信号赋值、推进、读取结果。 pytest fixtures：dut（模块级，负责覆盖率采样/收集交给 toffee_test）、env（函数级，给每个 test 一个全新环境）。 与 Guide_Doc 的关系：\ndut_fixture.md：夹具/环境（Fixture/Env）的组织、Step/StepRis 的用法与职责边界。 dut_api_instruction.md：API 设计（命名、参数约束、返回、示例、异常）和文档规范。 dut_function_coverage_def.md：如何将功能覆盖组接线到 DUT 并在 StepRis 内采样。 dut_line_coverage.md：如何设置行覆盖文件、忽略清单，并将数据上报给 toffee_test。 Adder_function_coverage_def.py\n作用：功能覆盖定义（Functional Coverage），声明 FG/FC/CK 并给出 watch_point 条件。\n定义覆盖组：FG-API、FG-ARITHMETIC、FG-BIT-WIDTH。 每组下定义 FC-_ 和 CK-_ 条件（如 CK-BASIC/CK-CARRY-IN/CK-OVERFLOW 等）。\nget_coverage_groups(dut): 初始化并返回覆盖组列表，供 Adder_api.py 绑定与采样。 与 Guide_Doc 的关系：\ndut_function_coverage_def.md：覆盖组/覆盖点的组织方式与命名规范、watch_point 的表达方式。 dut_functions_and_checks.md：FG/FC/CK 的命名体系与映射关系来源，测试中用 mark_function 标记覆盖时需与此保持一致。 test_Adder_api_basic.py\n作用：API 层面的基础功能测试，覆盖典型输入、进位、零值、溢出、边界等。\n使用 from Adder_api import * 来获取 fixtures（dut/env）与 API。 在每个测试中通过 env.dut.fc_cover[“FG-…”].mark_function(“FC-…”, \u003ctest_fn\u003e, [“CK-…”]) 标注功能覆盖命中关系。 与 Guide_Doc 的关系： dut_test_case.md：单测结构（目标/流程/预期）、命名与断言规范、可重复性、标记与日志。 dut_functions_and_checks.md：FG/FC/CK 的正确引用与标注。 dut_test_template.md：docstring 和结构写法的范式来源。 test_Adder_functional.py\n作用：功能行为测试（接近“场景/功能项”的角度），比 API 基测覆盖更全面的功能点验证。\n同样通过 mark_function 与 FG/FC/CK 标签体系对齐。 与 Guide_Doc 的关系： dut_test_case.md：功能类测试的编写规范与断言要求。 dut_functions_and_checks.md：功能覆盖标注的规范与完整性。 dut_test_template.md：测试函数组织的范式。 test_example.py\n作用：空白样例（脚手架），用于新增测试文件的最小模板参考。 与 Guide_Doc 的关系：\ndut_test_template.md：新建测试文件/函数时的结构、引入方式与标注方法的模板。 unity_test/*.md：验证相关文档\nAdder_basic_info.md\n用途：DUT 概览与接口说明（功能、端口、类型、粗粒度功能分类）。 参考：Guide_Doc/dut_functions_and_checks.md（接口/功能分类用语）、Guide_Doc/dut_fixture.md（从验证视角描述 I/O 与 Step 时可参考）。 Adder_verification_needs_and_plan.md\n用途：验证需求与计划（目标、风险点、测试项规划、方法论）。 参考：Guide_Doc/dut_test_program.md（编排与选择策略）、Guide_Doc/dut_test_case.md（单测质量要求）、Guide_Doc/dut_functions_and_checks.md（从需求到 FG/FC/CK 的映射）。 Adder_functions_and_checks.md\n用途：FG/FC/CK 真源清单，测试标注与功能覆盖定义需与此保持一致。 参考：Guide_Doc/dut_functions_and_checks.md（结构/命名）、Guide_Doc/dut_function_coverage_def.md（如何落地为覆盖实现）。 Adder_line_coverage_analysis.md\n用途：行覆盖率结论与分析，解释忽略清单、未命中行、补测建议。 参考：Guide_Doc/dut_line_coverage.md；配合同目录 tests 下的 Adder.ignore。 Adder_bug_analysis.md\n用途：缺陷分析报告，按 CK/TC 对应、置信度、根因、修复建议与回归方法撰写。 参考：Guide_Doc/dut_bug_analysis.md（结构/要素）、Guide_Doc/dut_functions_and_checks.md（命名一致）。 Adder_test_summary.md\n用途：阶段性/最终测试总结（执行统计、覆盖情况、缺陷分布、建议、结论）。 参考：Guide_Doc/dut_test_summary.md，与 Guide_Doc/dut_test_program.md 呼应。 流程总结 要做什么：\n将 DUT（如 Adder）打包为可测的 Python 模块 启动 UCAgent（可带 MCP Server），让 Code Agent 协作按阶段推进验证 依据 Guide_Doc 规范生成/完善 unity_test 文档与 tests，并以功能覆盖+行覆盖驱动测试 发现并分析缺陷，产出报告与结论 做了什么：\n用 picker 将 RTL 导出为 Python 包（output/Adder/），准备最小 README 与文件清单 启动 ucagent（含 --mcp-server/--mcp-server-no-file-tools），在 TUI/MCP 下协作 在 Guide_Doc 规范约束下，生成/补全： 功能清单与检测点：unity_test/Adder_functions_and_checks.md（FG/FC/CK） 夹具/环境与 API：tests/Adder_api.py（create_dut、AdderEnv、api_Adder_*） 功能覆盖定义：tests/Adder_function_coverage_def.py（绑定 StepRis 采样） 行覆盖配置与忽略：tests/Adder.ignore，分析文档 unity_test/Adder_line_coverage_analysis.md 用例实现：tests/test_*.py（标注 mark_function 与 FG/FC/CK） 缺陷分析与总结：unity_test/Adder_bug_analysis.md、unity_test/Adder_test_summary.md 通过工具编排推进：RunTestCases/Check/StdCheck/KillCheck/Complete/GoToStage 权限控制仅允许写 unity_test/ 与 tests（add_un_write_path/del_un_write_path） 实现的效果：\n自动/半自动地产出合规的文档与可回归的测试集，支持全量与定向回归 功能覆盖与行覆盖数据齐备，未命中点可定位与补测 缺陷根因、修复建议与验证方法有据可依，形成结构化报告（uc_test_report/index.html） 支持 MCP 集成与 TUI 协作，过程可暂停/检查/回补，易于迭代与复用 典型操作轨迹（卡住时）：\nCheck → StdCheck(lines=-1) → KillCheck → 修复 → Check → Complete ","categories":["教程"],"description":"工具介绍与安装。","excerpt":"工具介绍与安装。","ref":"/mlvp/docs/ucagent/introduce/","tags":["docs"],"title":"工具介绍"},{"body":"以Adder为例，各语言的验证代码和注释如下：\nCpp Java Scala Python Go #include \"UT_Adder.hpp\" int64_t random_int64() { static std::random_device rd; static std::mt19937_64 generator(rd()); static std::uniform_int_distribution distribution(INT64_MIN, INT64_MAX); return distribution(generator); } int main() { UTAdder *dut = new UTAdder(); dut-\u003eStep(1); printf(\"Initialized UTAdder\\n\"); struct input_t { uint64_t a; uint64_t b; uint64_t cin; }; struct output_t { uint64_t sum; uint64_t cout; }; for (int c = 0; c \u003c 114514; c++) { input_t i; output_t o_dut, o_ref; i.a = random_int64(); i.b = random_int64(); i.cin = random_int64() \u0026 1; auto dut_cal = [\u0026]() { dut-\u003ea = i.a; dut-\u003eb = i.b; dut-\u003ecin = i.cin; dut-\u003eStep(1); o_dut.sum = (uint64_t)dut-\u003esum; o_dut.cout = (uint64_t)dut-\u003ecout; }; auto ref_cal = [\u0026]() { uint64_t sum = i.a + i.b; bool carry = sum \u003c i.a; sum += i.cin; carry = carry || sum \u003c i.cin; o_ref.sum = sum; o_ref.cout = carry ; }; dut_cal(); ref_cal(); printf(\"[cycle %lu] a=0x%lx, b=0x%lx, cin=0x%lx\\n\", dut-\u003exclock.clk, i.a, i.b, i.cin); printf(\"DUT: sum=0x%lx, cout=0x%lx\\n\", o_dut.sum, o_dut.cout); printf(\"REF: sum=0x%lx, cout=0x%lx\\n\", o_ref.sum, o_ref.cout); Assert(o_dut.sum == o_ref.sum, \"sum mismatch\"); } dut-\u003eFinish(); printf(\"Test Passed, destory UTAdder\\n\"); return 0; } package com.ut; import java.math.BigInteger; // import the generated UT class import com.ut.UT_Adder; public class example { static public void main(String[] args){ UT_Adder adder = new UT_Adder(); for(int i=0; i\u003c10000; i++){ int N = 1000000; long a = (long) (Math.random() * N); long b = (long) (Math.random() * N); long c = (long) (Math.random() * N) \u003e 50 ? 1 : 0; // set inputs adder.a.Set(a); adder.b.Set(b); adder.cin.Set(c); // step adder.Step(); // reference model long sum = a + b; boolean carry = sum \u003c a ? true : false; sum += c; carry = carry || sum \u003c c; // assert assert adder.sum.U().longValue() == sum : \"sum mismatch: \" + adder.sum.U() + \" != \" + sum; assert adder.cout.U().intValue() == (carry ? 1 : 0) : \"carry mismatch: \" + adder.cout.U() + \" != \" + carry; } System.out.println(\"Java tests passed\"); adder.Finish(); } } package com.ut import java.lang.Math import com.ut.UT_Adder object example { def main(args: Array[String]): Unit = { val adder = new UT_Adder() for (i \u003c- 0 until 10000) { val N = 1000000 val a = (Math.random() * N).toLong val b = (Math.random() * N).toLong val c = if ((Math.random() * N).toLong \u003e 50) 1 else 0 // set inputs adder.a.Set(a) adder.b.Set(b) adder.cin.Set(c) // step adder.Step() // reference model var sum = a + b var carry = if (sum \u003c a) true else false sum += c carry = carry || (sum \u003c c) // assert assert(adder.sum.U().longValue() == sum, s\"sum mismatch: ${adder.sum.U()} != $sum\") assert(adder.cout.U().intValue() == (if (carry) 1 else 0), s\"carry mismatch: ${adder.cout.U()} != $carry\") println(s\"[cycle ${adder.xclock.getClk().intValue()}] a=${adder.a.U64()}, b=${adder.b.U64()}, cin=${adder.cin.U64()}\") } println(\"Scala tests passed\") adder.Finish() } } from Adder import * import random class input_t: def __init__(self, a, b, cin): self.a = a self.b = b self.cin = cin class output_t: def __init__(self): self.sum = 0 self.cout = 0 def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 128) - 1) def as_uint(x, nbits): return x \u0026 ((1 \u003c\u003c nbits) - 1) def main(): dut = DUTAdder() # Assuming USE_VERILATOR print(\"Initialized UTAdder\") for c in range(11451): i = input_t(random_int(), random_int(), random_int() \u0026 1) o_dut, o_ref = output_t(), output_t() def dut_cal(): dut.a.value, dut.b.value, dut.cin.value = i.a, i.b, i.cin dut.Step(1) o_dut.sum = dut.sum.value o_dut.cout = dut.cout.value def ref_cal(): sum = as_uint( i.a + i.b + i.cin, 128+1) o_ref.sum = as_uint(sum, 128) o_ref.cout = as_uint(sum \u003e\u003e 128, 1) dut_cal() ref_cal() print(f\"[cycle {dut.xclock.clk}] a=0x{i.a:x}, b=0x{i.b:x}, cin=0x{i.cin:x}\") print(f\"DUT: sum=0x{o_dut.sum:x}, cout=0x{o_dut.cout:x}\") print(f\"REF: sum=0x{o_ref.sum:x}, cout=0x{o_ref.cout:x}\") assert o_dut.sum == o_ref.sum, \"sum mismatch\" dut.Finish() print(\"Test Passed, destroy UTAdder\") if __name__ == \"__main__\": main() package main import ( \"fmt\" \"time\" \"math/rand\" ut \"UT_Adder\" ) func assert(cond bool, msg string) { if !cond { panic(msg) } } func main() { adder := ut.NewUT_Adder() rand.Seed(time.Now().UnixNano()) for i := 0; i \u003c 10000; i++ { N := 1000000 a := rand.Int63n(int64(N)) b := rand.Int63n(int64(N)) var c int64 if rand.Int63n(int64(N)) \u003e 50 { c = 1 } else { c = 0 } adder.A.Set(a) adder.B.Set(b) adder.Cin.Set(c) adder.Step() // reflerence model sum := a + b carry := sum \u003c a sum += c carry = carry || sum \u003c c // assert assert(adder.Sum.U64() == uint64(sum), fmt.Sprintf(\"sum mismatch: %d != %d\\n\", adder.Sum.U64(), uint64(sum))) var carry_bool uint64 if carry { carry_bool = 1 } else { carry_bool = 0 } assert(adder.Cout.U64() == carry_bool, fmt.Sprintf(\"carry mismatch: %d != %t\\n\", adder.Cout.U().Int64(), carry)) } adder.Finish(); fmt.Println(\"Golang tests passed\") } ","categories":["教程"],"description":"使用C++、Java、Python和Golang验证加法器的案例","excerpt":"使用C++、Java、Python和Golang验证加法器的案例","ref":"/mlvp/docs/multi-lang/examples/adder/","tags":["docs"],"title":"加法器"},{"body":" 在开始前本页会 简单的介绍什么是验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n芯片验证 芯片验证是确保芯片设计正确性和可靠性的重要环节，主要包括功能验证、形式验证和物理验证等形式，本学习材料仅仅包含对功能验证的介绍，且侧重于基于仿真器的芯片功能验证。芯片功能验证的流程和方法与软件测试有比较大的共同点，例如都有单元测试、系统测试、黑盒测试、白盒测试等。在验证指标上也有共同特点，例如功能覆盖率、代码覆盖率等等。从某种形式上说，除了使用的工具和编程语言不一样外，他们的目标和流程几乎相同。因此，在不考虑工具和编程语言的情况下，会软件测试的工程师应当就会芯片验证。 但在实际工作中，软件测试和芯片验证属于两个完全不相交的行业，其主要原因是验证工具和验证语言的不同，导致软件测试工程师很难实现跨界。在芯片验证领域，通常使用硬件描述语言进行验证（例如 Verilog 或者 System Verilog），使用专业商业工具进行电路仿真。硬件描述语言不同于C++/Python等高级软件编程语言，具有独特的“时钟”特性，对于软件领域的工程师不友好，学习成本高。\n为了打通芯片验证与传统软件测试之间的壁垒，让更多的人参与到芯片验证，本项目提供如下内容：\n多语言验证工具（Picker），让用户可以使用自己擅长的编程语言进行芯片验证 验证框架（toffee），如何在不关心时钟的情况下进行功能验证\n介绍基本电路、验证知识，方便软件背景爱好者更能容易的理解电路特征\n提供基本学习材料，学习基本验证知识\n提供真实高性能芯片验证案例，让爱好者可以远程参与验证工作\n基本术语 DUT： DUT（Design Under Test）指待测试设计，通常指设计好的RTL代码。\nRM： Reference Model （RM）指代待测试单元对应的参考模型，参考模型通常被认为是标准的，没有错误的。\nRTL： 指寄存器传输级（Register Transfer Level），通常指代芯片设计对应的 verilog 或者 vhdl 代码。\n覆盖率： 测试覆盖率是指测试范围与整个需求范围的百分比。在芯片验证领域，通常有代码行覆盖率、函数覆盖率、功能覆盖率等。\nDV： DV中的D通常指设计（Desgin），V指验证（Verification）。合在一起指设计与验证协同工作。\n差分测试（difftest）： 选取两个（或以上）功能相同的被测对象，选取符合被测对象要求的同一测试用例分别提交被测对象进行执行，以观测执行结果是否存在差异的过程。\n工具介绍 本学习材料用到的核心工具为 picker （https://github.com/XS-MLVP/picker），它的作用是将RTL编写的设计模块自动提供高级编程语言接口（Python/C++等）。基于该工具，软件开发（测试）背景的验证人员可以不用去学习 Verilog/VHDL 等硬件描述语言进行芯片验证。\n系统需求 建议操作系统：Ubuntu 22.04 LTS\n在系统结构开发、科研的过程中，Linux 是最为常用的平台，这主要是因为 Linux 拥有丰富的软件、工具资源：由于 Linux 的开源性，各大重要工具软件（如 Verilator）可以很容易地面向 Linux 进行开发。 在本课程的实验中，多语言验证工具Picker、Swig等工具都可以在 Linux 上稳定运行。 ","categories":["示例项目","教程"],"description":"如何使用开放验证平台的环境参与到硬件验证中来。","excerpt":"如何使用开放验证平台的环境参与到硬件验证中来。","ref":"/mlvp/docs/quick-start/","tags":["examples","docs"],"title":"快速开始"},{"body":"验证报告 https://github.com/XS-MLVP/Example-NutShellCache/blob/master/nutshell_cache_report_demo.pdf\n验证环境\u0026用例代码 https://github.com/XS-MLVP/Example-NutShellCache\n","categories":["示例项目","教程"],"description":"利用Python语言对果壳Cache进行验证，","excerpt":"利用Python语言对果壳Cache进行验证，","ref":"/mlvp/docs/advance_case/nutshellcache/","tags":["examples","docs"],"title":"完整果壳 Cache 验证"},{"body":" 本页简单介绍什么是芯片验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n芯片验证过程需要和企业、团队的实际情况契合，没有符合所有要求，必须参考的绝对标准。\n什么是芯片验证 芯片从设计到成品的过程主要包括芯片设计、芯片制造、芯片封测试三大阶段。在芯片设计中，又分前端设计和后端设计，前端设计也称之为逻辑设计，目标是让电路逻辑达到预期功能要求。后端设计也称为物理设计，主要工作是优化布局布线，减小芯片面积，降低功耗，提高频率等。芯片验证（Chip Verification）是芯片设计流程中的一个重要环节。它的目标是确保设计的芯片在功能、性能和功耗等方面都满足预定的规格。验证过程通常包括功能验证、时序验证和功耗验证等多个步骤，使用的方法和工具包括仿真、形式验证、硬件加速和原型制作等。针对本文，芯片验证仅包含对芯片前端设计的验证，验证设计的电路逻辑是否满足既定需求（“Does this proposed design do what is intended?\"），通常也称为功能验证（Functional verification），不包含功耗、频率等后端设计。\n对于芯片产品，一旦设计错误被制造出来修改成本将会非常高昂，因为可能需要召回产品，并重新制造芯片，无论是经济成本还是时间成本都十分昂贵。经典由于芯片验证不足导致失败的典型案例如下： Intel Pentium FDIV Bug：在1994年，Intel的Pentium处理器被发现存在一个严重的除法错误，这个错误被称为FDIV bug。这个错误是由于在芯片的浮点单元中，一个查找表中的几个条目错误导致的。这个错误在大多数应用中不会出现，但在一些特定的计算中会导致结果错误。由于这个错误，Intel不得不召回了大量的处理器，造成了巨大的经济损失。\nAriane 5 Rocket Failure：虽然这不是一个芯片的例子，但它展示了硬件验证的重要性。在1996年，欧洲空间局的Ariane 5火箭在发射后不久就爆炸了。原因是火箭的导航系统中的一个64位浮点数被转换为16位整数时溢出，导致系统崩溃。这个错误在设计阶段没有被发现，导致了火箭的失败。\nAMD Barcelona Bug：在2007年，AMD的Barcelona处理器被发现存在一个严重的转译查找缓冲（TLB）错误。这个错误会导致系统崩溃或者重启。AMD不得不通过降低处理器的频率和发布BIOS更新来解决这个问题，这对AMD的声誉和财务状况造成了重大影响。\n这些案例都强调了芯片验证的重要性。如果在设计阶段就能发现并修复这些错误，那么就可以避免这些昂贵的失败。验证不足的案例不仅发生在过去，也发生在现在，例如某新入局 ASIC 芯片市场的互联网企业打造一款 55 纳米芯片，极力追求面积缩减并跳过验证环节，最终导致算法失败，三次流片皆未通过测试，平均每次流片失败导致企业损失约 50 万美元。\n芯片验证流程 芯片设计和验证的耦合关系如上图所示，设计和验证有同样的输入，即规范文档（specification）。参考规范，设计与验证人员双方按照各自的理解，以及各自的需求进行独立编码实现。设计方需要满足的前提是编码的RTL代码“可综合”，需要考虑电路特性，而验证方一般只要考虑功能是否满足要求，编码限制少。双方完成模块开发后，需要进行健全性对比测试（Sanity Test），判定功能是否表现一致，若不一致需要进行协同排查，确定问题所在并进行修复，再进行对比测试，直到所有功能点都满足预期。由于芯片设计和芯片验证耦合度很高，因此有些企业在研发队伍上也进行了直接耦合，为每个子模块的设计团队都配置了对应的验证团队（DV）。上图中的设计与验证的耦合流程为粗粒度的关系，具体到具体芯片（例如Soc、DDR）、具体企业等都有其适合自身的合作模式。\n在上述对比测试中，设计方的产出的模块通常称为DUT（Design Under Test），验证方开发的模型通常称为RM（Reference Model）。针对图中的验证工作，按照流程可以有：编写验证计划、创建验证平台、整理功能点、构建测试用例、运行调试、收集Bug/覆盖率、回归测试、编写测试报告等多个阶段。\n验证计划： 验证计划描述了如何进行验证，以及如何保证验证质量，达到功能验证要求。在文档结构上通常包含验证目标，验证策略、验证环境、验证项、验证过程、风险防范、资源及时间表、结果和报告等部分。验证目标明确需要验证的功能或性能指标，这些目标应该直接从芯片的规范文档中提取。验证策略描述如何进行验证，包括可能使用的验证方法，例如仿真、形式化、FPGA加速等，以及如何组织验证任务。验证环境用于描述具体的测试环境，例如验证工具类型，版本号等。验证项包含了需要验证的具体项以及预期结果。验证计划可以有总计划，也可以针对具体验证的子任务进行编写。\n平台搭建： 验证平台是具体验证任务的执行环境，同一类验证任务可以使用相同的验证平台。验证平台的搭建是验证流程中的关键步骤、具体包含验证工具选择（例如是采用软件仿真，还是采用形式化验证，或者硬件加速）、环境配置（例如配置服务器环境，FPGA环境）、创建测试环境、基本测试案例等。创建好基本测试平台，跑通基本测试案例，也通常称为“冒烟测试”。后继具体的测试代码，都将基于该测试平台进行，因此测试平台需要具有可重用性。验证平台通常包含测试框架和被测试代码，以及对应的基本信号激励。\n功能点整理： 根据规范手册（spec）列出DUT的基本功能，并对其进行明确的描述，以及如何对该功能点进行测试。功能点整理过程中，需要根据重要性、风险、复杂性等因数对其进行优先级排序。功能点整理还需要对各个功能点进行追踪和状态，如果发现原始功能点有更新需要及时进行对应计划的同步。\n测试用例： 测试用例是指一组条件或变量，用于确定DUT是否满足特定需求并能正确运行。每个测试用例通常包含测试条件，输入数据，预期结果，实际结果和测试结果。通过运行测试用例并比较预期结果和实际结果，可以确定系统或应用是否正确实现了特定的功能或需求。在芯片验证中，测试用例是用来验证芯片设计是否满足规格要求的重要工具。\n编码实现： 编码实现即测试用例的具体执行过程，包括测试数据生成、测试框架选择、编程语言选择、参考模型编写等。编码实现是对功能点和测试用例充分理解后工作，如果理解不到位，可能导致DUT无法驱动，不能发现潜在bug等问题。\n收集bug/覆盖率： 验证的目标就是提前发现设计中存在的bug，因此需要对发现的bug进行收集和管理。每发现一个新缺陷，需要给定唯一标号，并同设计工程师进行bug定级，然后进行状态追踪。能发现bug最好，但在实际验证中不是每次测试都能发现bug，因此需要另外一个指标评价验证是否到位。该指标通常采用覆盖率，当覆盖率超过一点阈值（例如代码覆盖率大于90%）后方可任务进行了充分验证。\n回归测试： 验证和设计是一个相互迭代的过程，因此当验证出bug后，需要设计进行修正，且需要保证修正后的DUT仍然能正常工作。这种测试的目的是捕获可能由于修改而引入的新错误，或者重新激活旧错误。回归测试可以是全面的，也就是说，它涵盖了所有的功能，或者可以是选择性的，只针对某些特定的功能或系统部分。\n测试报告： 测试报告是对整个验证过程的总结，它提供了关于测试活动的全面视图，包括测试的目标、执行的测试用例、发现的问题和缺陷、测试覆盖率和测试效率等。\n芯片验证层次 按照验证对象的大小，芯片验证通常包含UT、BT、IT、ST四个层次。\n单元测试（Unit Testing， UT）： 这是最低的验证层次，主要针对单个模块或组件进行。目标是验证每个模块或组件的功能是否正确。\n块测试（Block Testing，BT）： 很多时候，单个模块和其他模块存在紧耦合，如果进行单独UT测试，可能存在信号处理复杂，功能验证不准确等问题，这时候可以把多个有耦合关系的模块合并成一个DUT块进行测试。\n集成测试（Integration Testing）： 在单元测试的基础上，将多个模块或组件组合在一起，验证它们能否正确地协同工作，通常用于测试子系统功能是否正常。\n系统测试（System Testing）： ST通常也称为Top验证，在集成测试的基础上，将所有的模块或组件组合在一起，形成一个完整的系统，验证系统的功能是否正确，以及系统的性能是否满足要求。\n理论上，这些层次的验证通常按照从低到高的顺序进行，每个层次的验证都建立在前一个层次的验证的基础上。但实际验证活动中，需要根据企业验证人员的规模、熟练度，功能需求等进行选择，不一定所有层次的验证都需要涉及。在每个层次，都需要编写相应的测试用例，运行测试，收集和分析结果，以确保芯片设计的正确性和质量。\n芯片验证指标 芯片验证的指标，通常包含功能正确性、测试覆盖率、缺陷密度、验证效率、验证成本等多个方面。功能正确性是最基本的验证指标，即芯片是否能够正确地执行其设计的功能。这通常通过运行一系列的功能测试用例来验证，包括正常情况下的功能测试，以及异常情况下的鲁棒性测试。测试覆盖率是指测试用例覆盖了多少设计的功能点，以及覆盖的程度如何。高的测试覆盖率通常意味着更高的验证质量。测试覆盖率可以进一步细分为代码覆盖率、功能覆盖率、条件覆盖率等。缺陷密度是指在一定的设计规模或代码量中，发现的缺陷的数量。低的缺陷密度通常意味着更高的设计质量。验证效率是指在一定的时间和资源下，能够完成的验证工作量。高的验证效率通常意味着更高的验证生产力。验证成本是指进行验证所需要的总体资源，包括人力、设备、时间等。低的验证成本通常意味着更高的验证经济性。\n功能正确性是验证的绝对指标，但在实践中，很多时候无法确定测试方案是否完备，所有测试空间是否全部测试到位，因此需要一个可量化的指标来指导验证是否足够充分，是否可以结束验证。该指标通常采用“测试覆盖率”。测试覆盖率通常有代码覆盖率（行，函数，分支）、功能覆盖率。\n代码行覆盖率： 即在测试过程中，DUT的设计代码中有多少行被执行；\n函数覆盖率： 即在测试过程中，DUT的设计代码中有多少函数被执行；\n分支覆盖率： 即在测试过程中，DUT的设计代码中有多少分支被执行（if else）；\n功能覆盖率： 即在测试过程中，有多少预定义功能被触发。\n高的代码覆盖率可以提高验证的质量和可靠性，但并不能保证验证的完全正确性，因为它不能覆盖所有的输入和状态组合。因此，除了追求高的代码覆盖率，还需要结合其他测试方法和指标，如功能测试、性能测试、缺陷密度等。\n芯片验证管理 芯片验证管理是一个涵盖了芯片验证过程中所有活动的管理过程，包括之前提到的验证策略的制定、验证环境的搭建、测试用例的编写和执行、结果的收集和分析、以及问题和缺陷的跟踪和修复等。芯片验证管理的目标是确保芯片设计满足所有的功能和性能要求，以及规格和标准。\n在芯片验证管理中，首先需要制定一个详细的验证策略，包括验证的目标、范围、方法、时间表等。然后，需要搭建一个适合的验证环境，包括硬件设备、软件工具、测试数据等。接下来，需要编写一系列的测试用例，覆盖所有的功能和性能点，然后执行这些测试用例，收集和分析结果，找出问题和缺陷。最后，需要跟踪和修复这些问题和缺陷，直到所有的测试用例都能通过。\n芯片验证管理是一个复杂的过程，需要多种技能和知识，包括芯片设计、测试方法、项目管理等。它需要与芯片设计、生产、销售等其他活动紧密协作，以确保芯片的质量和性能。芯片验证管理的效果直接影响到芯片的成功和公司的竞争力。因此，芯片验证管理是芯片开发过程中的一个重要环节。\n芯片验证管理过程可以基于“项目管理平台”和“bug管理平台”进行，基于平台的管理效率通常情况下明显高于基于人工的管理模式。\n芯片验证现状 当前，芯片验证通常是在芯片设计公司内部完成的，这一过程不仅技术上复杂，而且具有巨大的成本。从验收与设计的紧密关系来看，芯片验证不可避免地涉及芯片设计的源代码。然而，芯片设计公司通常将芯片设计源代码视为商业机密，这使得必须由公司内部人员来执行芯片验证，难以将验证工作外包。\n芯片验证的重要性在于确保设计的芯片在各种条件下能够可靠运行。验证工作不仅仅是为了满足技术规格，还需要应对不断增长的复杂性和新兴技术的要求。随着半导体行业的发展，芯片验证的工作量不断增加，尤其是对于复杂的芯片而言，验证工作已经超过了设计工作，占比超过70%。这使得在工程师人员配比上，验证工程师人数通常是设计工程师人数的2倍或以上（例如zeku的三千人规模团队中，大约有一千人的设计工程师，两千人的验证工程师。其他大型芯片设计公司的验证人员占比类似或更高）。\n由于验证工作的特殊性，需要对芯片设计源代码进行访问，这在很大程度上限制了芯片验证的外包可能性。芯片设计源代码被视为公司的核心商业机密，涉及到技术细节和创新，因此在安全和法律层面上不太可能与外部方共享。这也导致了公司内部人员必须承担验证工作的重任，增加了公司内部的工作负担和成本。\n在当前情况下，芯片验证工程师的需求持续增加。他们需要具备深厚的技术背景，熟悉各种验证工具和方法，并且对新兴技术有敏锐的洞察力。由于验证工作的复杂性，验证团队通常需要庞大的规模，这与设计团队规模形成鲜明对比。\n为了应对这一挑战，行业可能需要不断探索创新的验证方法和工具，以提高验证效率，降低成本。\n小结：复杂芯片验证成本昂贵，表现在如下几个方面 验证工作量大： 对于复杂芯片，验证工作在整个芯片设计工作中，占比超过 70%。\n人力成本高： 验证工程师人数是设计工程师人数的2倍，对于复杂业务，工程师数量在千人以上。\n内部验证： 芯片设计公司为了保证商业秘密（芯片设计代码）不被泄露，只能选择招聘大量验证工程师，在公司内部进行验证工作。\n芯片验证众包 相比与硬件，软件领域为了减少软件测试成本，测试外包（分包）已经成为常态，该领域的分包业务非常成熟，市场规模已经是千亿人民币级别，并朝万亿级别规模进发。从工作内容上看，软件测试和硬件验证，有非常大的共同特征（系统的目的不同的对象），如果以软件的方式对硬件验证进行分包是否可行？\n把芯片验证工作进行外包（分包）面临诸多挑战，例如： 从业人员基数少： 相比软件领域，硬件开发者数量少了几个数量级。例如在github的统计上（https://madnight.github.io/githut/#/pull_requests/2023/2），传统软件编程语言占（Python、Java、C++，Go）比接近 50%， 而硬件描述语言，verilog占比仅 0.076%，这能从侧面反应出各自领域的开发者数量。\n验证工具商业化： 企业中使用的验证工具（仿真器、形式化、数据分析）几乎都是商业工具，这类工具对于普通人来说几乎不可见，自学难度高。\n开放学习资料少： 芯片验证涉及到访问芯片设计的源代码，而这些源代码通常被视为公司的商业机密和专有技术。芯片设计公司可能不愿意公开详细的验证过程和技术，限制了学习材料的可用性。\n可行性分析 虽然芯片验证领域一直以来相对封闭，但从技术角度而言，采用分包的方式进行验证是一种可行的选择。这主要得益于以下几个因素：\n首先，随着开源芯片项目的逐渐增多，验证过程中所涉及的源代码已经变得更加开放和透明。这些开源项目在设计和验证过程中没有商业机密的顾虑，为学习和研究提供了更多的可能性。即使某些项目涉及商业机密，也可以通过采用加密等方式来隐藏设计代码，从而在一定程度上解决了商业机密的问题，使验证更容易实现。\n其次，芯片验证领域已经涌现出大量的基础验证工具，如verilator和systemc等。这些工具为验证工程师提供了强大的支持，帮助他们更高效地进行验证工作。通过这些工具，验证过程的复杂性和难度得到了一定程度的缓解，为采用分包的验证方法提供了更为可行的技术基础。\n在开源软件领域，已经有一些成功的案例可供参考。例如，Linux内核的验证过程采用了分包的方式，不同的开发者和团队分别负责不同的模块验证，最终形成一个整体完备的系统。类似地，机器学习领域的ImageNet项目也采用了分包标注的策略，通过众包的方式完成大规模的图像标注任务。这些案例为芯片验证领域提供了成功的经验，证明了分包验证在提高效率、降低成本方面的潜力。\n因此，尽管芯片验证领域相对于其他技术领域而言仍显得封闭，但技术的进步和开源项目的增多为采用分包验证提供了新的可能性。通过借鉴其他领域的成功经验和利用现有的验证工具，我们有望在芯片验证中推动更加开放、高效的验证方法的应用，进一步促进行业的发展。这种技术的开放性和灵活性将为验证工程师提供更多的选择，推动芯片验证领域迎来更为创新和多样化的发展。\n技术路线 为了克服挑战，让更多的人参与到芯片验证，本项目从如下几个技术方向进行持续尝试\n提供多语言验证工具： 传统芯片验证是基于System Verilog编程语言进行，但是该语言用户基数少，为了让其他软件开发/测试的技术人员参与到芯片验证，本项目提供多语言验证转换工具Picker，它可以让验证者使用自己熟悉的编程语言（例如C++/Python/Java/Go）基于开源验证工具参与验证工作。\n提供验证学习材料： 芯片验证学习材料少，主要原因由于商业公司几乎不可能公开其内部资料，为此本项目会持续更新学习材料，让验证人员可在线，免费学习所需要的技能。\n提供真实芯片验证案例： 为了让学习材料更具使用性，本项目以“香山昆明湖（工业级高性能risc-v处理器）IP核”作为基础，从中摘取模块持续更新验证案例。\n组织芯片设计分包验证： 学以致用是每个人学习的期望目标，为此本项目定期组织芯片设计的验证分包，让所有人（无论你是大学生、验证专家、软件开发测试者、还是中学生）都可以参与到真实芯片的设计工作中去。\n本项目的目标是达到如下愿景，“打开传统验证模式的黑盒，让所有感兴趣的人可以随时随地的，用自己擅长的编程语言参与芯片验证”。\n","categories":"","description":"关于芯片验证的基本概念\n","excerpt":"关于芯片验证的基本概念\n","ref":"/mlvp/docs/basic/ic_verify/","tags":"","title":"芯片验证"},{"body":"生成库文件 picker可以通过参数--lang指定转换的对应语言（参数已支持cpp、python、java、lua、scala、golang），由于不同编程语言对应的“库”不同，因此生成的库文件有所区别，例如java生成的是jar包，python生成的为文件夹。picker导出对应编程语言的库，需要xcomm的支持，可以通过picker --check查看支持情况：\n$picker --check [OK ] Version: 0.9.0-feat_performance_improve-b7001a6-2025-04-11-dirty [OK ] Exec path: /usr/local/share/lib/python3.11/site-packages/picker/bin/picker [OK ] Template path: /usr/local/share/lib/python3.11/site-packages/picker/share/picker/template [OK ] Support Cpp (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/lib' success) [OK ] Support Golang (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/golang' success) [OK ] Support Java (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/java/xspcomm-java.jar' success) [OK ] Support Lua (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/lua/luaxspcomm.so' success) [OK ] Support Python (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/python' success) [OK ] Support Scala (find: '/usr/local/share/lib/python3.11/site-packages/picker/share/picker/scala/xspcomm-scala.jar' success) 输出显示success表示支持，fail表示不支持。\nC++ 对于C++语言，picker生成的为so动态链接库，和对应的头文件。例如：\nUT_Adder/ ├── UT_Adder.cpp # DUT 文件 ├── UT_Adder.hpp # DUT 头文件 ├── UT_Adder_dpi.hpp # DPI 头文件 ├── dut_base.hpp # DUT base 头文件 ├── libDPIAdder.a # DPI 静态库 └── libUTAdder.so # DUT 动态库 在使用时，设置好LD路径，然后再测试代码中#include UT_Adder.hpp\nPython Python语言生成的为目录（Python module以目录的方式表示）\nUT_Adder/ ├── _UT_Adder.so ├── __init__.py ├── libUTAdder.so └── libUT_Adder.py 设置PYTHONPATH后，可以在test中import UT_Adder\nJava/scala 对于Java和scala基于JVM的编程语言，picker生成的为对应的jar包。\nUT_Adder/ ├── UT_Adder-scala.jar └── UT_Adder-java.jar go go语言生成的为目录（类似python）。\nUT_Adder/ └── golang └── src └── UT_Adder ├── UT_Adder.go ├── UT_Adder.so ├── UT_Adder_Wrapper.go ├── go.mod └── libUTAdder.so 设置GOPATH后，可直接进行import\n验证接口 DUT验证接口可以参考连接：https://github.com/XS-MLVP/picker/blob/master/doc/API.zh.md\nxspcomm库接口请参考连接：https://github.com/XS-MLVP/xcomm/blob/master/docs/APIs.cn.md\n","categories":["教程"],"description":"DUT文件与编程语言都支持的验证接口","excerpt":"DUT文件与编程语言都支持的验证接口","ref":"/mlvp/docs/multi-lang/interface/","tags":["docs"],"title":"验证接口"},{"body":" This page introduces the basics of digital circuits. Digital circuits use digital signals and are the foundation of most modern computers.\nWhat Are Digital Circuits Digital circuits are electronic circuits that use two discrete voltage levels to represent information. Typically, digital circuits use two power supply voltages to indicate high (H) and low (L) levels, representing the digits 1 and 0 respectively. This representation uses binary signals to transmit and process information.\nMost digital circuits are built using field-effect transistors, with MOSFETs (Metal-Oxide-Semiconductor Field-Effect Transistors) being the most common. MOSFETs are semiconductor devices that control current flow using an electric field, enabling digital signal processing.\nIn digital circuits, MOSFETs are combined to form various logic gates like AND, OR, and NOT gates. These logic gates are combined in different ways to create the various functions and operations in digital circuits. Here are some key features of digital circuits:\n(1) Voltage Representation： Digital circuits use two voltage levels, high and low, to represent digital information. Typically, a high level represents the digit 1, and a low level represents the digit 0.\n(2) MOSFET Implementation： MOSFETs are one of the most commonly used components in digital circuits. By controlling the on and off states of MOSFETs, digital signal processing and logic operations can be achieved.\n(3) Logic Gate Combinations： Logic gates, composed of MOSFETs, are the basic building blocks of digital circuits. By combining different logic gates, complex digital circuits can be built to perform various logical functions.\n(4) Binary Representation： Information in digital circuits is typically represented using the binary system. Each digit can be made up of a series of binary bits, which can be processed and operated on within digital circuits.\n(5) Signal Processing： Digital circuits convert and process signals through changes in voltage and logic operations. This discrete processing method makes digital circuits well-suited for computing and information processing tasks.\nWhy Learn Digital Circuits Learning digital circuits is fundamental and necessary for the chip verification process, primarily for the following reasons:\n(1) Understanding Design Principles： Digital circuits are the foundation of chip design. Knowing the basic principles and design methods of digital circuits is crucial for understanding the structure and function of chips. The goal of chip verification is to ensure that the designed digital circuits work according to specifications in actual hardware, and understanding digital circuits is key to comprehending the design.\n(2) Design Standards： Chip verification typically involves checking whether the design meets specific standards and functional requirements. Learning digital circuits helps in understanding these standards, thus building better test cases and verification processes to ensure thorough and accurate verification.\n(3) Timing and Clocks： Timing issues are common challenges in digital circuit design and verification. Learning digital circuits helps in understanding concepts of timing and clocks, ensuring that timing issues are correctly handled during verification, avoiding timing delays and conflicts in the circuit.\n(4) Logical Analysis： Chip verification often involves logical analysis to ensure circuit correctness. Learning digital circuits fosters a deep understanding of logic, aiding in logical analysis and troubleshooting.\n(5) Writing Test Cases： In chip verification, various test cases need to be written to ensure design correctness. Understanding digital circuits helps in designing comprehensive and targeted test cases, covering all aspects of the circuit.\n(6) Signal Integrity： Learning digital circuits helps in understanding signal propagation and integrity issues within circuits. Ensuring proper signal transmission under different conditions is crucial, especially in high-speed designs.\nOverall, learning digital circuits provides foundational knowledge and tools for chip verification, enabling verification engineers to better understand designs, write effective test cases, analyze verification results, and troubleshoot issues. Theoretical and practical experience with digital circuits is indispensable for chip verification engineers.\nDigital Circuits Basics You can learn digital circuits through the following online resources：\nTsinghua University’s Digital Circuits Basics USTC Digital Circuit Lab Digital Design and Computer Architecture MIT Analysis and Design of Digital Integrated Circuits Hardware Description Language Chisel Traditional Description Languages Hardware Description Languages (HDL) are languages used to describe digital circuits, systems, and hardware. They allow engineers to describe hardware structure, function, and behavior through text files, enabling abstraction and modeling of hardware designs.\nHDL is commonly used for designing and simulating digital circuits such as processors, memory, controllers, etc. It provides a formal method to describe the behavior and structure of hardware circuits, making it easier for design engineers to perform hardware design, verification, and simulation.\nCommon hardware description languages include:\nVerilog：One of the most used HDLs, Verilog is an event-driven language widely used for digital circuit design, verification, and simulation. VHDL：Another common HDL, VHDL is an object-oriented language offering richer abstraction and modular design methods. SystemVerilog：An extension of Verilog, SystemVerilog introduces advanced features like object-oriented programming and randomized testing, making Verilog more suitable for complex system design and verification. Chisel Chisel is a modern, advanced hardware description language that differs from traditional Verilog and VHDL. It’s a hardware construction language based on Scala. Chisel offers a more modern and flexible way to describe hardware, leveraging Scala’s features to easily implement parameterization, abstraction, and reuse while maintaining hardware-level efficiency and performance.\nChisel’s features include:\nModern Syntax: Chisel’s syntax is more similar to software programming languages like Scala, making hardware description more intuitive and concise. Parameterization and Abstraction: Chisel supports parameterization and abstraction, allowing for the creation of configurable and reusable hardware modules. Type Safety: Based on Scala, Chisel has type safety features, enabling many errors to be detected at compile-time. Generating Performance-Optimized Hardware: Chisel code can be converted to Verilog and then synthesized, placed, routed, and simulated by standard EDA toolchains to generate performance-optimized hardware. Strong Simulation Support: Chisel provides simulation support integrated with ScalaTest and Firrtl, making hardware simulation and verification more convenient and flexible. Chisel Example of a Full Adder The circuit design is shown below:\nComplete Chisel code:\npackage examples import chisel3._ class FullAdder extends Module { // Define IO ports val io = IO(new Bundle { val a = Input(UInt(1.W)) // Input port 'a' of width 1 bit val b = Input(UInt(1.W)) // Input port 'b' of width 1 bit val cin = Input(UInt(1.W)) // Input port 'cin' (carry-in) of width 1 bit val sum = Output(UInt(1.W)) // Output port 'sum' of width 1 bit val cout = Output(UInt(1.W))// Output port 'cout' (carry-out) of width 1 bit }) // Calculate sum bit (sum of a, b, and cin) val s1 = io.a ^ io.b // XOR operation between 'a' and 'b' io.sum := s1 ^ io.cin // XOR operation between 's1' and 'cin', result assigned to 'sum' // Calculate carry-out bit val s3 = io.a \u0026 io.b // AND operation between 'a' and 'b', result assigned to 's3' val s2 = s1 \u0026 io.cin // AND operation between 's1' and 'cin', result assigned to 's2' io.cout := s2 | s3 // OR operation between 's2' and 's3', result assigned to 'cout' } You can refer to Chisel learning materials from the official documentation: https://www.chisel-lang.org/docs\n","categories":"","description":"Basic concepts of digital circuits\n","excerpt":"Basic concepts of digital circuits\n","ref":"/mlvp/en/docs/basic/ic_base/","tags":"","title":"Digital Circuits"},{"body":"Direct Usage Based on local CLI and LLM. Requires an OpenAI‑compatible API and an embedding model API.\nConfigure via Environment Variables (Recommended) Config file content:\n# OpenAI-compatible API config openai: model_name: \"$(OPENAI_MODEL: Qwen/Qwen3-Coder-30B-A3B-Instruct)\" # model name openai_api_key: \"$(OPENAI_API_KEY: YOUR_API_KEY)\" # API key openai_api_base: \"$(OPENAI_API_BASE: http://10.156.154.242:8000/v1)\" # API base URL # Embedding model config # Used for doc search and memory features; can be disabled via --no-embed-tools embed: model_name: \"$(EMBED_MODEL: Qwen/Qwen3-Embedding-0.6B)\" # embedding model name openai_api_key: \"$(EMBED_OPENAI_API_KEY: YOUR_API_KEY)\" # embedding API key openai_api_base: \"$(EMBED_OPENAI_API_BASE: http://10.156.154.242:8001/v1)\" # embedding API URL dims: 4096 # embedding dimension UCAgent config supports Bash‑style env placeholders: $(VAR: default). On load, it will be replaced with the current env var VAR; if unset, the default is used.\nFor example, in the built‑in vagent/setting.yaml: openai.model_name: \"$(OPENAI_MODEL: \u003cyour_chat_model_name\u003e)\" openai.openai_api_key: \"$(OPENAI_API_KEY: [your_api_key])\" openai.openai_api_base: \"$(OPENAI_API_BASE: http://\u003cyour_chat_model_url\u003e/v1)\" embed.model_name: \"$(EMBED_MODEL: \u003cyour_embedding_model_name\u003e)\" Also supports other providers: model_type supports openai, anthropic, google_genai (see vagent/setting.yaml). You can switch models and endpoints just by exporting env vars, without editing the config file.\nExample: set chat model and endpoint\n# Specify chat model (OpenAI‑compatible) export OPENAI_MODEL='Qwen/Qwen3-Coder-30B-A3B-Instruct' # Specify API Key and Base (fill in according to your provider) export OPENAI_API_KEY='your_api_key' export OPENAI_API_BASE='https://your-openai-compatible-endpoint/v1' # Optional: embedding model (if using retrieval/memory features) export EMBED_MODEL='text-embedding-3-large' export EMBED_OPENAI_API_KEY=\"$OPENAI_API_KEY\" export EMBED_OPENAI_API_BASE=\"$OPENAI_API_BASE\" Then start UCAgent as described earlier. To persist, append the above exports to your default shell startup file (e.g., bash: ~/.bashrc, zsh: ~/.zshrc, fish: ~/.config/fish/config.fish), then reopen terminal or source it manually.\nConfigure via config.yaml Create and edit config.yaml at the project root to configure the AI model and embedding model: # OpenAI-compatible API config openai: openai_api_base: \u003cyour_openai_api_base_url\u003e # API base URL model_name: \u003cyour_model_name\u003e # model name, e.g., gpt-4o-mini openai_api_key: \u003cyour_openai_api_key\u003e # API key # Embedding model config # Used for doc search and memory features; can be disabled via --no-embed-tools embed: model_name: \u003cyour_embed_model_name\u003e # embedding model name openai_api_base: \u003cyour_openai_api_base_url\u003e # embedding API URL openai_api_key: \u003cyour_api_key\u003e # embedding API key dims: \u003cyour_embed_model_dims\u003e # embedding dimension, e.g., 1536 Start Step 1 is the same as in MCP mode: prepare RTL and the corresponding SPEC docs under examples/{dut}. {dut} is the module name; if it is Adder, the directory is examples/Adder. Step 2 differs: package RTL, put the docs into the workspace, and start UCAgent TUI: make test_{dut}, where {dut} is the module. For Adder, run make test_Adder (see all targets in Makefile). This will: Copy files from examples/{dut} to output/{dut} (.v/.sv/.md/.py, etc.) Run python3 ucagent.py output/ {dut} --config config.yaml -s -hm --tui -l Start UCAgent with TUI and automatically enter the loop Tip: verification artifacts are written to output/unity_test/ by default; to change it, use the CLI --output option to set the directory name.\nDirect CLI (without Makefile): Not installed (run inside project): python3 ucagent.py output/ Adder --config config.yaml -s -hm --tui -l Installed as command: ucagent output/ Adder --config config.yaml -s -hm --tui -l Options aligned with vagent/cli.py:\nworkspace: workspace directory (here output/) dut: DUT name (workspace subdirectory name, e.g., Adder) Common options: --tui start terminal UI -l/--loop --loop-msg \"...\" enter loop immediately after start and inject a hint -s/--stream-output stream output -hm/--human human‑intervention mode (can pause between stages) --no-embed-tools if retrieval/memory tools are not needed --skip/--unskip skip/unskip stages (can be passed multiple times) TUI Quick Reference (Direct Mode) List tools: tool_list Stage check: tool_invoke Check timeout=0 View logs: tool_invoke StdCheck lines=-1 (‑1 for all lines) Stop check: tool_invoke KillCheck Finish stage: tool_invoke Complete timeout=0 Run tests: Full: tool_invoke RunTestCases target='' timeout=0 Single test function: tool_invoke RunTestCases target='tests/test_checker.py::test_run' timeout=120 return_line_coverage=True Filter: tool_invoke RunTestCases target='-k add or mul' Jump stage: tool_invoke GoToStage index=2 (index starts from 0) Continue: loop 继续修复 ALU754 的未命中分支并重试用例 Recommended minimal write permission (only allow generation under verification artifacts):\nAllow only unity_test/ and unity_test/tests/ to be writable: add_un_write_path * del_un_write_path unity_test del_un_write_path unity_test/tests FAQ and Tips Check stuck/no output: First run tool_invoke StdCheck lines=-1 to view all logs; if needed tool_invoke KillCheck; fix then retry tool_invoke Check. Tool name not found: Run tool_list to confirm available tools; if missing, check whether in TUI mode and whether embedding tools were disabled (usually unrelated). Artifact location: By default under workspace/output_dir, i.e., output/unity_test/ for the examples on this page. Related Docs Full human‑AI collaboration flow and examples: see Human‑AI Collaboration MCP integration (e.g., gemini‑cli / qwen code): see MCP Integration Mode TUI interface and operations in detail: see TUI ","categories":["Tutorial"],"description":"Direct usage, options, TUI interface, and FAQ.","excerpt":"Direct usage, options, TUI interface, and FAQ.","ref":"/mlvp/en/docs/ucagent/usage/direct/","tags":["docs"],"title":"Direct Mode"},{"body":" This page will briefly introduce what verification is and concepts used in the examples, such as DUT (Design Under Test) and RM (Reference Model).\n","categories":["Sample Projects","Tutorials"],"description":"Detailed usage instructions for the Open Verification Platform environment.","excerpt":"Detailed usage instructions for the Open Verification Platform …","ref":"/mlvp/en/docs/env_usage/","tags":["examples","docs"],"title":"Environment Usage"},{"body":"","categories":["Example Projects","Tutorials"],"description":"Using TileLink Protocol for L2 Cache Driven by C++","excerpt":"Using TileLink Protocol for L2 Cache Driven by C++","ref":"/mlvp/en/docs/advance_case/tilelink/","tags":["examples","docs"],"title":"TileLink Protocol"},{"body":"","categories":["示例项目","教程"],"description":"基于C++驱动使用 TillLink 协议的 L2 Cache","excerpt":"基于C++驱动使用 TillLink 协议的 L2 Cache","ref":"/mlvp/docs/advance_case/tilelink/","tags":["examples","docs"],"title":"TileLink 协议"},{"body":"","categories":["Example Projects","Tutorial"],"description":"Two usage modes, options, TUI, and FAQ.","excerpt":"Two usage modes, options, TUI, and FAQ.","ref":"/mlvp/en/docs/ucagent/usage/","tags":["examples","docs"],"title":"Usage"},{"body":" Introduction to chip verification using the Guoke Cache as an example, covering the basic verification process and report writing.\n","categories":["Sample Projects","Tutorials"],"description":"Introduction to the basic knowledge required for working with the open verification platform.","excerpt":"Introduction to the basic knowledge required for working with the open …","ref":"/mlvp/en/docs/basic/","tags":["examples","docs"],"title":"Verification Basics"},{"body":"Usage When using the Picker tool to encapsulate the DUT, use the -w [wave_file] option to specify the waveform file to be saved. Different waveform file types are supported for different backend simulators, as follows:\nVerilator .vcd format waveform file. .fst format waveform file, a more efficient compressed file. VCS .fsdb format waveform file, a more efficient compressed file. Note that if you choose to generate the libDPI_____.so file yourself, the waveform file format is not restricted by the above constraints. The waveform file format is determined when the simulator constructs libDPI.so, so if you generate it yourself, you need to specify the waveform file format using the corresponding simulator’s configuration.\nPython Example Normally, the DUT needs to be explicitly declared complete to notify the simulator to perform post-processing tasks (writing waveform, coverage files, etc.). In Python, after completing all tests, call the .Finish() method of the DUT to notify the simulator that the task is complete, and then flush the files to disk.\nUsing the Adder Example, the test program is as follows:\nfrom Adder import * if __name__ == \"__main__\": dut = DUTAdder() for i in range(10): dut.a.value = i * 2 dut.b.value = int(i / 4) dut.Step(1) print(dut.sum.value, dut.cout.value) dut.Finish() # flush the wave file to disk After the run is completed, the waveform file with the specified name will be generated.\nViewing Results GTKWave Use GTKWave to open fst or vcd waveform files to view the waveform.\nVerdi Use Verdi to open fsdb or vcd waveform files to view the waveform.\n","categories":["Sample Projects","Tutorials"],"description":"Generate circuit waveforms.","excerpt":"Generate circuit waveforms.","ref":"/mlvp/en/docs/env_usage/wave/","tags":["examples","docs"],"title":"Waveform Generation"},{"body":"使用方法 在使用 Picker 工具封装 DUT 时，使用选项-w [wave_file]指定需要保存的波形文件。 针对不同的后端仿真器，支持不同的波形文件类型，具体如下：\nVerilator .vcd格式的波形文件。 .fst格式的波形文件，更高效的压缩文件。 VCS .fsdb格式的波形文件，更高效的压缩文件。 需要注意的是，如果你选择自行生成 libDPI_____.so 文件，那么波形文件格式不受上述约束的限制。因为波形文件是在仿真器构建 libDPI.so 时决定的，如果你自行生成，那么波形文件格式也需要自行用对应仿真器的配置指定。\nPython 示例 正常情况下，dut需要被显式地声明完成任务，以通知进行模拟器的后处理工作（写入波形、覆盖率等文件）。 在Python中，需要在完成所有测试后，调用dut的.Finish()方法以通知模拟器任务已完成，进而将文件flush到磁盘。\n以加法器为例，以下为测试程序：\nfrom Adder import * if __name__ == \"__main__\": dut = DUTAdder() for i in range(10): dut.a.value = i * 2 dut.b.value = int(i / 4) dut.Step(1) print(dut.sum.value, dut.cout.value) dut.Finish() # flush the wave file to disk 运行结束后即可生成指定文件名的波形文件。\n查看结果 GTKWave 使用 GTKWave 打开 fst 或 vcd 波形文件，即可查看波形图。\nVerdi 使用 Verdi 打开 fsdb 或 vcd 波形文件，即可查看波形图。\n","categories":["示例项目","教程"],"description":"生成电路波形","excerpt":"生成电路波形","ref":"/mlvp/docs/env_usage/wave/","tags":["examples","docs"],"title":"波形生成"},{"body":" 在开始前本页会 简单的介绍什么是验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n","categories":["示例项目","教程"],"description":"开放验证平台的基础工具的详细使用方法。","excerpt":"开放验证平台的基础工具的详细使用方法。","ref":"/mlvp/docs/env_usage/","tags":["examples","docs"],"title":"基础工具"},{"body":"","categories":["示例项目","教程"],"description":"两种使用方式、各个选项参数、TUI界面和FAQ说明。","excerpt":"两种使用方式、各个选项参数、TUI界面和FAQ说明。","ref":"/mlvp/docs/ucagent/usage/","tags":["examples","docs"],"title":"使用"},{"body":" 本页将介绍数字电路的基础知识。数字电路是利用数字信号的电子电路。近年来，绝大多数的计算机都是基于数字电路实现的。\n什么是数字电路 数字电路是一种利用两种不连续的电位来表示信息的电子电路。在数字电路中，通常使用两个电源电压，分别表示高电平（H）和低电平（L），分别代表数字1和0。这样的表示方式通过离散的电信号，以二进制形式传递和处理信息。\n大多数数字电路的实现基于场效应管，其中最常用的是 MOSFET（Metal-Oxide-Semiconductor Field-Effect Transistor，金属氧化物半导体场效应管）。MOSFET 是一种半导体器件，可以在电场的控制下调控电流流动，从而实现数字信号的处理。\n在数字电路中，MOSFET 被组合成各种逻辑电路，如与门、或门、非门等。这些逻辑门通过不同的组合方式，构建了数字电路中的各种功能和操作。以下是一些数字电路的基本特征：\n(1) 电位表示信息： 数字电路使用两种电位，即高电平和低电平，来表示数字信息。通常，高电平代表数字1，低电平代表数字0。\n(2) MOSFET 实现： MOSFET 是数字电路中最常用的元件之一。通过控制 MOSFET 的导通和截止状态，可以实现数字信号的处理和逻辑运算。\n(3) 逻辑门的组合： 逻辑门是数字电路的基本构建块，由 MOSFET 组成。通过组合不同的逻辑门，可以构建复杂的数字电路，实现各种逻辑功能。\n(4) 二进制表达： 数字电路中的信息通常使用二进制系统进行表示。每个数字都可以由一串二进制位组成，这些位可以在数字电路中被处理和操作。\n(5) 电平转换和信号处理： 数字电路通过电平的变化和逻辑操作，实现信号的转换和处理。这种离散的处理方式使得数字电路非常适用于计算和信息处理任务。\n为什么要学习数字电路 学习数字电路是芯片验证过程中的基础和必要前提，主要体现在以下多个方面：\n(1) 理解设计原理： 数字电路是芯片设计的基础，了解数字电路的基本原理和设计方法是理解芯片结构和功能的关键。芯片验证的目的是确保设计的数字电路在实际硬件中按照规格正常工作，而理解数字电路原理是理解设计的关键。\n(2) 设计规范： 芯片验证通常涉及验证设计是否符合特定的规范和功能要求。学习数字电路可以帮助理解这些规范，从而更好地构建测试用例和验证流程，确保验证的全面性和准确性。\n(3) 时序和时钟： 时序问题是数字电路设计和验证中的常见挑战。学习数字电路可以帮助理解时序和时钟的概念，以确保验证过程中能够正确处理时序问题，避免电路中的时序迟滞和冲突。\n(4) 逻辑分析： 芯片验证通常涉及对逻辑的分析，确保电路的逻辑正确性。学习数字电路可以培养对逻辑的深刻理解，从而更好地进行逻辑分析和故障排查。\n(5) 测试用例编写： 在芯片验证中，需要编写各种测试用例来确保设计的正确性。对数字电路的理解可以帮助设计更全面、有针对性的测试用例，涵盖电路的各个方面。\n(6) 信号完整性： 学习数字电路有助于理解信号在电路中的传播和完整性问题。在芯片验证中，确保信号在不同条件下的正常传递是至关重要的，特别是在高速设计中。\n整体而言，学习数字电路为芯片验证提供了基础知识和工具，使验证工程师能够更好地理解设计，编写有效的测试用例，分析验证结果，并解决可能出现的问题。数字电路的理论和实践经验对于芯片验证工程师来说都是不可或缺的。\n数字电路基础知识 可以通过以下在线资源进行数字电路学习：\n清华大学数字电路基础 中科大数字电路实验 数字设计和计算机体系结构 MIT 数字集成电路分析与设计 硬件描述语言Chisel 传统描述语言 硬件描述语言（Hardware Description Language，简称 HDL）是一种用于描述数字电路、系统和硬件的语言。它允许工程师通过编写文本文件来描述硬件的结构、功能和行为，从而实现对硬件设计的抽象和建模。\nHDL 通常被用于设计和仿真数字电路，如处理器、存储器、控制器等。它提供了一种形式化的方法来描述硬件电路的行为和结构，使得设计工程师可以更方便地进行硬件设计、验证和仿真。\n常见的硬件描述语言包括：\nVerilog：Verilog 是最常用的 HDL 之一，它是一种基于事件驱动的硬件描述语言，广泛应用于数字电路设计、验证和仿真。 VHDL：VHDL 是另一种常用的 HDL，它是一种面向对象的硬件描述语言，提供了更丰富的抽象和模块化的设计方法。 SystemVerilog：SystemVerilog 是 Verilog 的扩展，它引入了一些高级特性，如对象导向编程、随机化测试等，使得 Verilog 更适用于复杂系统的设计和验证。 Chisel Chisel 是一种现代化高级的硬件描述语言，与传统的 Verilog 和 VHDL 不同，它是基于 Scala 编程语言的硬件构建语言。Chisel 提供了一种更加现代化和灵活的方法来描述硬件，通过利用 Scala 的特性，可以轻松地实现参数化、抽象化和复用，同时保持硬件级别的效率和性能。\nChisel 的特点包括：\n现代化的语法：Chisel 的语法更加接近软件编程语言，如 Scala，使得硬件描述更加直观和简洁。 参数化和抽象化：Chisel 支持参数化和抽象化，可以轻松地创建可配置和可重用的硬件模块。 类型安全：Chisel 是基于 Scala 的，因此具有类型安全的特性，可以在编译时检测到许多错误。 生成性能优化的硬件：Chisel 代码可以被转换成 Verilog，然后由标准的 EDA 工具链进行综合、布局布线和仿真，生成性能优化的硬件。 强大的仿真支持：Chisel 提供了与 ScalaTest 和 Firrtl 集成的仿真支持，使得对硬件进行仿真和验证更加方便和灵活。 Chisel版的全加法器实例 电路设计如下图所示：\n完整的Chisel代码如下：\npackage examples import chisel3._ class FullAdder extends Module { // Define IO ports val io = IO(new Bundle { val a = Input(UInt(1.W)) // Input port 'a' of width 1 bit val b = Input(UInt(1.W)) // Input port 'b' of width 1 bit val cin = Input(UInt(1.W)) // Input port 'cin' (carry-in) of width 1 bit val sum = Output(UInt(1.W)) // Output port 'sum' of width 1 bit val cout = Output(UInt(1.W))// Output port 'cout' (carry-out) of width 1 bit }) // Calculate sum bit (sum of a, b, and cin) val s1 = io.a ^ io.b // XOR operation between 'a' and 'b' io.sum := s1 ^ io.cin // XOR operation between 's1' and 'cin', result assigned to 'sum' // Calculate carry-out bit val s3 = io.a \u0026 io.b // AND operation between 'a' and 'b', result assigned to 's3' val s2 = s1 \u0026 io.cin // AND operation between 's1' and 'cin', result assigned to 's2' io.cout := s2 | s3 // OR operation between 's2' and 's3', result assigned to 'cout' } Chisel 学习材料可以参考官方文档：https://www.chisel-lang.org/docs\n","categories":"","description":"关于数字电路的基本概念\n","excerpt":"关于数字电路的基本概念\n","ref":"/mlvp/docs/basic/ic_base/","tags":"","title":"数字电路"},{"body":" 本页将展示使用多种语言验证的各种案例。\n","categories":["教程"],"description":"多语言案例介绍","excerpt":"多语言案例介绍","ref":"/mlvp/docs/multi-lang/examples/","tags":["docs"],"title":"验证案例"},{"body":" 介绍芯片验证，以果壳 Cache 为例，介绍基本的验证流程、报告撰写。\n","categories":["示例项目","教程"],"description":"介绍开放验证平台工作所需要的基础知识。","excerpt":"介绍开放验证平台工作所需要的基础知识。","ref":"/mlvp/docs/basic/","tags":["examples","docs"],"title":"验证基础"},{"body":"直接使用 基于本地 CLI 和大模型的使用方式。需要准备好 OpenAI 兼容的 API 和嵌入模型 API。\n使用环境变量配置（推荐） 配置文件内容：\n# OpenAI兼容的API配置 openai: model_name: \"$(OPENAI_MODEL: Qwen/Qwen3-Coder-30B-A3B-Instruct)\" # 模型名称 openai_api_key: \"$(OPENAI_API_KEY: YOUR_API_KEY)\" # API密钥 openai_api_base: \"$(OPENAI_API_BASE: http://10.156.154.242:8000/v1)\" # API基础URL # 向量嵌入模型配置 # 用于文档搜索和记忆功能，不需要可通过 --no-embed-tools 关闭 embed: model_name: \"$(EMBED_MODEL: Qwen/Qwen3-Embedding-0.6B)\" # 嵌入模型名称 openai_api_key: \"$(EMBED_OPENAI_API_KEY: YOUR_API_KEY)\" # 嵌入模型API密钥 openai_api_base: \"$(EMBED_OPENAI_API_BASE: http://10.156.154.242:8001/v1)\" # 嵌入模型API URL dims: 4096 # 嵌入维度 UCAgent 的配置文件支持 Bash 风格的环境变量占位：$(VAR: default)。加载时会用当前环境变量 VAR 的值替换；若未设置，则使用 default。\n例如在内置配置 vagent/setting.yaml 中： openai.model_name: \"$(OPENAI_MODEL: \u003cyour_chat_model_name\u003e)\" openai.openai_api_key: \"$(OPENAI_API_KEY: [your_api_key])\" openai.openai_api_base: \"$(OPENAI_API_BASE: http://\u003cyour_chat_model_url\u003e/v1)\" embed.model_name: \"$(EMBED_MODEL: \u003cyour_embedding_model_name\u003e)\" 也支持其他提供商：model_type 可选 openai、anthropic、google_genai（详见 vagent/setting.yaml）。 你可以仅通过导出环境变量完成模型与端点切换，而无需改动配置文件。\n示例：设置聊天模型与端点\n# 指定聊天模型（OpenAI 兼容） export OPENAI_MODEL='Qwen/Qwen3-Coder-30B-A3B-Instruct' # 指定 API Key 与 Base（按你的服务商填写） export OPENAI_API_KEY='你的API密钥' export OPENAI_API_BASE='https://你的-openai-兼容端点/v1' # 可选：嵌入模型（若使用检索/记忆等功能） export EMBED_MODEL='text-embedding-3-large' export EMBED_OPENAI_API_KEY=\"$OPENAI_API_KEY\" export EMBED_OPENAI_API_BASE=\"$OPENAI_API_BASE\" 然后按前述命令启动 UCAgent 即可。若要长期生效，可将上述 export 追加到你的默认 shell 启动文件（例如 bash: ~/.bashrc，zsh: ~/.zshrc，fish: ~/.config/fish/config.fish），保存后重新打开终端或手动加载。\n使用 config.yaml 来配置 在项目根目录创建并编辑 config.yaml 文件，配置 AI 模型和嵌入模型： # OpenAI兼容的API配置 openai: openai_api_base: \u003cyour_openai_api_base_url\u003e # API基础URL model_name: \u003cyour_model_name\u003e # 模型名称，如 gpt-4o-mini openai_api_key: \u003cyour_openai_api_key\u003e # API密钥 # 向量嵌入模型配置 # 用于文档搜索和记忆功能，不需要可通过 --no-embed-tools 关闭 embed: model_name: \u003cyour_embed_model_name\u003e # 嵌入模型名称 openai_api_base: \u003cyour_openai_api_base_url\u003e # 嵌入模型API URL openai_api_key: \u003cyour_api_key\u003e # 嵌入模型API密钥 dims: \u003cyour_embed_model_dims\u003e # 嵌入维度，如 1536 开始使用 第一步和 MCP 模式相同，准备 RTL 和对应的 SPEC 文档放入examples/{dut}文件夹。{dut}是模块的名称，如果是Adder，目录则为examples/Adder。 第二步开始就不同了，打包 RTL，将文档放入工作目录并启动 UCAgent TUI：make test_{dut}，{dut}为对应的模块。若使用 Adder，命令为 make test_Adder（可在 Makefile 查看全部目标）。该命令会： 将 examples/{dut} 下文件拷贝到 output/{dut}（含 .v/.sv/.md/.py 等） 执行 python3 ucagent.py output/ {dut} --config config.yaml -s -hm --tui -l 启动带 TUI 的 UCAgent，并自动进入任务循环（loop） 提示：验证产物默认写入 output/unity_test/，若需更改可通过 CLI 的 --output 参数指定目录名。\n直接用 CLI 启动（不经 Makefile） 未安装命令时（项目内运行）： python3 ucagent.py output/ Adder --config config.yaml -s -hm --tui -l 安装为命令后： ucagent output/ Adder --config config.yaml -s -hm --tui -l 参数对齐 vagent/cli.py：\nworkspace：工作区目录（此处为 output/） dut：DUT 名称（工作区子目录名，如 Adder） 常用可选项： --tui 启动终端界面 -l/--loop --loop-msg \"...\" 启动后立即进入循环并注入提示 -s/--stream-output 实时输出 -hm/--human 进入人工可干预模式（在阶段间可暂停） --no-embed-tools 如不需要检索/记忆工具 --skip/--unskip 跳过/取消跳过阶段（可多次传入） 常用 TUI 命令速查（直接使用模式） 列出工具：tool_list 阶段检查：tool_invoke Check timeout=0 查看日志：tool_invoke StdCheck lines=-1（-1 表示所有行） 终止检查：tool_invoke KillCheck 阶段完成：tool_invoke Complete timeout=0 运行用例： 全量：tool_invoke RunTestCases target='' timeout=0 单测函数：tool_invoke RunTestCases target='tests/test_checker.py::test_run' timeout=120 return_line_coverage=True 过滤：tool_invoke RunTestCases target='-k add or mul' 阶段跳转：tool_invoke GoToStage index=2（索引从 0 开始） 继续执行：loop 继续修复 ALU754 的未命中分支并重试用例 建议的最小可写权限（只允许生成验证产物处可写）：\n仅允许 unity_test/ 与 unity_test/tests/ 可写： add_un_write_path * del_un_write_path unity_test del_un_write_path unity_test/tests 常见问题与提示 检查卡住/无输出： 先 tool_invoke StdCheck lines=-1 查看全部日志；必要时 tool_invoke KillCheck；修复后重试 tool_invoke Check。 没找到工具名： 先执行 tool_list 确认可用工具；若缺失，检查是否在 TUI 模式、是否禁用了嵌入工具（通常无关）。 产物位置： 默认在 workspace/output_dir，即本页示例为 output/unity_test/。 相关文档 人机协同的完整流程与示例，见人机协同验证 MCP 集成（如 gemini-cli / qwen code），见MCP集成模式 TUI 界面与操作详解，见TUI ","categories":["教程"],"description":"直接使用方式、各个选项参数、TUI界面和FAQ说明。","excerpt":"直接使用方式、各个选项参数、TUI界面和FAQ说明。","ref":"/mlvp/docs/ucagent/usage/direct/","tags":["docs"],"title":"直接使用模式"},{"body":"RTL Source Code In this case, we drive a 64-bit adder (combinational circuit) with the following source code:\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-1:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule This adder contains a 64-bit adder with inputs of two 64-bit numbers and a carry-in signal, outputting a 64-bit sum and a carry-out signal.\nTesting Process During the testing process, we will create a folder named Adder, containing a file called Adder.v. This file contains the above RTL source code.\nExporting RTL to Python Module Generating Intermediate Files Navigate to the Adder folder and execute the following command:\npicker export --autobuild=false Adder.v -w Adder.fst --sname Adder --tdir picker_out_adder --lang python -e --sim verilator This command performs the following actions:\nUses Adder.v as the top file, with Adder as the top module, and generates a dynamic library using the Verilator simulator with Python as the target language.\nEnables waveform output, with the target waveform file as Adder.fst.\nIncludes files for driving the example project (-e), and does not automatically compile after code generation (-autobuild=false).\nThe final file output path is picker_out_adder.\nSome command-line parameters were not used in this command, and they will be introduced in later sections. The output directory structure is as follows. Note that these are all intermediate files and cannot be used directly:\npicker_out_adder |-- Adder.v # Original RTL source code |-- Adder_top.sv # Generated Adder_top top-level wrapper, using DPI to drive Adder module inputs and outputs |-- Adder_top.v # Generated Adder_top top-level wrapper in Verilog, needed because Verdi does not support importing SV source code |-- CMakeLists.txt # For invoking the simulator to compile the basic C++ class and package it into a bare DPI function binary dynamic library (libDPIAdder.so) |-- Makefile # Generated Makefile for invoking CMakeLists.txt, allowing users to compile libAdder.so through the make command, with manual adjustment of Makefile configuration parameters, or to compile the example project |-- cmake # Generated cmake folder for invoking different simulators to compile RTL code | |-- vcs.cmake | `-- verilator.cmake |-- cpp # CPP example directory containing sample code | |-- CMakeLists.txt # For wrapping libDPIAdder.so using basic data types into a directly operable class (libUTAdder.so), not just bare DPI functions | |-- Makefile | |-- cmake | | |-- vcs.cmake | | `-- verilator.cmake | |-- dut.cpp # Generated CPP UT wrapper, including calls to libDPIAdder.so, and UTAdder class declaration and implementation | |-- dut.hpp # Header file | `-- example.cpp # Sample code calling UTAdder class |-- dut_base.cpp # Base class for invoking and driving simulation results from different simulators, encapsulated into a unified class to hide all simulator-related code details |-- dut_base.hpp |-- filelist.f # Additional file list for multi-file projects, check the -f parameter introduction. Empty in this case |-- mk | |-- cpp.mk # Controls Makefile when targeting C++ language, including logic for compiling example projects (-e, example) | `-- python.mk # Same as above, but with Python as the target language `-- python |-- CMakeLists.txt |-- Makefile |-- cmake | |-- vcs.cmake | `-- verilator.cmake |-- dut.i # SWIG configuration file for exporting libDPIAdder.so’s base class and function declarations to Python, enabling Python calls `-- dut.py # Generated Python UT wrapper, including calls to libDPIAdder.so, and UTAdder class declaration and implementation, equivalent to libUTAdder.so Building Intermediate Files Navigate to the picker_out_adder directory and execute the make command to generate the final files.\nUse the simulator invocation script defined by cmake/*.cmake to compile Adder_top.sv and related files into the libDPIAdder.so dynamic library.Use the compilation script defined by CMakeLists.txt to wrap libDPIAdder.so into the libUTAdder.so dynamic library through dut_base.cpp. Both outputs from steps 1 and 2 are copied to the UT_Adder directory.Generate the wrapper layer using the SWIG tool with dut_base.hpp and dut.hpp header files, and finally build a Python module in the UT_Adder directory.If the -e parameter is included, the pre-defined example.py is placed in the parent directory of the UT_Adder directory as a sample code for calling this Python module. The final directory structure is:\n. |-- Adder.fst # Waveform file for testing |-- UT_Adder | |-- Adder.fst.hier | |-- _UT_Adder.so # Wrapper dynamic library generated by SWIG | |-- __init__.py # Python module initialization file, also the library definition file | |-- libDPIAdder.a # Library file generated by the simulator | |-- libUTAdder.so # DPI dynamic library wrapper generated based on dut_base | `-- libUT_Adder.py # Python module generated by SWIG | `-- xspcomm # Base library folder, no need to pay attention to this `-- example.py # Sample code Setting Up Test Code Replace the content in example.py with the following Python test code.\nfrom Adder import * import random # Generate unsigned random numbers def random_int(): return random.randint(-(2**63), 2**63 - 1) \u0026 ((1 \u003c\u003c 63) - 1) # Reference model for the adder implemented in Python def reference_adder(a, b, cin): sum = (a + b) \u0026 ((1 \u003c\u003c 64) - 1) carry = sum \u003c a sum += cin carry = carry or sum \u003c cin return sum, 1 if carry else 0 def random_test(): # Create DUT dut = DUTAdder() # By default, pin assignments do not write immediately but write on the next clock rising edge, which is suitable for sequential circuits. However, since the Adder is a combinational circuit, we need to write immediately # Therefore, the AsImmWrite() method is called to change pin assignment behavior dut.a.AsImmWrite() dut.b.AsImmWrite() dut.cin.AsImmWrite() # Loop test for i in range 114514): a, b, cin = random_int(), random_int(), random_int() \u0026 1 # DUT: Assign values to Adder circuit pins, then drive the combinational circuit (for sequential circuits or waveform viewing, use dut.Step() to drive) dut.a.value, dut.b.value, dut.cin.value = a, b, cin dut.RefreshComb() # Reference model: Calculate results ref_sum, ref_cout = reference_adder(a, b, cin) # Check results assert dut.sum.value == ref_sum, \"sum mismatch: 0x{dut.sum.value:x} != 0x{ref_sum:x}\" assert dut.cout.value == ref_cout, \"cout mismatch: 0x{dut.cout.value:x} != 0x{ref_cout:x}\" print(f\"[test {i}] a=0x{a:x}, b=0x{b:x}, cin=0x{cin:x} =\u003e sum: 0x{ref_sum}, cout: 0x{ref_cout}\") # Test complete dut.Finish() print(\"Test Passed\") if __name__ == \"__main__\": random_test() Running the Test In the picker_out_adder directory, execute the python3 example.py command to run the test. After the test is complete, we can see the output of the example project.\n[...] [test 114507] a=0x7adc43f36682cffe, b=0x30a718d8cf3cc3b1, cin=0x0 =\u003e sum: 0x12358823834579604399, cout: 0x0 [test 114508] a=0x3eb778d6097e3a72, b=0x1ce6af17b4e9128, cin=0x0 =\u003e sum: 0x4649372636395916186, cout: 0x0 [test 114509] a=0x42d6f3290b18d4e9, b=0x23e4926ef419b4aa, cin=0x1 =\u003e sum: 0x7402657300381600148, cout: 0x0 [test 114510] a=0x505046adecabcc, b=0x6d1d4998ed457b06, cin=0x0 =\u003e sum: 0x7885127708256118482, cout: 0x0 [test 114511] a=0x16bb10f22bd0af50, b=0x5813373e1759387, cin=0x1 =\u003e sum: 0x2034576336764682968, cout: 0x0 [test 114512] a=0xc46c9f4aa798106, b=0x4d8f52637f0417c4, cin=0x0 =\u003e sum: 0x6473392679370463434, cout: 0x0 [test 114513] a=0x3b5387ba95a7ac39, b=0x1a378f2d11b38412, cin=0x0 =\u003e sum: 0x6164045699187683403, cout: 0x0 Test Passed ","categories":["Example Projects","Tutorials"],"description":"Demonstrates the principles and usage of the tool based on a simple adder verification. This adder is implemented using simple combinational logic.","excerpt":"Demonstrates the principles and usage of the tool based on a simple …","ref":"/mlvp/en/docs/quick-start/eg-adder/","tags":["examples","docs"],"title":"Case 1: Adder"},{"body":"RTL源码 在本案例中，我们驱动一个 64 位的加法器（组合电路），其源码如下：\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-1:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule 该加法器包含一个 64 位的加法器，其输入为两个 64 位的数和一个进位信号，输出为一个 64 位的和和一个进位信号。\n测试过程 在测试过程中，我们将创建一个名为 Adder 的文件夹，其中包含一个 Adder.v 文件。该文件内容即为上述的 RTL 源码。\n将RTL导出为 Python Module 生成中间文件 进入 Adder 文件夹，执行如下命令：\npicker export --autobuild=false Adder.v -w Adder.fst --sname Adder --tdir picker_out_adder/ --lang python -e --sim verilator *注：–tdir 指定的是目标构建目录，如果该参数值为空或者以“/”结尾，picker则会自动以DUT的目标模块名创建构建目录。例如 --tdir picker_out_adder指定了当前目录下的picker_out_adder为构建目录，而参数--tdir picker_out_adder/则指定picker在当前目录picker_out_adder中创建Adder目录作为目标构建目录。\n该命令的含义是：\n将 Adder.v 作为 Top 文件，并将 Adder 作为 Top Module，基于 verilator 仿真器生成动态库，生成目标语言为 Python。 启用波形输出，目标波形文件为Adder.fst。 包含用于驱动示例项目的文件(-e)，同时codegen完成后不自动编译(-autobuild=false)。 最终的文件输出路径是 picker_out_adder 在使用该命令时，还有部分命令行参数没有使用，这些命令将在后续的章节中介绍。\n输出的目录结构如下，请注意这部分均为中间文件，不能直接使用：\npicker_out_adder/ └── Adder |-- Adder.v # 原始的RTL源码 |-- Adder_top.sv # 生成的Adder_top顶层封装，使用DPI驱动Adder模块的inputs和outputs |-- Adder_top.v # 生成的Adder_top顶层封装，因为Verdi不支持导入SV源码使用，因此需要生成一个Verilog版本 |-- CMakeLists.txt # 用于调用仿真器编译基本的cpp class并将其打包成有裸DPI函数二进制动态库(libDPIAdder.so) |-- Makefile # 生成的Makefile，用于调用CMakeLists.txt，并让用户可以通过make命令编译出libAdder.so，并手动调整Makefile的配置参数。或者编译示例项目 |-- cmake # 生成的cmake文件夹，用于调用不同仿真器编译RTL代码 | |-- vcs.cmake | `-- verilator.cmake |-- cpp # CPP example目录，包含示例代码 | |-- CMakeLists.txt # 用于将libDPIAdder.so使用基础数据类型封装为一个可直接操作的类（libUTAdder.so），而非裸DPI函数。 | |-- Makefile | |-- cmake | | |-- vcs.cmake | | `-- verilator.cmake | |-- dut.cpp # 生成的cpp UT封装，包含了对libDPIAdder.so的调用，及UTAdder类的声明及实现 | |-- dut.hpp # 头文件 | `-- example.cpp # 调用UTAdder类的示例代码 |-- dut_base.cpp # 用于调用与驱动不同仿真器编译结果的基类，通过继承封装为统一的类，用于隐藏所有仿真器相关的代码细节。 |-- dut_base.hpp |-- filelist.f # 多文件项目使用的其他文件列表，请查看 -f 参数的介绍。本案例中为空 |-- mk | |-- cpp.mk # 用于控制以cpp为目标语言时的Makefile，包含控制编译示例项目（-e，example）的逻辑 | `-- python.mk # 同上，目标语言是python `-- python |-- CMakeLists.txt |-- Makefile |-- cmake | |-- vcs.cmake | `-- verilator.cmake |-- dut.i # SWIG配置文件，用于将libDPIAdder.so的基类与函数声明，依据规则用swig导出到python，提供python调用的能力 `-- dut.py # 生成的python UT封装，包含了对libDPIAdder.so的调用，及UTAdder类的声明及实现，等价于 libUTAdder.so 构建中间文件 进入 picker_out_adder/Adder 目录并执行 make 命令，即可生成最终的文件。\n由 Makefile 定义的自动编译过程流如下：\n通过 cmake/*.cmake 定义的仿真器调用脚本，编译 Adder_top.sv 及相关文件为 libDPIAdder.so 动态库。 通过 CMakelists.txt 定义的编译脚本，将 libDPIAdder.so 通过 dut_base.cpp 封装为 libUTAdder.so 动态库。并将1、2步产物拷贝到 UT_Adder 目录下。 通过 dut_base.hpp 及 dut.hpp 等头文件，利用 SWIG 工具生成封装层，并最终在 UT_Adder 这一目录中构建一个 Python Module。 如果有 -e 参数，则将预先定义好的 example.py 置于 UT_Adder 目录的上级目录，作为如何调用该 Python Module 的示例代码。 最终目录结果为：\npicker_out_adder/ └── Adder |-- _UT_Adder.so # Swig生成的wrapper动态库 |-- __init__.py # Python Module的初始化文件，也是库的定义文件 |-- libDPIAdder.a # 仿真器生成的库文件 |-- libUTAdder.so # 基于dut_base生成的libDPI动态库封装 |-- libUT_Adder.py # Swig生成的Python Module `-- xspcomm # xspcomm基础库，固定文件夹，不需要关注 配置测试代码 在picker_out_adder中添加 example.py：\nfrom Adder import * import random # 生成无符号随机数 def random_int(): return random.randint(-(2**63), 2**63 - 1) \u0026 ((1 \u003c\u003c 63) - 1) # 通过python实现的加法器参考模型 def reference_adder(a, b, cin): sum = (a + b) \u0026 ((1 \u003c\u003c 64) - 1) carry = sum \u003c a sum += cin carry = carry or sum \u003c cin return sum, 1 if carry else 0 def random_test(): # 创建DUT dut = DUTAdder() # 默认情况下，引脚赋值不会立马写入，而是在下一次时钟上升沿写入，这对于时序电路适用，但是Adder为组合电路，所以需要立即写入 # 因此需要调用AsImmWrite()方法更改引脚赋值行为 dut.a.AsImmWrite() dut.b.AsImmWrite() dut.cin.AsImmWrite() # 循环测试 for i in range(114514): a, b, cin = random_int(), random_int(), random_int() \u0026 1 # DUT：对Adder电路引脚赋值，然后驱动组合电路 （对于时序电路，或者需要查看波形，可通过dut.Step()进行驱动） dut.a.value, dut.b.value, dut.cin.value = a, b, cin dut.RefreshComb() # 参考模型：计算结果 ref_sum, ref_cout = reference_adder(a, b, cin) # 检查结果 assert dut.sum.value == ref_sum, \"sum mismatch: 0x{dut.sum.value:x} != 0x{ref_sum:x}\" assert dut.cout.value == ref_cout, \"cout mismatch: 0x{dut.cout.value:x} != 0x{ref_cout:x}\" print(f\"[test {i}] a=0x{a:x}, b=0x{b:x}, cin=0x{cin:x} =\u003e sum: 0x{ref_sum}, cout: 0x{ref_cout}\") # 完成测试 dut.Finish() print(\"Test Passed\") if __name__ == \"__main__\": random_test() 运行测试 在 picker_out_adder 目录下执行 python3 example.py 命令，即可运行测试。在测试完成后我们即可看到 example 示例项目的输出。\n[...] [test 114507] a=0x7adc43f36682cffe, b=0x30a718d8cf3cc3b1, cin=0x0 =\u003e sum: 0x12358823834579604399, cout: 0x0 [test 114508] a=0x3eb778d6097e3a72, b=0x1ce6af17b4e9128, cin=0x0 =\u003e sum: 0x4649372636395916186, cout: 0x0 [test 114509] a=0x42d6f3290b18d4e9, b=0x23e4926ef419b4aa, cin=0x1 =\u003e sum: 0x7402657300381600148, cout: 0x0 [test 114510] a=0x505046adecabcc, b=0x6d1d4998ed457b06, cin=0x0 =\u003e sum: 0x7885127708256118482, cout: 0x0 [test 114511] a=0x16bb10f22bd0af50, b=0x5813373e1759387, cin=0x1 =\u003e sum: 0x2034576336764682968, cout: 0x0 [test 114512] a=0xc46c9f4aa798106, b=0x4d8f52637f0417c4, cin=0x0 =\u003e sum: 0x6473392679370463434, cout: 0x0 [test 114513] a=0x3b5387ba95a7ac39, b=0x1a378f2d11b38412, cin=0x0 =\u003e sum: 0x6164045699187683403, cout: 0x0 Test Passed ","categories":["示例项目","教程"],"description":"基于一个简单的加法器验证展示工具的原理和使用方法，这个加法器内部是简单的组合逻辑。","excerpt":"基于一个简单的加法器验证展示工具的原理和使用方法，这个加法器内部是简单的组合逻辑。","ref":"/mlvp/docs/quick-start/eg-adder/","tags":["examples","docs"],"title":"案例一：简单加法器"},{"body":"CoupledL2是一个非阻塞的L2 Cache。\n下面的代码会对CoupledL2进行简单的验证，并使用数组作为参考模型，验证过程如下：\n生成随机的地址addr、执行AcquireBlock，请求读取addr的数据。 执行GrantData，接收DUT响应的数据。 把接收到的数据和参考模型的内容进行比较，验证行为是否一致。 执行GrantAck，响应DUT。 执行ReleaseData，向DUT请求在addr写入随机数据data。 同步参考模型，把addr的数据更新为data。 执行ReleaseAck，接收DUT的写入响应。 上述步骤会重复4000次。\n验证代码：\nCpp Java Python #include \"UT_CoupledL2.hpp\" using TLDataArray = std::array; enum class OpcodeA : uint32_t { PutFullData = 0x0, PutPartialData = 0x1, ArithmeticData = 0x2, LogicalData = 0x3, Get = 0x4, Hint = 0x5, AcquireBlock = 0x6, AcquirePerm = 0x7, }; enum class OpcodeB : uint32_t { ProbeBlock = 0x6, ProbePerm = 0x7 }; enum class OpcodeC : uint32_t { ProbeAck = 0x4, ProbeAckData = 0x5, Release = 0x6, ReleaseData = 0x7 }; enum class OpcodeD : uint32_t { AccessAck, AccessAckData, HintAck, Grant = 0x4, GrantData = 0x5, ReleaseAck = 0x6 }; enum class OpcodeE : uint32_t { GrantAck = 0x4 }; constexpr std::initializer_list ARGS = {\"+verilator+rand+reset+0\"}; auto dut = UTCoupledL2(ARGS); auto \u0026clk = dut.xclock; void sendA(OpcodeA opcode, uint32_t size, uint32_t address) { const auto \u0026valid = dut.master_port_0_0_a_valid; const auto \u0026ready = dut.master_port_0_0_a_ready; while (ready.value == 0x0) clk.Step(); valid.value = 1; dut.master_port_0_0_a_bits_opcode.value = opcode; dut.master_port_0_0_a_bits_size.value = size; dut.master_port_0_0_a_bits_address.value = address; clk.Step(); valid.value = 0; } void getB() { assert(false); const auto \u0026valid = dut.master_port_0_0_b_valid; const auto \u0026ready = dut.master_port_0_0_b_ready; ready.value = 1; while (valid.value == 0) clk.Step(); dut.master_port_0_0_b_bits_opcode = 0x0; dut.master_port_0_0_b_bits_param = 0x0; dut.master_port_0_0_b_bits_size = 0x0; dut.master_port_0_0_b_bits_source = 0x0; dut.master_port_0_0_b_bits_address = 0x0; dut.master_port_0_0_b_bits_mask = 0x0; dut.master_port_0_0_b_bits_data = 0x0; dut.master_port_0_0_b_bits_corrupt = 0x0; clk.Step(); ready.value = 0; } void sendC(OpcodeC opcode, uint32_t size, uint32_t address, uint64_t data) { const auto \u0026valid = dut.master_port_0_0_c_valid; const auto \u0026ready = dut.master_port_0_0_c_ready; while (ready.value == 0) clk.Step(); valid.value = 1; dut.master_port_0_0_c_bits_opcode.value = opcode; dut.master_port_0_0_c_bits_size.value = size; dut.master_port_0_0_c_bits_address.value = address; dut.master_port_0_0_c_bits_data.value = data; clk.Step(); valid.value = 0; } void getD() { const auto \u0026valid = dut.master_port_0_0_d_valid; const auto \u0026ready = dut.master_port_0_0_d_ready; ready.value = 1; clk.Step(); while (valid.value == 0) clk.Step(); ready.value = 0; } void sendE(uint32_t sink) { const auto \u0026valid = dut.master_port_0_0_e_valid; const auto \u0026ready = dut.master_port_0_0_e_ready; while (ready.value == 0) clk.Step(); valid.value = 1; dut.master_port_0_0_e_bits_sink.value = sink; clk.Step(); valid.value = 0; } void AcquireBlock(uint32_t address) { sendA(OpcodeA::AcquireBlock, 0x6, address); } void GrantData(TLDataArray \u0026r_data) { const auto \u0026opcode = dut.master_port_0_0_d_bits_opcode; const auto \u0026data = dut.master_port_0_0_d_bits_data; for (int i = 0; i \u003c 2; i++) { do { getD(); } while (opcode.value != OpcodeD::GrantData); r_data[i] = data.value; } } void GrantAck(uint32_t sink) { sendE(sink); } void ReleaseData(uint32_t address, const TLDataArray \u0026data) { for (int i = 0; i \u003c 2; i++) sendC(OpcodeC::ReleaseData, 0x6, address, data[i]); } void ReleaseAck() { const auto \u0026opcode = dut.master_port_0_0_d_bits_opcode; do { getD(); } while (opcode.value != OpcodeD::ReleaseAck); } int main() { TLDataArray ref_data[16] = {}; /* Random generator */ std::random_device rd; std::mt19937_64 gen_rand(rd()); std::uniform_int_distribution distrib(0, 0xf - 1); /* DUT init */ dut.InitClock(\"clock\"); dut.reset.SetWriteMode(xspcomm::WriteMode::Imme); dut.reset.value = 1; clk.Step(); dut.reset.value = 0; for (int i = 0; i \u003c 100; i++) clk.Step(); /* Test loop */ for (int test_loop = 0; test_loop \u003c 4000; test_loop++) { uint32_t d_sink; TLDataArray data{}, r_data{}; /* Generate random */ const auto address = distrib(gen_rand) \u003c\u003c 6; for (auto \u0026i : data) i = gen_rand(); printf(\"[CoupledL2 Test\\t%d]: At address(0x%03x), \", test_loop + 1, address); /* Read */ AcquireBlock(address); GrantData(r_data); // Print read result printf(\"Read: \"); for (const auto \u0026x : r_data) printf(\"%08lx\", x); d_sink = dut.master_port_0_0_d_bits_sink.value; assert ((r_data == ref_data[address \u003e\u003e 6]) \u0026\u0026 \"Read Failed\"); GrantAck(d_sink); /* Write */ ReleaseData(address, data); ref_data[address \u003e\u003e 6] = data; ReleaseAck(); // Print write data printf(\", Write: \"); for (const auto \u0026x : data) printf(\"%08lx\", x); printf(\".\\n\"); } return 0; } import com.ut.UT_CoupledL2; import com.xspcomm.WriteMode; import java.io.BufferedWriter; import java.io.IOException; import java.io.OutputStreamWriter; import java.io.PrintWriter; import java.math.BigInteger; import java.util.Arrays; import java.util.Random; import java.util.random.RandomGenerator; class Opcode { public enum A { PutFullData(0x0), PutPartialData(0x1), ArithmeticData(0x2), LogicalData(0x3), Get(0x4), Hint(0x5), AcquireBlock(0x6), AcquirePerm(0x7); private final int value; A(int value) { this.value = value; } public int getValue() { return value; } } public enum B { ProbeBlock(0x6), ProbePerm(0x7); private final int value; B(int value) { this.value = value; } public int getValue() { return value; } } public enum C { ProbeAck(0x4), ProbeAckData(0x5), Release(0x6), ReleaseData(0x7); private final int value; C(int value) { this.value = value; } public int getValue() { return value; } } public enum D { AccessAck(0x0), AccessAckData(0x1), HintAck(0x2), Grant(0x4), GrantData(0x5), ReleaseAck(0x6); private final int value; D(int value) { this.value = value; } public int getValue() { return value; } } public enum E { GrantAck(0x4); private final int value; E(int value) { this.value = value; } public int getValue() { return value; } } } public class TestCoupledL2 { static PrintWriter pwOut = new PrintWriter(new BufferedWriter(new OutputStreamWriter(System.out))); static UT_CoupledL2 dut; static void sendA(int opcode, int size, int address) { var valid = dut.master_port_0_0_a_valid; var ready = dut.master_port_0_0_a_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_a_bits_opcode.Set(opcode); dut.master_port_0_0_a_bits_size.Set(size); dut.master_port_0_0_a_bits_address.Set(address); dut.xclock.Step(); valid.Set(0); } static void getB() { var valid = dut.master_port_0_0_b_valid; var ready = dut.master_port_0_0_b_ready; ready.Set(1); while (!valid.B()) dut.xclock.Step(); ready.Set(0); } static void sendC(int opcode, int size, int address, long data) { var valid = dut.master_port_0_0_c_valid; var ready = dut.master_port_0_0_c_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_c_bits_opcode.Set(opcode); dut.master_port_0_0_c_bits_size.Set(size); dut.master_port_0_0_c_bits_address.Set(address); dut.master_port_0_0_c_bits_data.Set(data); dut.xclock.Step(); valid.Set(0); } static void getD() { var valid = dut.master_port_0_0_d_valid; var ready = dut.master_port_0_0_d_ready; ready.Set(1); dut.xclock.Step(); while (!valid.B()) dut.xclock.Step(); ready.Set(0); } static void sendE(int sink) { var valid = dut.master_port_0_0_e_valid; var ready = dut.master_port_0_0_e_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_e_bits_sink.Set(sink); dut.xclock.Step(); valid.Set(0); } static void AcquireBlock(int address) { sendA(Opcode.A.AcquireBlock.getValue(), 0x6, address); } static BigInteger GrantData() { var opcode = dut.master_port_0_0_d_bits_opcode; var data = dut.master_port_0_0_d_bits_data; do { getD(); } while (opcode.Get().intValue() != Opcode.D.GrantData.getValue()); var r_data = data.U64().shiftLeft(64); do { getD(); } while (opcode.Get().intValue() != Opcode.D.GrantData.getValue()); return r_data.or(data.U64()); } static void GrantAck(int sink) { sendE(sink); } static void ReleaseData(int address, BigInteger data) { sendC(Opcode.C.ReleaseData.getValue(), 0x6, address, data.longValue()); sendC(Opcode.C.ReleaseData.getValue(), 0x6, address, data.shiftRight(64).longValue()); } static void ReleaseAck() { var opcode = dut.master_port_0_0_d_bits_opcode; do { getD(); } while (opcode.Get().intValue() != Opcode.D.ReleaseAck.getValue()); } public static void main(String[] args) throws IOException { /* Random Generator */ var gen_rand = RandomGenerator.getDefault(); /* DUT init */ final String[] ARGS = {\"+verilator+rand+reset+0\"}; dut = new UT_CoupledL2(ARGS); dut.InitClock(\"clock\"); dut.reset.SetWriteMode(WriteMode.Imme); dut.reset.Set(1); dut.xclock.Step(); dut.reset.Set(0); for (int i = 0; i \u003c 100; i++) dut.xclock.Step(); dut.xclock.Step(); /* Ref */ BigInteger[] ref_data = new BigInteger[16]; Arrays.fill(ref_data, BigInteger.ZERO); /* Test loop */ for (int test_loop = 0; test_loop \u003c 4000; test_loop++) { var address = gen_rand.nextInt(0xf) \u003c\u003c 6; var data = new BigInteger(128, Random.from(gen_rand)); pwOut.print(\"[CoupledL2 Test%d]: At address(%#03x), \".formatted(test_loop + 1, address)); /* Read */ AcquireBlock(address); var r_data = GrantData(); assert (r_data.equals(ref_data[address \u003e\u003e 6])); var sink = dut.master_port_0_0_d_bits_sink.Get().intValue(); GrantAck(sink); /* Write */ ReleaseData(address, data); ref_data[address \u003e\u003e 6] = data; ReleaseAck(); pwOut.println(\"Read: %s, Write: %s\".formatted(r_data.toString(), data.toString())); pwOut.flush(); } } } ################ bundle.py ################ from toffee import Bundle, Signals, Signal class DecoupledBundle(Bundle): ready, valid = Signals(2) class TileLinkBundleA(DecoupledBundle): opcode, param, size, source, address, user_alias, mask, data, corrupt = Signals(9) class TileLinkBundleB(DecoupledBundle): opcode, param, size, source, address, mask, data, corrupt = Signals(8) class TileLinkBundleC(DecoupledBundle): opcode, param, size, source, address, user_alias, data, corrupt = Signals(8) class TileLinkBundleD(DecoupledBundle): opcode, param, size, source, sink, denied, data, corrupt = Signals(8) class TileLinkBundleE(DecoupledBundle): sink = Signal() class TileLinkBundle(Bundle): a = TileLinkBundleA.from_regex(r\"a_(?:(valid|ready)|bits_(.*))\") b = TileLinkBundleB.from_regex(r\"b_(?:(valid|ready)|bits_(.*))\") c = TileLinkBundleC.from_regex(r\"c_(?:(valid|ready)|bits_(.*))\") d = TileLinkBundleD.from_regex(r\"d_(?:(valid|ready)|bits_(.*))\") e = TileLinkBundleE.from_regex(r\"e_(?:(valid|ready)|bits_(.*))\") ################ agent.py ################ from toffee import Agent, driver_method from toffee.triggers import Value from bundle import TileLinkBundle class TilelinkOPCodes: class A: PutFullData = 0x0 PutPartialData = 0x1 ArithmeticData = 0x2 LogicalData = 0x3 Get = 0x4 Hint = 0x5 AcquireBlock = 0x6 AcquirePerm = 0x7 class B: Probe = 0x8 class C: ProbeAck = 0x4 ProbeAckData = 0x5 Release = 0x6 ReleaseData = 0x7 class D: AccessAck = 0x0 AccessAckData = 0x1 HintAck = 0x2 Grant = 0x4 GrantData = 0x5 ReleaseAck = 0x6 class E: GrantAck = 0x4 class TileLinkAgent(Agent): def __init__(self, tlbundle: TileLinkBundle): super().__init__(tlbundle.step) self.tlbundle = tlbundle @driver_method() async def put_a(self, dict): dict[\"valid\"] = 1 self.tlbundle.a.assign(dict) await Value(self.tlbundle.a.ready, 1) self.tlbundle.a.valid.value = 0 @driver_method() async def get_d(self): self.tlbundle.d.ready.value = 1 await Value(self.tlbundle.d.valid, 1) result = self.tlbundle.d.as_dict() self.tlbundle.d.ready.value = 0 return result @driver_method() async def get_b(self): self.tlbundle.b.ready.value = 1 await Value(self.tlbundle.b.valid, 1) result = self.tlbundle.b.as_dict() self.tlbundle.b.ready.value = 0 return result @driver_method() async def put_c(self, dict): dict[\"valid\"] = 1 self.tlbundle.c.assign(dict) await Value(self.tlbundle.c.ready, 1) self.tlbundle.c.valid.value = 0 @driver_method() async def put_e(self, dict): dict[\"valid\"] = 1 self.tlbundle.e.assign(dict) await Value(self.tlbundle.e.ready, 1) self.tlbundle.e.valid.value = 0 ################################ async def aquire_block(self, address): await self.put_a( { \"*\": 0, \"size\": 0x6, \"opcode\": TilelinkOPCodes.A.AcquireBlock, \"address\": address, } ) data = 0x0 for i in range(2): ret = await self.get_d() while ret[\"opcode\"] != TilelinkOPCodes.D.GrantData: ret = await self.get_d() data = (ret[\"data\"] \u003c\u003c (256 * i)) | data await self.put_e({\"sink\": ret[\"sink\"]}) return data async def release_data(self, address, data): for _ in range(2): await self.put_c( { \"*\": 0, \"size\": 0x6, \"opcode\": TilelinkOPCodes.C.ReleaseData, \"address\": address, \"data\": data % (2**256), } ) data = data \u003e\u003e 256 x = await self.get_d() while x[\"opcode\"] != TilelinkOPCodes.D.ReleaseAck: x = await self.get_d() ################ test.py ################ import toffee import random from toffee.triggers import ClockCycles from CoupledL2 import DUTCoupledL2 from bundle import TileLinkBundle from agent import TileLinkAgent async def test_top(dut: DUTCoupledL2): toffee.start_clock(dut) dut.reset.value = 1 await ClockCycles(dut, 100) dut.reset.value = 0 tlbundle = TileLinkBundle.from_prefix(\"master_port_0_0_\").bind(dut) tlbundle.set_all(0) tlagent = TileLinkAgent(tlbundle) await ClockCycles(dut, 20) ref_data = [0] * 0x10 for _ in range(4000): # Read address = random.randint(0, 0xF) \u003c\u003c 6 r_data = await tlagent.aquire_block(address) print(f\"Read {address} = {hex(r_data)}\") assert r_data == ref_data[address \u003e\u003e 6] # Write send_data = random.randint(0, 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF) await tlagent.release_data(address, send_data) ref_data[address \u003e\u003e 6] = send_data print(f\"Write {address} = {hex(send_data)}\") if __name__ == \"__main__\": toffee.setup_logging(toffee.INFO) dut = DUTCoupledL2([\"+verilator+rand+reset+0\"]) dut.InitClock(\"clock\") dut.reset.AsImmWrite() toffee.run(test_top(dut)) dut.Finish() ","categories":["教程"],"description":"用C++、Java和Python简单验证香山L2 Cache的案例","excerpt":"用C++、Java和Python简单验证香山L2 Cache的案例","ref":"/mlvp/docs/multi-lang/examples/coupledl2/","tags":["docs"],"title":"CoupledL2"},{"body":" Using Guoke Cache as an example, this document introduces how to create a DUT based on Chisel.\nIn this document, a DUT (Design Under Test) refers to the circuit or system being verified during the chip verification process. The DUT is the primary subject of verification. When creating a DUT based on the picker tool, it is essential to consider the functionality, performance requirements, and verification goals of the subject under test. These goals may include the need for faster execution speed or more detailed test information. Generally, the DUT, written in RTL, is combined with its surrounding environment to form the verification environment (test_env), where test cases are written. In this project, the DUT is the Python module that needs to be tested and converted through RTL. Traditional RTL languages include Verilog, System Verilog, VHDL, etc. However, as an emerging RTL design language, （https://www.chisel-lang.org/） is playing an increasingly important role in RTL design due to its object-oriented features and ease of use. This chapter introduces how to create a DUT using the conversion of the cache source code from the Guoke Processor-NutShell to a Python module as an example.\nChisel and Guoke Chisel is a high-level hardware construction language (HCL) based on the Scala language. Traditional HDLs describe circuits, while HCLs generate circuits, making them more abstract and advanced. The Stage package provided in Chisel can convert HCL designs into traditional HDL languages such as Verilog and System Verilog. With tools like Mill and Sbt, automation in development can be achieved.\nGuoke is a sequential single-issue processor implementation based on the RISC-V RV64 open instruction set, modularly designed using the Chisel language. For a more detailed introduction to Guoke, please refer to the link: https://oscpu.github.io/NutShell-doc/.\nGuoke cache The Guoke Cache (Nutshell Cache) is the cache module used in the Guoke processor. It features a three-stage pipeline design. When the third stage pipeline detects that the current request is MMIO or a refill occurs, it will block the pipeline. The Guoke Cache also uses a customizable modular design that can generate different-sized L1 Caches or L2 Caches by changing parameters. Additionally, the Guoke Cache has a coherence interface to handle coherence-related requests.\nChisel to Verilog The stage library in Chisel helps generate traditional HDL code such as Verilog and System Verilog from Chisel code. Below is a brief introduction on how to convert a cache implementation based on Chisel into the corresponding Verilog circuit description.\nInitializing the Guoke Environment First, download the entire Guoke source code from the source repository and initialize it:\nmkdir cache-ut cd cache-ut git clone https://github.com/OSCPU/NutShell.git cd NutShell \u0026\u0026 git checkout 97a025d make init Creating Scala Compilation Configuration Then, create build.sc in the cache-ut directory with the following content:\nimport $file.NutShell.build import mill._, scalalib._ import coursier.maven.MavenRepository import mill.scalalib.TestModule._ // Specify Nutshell dependencies object difftest extends NutShell.build.CommonNS { override def millSourcePath = os.pwd / \"NutShell\" / \"difftest\" } // Nutshell configuration object NtShell extends NutShell.build.CommonNS with NutShell.build.HasChiselTests { override def millSourcePath = os.pwd / \"NutShell\" override def moduleDeps = super.moduleDeps ++ Seq( difftest, ) } // UT environment configuration object ut extends NutShell.build.CommonNS with ScalaTest{ override def millSourcePath = os.pwd override def moduleDeps = super.moduleDeps ++ Seq( NtShell ) } Instantiating cache After creating the configuration information, create the src/main/scala source code directory according to the Scala specification. Then, in the source code directory, create nut_cache.scala and use the following code to instantiate the Cache and convert it into Verilog code:\npackage ut_nutshell import chisel3._ import chisel3.util._ import nutcore._ import top._ import chisel3.stage._ object CacheMain extends App { (new ChiselStage).execute(args, Seq( ChiselGeneratorAnnotation(() =\u003e new Cache()(CacheConfig(ro = false, name = \"tcache\", userBits = 16))) )) } Generating RTL After creating all the files (build.sc, src/main/scala/nut_cache.scala), execute the following command in the cache-ut directory:\nmkdir build mill --no-server -d ut.runMain ut_nutshell.CacheMain --target-dir build --output-file Cache Note: For the Mill environment configuration, please refer to https://mill-build.com/mill/Intro_to_Mill.html.\nAfter successfully executing the above command, a Verilog file Cache.v will be generated in the build directory. Then, the picker tool can be used to convert Cache.v into a Python module. Besides Chisel, almost all other HCL languages can generate corresponding RTL codes, so the basic process above also applies to other HCLs.\nDUT Compilation Generally, if you need the DUT to generate waveforms, coverage, etc., it will slow down the DUT’s execution speed. Therefore, when generating a Python module through the picker tool, it will be generated according to various configurations: (1) Turn off all debug information; (2) Enable waveforms; (3) Enable code line coverage. The first configuration aims to quickly build the environment for regression testing, etc.; the second is used to analyze specific errors, timing, etc.; the third is used to improve coverage.\n","categories":["Sample Projects","Learning Materials"],"description":"Using Guoke Cache as an example, this document introduces how to create a DUT based on Chisel.","excerpt":"Using Guoke Cache as an example, this document introduces how to …","ref":"/mlvp/en/docs/basic/create_dut/","tags":["examples","docs"],"title":"Creating DUT"},{"body":"UCAgent supports human‑AI collaborative verification. You can pause AI execution, intervene manually, then continue AI execution. This mode applies to scenarios needing fine control or complex decisions.\nCollaboration Flow:\nPause AI execution: Direct LLM access mode: press Ctrl+C to pause. Code Agent collaboration mode: pause according to the agent’s method (e.g. Gemini-cli uses Esc). Human intervention: Manually edit files, test cases or configuration. Use interactive commands for debugging or adjustment. Stage control: Use tool_invoke Check to check current stage status. Use tool_invoke Complete to mark stage complete and enter next stage. Continue execution: Use loop [prompt] to continue AI execution and optionally provide extra prompt info. In Code Agent mode, input prompts via the agent console. Permission management: Use add_un_write_path, del_un_write_path to set file write permissions, controlling whether AI can edit specific files. Applies to direct LLM access or forced use of UCAgent file tools. ","categories":["Tutorial"],"description":"How to collaborate with AI to verify a module.","excerpt":"How to collaborate with AI to verify a module.","ref":"/mlvp/en/docs/ucagent/usage/assit/","tags":["docs"],"title":"Human-AI Collaborative Verification"},{"body":"Multi-File Input and Output In many cases, a module in one file may instantiate modules in other files. In such cases, you can use the picker tool’s -f option to process multiple Verilog source files. For example, suppose you have three source files: Cache.sv, CacheStage.sv, and CacheMeta.sv:\nFile List Cache.sv // In module Cache( ... ); CacheStage s1( ... ); CacheStage s2( ... ); CacheStage s3( ... ); CacheMeta cachemeta( ... ); endmodule CacheStage.sv // In CacheStage.sv module CacheStage( ... ); ... endmodule CacheMeta.sv // In CacheMeta.sv module CacheMeta( ... ); ... endmodule Usage In this case, the module under test is Cache, which is in Cache.sv. You can generate the DUT using the following command:\nCommand Line Specification picker export Cache.sv --fs CacheStage.sv,CacheMeta.sv --sname Cache Specification through a File List File You can also use a .txt file to specify multiple input files:\npicker export Cache.sv --fs src.txt --sname Cache Where the contents of src.txt are:\nCacheStage.sv CacheMeta.sv Notes It is important to note that even when using multiple file inputs, you still need to specify the file containing the top-level module under test, as shown in the example above with Cache.sv. When using multiple file inputs, Picker will pass all files to the simulator, which will compile them simultaneously. Therefore, it is necessary to ensure that the module names in all files are unique. ","categories":["Sample Projects","Tutorials"],"description":"Handling multiple Verilog source files","excerpt":"Handling multiple Verilog source files","ref":"/mlvp/en/docs/env_usage/multifile/","tags":["examples","docs"],"title":"Multi-File Input"},{"body":"Toffee is a hardware vafication framework written in Python that relies on the multilingual conversion tool Picker. The tool is able to convert the Verilog code of the hardware design into a Python Package, allowing users to use Python to drive and verify the hardware design.\nIt absorbs part of the UVM verification methodology to ensure the standardization and reusability of the verification environment, and re-designs the construction of the whole verification environment to make it more in line with the use habits of developers in the software field, so that software developers can easily perform hardware verification work.\nSee detailed instructions for using Toffee in Toffee Documentation.\n","categories":"","description":"Toffee is the framework for building the hardware verification environment","excerpt":"Toffee is the framework for building the hardware verification …","ref":"/mlvp/en/docs/mlvp/","tags":"","title":"Vafication Framework"},{"body":"The whole process adopts an “stage‑by‑stage progressive advancement” approach: each stage has a clear goal, outputs and pass criteria; after completion you use tool Check to verify and tool Complete to enter the next stage. If a stage contains sub‑stages, you must finish the sub‑stages one by one in order and each must pass Check.\nTotal top‑level stages: 11 (see vagent/lang/zh/config/default.yaml) Advancement principle: a stage that has not passed cannot be jumped; use tool CurrentTips to get detailed guidance for the current stage; when backfilling is needed use GotoStage to return to a specified stage. Three ways to skip / unskip a stage: In project root config.yaml under some stage list element’s - name entry set key skip: true/false to skip / not skip. At CLI startup use --skip / --unskip someStage to control skipping / not skipping a stage. After TUI starts use skip_stage / unskip_stage someStage to temporarily skip / unskip a stage. Overall Flow Overview (11 Stages) Current flow contains:\nRequirement Analysis \u0026 Verification Planning → 2) {DUT} Function Understanding → 3) Functional Specification Analysis \u0026 Test Point Definition → 4) Test Platform Basic Architecture Design → 5) Functional Coverage Model Implementation → 6) Basic API Implementation → 7) Basic API Functional Testing → 8) Test Framework Scaffolding → 9) Comprehensive Verification Execution \u0026 Bug Analysis → 10) Code Line Coverage Analysis \u0026 Improvement (skipped by default, can be enabled) → 11) Verification Review \u0026 Summary Use the actual workflow as final; the diagram below is for reference only. Note: in the paths below defaults to the output directory name under the working directory (default unity_test). For example docs are output to \u003cworkspace\u003e/unity_test/.\nStage 1: Requirement Analysis \u0026 Verification Planning\nGoal: understand the task, clarify verification scope and strategy. How: Read {DUT}/README.md, sort out “which functions / inputs / outputs / boundaries and risks need testing”. Form an executable verification plan and goal list. Output: \u003cOUT\u003e/{DUT}_verification_needs_and_plan.md (written in Chinese). Pass criteria: document exists, structure conforms (auto check markdown_file_check). Checker: UnityChipCheckerMarkdownFileFormat Role: verify Markdown file existence and format; forbids writing newline as literal “\\n”. Parameters: markdown_file_list (str | List[str]): path or list of MD files to check. Example: {OUT}/{DUT}_verification_needs_and_plan.md no_line_break (bool): whether to forbid newline written as literal “\\n”; true forbids. Stage 2: {DUT} Function Understanding\nGoal: grasp DUT interfaces and basic info; clarify if combinational or sequential circuit. How: Read {DUT}/README.md and {DUT}/__init__.py. Analyze IO ports, clock/reset needs and function scope. Output: \u003cOUT\u003e/{DUT}_basic_info.md. Pass criteria: document exists, format conforms (markdown_file_check). Checker: UnityChipCheckerMarkdownFileFormat (same parameter meanings). Stage 3: Functional Specification Analysis \u0026 Test Point Definition (with sub‑stages FG/FC/CK)\nGoal: structure Function Groups (FG), Function Points (FC) and Check Points (CK) as basis for subsequent automation. How: Read {DUT}/*.md and produced docs, build FG/FC/CK structure of {DUT}_functions_and_checks.md. Normalize labels: , , ; each function point must have at least 1 check point. Sub‑stages: 3.1 Functional grouping \u0026 hierarchy (FG): checker UnityChipCheckerLabelStructure(FG) 3.2 Function point definition (FC): checker UnityChipCheckerLabelStructure(FC) 3.3 Check point design (CK): checker UnityChipCheckerLabelStructure(CK) Output: \u003cOUT\u003e/{DUT}_functions_and_checks.md. Pass criteria: all three label structures pass corresponding checks. Corresponding checkers (default configuration): 3.1 UnityChipCheckerLabelStructure Role: parse label structure in {DUT}_functions_and_checks.md and validate hierarchy \u0026 counts (FG). Parameters: doc_file (str): path to function/check doc. Example: {OUT}/{DUT}_functions_and_checks.md leaf_node (“FG” | “FC” | “CK”): leaf type to validate. Example: \"FG\" min_count (int, default 1): minimum count threshold. must_have_prefix (str, default “FG-API”): required prefix for FG names for normalized grouping. 3.2 UnityChipCheckerLabelStructure (FC) Role: parse document and validate function point definitions. Same parameters; leaf_node \"FC\". 3.3 UnityChipCheckerLabelStructure (CK) Role: parse document and validate check point design (CK) and cache CK list for subsequent batch implementation. Extra parameter: data_key (str) e.g. \"COVER_GROUP_DOC_CK_LIST\" for caching CK list. Stage 4: Test Platform Basic Architecture Design (fixture / API framework)\nGoal: provide unified DUT creation and test lifecycle management capability. How: In \u003cOUT\u003e/tests/{DUT}_api.py implement create_dut(); for sequential circuit configure clock (InitClock); combinational circuits need no clock. Implement pytest fixture dut for init/cleanup and optional waveform / line coverage switches. Output: \u003cOUT\u003e/tests/{DUT}_api.py (with comments \u0026 docstrings). Pass criteria: DUT creation and fixture checks pass (UnityChipCheckerDutCreation / UnityChipCheckerDutFixture / UnityChipCheckerEnvFixture). Sub‑stage checkers: UnityChipCheckerDutCreation: validate create_dut(request) signature, clock/reset, coverage path. UnityChipCheckerDutFixture: validate lifecycle management, yield/cleanup, coverage collection call presence. UnityChipCheckerEnvFixture: validate existence/count of env* fixtures and (optionally) Bundle encapsulation (min_env default 1). Coverage path specification (important):\nIn create_dut(request) you must obtain a new line coverage file path via get_coverage_data_path(request, new_path=True) and pass into dut.SetCoverage(...). In cleanup phase of fixture dut you must obtain existing path via get_coverage_data_path(request, new_path=False) and call set_line_coverage(request, \u003cpath\u003e, ignore=...) to write statistics. If such calls are missing the checker will error directly and give fix tips (including tips_of_get_coverage_data_path example). Stage 5: Functional Coverage Model Implementation\nGoal: turn FG/FC/CK into countable coverage structures supporting progress measurement \u0026 regression. How: In \u003cOUT\u003e/tests/{DUT}_function_coverage_def.py implement get_coverage_groups(dut). Build a CovGroup for each FG; for FC/CK build watch_point and check function (prefer lambda, else normal function). Sub‑stages: 5.1 Coverage group creation (FG) 5.2 Coverage point \u0026 check implementation (FC/CK), supporting “batch implementation” tips (COMPLETED_POINTS/TOTAL_POINTS). Output: \u003cOUT\u003e/tests/{DUT}_function_coverage_def.py. Pass criteria: coverage group checks (FG/FC/CK) and batch implementation check pass. Sub‑stage checkers: 5.1 UnityChipCheckerCoverageGroup: compare coverage group definitions to doc FG consistency. 5.2 UnityChipCheckerCoverageGroup: compare coverage point / check point implementation to doc FC/CK consistency. 5.2 (batch) UnityChipCheckerCoverageGroupBatchImplementation: batch advance CK implementation \u0026 alignment, maintain progress (TOTAL/COMPLETED) with batch_size (default 20) and data_key \"COVER_GROUP_DOC_CK_LIST\". Stage 6: Basic API Implementation\nGoal: provide reusable operation encapsulations with prefix api_{DUT}_* hiding low‑level signal details. How: In \u003cOUT\u003e/tests/{DUT}_api.py implement at least 1 basic API; recommend differentiating “low‑level functional API” and “task functional API”. Add detailed docstring: function, parameters, return, exceptions. Output: \u003cOUT\u003e/tests/{DUT}_api.py. Pass criteria: UnityChipCheckerDutApi passes (prefix must be api_{DUT}_). Checker UnityChipCheckerDutApi: scans/validates count, naming, signature and docstring completeness of api_{DUT}_* functions (min_apis default 1). Stage 7: Basic API Functional Correctness Testing\nGoal: write at least 1 basic functional test case per implemented API and mark coverage. How: Create \u003cOUT\u003e/tests/test_{DUT}_api_*.py; import from {DUT}_api import *. First line of each test function: dut.fc_cover['FG-API'].mark_function('FC-API-NAME', test_func, ['CK-XXX']). Design typical / boundary / exceptional data; assert expected output. Use tool RunTestCases for execution \u0026 regression. Output: \u003cOUT\u003e/tests/test_{DUT}_api_*.py and defect records if bugs found. Pass criteria: UnityChipCheckerDutApiTest passes (coverage, case quality, documentation record complete). Stage 8: Test Framework Scaffolding Build\nGoal: bulk generate “placeholder” test templates for not‑yet‑implemented function points ensuring full coverage map. How: Based on {DUT}_functions_and_checks.md create test_*.py under \u003cOUT\u003e/tests/ with semantic file \u0026 case naming. First line mark coverage; add TODO comment describing what to test; end with assert False, 'Not implemented' to prevent false pass. Output: batch test templates; coverage progress indicators (COVERED_CKS/TOTAL_CKS). Pass criteria: UnityChipCheckerTestTemplate passes (structure / marking / explanation complete). Stage 9: Comprehensive Verification Execution \u0026 Bug Analysis\nGoal: turn templates into real tests, systematically discover and analyze DUT bugs. How: Fill logic in test_*.py, prefer API calls not direct signal manipulation. Design sufficient data and assertions; run RunTestCases; for Fail perform source‑based defect localization and record. Sub‑stage: 9.1 Batch test case implementation \u0026 corresponding defect analysis (COMPLETED_CASES/TOTAL_CASES). Output: systematic test set and /{DUT}_bug_analysis.md. Pass criteria: UnityChipCheckerTestCase passes (quality / coverage / bug analysis). Parent checker UnityChipCheckerTestCase; sub‑stage batch checker UnityChipCheckerBatchTestsImplementation (maintains implementation progress with batch_size default 10, data_key \"TEST_TEMPLATE_IMP_REPORT\"). TC bug labeling norms \u0026 consistency (strongly associated with docs/report):\nTerm: uniformly use “TC bug” (no longer use “CK bug”). Label structure: \u003cFG-*\u003e/\u003cFC-*\u003e/\u003cCK-*\u003e/\u003cBG-NAME-XX\u003e/\u003cTC-test_file.py::[ClassName]::test_case\u003e; BG confidence XX integer 0–100. Failed case vs documentation relationship: \u003cTC-*\u003e appearing in documentation must one‑to‑one match failed test cases in report (file/class/test names). Failed test cases must mark their associated check point (CK) else judged “unmarked”. Failed cases not recorded in bug doc will be warned as “undocumented failed test”. Stage 10: Code Line Coverage Analysis \u0026 Improvement (default skipped, can enable)\nGoal: review uncovered code lines, add targeted supplements. How: run Check to get line coverage; if below threshold, add tests targeting uncovered lines and regress; loop until threshold reached. Output: line coverage report and supplemental tests. Pass criteria: UnityChipCheckerTestCaseWithLineCoverage meets threshold (default 0.9 adjustable in config). Note: stage marked skip=true in config; enable via --unskip specifying index. Stage 11: Verification Review \u0026 Summary\nGoal: precipitate results, review process, provide improvement suggestions. How: Improve defect entries in /{DUT}_bug_analysis.md (source‑based analysis). Summarize and write /{DUT}_test_summary.md, re‑examine whether plan achieved; use GotoStage for backfill when necessary. Output: \u003cOUT\u003e/{DUT}_test_summary.md and final conclusion. Pass criteria: UnityChipCheckerTestCase re‑check passes. Tips \u0026 Best Practices\nUse tools anytime: Detail / Status to view Mission progress \u0026 current stage; CurrentTips for step‑level guidance; Check / Complete to advance stage. Left Mission panel in TUI shows stage index, skip status and failure count; can combine CLI --skip/--unskip/--force-stage-index for control. Customizing Workflow (add / remove stages / sub‑stages) Principle Explanation Workflow is defined in language config vagent/lang/zh/config/default.yaml top‑level stage: list. Config load order: setting.yaml → ~/.ucagent/setting.yaml → language default (including stage) → project root config.yaml → CLI --override. Note: list types (such as stage list) merge as “whole overwrite” not element‑level. Therefore to add / remove / modify stages, copy the default stage list into your project config.yaml and edit on that basis. Temporarily not executing a stage: prefer CLI --skip \u003cindex\u003e or tool Skip/Goto during run; for persistent skipping write skip: true on that stage entry in your config.yaml (must still provide full stage list). Add a Stage Need: after “comprehensive verification execution” add a “static check \u0026 Lint report” stage requiring generation of \u003cOUT\u003e/{DUT}_lint_report.md and format check. Method: in project root config.yaml provide full stage: list and insert entry at suitable position (fragment example only shows new item; actual needs your full list): stage: # ...previous existing stages... - name: static_lint_and_style_check desc: \"静态分析与代码风格检查报告\" task: - \"目标：完成 DUT 的静态检查/Lint，并输出报告\" - \"第1步：运行 lint 工具（按项目需要）\" - \"第2步：将结论整理为 \u003cOUT\u003e/{DUT}_lint_report.md（中文）\" - \"第3步：用 Check 校验报告是否存在且格式规范\" checker: - name: markdown_file_check clss: \"UnityChipCheckerMarkdownFileFormat\" args: markdown_file_list: \"{OUT}/{DUT}_lint_report.md\" # MD 文件路径或列表 no_line_break: true # 禁止字面量 \"\\n\" 作为换行 reference_files: [] output_files: - \"{OUT}/{DUT}_lint_report.md\" skip: false # ...subsequent existing stages... Remove a Sub‑Stage Scenario: in “functional specification analysis \u0026 test point definition” temporarily not executing “function point definition (FC)” sub‑stage. Recommended approach: at runtime use CLI --skip to skip index; if long‑term config needed copy default stage: list to your config.yaml then in parent stage functional_specification_analysis remove corresponding sub‑stage entry from its stage: child list, or add skip: true to that sub‑stage. Sub‑stage removal (fragment example only shows parent stage structure \u0026 its sub‑stage list):\nstage: - name: functional_specification_analysis desc: \"功能规格分析与测试点定义\" task: - \"目标：将芯片功能拆解成可测试的小块，为后续测试做准备\" # ...省略父阶段任务... stage: - name: functional_grouping # 保留 FG 子阶段 # ...原有配置... # - name: function_point_definition # 原来的 FC 子阶段（此行及其内容整体删除，或在其中加 skip: true） - name: check_point_design # 保留 CK 子阶段 # ...原有配置... # ...其他字段... Tips\nOnly temporary skip needed: use --skip / --unskip fastest; no config file edit. Need permanent add/remove: copy default stage: list to project config.yaml, edit then commit; note list is whole overwrite—do not paste only fragment of added / removed items. New stage’s checkers can reuse existing classes (Markdown / Fixture / API / Coverage / TestCase etc.) or extend custom checkers (put under vagent/checkers/ and fill import path in clss). Customizing Checkers (checker) Principle Explanation\nEach (sub) stage has a checker: list; when executing Check all checkers in that list are run sequentially. Config fields: name: identifier of the checker inside the stage (readability / logs) clss: checker class name; short name imported from vagent.checkers namespace; can also write full module path (e.g. mypkg.mychk.MyChecker) args: parameters passed to checker constructor; supports template variables (e.g. {OUT}, {DUT}) extra_args: optional; some checkers support custom tips / strategy (e.g. fail_msg, batch_size, pre_report_file etc.) Parsing \u0026 instantiation: vagent/stage/vstage.py reads checker: and generates instances per clss/args; at runtime ToolStdCheck/Check calls do_check(). Merge semantics: when merging config lists are “whole replacement”; to modify checker: of a stage in project config.yaml, copy that stage entry and replace its entire checker: list. Add a Checker In parent stage “functional specification analysis \u0026 test point definition” add a “document format check” ensuring {OUT}/{DUT}_functions_and_checks.md does not write newline as literal \\n.\n# Fragment example: needs placement into your full stage list corresponding stage - name: functional_specification_analysis desc: \"功能规格分析与测试点定义\" # ...existing fields... output_files: - \"{OUT}/{DUT}_functions_and_checks.md\" checker: - name: functions_and_checks_doc_format clss: \"UnityChipCheckerMarkdownFileFormat\" args: markdown_file_list: \"{OUT}/{DUT}_functions_and_checks.md\" # 功能/检查点文档 no_line_break: true # 禁止字面量 \"\\n\" stage: # ...子阶段 FG/FC/CK 原有配置... (Extensible) Custom checker (minimal implementation, place in vagent/checkers/unity_test.py)\nIn many scenarios the “added checker” is not reusing an existing checker but needs a new one. Minimal implementation steps:\nCreate a new class inheriting base vagent.checkers.base.Checker In __init__ declare needed parameters (matching YAML args) Implement do_check(self, timeout=0, **kw) -\u003e tuple[bool, object] returning (pass?, structured message) For reading/writing workspace files use self.get_path(rel) to get absolute path; for cross‑stage shared data use self.smanager_set_value / get_value If you want short name reference in clss, export the class in vagent/checkers/__init__.py (or write full module path in clss) Minimal code skeleton (example):\n# File: vagent/checkers/unity_test.py from typing import Tuple import os from vagent.checkers.base import Checker class UnityChipCheckerMyCustomCheck(Checker): def __init__(self, target_file: str, threshold: int = 1, **kw): self.target_file = target_file self.threshold = threshold def do_check(self, timeout=0, **kw) -\u003e Tuple[bool, object]: \"\"\"Check whether target_file exists and perform simple rule validation.\"\"\" real = self.get_path(self.target_file) if not os.path.exists(real): return False, {\"error\": f\"file '{self.target_file}' not found\"} # TODO: write your specific validation logic here (count / parse / compare etc.) return True, {\"message\": \"MyCustomCheck passed\"} Reference in stage YAML (same as adding a checker):\nchecker: - name: my_custom_check clss: \"UnityChipCheckerMyCustomCheck\" # If not exported in __init__.py write full path mypkg.mychk.UnityChipCheckerMyCustomCheck args: target_file: \"{OUT}/{DUT}_something.py\" threshold: 2 extra_args: fail_msg: \"未满足自定义阈值，请完善实现或调低阈值。\" # Optional: customize default failure tip via extra_args Advanced tips (as needed):\nLong task / external process: when running subprocess call self.set_check_process(p, timeout) so tools KillCheck / StdCheck can manage \u0026 view output. Template rendering: implement get_template_data() to render progress / stats into stage title and task text. Initialization hook: implement on_init() to load cache / prepare batch tasks (same as Batch series checkers). Delete a Checker If temporarily not using “Stage 2 basic info document format check”, set that stage’s checker: empty or remove that item:\n- name: dut_function_understanding desc: \"{DUT}功能理解\" # ...existing fields... checker: [] # Remove original markdown_file_check Modify a Checker Change line coverage check threshold from 0.9 to 0.8 and customize failure message:\n- name: line_coverage_analysis_and_improvement desc: \"代码行覆盖率分析与提升{COVERAGE_COMPLETE}\" # ...existing fields... checker: - name: line_coverage_check clss: \"UnityChipCheckerTestCaseWithLineCoverage\" args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" min_line_coverage: 0.8 # Lower threshold extra_args: fail_msg: \"未达到 80% 的行覆盖率，请补充针对未覆盖行的测试。\" Optional: custom checker class\nAdd new class under vagent/checkers/, inherit vagent.checkers.base.Checker and implement do_check(). After exporting in vagent/checkers/__init__.py you can use short name in clss; or directly write full module path. Strings in args support template variable rendering; extra_args can customize failure message (depends on checker implementation). Common Checker Parameters (Structured) Below parameters all come from actual code implementation (vagent/checkers/unity_test.py); names, defaults and types align with code. Example fragments can be placed directly in phase YAML checker[].args.\nUnityChipCheckerMarkdownFileFormat Parameters: markdown_file_list (str | List[str]): Markdown file path or list to check. no_line_break (bool, default false): whether to forbid newline written as literal “\\n”. Example: args: markdown_file_list: \"{OUT}/{DUT}_basic_info.md\" no_line_break: true UnityChipCheckerLabelStructure Parameters: doc_file (str) leaf_node (“FG”|“FC”|“CK”) min_count (int, default 1) must_have_prefix (str, default “FG-API”) data_key (str, optional) Example: args: doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" leaf_node: \"CK\" data_key: \"COVER_GROUP_DOC_CK_LIST\" UnityChipCheckerDutCreation args: target_file: \"{OUT}/tests/{DUT}_api.py\" UnityChipCheckerDutFixture args: target_file: \"{OUT}/tests/{DUT}_api.py\" UnityChipCheckerEnvFixture args: target_file: \"{OUT}/tests/{DUT}_api.py\" min_env: 1 UnityChipCheckerDutApi args: api_prefix: \"api_{DUT}_\" target_file: \"{OUT}/tests/{DUT}_api.py\" min_apis: 1 UnityChipCheckerCoverageGroup args: test_dir: \"{OUT}/tests\" cov_file: \"{OUT}/tests/{DUT}_function_coverage_def.py\" doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" check_types: [\"FG\", \"FC\", \"CK\"] UnityChipCheckerCoverageGroupBatchImplementation args: test_dir: \"{OUT}/tests\" cov_file: \"{OUT}/tests/{DUT}_function_coverage_def.py\" doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" batch_size: 20 data_key: \"COVER_GROUP_DOC_CK_LIST\" UnityChipCheckerTestTemplate args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" test_dir: \"{OUT}/tests\" ignore_ck_prefix: \"test_api_{DUT}_\" data_key: \"TEST_TEMPLATE_IMP_REPORT\" batch_size: 20 UnityChipCheckerDutApiTest args: api_prefix: \"api_{DUT}_\" target_file_api: \"{OUT}/tests/{DUT}_api.py\" target_file_tests: \"{OUT}/tests/test_{DUT}_api*.py\" doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" UnityChipCheckerBatchTestsImplementation args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" ignore_ck_prefix: \"test_api_{DUT}_\" batch_size: 10 data_key: \"TEST_TEMPLATE_IMP_REPORT\" pre_report_file: \"{OUT}/{DUT}/.TEST_TEMPLATE_IMP_REPORT.json\" UnityChipCheckerTestCase args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" UnityChipCheckerTestCaseWithLineCoverage args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" cfg: \"\u003cCONFIG_OBJECT_OR_DICT\u003e\" min_line_coverage: 0.9 Hint: above “Example” fragments only show args snippet; actually they need to be placed under a phase entry checker[].args.\n","categories":["Tutorial"],"description":"Overall workflow explanation.","excerpt":"Overall workflow explanation.","ref":"/mlvp/en/docs/ucagent/workflow/","tags":["docs"],"title":"Workflow"},{"body":" 以果壳cache为例，介绍如何创建基于Chisel的DUT\n在本文档中，DUT（Design Under Test）是指在芯片验证过程中，被验证的对象电路或系统。DUT是验证的主体，在基于picker工具创建DUT时，需要考虑被测对象的功能、性能要求和验证目标，例如是需要更快的执行速度，还是需要更详细的测试信息。通常情况下DUT，即RTL编写的源码，与外围环境一起构成验证环境（test_env），然后基于该验证环境编写测试用例。在本项目中，DUT是需要测试的Python模块，需要通过RTL进行转换。传统的RTL语言包括Verilog、System Verilog、VHDL等，然而作为新兴的RTL设计语言，Chisel（https://www.chisel-lang.org/）也以其面向对象的特征和便捷性，逐渐在RTL设计中扮演越来越重要的角色。本章以果壳处理器-NutShell中的cache源代码到Python模块的转换为例进行介绍如何创建DUT。\nChisel与果壳 准确来说，Chisel是基于Scala语言的高级硬件构造（HCL）语言。传统HDL是描述电路，而HCL则是生成电路，更加抽象和高级。同时Chisel中提供的Stage包则可以将HCL设计转化成Verilog、System Verilog等传统的HDL语言设计。配合上Mill、Sbt等Scala工具则可以实现自动化的开发。\n果壳是使用 Chisel 语言模块化设计的、基于 RISC-V RV64 开放指令集的顺序单发射处理器实现。果壳更详细的介绍请参考链接：https://oscpu.github.io/NutShell-doc/\n果壳 cache 果壳cache（Nutshell Cache）是果壳处理器中使用的缓存模块。其采用三级流水设计，当第三级流水检出当前请求为MMIO或者发生重填（refill）时，会阻塞流水线。同时，果壳cache采用可定制的模块化设计，通过改变参数可以生成存储空间大小不同的一级cache（L1 Cache）或者二级cache（L2 Cache）。此外，果壳cache留有一致性（coherence）接口，可以处理一致性相关的请求。\nChisel 转 Verilog Chisel中的stage库可以帮助由Chisel代码生成Verilog、System Verilog等传统的HDL代码。以下将简单介绍如何由基于Chisel的cache实现转换成对应的Verilog电路描述。\n初始化果壳环境 首先从源仓库下载整个果壳源代码，并进行初始化：\nmkdir cache-ut cd cache-ut git clone https://github.com/OSCPU/NutShell.git cd NutShell \u0026\u0026 git checkout 97a025d make init 创建scala编译配置 在cache-ut目录下创建build.sc，其中内容如下：\nimport $file.NutShell.build import mill._, scalalib._ import coursier.maven.MavenRepository import mill.scalalib.TestModule._ // 指定Nutshell的依赖 object difftest extends NutShell.build.CommonNS { override def millSourcePath = os.pwd / \"NutShell\" / \"difftest\" } // Nutshell 配置 object NtShell extends NutShell.build.CommonNS with NutShell.build.HasChiselTests { override def millSourcePath = os.pwd / \"NutShell\" override def moduleDeps = super.moduleDeps ++ Seq( difftest, ) } // UT环境配置 object ut extends NutShell.build.CommonNS with ScalaTest{ override def millSourcePath = os.pwd override def moduleDeps = super.moduleDeps ++ Seq( NtShell ) } 实例化 cache 创建好配置信息后，按照scala规范，创建src/main/scala源代码目录。之后，就可以在源码目录中创建nut_cache.scala，利用如下代码实例化Cache并转换成Verilog代码：\npackage ut_nutshell import chisel3._ import chisel3.util._ import nutcore._ import top._ import chisel3.stage._ object CacheMain extends App { (new ChiselStage).execute(args, Seq( ChiselGeneratorAnnotation(() =\u003e new Cache()(CacheConfig(ro = false, name = \"tcache\", userBits = 16))) )) } 生成RTL 完成上述所有文件的创建后（build.sc，src/main/scala/nut_cache.scala），在cache-ut目录下执行如下命令：\nmkdir build mill --no-server -d ut.runMain ut_nutshell.CacheMain --target-dir build --output-file Cache 注：mill环境的配置请参考 https://mill-build.com/mill/Intro_to_Mill.html\n上述命令成功执行完成后，会在build目录下生成verilog文件：Cache.v。之后就可以通过picker工具进行Cache.v到 Python模块的转换。除去chisel外，其他HCL语言几乎都能生成对应的 RTL代码，因此上述基本流程也适用于其他HCL。\nDUT编译 一般情况下，如果需要DUT生成波形、覆盖率等会导致DUT的执行速度变慢，因此在通过picker工具生成python模块时会根据多种配置进行生成：（1）关闭所有debug信息；（2）开启波形；（3）开启代码行覆盖率。其中第一种配置的目标是快速构建环境，进行回归测试等；第二种配置用于分析具体错误，时序等；第三种用于提升覆盖率。\n","categories":["示例项目","学习材料"],"description":"以果壳cache为例，介绍如何创建基于chisel的DUT","excerpt":"以果壳cache为例，介绍如何创建基于chisel的DUT","ref":"/mlvp/docs/basic/create_dut/","tags":["examples","docs"],"title":"创建DUT"},{"body":"多文件输入输出 在许多情况中，某文件下的某个模块会例化其他文件下的模块，在这种情况下您可以使用Picker工具的-f选项处理多个verilog源文件。例如，假设您有Cache.sv, CacheStage.sv以及CacheMeta.sv三个源文件：\n文件列表 Cache.sv // In module Cache( ... ); CacheStage s1( ... ); CacheStage s2( ... ); CacheStage s3( ... ); CacheMeta cachemeta( ... ); endmodule CacheStage.sv // In CacheStage.sv module CacheStage( ... ); ... endmodule CacheMeta.sv // In CacheMeta.sv module CacheMeta( ... ); ... endmodule 应用方式 其中，待测模块为Cache，位于Cache.sv中，则您可以通过以下命令生成DUT：\n命令行指定 picker export Cache.sv --fs CacheStage.sv,CacheMeta.sv --sname Cache 通过文件列表文件指定 您也可以通过传入.txt文件的方式来实现多文件输入：\npicker export Cache.sv --fs src.txt --sname Cache 其中src.txt的内容为:\nCacheStage.sv CacheMeta.sv 注意事项 需要注意的是，使用多文件输入时仍需要指定待测顶层模块所在的文件，例如上文中所示的Cache.sv。 在使用多文件输入时，Picker会将所有文件都交给仿真器，仿真器同时进行编译，因此需要保证所有文件中的模块名不重复。 ","categories":["示例项目","教程"],"description":"处理多个Verilog源文件","excerpt":"处理多个Verilog源文件","ref":"/mlvp/docs/env_usage/multifile/","tags":["examples","docs"],"title":"多文件输入"},{"body":"整体采用“按阶段渐进推进”的方式，每个阶段都有明确目标、产出与通过标准；完成后用工具 Check 验证并用 Complete 进入下一阶段。若阶段包含子阶段，需按顺序 逐一完成子阶段并各自通过 Check。\n顶层阶段总数：11（见 vagent/lang/zh/config/default.yaml） 推进原则：未通过的阶段不可跳转；可用工具 CurrentTips 获取当前阶段详细指导；需要回补时可用 GotoStage 回到指定阶段。 三种跳/不跳过阶段方法： 在项目根 config.yaml 的某个 stage 字段下面 -name 元素里的 skip 键配置 true/false 来跳过/不跳过。 命令行启动时可用 --skip/- -unskip someStage 来控制跳过/不跳过某阶段。 在tui启动后可用 skip_stage/unskip_stage someStage 来控制临时跳过/不跳过某阶段。 整体流程概览（11 个阶段） 目前的流程包含：\n需求分析与验证规划 → 2) {DUT} 功能理解 → 3) 功能规格分析与测试点定义 → 4) 测试平台基础架构设计 → 5) 功能覆盖率模型实现 → 6) 基础 API 实现 → 7) 基础 API 功能测试 → 8) 测试框架脚手架 → 9) 全面验证执行与缺陷分析 → 10) 代码行覆盖率分析与提升（默认跳过，可启用）→ 11) 验证审查与总结 以实际的工作流为准，下图仅供参考。 说明：以下路径中的 默认为工作目录下的输出目录名（默认 unity_test）。例如文档输出到 \u003cworkspace\u003e/unity_test/。\n阶段 1：需求分析与验证规划\n目标：理解任务、明确验证范围与策略。 怎么做： 阅读 {DUT}/README.md，梳理“需要测哪些功能/输入输出/边界与风险”。 形成可执行的验证计划与目标清单。 产出：\u003cOUT\u003e/{DUT}_verification_needs_and_plan.md（中文撰写）。 通过标准：文档存在、结构规范（自动检查 markdown_file_check）。 检查器： UnityChipCheckerMarkdownFileFormat 作用：校验 Markdown 文件存在与格式，禁止把换行写成字面量“\\n”。 参数： markdown_file_list (str | List[str]): 待检查的 MD 文件路径或路径列表。示例：{OUT}/{DUT}_verification_needs_and_plan.md no_line_break (bool): 是否禁止把换行写成字面量 “\\n”；true 表示禁止。 阶段 2：{DUT} 功能理解\n目标：掌握 DUT 的接口与基本信息，明确是组合/时序电路。 怎么做： 阅读 {DUT}/README.md 与 {DUT}/__init__.py。 分析 IO 端口、时钟/复位需求与功能范围。 产出：\u003cOUT\u003e/{DUT}_basic_info.md。 通过标准：文档存在、格式规范（markdown_file_check）。 检查器： UnityChipCheckerMarkdownFileFormat 作用：校验 Markdown 文件存在与格式，禁止把换行写成字面量“\\n”。 参数： markdown_file_list (str | List[str]): 待检查的 MD 文件路径或路径列表。示例：{OUT}/{DUT}_basic_info.md no_line_break (bool): 是否禁止把换行写成字面量 “\\n”；true 表示禁止。 阶段 3：功能规格分析与测试点定义（含子阶段 FG/FC/CK）\n目标：把功能分组（FG）、功能点（FC）和检测点（CK）结构化，作为后续自动化的依据。 怎么做： 阅读 {DUT}/*.md 与已产出文档，建立 {DUT}_functions_and_checks.md 的 FG/FC/CK 结构。 规范标签：\u003cFG-组名\u003e、\u003cFC-功能名\u003e、\u003cCK-检测名\u003e，每个功能点至少 1 个检测点。 子阶段： 3.1 功能分组与层次（FG）：检查器 UnityChipCheckerLabelStructure(FG) 3.2 功能点定义（FC）：检查器 UnityChipCheckerLabelStructure(FC) 3.3 检测点设计（CK）：检查器 UnityChipCheckerLabelStructure(CK) 产出：\u003cOUT\u003e/{DUT}_functions_and_checks.md。 通过标准：三类标签结构均通过对应检查。 对应检查器（默认配置）： 3.1 UnityChipCheckerLabelStructure 作用：解析 {DUT}_functions_and_checks.md 中的标签结构并校验层级与数量（FG）。 参数： doc_file (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md leaf_node (“FG” | “FC” | “CK”): 需要校验的叶子类型。示例：\"FG\" min_count (int, 默认 1): 该叶子类型的最小数量阈值。 must_have_prefix (str, 默认 “FG-API”): FG 名称要求的前缀，用于规范化分组命名。 3.2 UnityChipCheckerLabelStructure 作用：解析文档并校验功能点定义（FC）。 参数： doc_file (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md leaf_node (“FG” | “FC” | “CK”): 需要校验的叶子类型。示例：\"FC\" min_count (int, 默认 1): 该叶子类型的最小数量阈值。 must_have_prefix (str, 默认 “FG-API”): 所属 FG 的前缀规范，用于一致性检查。 3.3 UnityChipCheckerLabelStructure 作用：解析文档并校验检测点设计（CK），并缓存 CK 列表用于后续分批实现。 参数： doc_file (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md leaf_node (“FG” | “FC” | “CK”): 需要校验的叶子类型。示例：\"CK\" data_key (str): 共享数据键名，用于缓存 CK 列表（供后续分批实现使用）。示例：\"COVER_GROUP_DOC_CK_LIST\" min_count (int, 默认 1): 该叶子类型的最小数量阈值。 must_have_prefix (str, 默认 “FG-API”): 所属 FG 的前缀规范，用于一致性检查。 阶段 4：测试平台基础架构设计（fixture/API 框架）\n目标：提供统一的 DUT 创建与测试生命周期管理能力。 怎么做： 在 \u003cOUT\u003e/tests/{DUT}_api.py 实现 create_dut()；时序电路配置时钟（InitClock），组合电路无需时钟。 实现 pytest fixture dut，负责初始化/清理与可选的波形/行覆盖率开关。 产出：\u003cOUT\u003e/tests/{DUT}_api.py（含注释与文档字符串）。 通过标准：DUT 创建与 fixture 检查通过（UnityChipCheckerDutCreation / UnityChipCheckerDutFixture）。 子阶段检查器： DUT 创建：UnityChipCheckerDutCreation 作用：校验 {DUT}_api.py 中的 create_dut(request) 是否实现规范（签名、时钟/复位、覆盖率路径等约定）。 参数： target_file (str): DUT API 与 fixture 所在文件路径。示例：{OUT}/tests/{DUT}_api.py dut fixture：UnityChipCheckerDutFixture 作用：校验 pytest fixture dut 的生命周期管理、yield/清理，以及覆盖率收集调用是否到位。 参数： target_file (str): 包含 dut fixture 的文件路径。示例：{OUT}/tests/{DUT}_api.py env fixture：UnityChipCheckerEnvFixture 作用：校验 env* 系列 fixture 的存在、数量与 Bundle 封装是否符合要求。 参数： target_file (str): 包含 env* 系列 fixture 的文件路径。示例：{OUT}/tests/{DUT}_api.py min_env (int, 默认 1): 至少需要存在的 env* fixture 数量。示例：1 force_bundle (bool, 当前未使用): 是否强制要求 Bundle 封装。 覆盖率路径规范（重要）：\n在 create_dut(request) 中，必须通过 get_coverage_data_path(request, new_path=True) 获取新的行覆盖率文件路径，并传入 dut.SetCoverage(...)。 在 dut fixture 的清理阶段，必须通过 get_coverage_data_path(request, new_path=False) 获取已有路径，并调用 set_line_coverage(request, \u003cpath\u003e, ignore=...) 写入统计。 若缺失上述调用，检查器会直接报错，并给出修复提示（含 tips_of_get_coverage_data_path 示例）。 阶段 5：功能覆盖率模型实现\n目标：将 FG/FC/CK 转为可统计的覆盖结构，支撑进度度量与回归。 怎么做： 在 \u003cOUT\u003e/tests/{DUT}_function_coverage_def.py 实现 get_coverage_groups(dut)。 为每个 FG 建立 CovGroup；为 FC/CK 建 watch_point 与检查函数（优先用 lambda，必要时普通函数）。 子阶段： 5.1 覆盖组创建（FG） 5.2 覆盖点与检查实现（FC/CK），支持“分批实现”提示（COMPLETED_POINTS/TOTAL_POINTS）。 产出：\u003cOUT\u003e/tests/{DUT}_function_coverage_def.py。 通过标准：CoverageGroup 检查（FG/FC/CK）与批量实现检查通过。 子阶段检查器： 5.1 UnityChipCheckerCoverageGroup 作用：比对覆盖组定义与文档 FG 一致性。 参数： test_dir (str): 测试目录根路径。示例：{OUT}/tests cov_file (str): 覆盖率模型定义文件路径。示例：{OUT}/tests/{DUT}_function_coverage_def.py doc_file (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md check_types (str | List[str]): 检查的类型集合。示例：\"FG\" 5.2 UnityChipCheckerCoverageGroup 作用：比对覆盖点/检查点实现与文档 FC/CK 一致性。 参数： test_dir (str): 测试目录根路径。示例同上 cov_file (str): 覆盖率模型定义文件路径。示例同上 doc_file (str): 功能/检查点文档路径。示例同上 check_types (List[str]): 检查类型集合。示例：[\"FC\", \"CK\"] 5.2（分批）UnityChipCheckerCoverageGroupBatchImplementation 作用：按 CK 分批推进实现与对齐检查，维护进度（TOTAL/COMPLETED）。 参数： test_dir (str): 测试目录根路径。 cov_file (str): 覆盖率模型定义文件路径。 doc_file (str): 功能/检查点文档路径。 batch_size (int, 默认 20): 每批实现与校验的 CK 数量上限。示例：20 data_key (str): 共享数据键名，用于读取 CK 列表。示例：\"COVER_GROUP_DOC_CK_LIST\" 阶段 6：基础 API 实现\n目标：用 api_{DUT}_* 前缀提供可复用的操作封装，隐藏底层信号细节。 怎么做： 在 \u003cOUT\u003e/tests/{DUT}_api.py 实现至少 1 个基础 API；建议区分“底层功能 API”与“任务功能 API”。 补充详细 docstring：功能、参数、返回值、异常。 产出：\u003cOUT\u003e/tests/{DUT}_api.py。 通过标准：UnityChipCheckerDutApi 通过（前缀必须为 api_{DUT}_）。 检查器： UnityChipCheckerDutApi 作用：扫描/校验 api_{DUT}_* 函数的数量、命名、签名与 docstring 完整度。 参数： api*prefix (str): API 前缀匹配表达式。建议：\"api*{DUT}\\_\" target_file (str): API 定义所在文件。示例：{OUT}/tests/{DUT}_api.py min_apis (int, 默认 1): 至少需要的 API 数量。 阶段 7：基础 API 功能正确性测试\n目标：为每个已实现 API 编写至少 1 个基础功能用例，并标注覆盖率。 怎么做： 在 \u003cOUT\u003e/tests/test_{DUT}_api_*.py 新建测试；导入 from {DUT}_api import *。 每个测试函数的第一行：dut.fc_cover['FG-API'].mark_function('FC-API-NAME', test_func, ['CK-XXX'])。 设计典型/边界/异常数据，断言预期输出。 用工具 RunTestCases 执行与回归。 产出：\u003cOUT\u003e/tests/test_{DUT}_api_*.py 与缺陷记录（若发现 bug）。 通过标准：UnityChipCheckerDutApiTest 通过（覆盖、用例质量、文档记录齐备）。 检查器： UnityChipCheckerDutApiTest 作用：运行 pytest 并检查每个 API 至少 1 个基础功能用例且正确覆盖标记；核对缺陷记录与文档一致。 参数： api*prefix (str): API 前缀匹配表达式。建议：\"api*{DUT}\\_\" target_file_api (str): API 文件路径。示例：{OUT}/tests/{DUT}_api.py target*file_tests (str): 测试文件 Glob。示例：{OUT}/tests/test*{DUT}\\_api\\*.py doc_func_check (str): 功能/检查点文档。示例：{OUT}/{DUT}_functions_and_checks.md doc_bug_analysis (str): 缺陷分析文档。示例：{OUT}/{DUT}_bug_analysis.md min_tests (int, 默认 1): 单 API 最少测试用例数。 timeout (int, 默认 15): 单次测试运行超时（秒）。 阶段 8：测试框架脚手架构建\n目标：为尚未实现的功能点批量生成“占位”测试模板，确保覆盖版图完整。 怎么做： 依据 {DUT}_functions_and_checks.md，在 \u003cOUT\u003e/tests/ 创建 test_*.py，文件与用例命名语义化。 每个函数首行标注覆盖率 mark；补充 TODO 注释说明要测什么；末尾添加 assert False, 'Not implemented' 防误通过。 产出：批量测试模板；覆盖率进度指标（COVERED_CKS/TOTAL_CKS）。 通过标准：UnityChipCheckerTestTemplate 通过（结构/标记/说明完整）。 检查器： UnityChipCheckerTestTemplate 作用：检查模板文件/用例结构、覆盖标记、TODO 注释与防误通过断言；统计覆盖进度。 参数： doc_func_check (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md test_dir (str): 测试目录根路径。示例：{OUT}/tests ignore*ck_prefix (str): 统计覆盖时忽略的 CK 前缀（通常为基础 API 的用例）。示例：\"test_api*{DUT}\\_\" data_key (str): 共享数据键名，用于生成/读取模板实现进度。示例：\"TEST_TEMPLATE_IMP_REPORT\" batch_size (int, 默认 20): 每批模板检查数量。 min_tests (int, 默认 1): 最少要求的模板测试数。 timeout (int, 默认 15): 测试运行超时（秒）。 阶段 9：全面验证执行与缺陷分析\n目标：将模板填充为真实测试，系统发现并分析 DUT bug。 怎么做： 在 test_*.py 填充逻辑，优先通过 API 调用，不直接操纵底层信号。 设计充分数据并断言；用 RunTestCases 运行；对 Fail 进行基于源码的缺陷定位与记录。 子阶段： 9.1 分批测试用例实现与对应缺陷分析（COMPLETED_CASES/TOTAL_CASES）。 产出：成体系的测试集与 /{DUT}_bug_analysis.md。 通过标准：UnityChipCheckerTestCase（质量/覆盖/缺陷分析）通过。 检查器： 父阶段：UnityChipCheckerTestCase 作用：运行整体测试并对照功能/缺陷文档检查质量、覆盖与记录一致性。 参数： doc_func_check (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md doc_bug_analysis (str): 缺陷分析文档路径。示例：{OUT}/{DUT}_bug_analysis.md test_dir (str): 测试目录根路径。示例：{OUT}/tests min_tests (int, 默认 1): 最少要求的测试用例数量。 timeout (int, 默认 15): 测试运行超时（秒）。 子阶段（分批实现）：UnityChipCheckerBatchTestsImplementation 作用：分批将模板落地为真实用例并回归，维护实现进度与报告。 参数： doc_func_check (str): 功能/检查点文档路径。 doc_bug_analysis (str): 缺陷分析文档路径。 test_dir (str): 测试目录根路径。 ignore*ck_prefix (str): 统计覆盖时忽略的 CK 前缀。示例：\"test_api*{DUT}\\_\" batch_size (int, 默认 10): 每批转化并执行的用例数量。 data_key (str): 共享数据键名（必填），用于保存分批实现进度。示例：\"TEST_TEMPLATE_IMP_REPORT\" pre_report_file (str): 历史进度报告路径。示例：{OUT}/{DUT}/.TEST_TEMPLATE_IMP_REPORT.json timeout (int, 默认 15): 测试运行超时（秒）。 TC bug 标注规范与一致性（与文档/报告强关联）：\n术语：统一使用 “TC bug”（不再使用 “CK bug”）。 标注结构：\u003cFG-*\u003e/\u003cFC-*\u003e/\u003cCK-*\u003e/\u003cBG-NAME-XX\u003e/\u003cTC-test_file.py::[ClassName]::test_case\u003e；其中 BG 的置信度 XX 为 0–100 的整数。 失败用例与文档关系： 文档中出现的 \u003cTC-*\u003e 必须能与测试报告中的失败用例一一对应（文件名/类名/用例名匹配）。 失败的测试用例必须标注其关联检查点（CK），否则会被判定为“未标记”。 若存在失败用例未在 bug 文档中记录，将被提示为“未文档化的失败用例”。 阶段 10：代码行覆盖率分析与提升（默认跳过，可启用）\n目标：回顾未覆盖代码行，定向补齐。 怎么做： 运行 Check 获取行覆盖率；若未达标，围绕未覆盖行增补测试并回归；循环直至满足阈值。 产出：行覆盖率报告与补充测试。 通过标准：UnityChipCheckerTestCaseWithLineCoverage 达标（默认阈值 0.9，可在配置中调整）。 说明：该阶段在配置中标记 skip=true，可用 --unskip 指定索引启用。 检查器： UnityChipCheckerTestCaseWithLineCoverage 作用：在 TestCase 基础上统计行覆盖率并对比阈值。 参数： doc_func_check (str): 功能/检查点文档路径。示例：{OUT}/{DUT}_functions_and_checks.md doc_bug_analysis (str): 缺陷分析文档路径。示例：{OUT}/{DUT}_bug_analysis.md test_dir (str): 测试目录根路径。示例：{OUT}/tests cfg (dict | Config): 必填，用于推导默认路径以及环境配置。 min_line_coverage (float, 默认按配置，未配置则 0.8): 最低行覆盖率阈值。 coverage_json (str, 可选): 行覆盖率 JSON 路径。默认：uc_test_report/line_dat/code_coverage.json coverage_analysis (str, 可选): 行覆盖率分析 MD 输出。默认：unity_test/{DUT}_line_coverage_analysis.md coverage_ignore (str, 可选): 忽略文件清单。默认：unity_test/tests/{DUT}.ignore 阶段 11：验证审查与总结\n目标：沉淀成果、复盘流程、给出改进建议。 怎么做： 完善 /{DUT}_bug_analysis.md 的缺陷条目（基于源码分析）。 汇总并撰写 /{DUT}_test_summary.md，回看规划是否达成；必要时用 GotoStage 回补。 产出：\u003cOUT\u003e/{DUT}_test_summary.md 与最终结论。 通过标准：UnityChipCheckerTestCase 复核通过。 检查器： UnityChipCheckerTestCase 作用：复核整体测试结果与文档一致性，形成最终结论。 参数：doc_func_check: “{OUT}/{DUT}_functions_and_checks.md”；doc_bug_analysis: “{OUT}/{DUT}_bug_analysis.md”；test_dir: “{OUT}/tests”。 提示与最佳实践\n随时用工具：Detail/Status 查看 Mission 进度与当前阶段；CurrentTips 获取步骤级指导；Check/Complete 推进阶段。 TUI 左侧 Mission 会显示阶段序号、跳过状态与失败计数；可结合命令行 --skip/--unskip/--force-stage-index 控制推进。 定制工作流（增删阶段/子阶段） 原理说明 工作流定义在语言配置 vagent/lang/zh/config/default.yaml 的顶层 stage: 列表。 配置加载顺序：setting.yaml → ~/.ucagent/setting.yaml → 语言默认（含 stage）→ 项目根 config.yaml → CLI --override。 注意：列表类型（如 stage 列表）在合并时是“整体覆盖”，不是元素级合并；因此要“增删改”阶段，建议把默认的 stage 列表复制到你的项目 config.yaml，在此基础上编辑。 临时不执行某阶段：优先使用 CLI --skip \u003cindex\u003e 或在运行中用工具 Skip/Goto；持久跳过可在你的 config.yaml 中把该阶段条目的 skip: true 写上（同样需要提供完整的 stage 列表）。 增加阶段 需求：在“全面验证执行”之后新增一个“静态检查与 Lint 报告”阶段，要求生成 \u003cOUT\u003e/{DUT}_lint_report.md 并做格式检查。 做法：在项目根 config.yaml 中提供完整的 stage: 列表，并在合适位置插入如下条目（片段示例，仅展示新增项，实际需要放入你的完整 stage 列表里）。 stage: # ...前面的既有阶段... - name: static_lint_and_style_check desc: \"静态分析与代码风格检查报告\" task: - \"目标：完成 DUT 的静态检查/Lint，并输出报告\" - \"第1步：运行 lint 工具（按项目需要）\" - \"第2步：将结论整理为 \u003cOUT\u003e/{DUT}_lint_report.md（中文）\" - \"第3步：用 Check 校验报告是否存在且格式规范\" checker: - name: markdown_file_check clss: \"UnityChipCheckerMarkdownFileFormat\" args: markdown_file_list: \"{OUT}/{DUT}_lint_report.md\" # MD 文件路径或列表 no_line_break: true # 禁止字面量 \"\\n\" 作为换行 reference_files: [] output_files: - \"{OUT}/{DUT}_lint_report.md\" skip: false # ...后续既有阶段... 减少子阶段 场景：在“功能规格分析与测试点定义”中，临时不执行“功能点定义（FC）”子阶段。 推荐做法：运行时使用 CLI --skip 跳过该索引；若需长期配置，复制默认 stage: 列表到你的 config.yaml，在父阶段 functional_specification_analysis 的 stage: 子列表里移除对应子阶段条目，或为该子阶段加 skip: true。 子阶段移除（片段示例，仅展示父阶段结构与其子阶段列表）：\nstage: - name: functional_specification_analysis desc: \"功能规格分析与测试点定义\" task: - \"目标：将芯片功能拆解成可测试的小块，为后续测试做准备\" # ...省略父阶段任务... stage: - name: functional_grouping # 保留 FG 子阶段 # ...原有配置... # - name: function_point_definition # 原来的 FC 子阶段（此行及其内容整体删除，或在其中加 skip: true） - name: check_point_design # 保留 CK 子阶段 # ...原有配置... # ...其他字段... 小贴士\n仅需临时跳过：用 --skip/--unskip 最快，无需改配置文件。 需要永久增删：复制默认 stage: 列表到项目 config.yaml，编辑后提交到仓库；注意列表是整体覆盖，别只贴新增/删减的片段。 新增阶段的检查器可复用现有类（如 Markdown/Fixture/API/Coverage/TestCase 等），也可以扩展自定义检查器（放在 vagent/checkers/ 并以可导入路径填写到 clss）。 定制校验器（checker） 原理说明\n每个（子）阶段下的 checker: 是一个列表；执行 Check 时会依次运行该列表里的所有检查器。 配置字段： name: 该检查器在阶段内的标识（便于阅读/日志） clss: 检查器类名；短名默认从 vagent.checkers 命名空间导入，也可写完整模块路径（如 mypkg.mychk.MyChecker） args: 传给检查器构造函数的参数，支持模板变量（如 {OUT}、{DUT}） extra_args: 可选，部分检查器支持自定义提示/策略（如 fail_msg、batch_size、pre_report_file 等） 解析与实例化：vagent/stage/vstage.py 会读取 checker:，按 clss/args 生成实例；运行期由 ToolStdCheck/Check 调用其 do_check()。 合并语义：配置合并时列表是“整体替换”，要在项目 config.yaml 修改某个阶段的 checker:，建议复制该阶段条目并完整替换其 checker: 列表。 增加 checker 在“功能规格分析与测试点定义”父阶段，新增“文档格式检查”，确保 {OUT}/{DUT}_functions_and_checks.md 没有把换行写成字面量 \\n。\n# 片段示例：需要放入你的完整 stage 列表对应阶段中 - name: functional_specification_analysis desc: \"功能规格分析与测试点定义\" # ...existing fields... output_files: - \"{OUT}/{DUT}_functions_and_checks.md\" checker: - name: functions_and_checks_doc_format clss: \"UnityChipCheckerMarkdownFileFormat\" args: markdown_file_list: \"{OUT}/{DUT}_functions_and_checks.md\" # 功能/检查点文档 no_line_break: true # 禁止字面量 \"\\n\" stage: # ...子阶段 FG/FC/CK 原有配置... （可扩展）自定义检查器（最小实现，放在 vagent/checkers/unity_test.py）\n很多场景下“增加的 checker”并非复用已有检查器，而是需要自己实现一个新的检查器。最小实现步骤：\n新建类并继承基类 vagent.checkers.base.Checker 在 __init__ 里声明你需要的参数（与 YAML args 对应） 实现 do_check(self, timeout=0, **kw) -\u003e tuple[bool, object]，返回 (是否通过, 结构化消息) 如需读/写工作区文件，使用 self.get_path(rel) 获取绝对路径；如需跨阶段共享数据，使用 self.smanager_set_value/get_value 若想用短名 clss 引用，请在 vagent/checkers/__init__.py 导出该类（或在 clss 写完整模块路径） 最小代码骨架（示例）：\n# 文件：vagent/checkers/unity_test.py from typing import Tuple import os from vagent.checkers.base import Checker class UnityChipCheckerMyCustomCheck(Checker): def __init__(self, target_file: str, threshold: int = 1, **kw): self.target_file = target_file self.threshold = threshold def do_check(self, timeout=0, **kw) -\u003e Tuple[bool, object]: \"\"\"检查 target_file 是否存在并做简单规则校验。\"\"\" real = self.get_path(self.target_file) if not os.path.exists(real): return False, {\"error\": f\"file '{self.target_file}' not found\"} # TODO: 这里写你的具体校验逻辑，例如统计/解析/比对等 return True, {\"message\": \"MyCustomCheck passed\"} 在阶段 YAML 中引用（与“增加一个 checker”一致）：\nchecker: - name: my_custom_check clss: \"UnityChipCheckerMyCustomCheck\" # 若未在 __init__.py 导出，写完整路径 mypkg.mychk.UnityChipCheckerMyCustomCheck args: target_file: \"{OUT}/{DUT}_something.py\" threshold: 2 extra_args: fail_msg: \"未满足自定义阈值，请完善实现或调低阈值。\" # 可选：通过 extra_args 自定义默认失败提示 进阶提示（按需）：\n长时任务/外部进程：在运行子进程时调用 self.set_check_process(p, timeout)，即可用工具 KillCheck/StdCheck 管理与查看进程输出。 模板渲染：实现 get_template_data() 可将进度/统计渲染到阶段标题与任务文案中。 初始化钩子：实现 on_init() 以在阶段开始时加载缓存/准备批任务（与 Batch 系列 checker 一致）。 删除 checker 如临时不要“第 2 阶段 基本信息文档格式检查”，将该阶段的 checker: 置空或移除该项：\n- name: dut_function_understanding desc: \"{DUT}功能理解\" # ...existing fields... checker: [] # 删除原本的 markdown_file_check 修改 checker 把“行覆盖率检查”的阈值从 0.9 调整到 0.8，并自定义失败提示：\n- name: line_coverage_analysis_and_improvement desc: \"代码行覆盖率分析与提升{COVERAGE_COMPLETE}\" # ...existing fields... checker: - name: line_coverage_check clss: \"UnityChipCheckerTestCaseWithLineCoverage\" args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" min_line_coverage: 0.8 # 调低阈值 extra_args: fail_msg: \"未达到 80% 的行覆盖率，请补充针对未覆盖行的测试。\" 可选：自定义检查器类\n在 vagent/checkers/ 新增类，继承 vagent.checkers.base.Checker 并实现 do_check()； 在 vagent/checkers/__init__.py 导出类后，可在 clss 用短名；或直接写完整模块路径； args 中的字符串支持模板变量渲染；extra_args 可用于自定义提示文案（具体视检查器实现而定）。 常用 checker 参数（结构化） 以下参数均来自实际代码实现（vagent/checkers/unity_test.py），名称、默认值与类型与代码保持一致；示例片段可直接放入阶段 YAML 的 checker[].args。\nUnityChipCheckerMarkdownFileFormat 参数： markdown_file_list (str | List[str]): 要检查的 Markdown 文件路径或路径列表。 no_line_break (bool, 默认 false): 是否禁止把换行写成字面量 “\\n”；true 表示禁止。 示例： args: markdown_file_list: \"{OUT}/{DUT}_basic_info.md\" no_line_break: true UnityChipCheckerLabelStructure 参数： doc_file (str) leaf_node (“FG”|“FC”|“CK”) min_count (int, 默认 1) must_have_prefix (str, 默认 “FG-API”) data_key (str, 可选) 示例： args: doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" leaf_node: \"CK\" data_key: \"COVER_GROUP_DOC_CK_LIST\" UnityChipCheckerDutCreation 参数： target_file (str) 示例： args: target_file: \"{OUT}/tests/{DUT}_api.py\" UnityChipCheckerDutFixture 参数： target_file (str) 示例： args: target_file: \"{OUT}/tests/{DUT}_api.py\" UnityChipCheckerEnvFixture 参数： target_file (str) min_env (int, 默认 1) force_bundle (bool, 当前未使用) 示例： args: target_file: \"{OUT}/tests/{DUT}_api.py\" min_env: 1 UnityChipCheckerDutApi 参数： api_prefix (str) target_file (str) min_apis (int, 默认 1) 示例： args: api_prefix: \"api_{DUT}_\" target_file: \"{OUT}/tests/{DUT}_api.py\" min_apis: 1 UnityChipCheckerCoverageGroup 参数： test_dir (str) cov_file (str) doc_file (str) check_types (str|List[str]) 示例： args: test_dir: \"{OUT}/tests\" cov_file: \"{OUT}/tests/{DUT}_function_coverage_def.py\" doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" check_types: [\"FG\", \"FC\", \"CK\"] UnityChipCheckerCoverageGroupBatchImplementation 参数： test_dir (str) cov_file (str) doc_file (str) batch_size (int) data_key (str) 示例： args: test_dir: \"{OUT}/tests\" cov_file: \"{OUT}/tests/{DUT}_function_coverage_def.py\" doc_file: \"{OUT}/{DUT}_functions_and_checks.md\" batch_size: 20 data_key: \"COVER_GROUP_DOC_CK_LIST\" UnityChipCheckerTestTemplate 基类参数：doc_func_check (str), test_dir (str), min_tests (int, 默认 1), timeout (int, 默认 15) 扩展参数（extra_args）：batch_size (默认 20), ignore_ck_prefix (str), data_key (str) 示例： args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" test_dir: \"{OUT}/tests\" ignore_ck_prefix: \"test_api_{DUT}_\" data_key: \"TEST_TEMPLATE_IMP_REPORT\" batch_size: 20 UnityChipCheckerDutApiTest 参数： api_prefix (str) target_file_api (str) target_file_tests (str) doc_func_check (str) doc_bug_analysis (str) min_tests (int, 默认 1) timeout (int, 默认 15) 示例： args: api_prefix: \"api_{DUT}_\" target_file_api: \"{OUT}/tests/{DUT}_api.py\" target_file_tests: \"{OUT}/tests/test_{DUT}_api*.py\" doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" UnityChipCheckerBatchTestsImplementation 基类参数：doc_func_check (str), test_dir (str), doc_bug_analysis (str), ignore_ck_prefix (str), timeout (int, 默认 15) 进度参数：data_key (str, 必填) 扩展参数（extra_args）：batch_size (默认 5), pre_report_file (str) 示例： args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" ignore_ck_prefix: \"test_api_{DUT}_\" batch_size: 10 data_key: \"TEST_TEMPLATE_IMP_REPORT\" pre_report_file: \"{OUT}/{DUT}/.TEST_TEMPLATE_IMP_REPORT.json\" UnityChipCheckerTestCase 参数： doc_func_check (str) doc_bug_analysis (str) test_dir (str) min_tests (int, 默认 1) timeout (int, 默认 15) 示例： args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" UnityChipCheckerTestCaseWithLineCoverage 基础参数同 UnityChipCheckerTestCase 额外必需：cfg (dict|Config) 额外可选（extra_args）： min_line_coverage (float, 默认按配置，未配置则 0.8) coverage_json (str, 默认 uc_test_report/line_dat/code_coverage.json) coverage_analysis (str, 默认 unity_test/{DUT}_line_coverage_analysis.md) coverage_ignore (str, 默认 unity_test/tests/{DUT}.ignore) 示例： args: doc_func_check: \"{OUT}/{DUT}_functions_and_checks.md\" doc_bug_analysis: \"{OUT}/{DUT}_bug_analysis.md\" test_dir: \"{OUT}/tests\" cfg: \"\u003cCONFIG_OBJECT_OR_DICT\u003e\" min_line_coverage: 0.9 提示：上面的 示例 仅展示 args 片段；实际需置于阶段条目下的 checker[].args。\n","categories":["教程"],"description":"整体的工作流说明。","excerpt":"整体的工作流说明。","ref":"/mlvp/docs/ucagent/workflow/","tags":["docs"],"title":"工作流"},{"body":"UCAgent 支持在验证过程中进行人机协同，允许用户暂停 AI 执行，人工干预验证过程，然后继续 AI 执行。这种模式适用于需要精细控制或复杂决策的场景。\n协同流程：\n暂停 AI 执行：\n在直接接入 LLM 模式下：按 Ctrl+C 暂停。 在 Code Agent 协同模式下：根据 Agent 的暂停方式（如 Gemini-cli 使用 Esc）暂停。 人工干预：\n手动编辑文件、测试用例或配置。 使用交互命令进行调试或调整。 阶段控制：\n使用 tool_invoke Check 检查当前阶段状态。 使用 tool_invoke Complete 标记阶段完成并进入下一阶段。 继续执行：\n使用 loop [prompt] 命令继续 AI 执行，并可提供额外的提示信息。 在 Code Agent 模式下，通过 Agent 的控制台输入提示。 权限管理：\n可使用 add_un_write_path，del_un_write_path 等命令设置文件写权限，控制 AI 是否可以编辑特定文件。 适用于直接接入 LLM 或强制使用 UCAgent 文件工具。 ","categories":["教程"],"description":"如何与AI配合来验证模块。","excerpt":"如何与AI配合来验证模块。","ref":"/mlvp/docs/ucagent/usage/assit/","tags":["docs"],"title":"人机协同验证"},{"body":"","categories":["Example Projects","Tutorials"],"description":"Complex case studies completed using the open verification platform.","excerpt":"Complex case studies completed using the open verification platform.","ref":"/mlvp/en/docs/advance_case/","tags":["examples","docs"],"title":"Advanced Case Studies"},{"body":"RTL Source Code In this example, we drive a random number generator, with the source code as follows:\nmodule RandomGenerator ( input wire clk, input wire reset, input [15:0] seed, output [15:0] random_number ); reg [15:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[14:0], lfsr[15] ^ lfsr[14]}; end end assign random_number = lfsr; endmodule This random number generator contains a 16-bit LFSR, with a 16-bit seed as input and a 16-bit random number as output. The LFSR is updated according to the following rules:\nXOR the highest bit and the second-highest bit of the current LFSR to generate a new_bit.\nShift the original LFSR left by one bit, and place new_bit in the lowest bit.\nDiscard the highest bit.\nTesting Process During testing, we will create a folder named RandomGenerator, which contains a RandomGenerator.v file. The content of this file is the RTL source code mentioned above.\nBuilding the RTL into a Python Module Generating Intermediate Files Navigate to the RandomGenerator folder and execute the following command:\npicker export --autobuild=false RandomGenerator.v -w RandomGenerator.fst --sname RandomGenerator --tdir picker_out_rmg --lang python -e --sim verilator This command does the following:\nUses RandomGenerator.v as the top file and RandomGenerator as the top module, generating a dynamic library with the Verilator simulator, targeting Python as the output language.\nEnables waveform output, with the target waveform file being RandomGenerator.fst.\nIncludes files for driving the example project (-e), and does not automatically compile after code generation (-autobuild=false).\nOutputs the final files to the picker_out_rmg directory. The output directory structure is similar to Adder Verification - Generating Intermediate Files , so it will not be elaborated here.\nBuilding Intermediate Files Navigate to the picker_out_rmg directory and execute the make command to generate the final files.\nNote: The compilation process is similar to Adder Verification - Compilation Process , so it will not be elaborated here. The final directory structure will be:\npicker_out_rmg |-- RandomGenerator.fst # Waveform file from the test |-- UT_RandomGenerator | |-- RandomGenerator.fst.hier | |-- _UT_RandomGenerator.so # Swig-generated wrapper dynamic library | |-- __init__.py # Initialization file for the Python module, also the library definition file | |-- libDPIRandomGenerator.a # Library file generated by the simulator | |-- libUTRandomGenerator.so # libDPI dynamic library wrapper generated based on dut_base | `-- libUT_RandomGenerator.py # Python module generated by Swig | `-- xspcomm # xspcomm base library, fixed folder, no need to pay attention to it `-- example.py # Example code Configuring the Test Code Replace the content of example.py with the following code.\nfrom RandomGenerator import * import random # Define the reference model class LFSR_16: def __init__(self, seed): self.state = seed \u0026 ((1 \u003c\u003c 16) - 1) def Step(self): new_bit = (self.state \u003e\u003e 15) ^ (self.state \u003e\u003e 14) \u0026 1 self.state = ((self.state \u003c\u003c 1) | new_bit ) \u0026 ((1 \u003c\u003c 16) - 1) if __name__ == \"__main__\": dut = DUTRandomGenerator() # Create the DUT dut.InitClock(\"clk\") # Specify the clock pin and initialize the clock seed = random.randint(0, 2**16 - 1) # Generate a random seed dut.seed.value = seed # Set the DUT seed ref = LFSR_16(seed) # Create a reference model for comparison # Reset the DUT dut.reset.value = 1 # Set reset signal to 1 dut.Step() # Advance one clock cycle (DUTRandomGenerator is a sequential circuit, it requires advancing via Step) dut.reset.value = 0 # Set reset signal to 0 dut.Step() # Advance one clock cycle for i in range(65536): # Loop 65536 times dut.Step() # Advance one clock cycle for the DUT, generating a random number ref.Step() # Advance one clock cycle for the reference model, generating a random number assert dut.random_number.value == ref.state, \"Mismatch\" # Compare the random numbers generated by the DUT and the reference model print(f\"Cycle {i}, DUT: {dut.random_number.value:x}, REF: {ref.state:x}\") # Print the results # Complete the test print(\"Test Passed\") dut.Finish() # Finish function will complete the writing of waveform, coverage, and other files Running the Test Program Execute python example.py in the picker_out_rmg directory to run the test program. After the execution, if Test Passed is output, the test is considered passed. After the run is complete, a waveform file RandomGenerator.fst will be generated, which can be viewed in the terminal using the following command:\ngtkwave RandomGenerator.fst Example output:\n··· Cycle 65529, DUT: d9ea, REF: d9ea Cycle 65530, DUT: b3d4, REF: b3d4 Cycle 65531, DUT: 67a9, REF: 67a9 Cycle 65532, DUT: cf53, REF: cf53 Cycle 65533, DUT: 9ea6, REF: 9ea6 Cycle 65534, DUT: 3d4d, REF: 3d4d Cycle 65535, DUT: 7a9a, REF: 7a9a Test Passed, destroy UT_RandomGenerator ","categories":["Example Projects","Tutorials"],"description":"Demonstrating the tool usage with a 16-bit LFSR random number generator, which includes a clock signal, sequential logic, and registers.","excerpt":"Demonstrating the tool usage with a 16-bit LFSR random number …","ref":"/mlvp/en/docs/quick-start/eg-rmg/","tags":["examples","docs"],"title":"Case 2: Random Number Generator"},{"body":" The Picker tool supports generating code line coverage reports, and the MLVP（https://github.com/XS-MLVP/mlvp）project supports generating functional coverage reports.\nCode Line Coverage Currently, the Picker tool supports generating code line coverage reports based on the Verilator simulator.\nVerilator The Verilator simulator provides coverage support. The implementation is as follows:\nUse the verilator_coverage tool to process or merge coverage databases, ultimately generating a coverage.info file for multiple DUTs. Use the genhtml command of the lcov tool based on coverage.info and RTL code source files to generate a complete code coverage report. The process is as follows:\nEnable the COVERAGE feature when generating the DUT with Picker (add the -c option). After the simulator runs, a coverage database file V{DUT_NAME}.dat will be generated after dut.Finish() is called. Use the write-info function of verilator_coverage to convert it to a .info file. Use the genhtml function of lcov to generate an HTML report using the .info file and the RTL source files specified in the file. Note: The RTL source files specified in the file refer to the source file paths used when generating the DUT, and these paths need to be valid in the current environment. In simple terms, all .sv/.v files used for compilation need to exist in the current environment, and the directory remains unchanged.\nverilator_coverage The verilator_coverage tool is used to process coverage data generated by the DUT after running .dat files. The tool can process and merge multiple .dat files and has two main functions:\nGenerate a .info file based on the .dat file for subsequent generation of a web page report.\n-annotate \u003coutput_dir\u003e：Present the coverage situation in the source file in annotated form, and save the result to output_dir. The format is as follows:\n100000 input logic a; // Begins with whitespace, because // number of hits (100000) is above the limit. %000000 input logic b; // Begins with %, because // number of hits (0) is below the limit. -annotate-min \u003ccount\u003e：Specify the limit as count for the above.\nCombine the .dat file with the source code file, and write the coverage data in annotated form into the specified directory.\n-write \u003cmerged-datafile\u003e -read \u003cdatafiles\u003e：Merge several .dat (datafiles) files into one .dat file. -write-info \u003cmerged-info\u003e -read \u003cdatafiles\u003e：Merge several .dat (datafiles) files into one .info file. genhtml The genhtml provided by the lcov package can export a more readable HTML report from the .info file. The command format is: genhtml [OPTIONS] \u003cinfofiles\u003e. It is recommended to use the -o \u003coutputdir\u003e option to output the results to a specified directory.\nFor example, in theAddr project. Usage Example If you enable the -c option when using Picker, after the simulation ends, a V{DUT_NAME}.dat file will be generated. And there will be a Makefile in the top-level directory, which contains the command to generate the coverage report.\nThe command is as follows:\ncoverage: ... verilator_coverage -write-info coverage.info ./${TARGET}/V${PROJECT}_coverage.dat genhtml coverage.info --output-directory coverage ... Enter make coverage in the shell, which will generate coverage.info based on the generated .dat file and then use genhtml to generate an html report in the coverage directory.\nVCS Documentation for VCS is currently being finalized.\n","categories":["Sample Projects","Tutorials"],"description":"Coverage tools","excerpt":"Coverage tools","ref":"/mlvp/en/docs/env_usage/coverage/","tags":["examples","docs"],"title":"Coverage Statistics"},{"body":"Add Tools and MCP Server Tools For advanced users who can modify this repository code, the following explains how to:\nAdd a new tool (for local / in‑agent invocation) Expose the tool as an MCP Server tool (for external IDE / client invocation) Control which tools are exposed and how they are invoked Key locations involved:\nvagent/tools/uctool.py: tool base class UCTool, to_fastmcp (LangChain Tool → FastMCP Tool) vagent/util/functions.py: import_and_instance_tools (import \u0026 instantiate by name), create_verify_mcps (start FastMCP) vagent/verify_agent.py: assemble tool list, start_mcps to combine and launch server vagent/cli.py / vagent/verify_pdb.py: CLI and TUI MCP start commands 1) Tool System and Assembly Tool base class UCTool: Inherits LangChain BaseTool; built‑in: call_count, call_time_out, streaming / blocking tips, MCP Context injection (ctx.info), re‑entry prevention, etc. It is recommended to make custom tools inherit UCTool to obtain better MCP behavior and debugging experience. Runtime assembly (during VerifyAgent initialization): Basic tools: RoleInfo, ReadTextFile Embed tools: reference retrieval \u0026 memory (unless --no-embed-tools) File tools: read / write / search / path etc. (can be removed in MCP no‑file tools mode) Stage tools: dynamically provided by StageManager according to workflow External tools: from config item ex_tools and CLI --ex-tools (instantiated with zero parameters via import_and_instance_tools) Name resolution: Short name: class / factory function must be exported in vagent/tools/__init__.py (e.g. from .mytool import HelloTool), then you can write HelloTool in ex_tools Full path: mypkg.mytools.HelloTool / mypkg.mytools.Factory 2) Add a New Tool (Local / In‑Agent) Specification requirements:\nUnique name, clear description Use pydantic BaseModel to define args_schema (MCP conversion depends on it) Implement _run (sync) or _arun (async); inheriting UCTool gives timeout, streaming and ctx injection automatically Example 1: synchronous tool (counted greeting)\nfrom pydantic import BaseModel, Field from vagent.tools.uctool import UCTool class HelloArgs(BaseModel): who: str = Field(..., description=\"Person to greet\") class HelloTool(UCTool): name: str = \"Hello\" description: str = \"Greet a target and count calls\" args_schema = HelloArgs def _run(self, who: str, run_manager=None) -\u003e str: return f\"Hello, {who}! (called {self.call_count+1} times)\" Register \u0026 use:\nTemporary: --ex-tools mypkg.mytools.HelloTool Persistent: project config.yaml ex_tools: - mypkg.mytools.HelloTool (Optional) short name registration: export HelloTool in vagent/tools/__init__.py, then you can write --ex-tools HelloTool.\nExample 2: asynchronous streaming tool (ctx.info + timeout)\nfrom pydantic import BaseModel, Field from vagent.tools.uctool import UCTool import asyncio class ProgressArgs(BaseModel): steps: int = Field(5, ge=1, le=20, description=\"Number of progress steps\") class ProgressTool(UCTool): name: str = \"Progress\" description: str = \"Demonstrate streaming output and timeout handling\" args_schema = ProgressArgs async def _arun(self, steps: int, run_manager=None): for i in range(steps): self.put_alive_data(f\"step {i+1}/{steps}\") # for blocking prompt / log buffer await asyncio.sleep(0.5) return \"done\" Explanation: UCTool.ainvoke will inject ctx in MCP mode and start a blocking prompt thread; when sync_block_log_to_client=True it periodically pushes logs via ctx.info, on timeout returns error plus buffered logs.\n3) Expose as MCP Server Tools Tool → MCP conversion (vagent/tools/uctool.py::to_fastmcp):\nRequired: args_schema inherits BaseModel; “injected parameter” signatures are not supported. UCTool subclasses get FastMCP tools with context_kwarg=\"ctx\" and streaming interaction capability. Server side startup:\nVerifyAgent.start_mcps combines tools: tool_list_base + tool_list_task + tool_list_ext + [tool_list_file] vagent/util/functions.py::create_verify_mcps converts tool sequence into FastMCP tools and starts uvicorn (mcp.streamable_http_app()). How to choose exposure scope:\nCLI: Start (with file tools): --mcp-server Start (without file tools): --mcp-server-no-file-tools Host: --mcp-server-host, Port: --mcp-server-port TUI commands: start_mcp_server [host] [port] / start_mcp_server_no_file_ops [host] [port] 4) Client Call Flow FastMCP Python client (see tests/test_mcps.py):\nfrom fastmcp import Client client = Client(\"http://127.0.0.1:5000/mcp\", timeout=10) print(client.list_tools()) print(client.call_tool(\"Hello\", {\"who\": \"UCAgent\"})) IDE / Agent (Claude Code, Copilot, Qwen Code, etc.): set httpUrl to http://\u003chost\u003e:\u003cport\u003e/mcp to discover and call tools.\n5) Lifecycle, Concurrency and Timeout Counting: UCTool has call_count; non‑UCTool tools are wrapped with counting by import_and_instance_tools. Concurrency protection: is_in_streaming / is_alive_loop prevent re‑entry; the same instance disallows concurrent execution. Timeout: call_time_out (default 20s) + client timeout; when blocking can use put_alive_data + sync_block_log_to_client=True to push heartbeat. 6) Configuration Strategy and Best Practices ex_tools list is a “whole overwrite”; project config.yaml must write full list. Short name vs full path: short name is more convenient; full path applies when private package without modifying this repo. No‑arg constructor / factory: assembler directly calls (...)(); complex configuration should be handled inside factory (read env / config file). File write permission: in MCP no‑file tools mode do not expose write‑type tools; if writing is needed, use inside local agent or explicitly allow write directory. Inject External Tools via Environment Variable (EX_TOOLS) Configuration files support Bash style environment variable placeholder: $(VAR: default). You can let ex_tools inject a list of tool classes from env (supports full module name or short name under vagent.tools).\nIn project config.yaml or user ~/.ucagent/setting.yaml write: ex_tools: $(EX_TOOLS: []) Provide list via environment variable (must be YAML parsable array literal): export EX_TOOLS='[\"SqThink\",\"HumanHelp\"]' # Or full class path: # export EX_TOOLS='[\"vagent.tools.extool.SqThink\",\"vagent.tools.human.HumanHelp\"]' After startup these tools appear in local dialog and MCP Server. Short name needs export in vagent/tools/__init__.py; otherwise use full module path.\nCombined with CLI --ex-tools option (both sides assembled).\n7) Common Issue Troubleshooting Tool not in MCP list: not assembled (ex_tools not configured / not exported), args_schema not BaseModel, server not started as expected. Call reports “injected parameter not supported”: tool definition includes LangChain injected args; change to explicit args_schema parameters. Timeout: increase call_time_out or client timeout; in long tasks output progress to maintain heartbeat. Short name invalid: not exported in vagent/tools/__init__.py; use full path or export it. After completing the above steps: your tool can be automatically invoked by ReAct locally, and can also be exposed via MCP Server for unified invocation by external IDE / clients.\n","categories":["Tutorial"],"description":"How to define parameters, workflow and tools.","excerpt":"How to define parameters, workflow and tools.","ref":"/mlvp/en/docs/ucagent/customize/","tags":["docs"],"title":"Customize"},{"body":"Arguments and Options Use UCAgent:\nucagent \u003cworkspace\u003e \u003cdut_name\u003e {options} Inputs workspace: working directory: workspace/\u003cDUT_DIR\u003e: device under test (DUT), i.e., the Python package \u003cDUT_DIR\u003e exported by the picker; e.g., Adder workspace/\u003cDUT_DIR\u003e/README.md: natural‑language description of verification requirements and goals for this DUT workspace/\u003cDUT_DIR\u003e/*.md: other reference documents workspace/\u003cDUT_DIR\u003e/*.v/sv/scala: source files used for bug analysis Other verification‑related files (e.g., provided test cases, requirement specs, etc.) dut_name: the name of the DUT, i.e., \u003cDUT_DIR\u003e, for example: Adder Outputs workspace: working directory: workspace/Guide_Doc: guidance and documents followed during the verification process workspace/uc_test_report: generated Toffee‑test report workspace/unity_test/tests: auto‑generated test cases workspace/*.md: generated docs including bug analysis, checkpoints, verification plan, and conclusion See also the detailed outputs in Introduction.\nPositional Arguments Argument Required Description Example workspace Yes Working directory ./output dut Yes DUT name (subdirectory under workspace) Adder Execution and Interaction Option Short Values/Type Default Description –stream-output -s flag off Stream output to console –human -hm flag off Enter human input/breakpoint mode on start –interaction-mode -im standard/enhanced/advanced standard Interaction mode; enhanced includes planning and memory mgmt, advanced adds adaptive strategy –tui flag off Enable terminal TUI –loop -l flag off Enter main loop immediately after start (with –loop-msg); for direct mode –loop-msg str empty First message injected when entering loop –seed int random Random seed (auto random if unspecified) –sys-tips str empty Override system prompt Config and Templates Option Short Values/Type Default Description –config path none Config file path, e.g., --config config.yaml –template-dir path none Custom template directory –template-overwrite flag no Allow overwriting existing files when rendering templates into workspace –output dir unity_test Output directory name –override A.B.C=VALUE[,X.Y=VAL2,…] none Override config with dot‑path assignments; strings need quotes, others parsed as Python literals –gen-instruct-file -gif file none Generate an external Agent guide file under workspace (overwrite if exists) –guid-doc-path path none Use a custom Guide_Doc directory (default uses internal copy) Planning and ToDo Option Short Values/Type Default Description –force-todo -fp flag no Enable ToDo tools in standard mode and include ToDo info in each round –use-todo-tools -utt flag no Enable ToDo‑related tools (not limited to standard mode) ToDo Tools Overview \u0026 Examples Note: ToDo tools are for enhancing model planning; users can define the model’s ToDo list. This feature requires strong model capability and is disabled by default.\nEnabling: use --use-todo-tools in any mode; or in standard mode use --force-todo to force enable and include ToDo info in each round.\nConventions and limits: step indices are 1‑based; number of steps must be 2–20; length of notes and each step text ≤ 100; exceeding limits will be rejected with an error string.\nTool overview\nClass Call Name Main Function Parameters Return Key Constraints/Behavior CreateToDo CreateToDo Create current ToDo (overwrite) task_description: str; steps: List[str] Success msg + summary Validate step count/length; write then return summary CompleteToDoSteps CompleteToDoSteps Mark steps as completed, with note completed_steps: List[int]=[]; notes: str=\"\" Success (count) + summary Only affects incomplete steps; prompt to create if none; ignore out‑of‑range UndoToDoSteps UndoToDoSteps Undo completion status, with note steps: List[int]=[]; notes: str=\"\" Success (count) + summary Only affects completed steps; prompt to create if none; ignore out‑of‑range ResetToDo ResetToDo Reset/clear current ToDo none Reset success msg Clear steps and notes; can recreate afterwards GetToDoSummary GetToDoSummary Get current ToDo summary none Summary / no‑ToDo prompt Read‑only, no state change ToDoState ToDoState Get status phrase (kanban/status) none Status description Dynamic display: no ToDo/completed/progress stats, etc. Invocation examples (MCP/internal tool call, JSON args):\n{ \"tool\": \"CreateToDo\", \"args\": { \"task_description\": \"Complete verification closure for Adder core functions\", \"steps\": [ \"Read README and spec, summarize features\", \"Define checkpoints and pass criteria\", \"Generate initial unit tests\", \"Run and fix failing tests\", \"Fill coverage and output report\" ] } } { \"tool\": \"CompleteToDoSteps\", \"args\": { \"completed_steps\": [1, 2], \"notes\": \"Initial issues resolved, ready to add tests\" } } { \"tool\": \"UndoToDoSteps\", \"args\": { \"steps\": [2], \"notes\": \"Step 2 needs checkpoint tweaks\" } } { \"tool\": \"ResetToDo\", \"args\": {} } { \"tool\": \"GetToDoSummary\", \"args\": {} } { \"tool\": \"ToDoState\", \"args\": {} } External and Embedding Tools Option Short Values/Type Default Description –ex-tools name1[,name2…] none Comma‑separated external tool class names (e.g., SqThink) –no-embed-tools flag no Disable built‑in retrieval/memory embedding tools Logging Option Short Values/Type Default Description –log flag no Enable logging –log-file path auto Log output file (use default if unspecified) –msg-file path auto Message log file (use default if unspecified) MCP Server Option Short Values/Type Default Description –mcp-server flag no Start MCP server (with file tools) –mcp-server-no-file-tools flag no Start MCP server (without file tools) –mcp-server-host host 127.0.0.1 Server listen address –mcp-server-port int 5000 Server port Stage Control and Safety Option Short Values/Type Default Description –force-stage-index int 0 Force start from specified stage index –skip int (repeatable) [] Skip specified stage index; can be provided multiple times –unskip int (repeatable) [] Unskip specified stage index; can be provided multiple times –no-write / –nw path1 path2 … none Restrict writable targets; must exist within workspace Version and Check Option Short Values/Type Default Description –check flag no Check default config, lang directories, templates, and Guide_Doc then exit –version flag Print version and exit Example python3 ucagent.py ./output Adder \\ \\ -s \\ -hm \\ -im enhanced \\ --tui \\ -l \\ --loop-msg 'start verification' \\ --seed 12345 \\ --sys-tips '按规范完成Adder的验证' \\ \\ --config config.yaml \\ --template-dir ./templates \\ --template-overwrite \\ --output unity_test \\ --override 'conversation_summary.max_tokens=16384,conversation_summary.max_summary_tokens=2048,conversation_summary.use_uc_mode=True,lang=\"zh\",openai.model_name=\"gpt-4o-mini\"' \\ --gen-instruct-file GEMINI.md \\ --guid-doc-path ./output/Guide_Doc \\ \\ --use-todo-tools \\ \\ --ex-tools 'SqThink,AnotherTool' \\ --no-embed-tools \\ \\ --log \\ --log-file ./output/ucagent.log \\ --msg-file ./output/ucagent.msg \\ \\ --mcp-server-no-file-tools \\ --mcp-server-host 127.0.0.1 \\ --mcp-server-port 5000 \\ \\ --force-stage-index 2 \\ --skip 5 --skip 7 \\ --unskip 6 \\ --nw ./output/Adder ./output/unity_test Positional arguments ./output: workspace working directory Adder: dut subdirectory name Execution and interaction -s: stream output -hm: human intervention on start -im enhanced: enhanced interaction mode (with planning and memory) –tui: enable TUI -l: enter loop immediately after start –loop/–loop-msg: inject first message when entering loop –seed 12345: fix random seed –sys-tips: custom system prompt Config and templates –config config.yaml: load project config from config.yaml –template-dir ./templates: set template directory to ./templates –template-overwrite: allow overwrite when rendering templates –output unity_test: output directory name unity_test –override ‘…’: override config keys (dot‑path=value, multiple comma‑separated; string values require inner quotes and wrap the whole with single quotes); the example sets conversation summary limits, enables trimming, sets doc language to Chinese, and model name to gpt‑4o‑mini -gif/–gen-instruct-file GEMINI.md: generate external collaboration guide at \u003cworkspace\u003e/GEMINI.md –guid-doc-path ./output/Guide_Doc: customize Guide_Doc directory as ./output/Guide_Doc Planning and ToDo –use-todo-tools: enable ToDo tools and force attaching ToDo info External and embedding tools –ex-tools ‘SqThink,AnotherTool’: enable external tools SqThink,AnotherTool –no-embed-tools: disable built‑in retrieval/memory tools Logging –log: enable log file –log-file ./output/ucagent.log: set log output file to ./output/ucagent.log –msg-file ./output/ucagent.msg: set message log file to ./output/ucagent.msg MCP Server –mcp-server-no-file-tools: start MCP (without file tools) –mcp-server-host: server listen address 127.0.0.1 –mcp-server-port: server listen port 5000 Stage control and safety –force-stage-index 2: start from stage index 2 –skip 5 –skip 7: skip stage 5 and stage 7 –unskip 7: unskip stage 7 –nw ./output/Adder ./output/unity_test: restrict writable paths to ./output/Adder and ./output/unity_test Notes –check and –version exit immediately, not combined with run –mcp-server and –mcp-server-no-file-tools are mutually exclusive; here we choose the latter. Path arguments (e.g., –template-dir/–guid-doc-path/–nw) must exist, otherwise an error occurs String values in –override must be quoted, and wrap the whole argument in single quotes to prevent the shell from consuming the inner quotes (the example already does this) ","categories":["Tutorial"],"description":"Explanation of CLI arguments and flags.","excerpt":"Explanation of CLI arguments and flags.","ref":"/mlvp/en/docs/ucagent/usage/option/","tags":["docs"],"title":"Options"},{"body":"RTL源码 在本案例中，我们驱动一个随机数生成器，其源码如下：\nmodule RandomGenerator ( input wire clk, input wire reset, input [15:0] seed, output [15:0] random_number ); reg [15:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[14:0], lfsr[15] ^ lfsr[14]}; end end assign random_number = lfsr; endmodule 该随机数生成器包含一个 16 位的 LFSR，其输入为一个 16 位的种子数，输出为一个 16 位的随机数。LFSR 的更新规则为：\n将当前的 LFSR 的最高位与次高位异或，称为new_bit。 将原来的 LFSR 向左平移一位，将 new_bit 放在最低位。 丢弃最高位。 测试过程 在测试过程中，我们将创建一个名为 RandomGenerator 的文件夹，其中包含一个 RandomGenerator.v 文件。该文件内容即为上述的 RTL 源码。\n将RTL构建为 Python Module 生成中间文件 进入 RandomGenerator 文件夹，执行如下命令：\npicker export --autobuild=false RandomGenerator.v -w RandomGenerator.fst --sname RandomGenerator --tdir picker_out_rmg/ --lang python -e --sim verilator 该命令的含义是：\n将RandomGenerator.v作为 Top 文件，并将RandomGenerator作为 Top Module，基于 verilator 仿真器生成动态库，生成目标语言为 Python。 启用波形输出，目标波形文件为RandomGenerator.fst 包含用于驱动示例项目的文件(-e)，同时codegen完成后不自动编译(-autobuild=false)。 最终的文件输出路径是 picker_out_rmg 输出的目录类似加法器验证-生成中间文件，这里不再赘述。\n构建中间文件 进入 picker_out_rmg 目录并执行 make 命令，即可生成最终的文件。\n备注：其编译过程类似于 加法器验证-编译流程，这里不再赘述。\n最终目录结果为：\npicker_out_rmg └── RandomGenerator |-- RandomGenerator.fst # 测试的波形文件 |-- UT_RandomGenerator | |-- RandomGenerator.fst.hier | |-- _UT_RandomGenerator.so # Swig生成的wrapper动态库 | |-- __init__.py # Python Module的初始化文件，也是库的定义文件 | |-- libDPIRandomGenerator.a # 仿真器生成的库文件 | |-- libUTRandomGenerator.so # 基于dut_base生成的libDPI动态库封装 | `-- libUT_RandomGenerator.py # Swig生成的Python Module | `-- xspcomm # xspcomm基础库，固定文件夹，不需要关注 `-- example.py # 示例代码 配置测试代码 在picker_out_rmg中创建 example.py：\nfrom RandomGenerator import * import random # 定义参考模型 class LFSR_16: def __init__(self, seed): self.state = seed \u0026 ((1 \u003c\u003c 16) - 1) def Step(self): new_bit = (self.state \u003e\u003e 15) ^ (self.state \u003e\u003e 14) \u0026 1 self.state = ((self.state \u003c\u003c 1) | new_bit ) \u0026 ((1 \u003c\u003c 16) - 1) if __name__ == \"__main__\": dut = DUTRandomGenerator() # 创建DUT dut.InitClock(\"clk\") # 指定时钟引脚，初始化时钟 seed = random.randint(0, 2**16 - 1) # 生成随机种子 dut.seed.value = seed # 设置DUT种子 ref = LFSR_16(seed) # 创建参考模型用于对比 # reset DUT dut.reset.value = 1 # reset 信号置1 dut.Step() # 推进一个时钟周期（DUTRandomGenerator是时序电路，需要通过Step推进） dut.reset.value = 0 # reset 信号置0 dut.Step() # 推进一个时钟周期 for i in range(65536): # 循环65536次 dut.Step() # dut 推进一个时钟周期，生成随机数 ref.Step() # ref 推进一个时钟周期，生成随机数 assert dut.random_number.value == ref.state, \"Mismatch\" # 对比DUT和参考模型生成的随机数 print(f\"Cycle {i}, DUT: {dut.random_number.value:x}, REF: {ref.state:x}\") # 打印结果 # 完成测试 print(\"Test Passed\") dut.Finish() # Finish函数会完成波形、覆盖率等文件的写入 运行测试程序 在 picker_out_rmg 目录下执行 python example.py 即可运行测试程序。在运行完成后，若输出 Test Passed，则表示测试通过。完成运行后，会生成波形文件：RandomGenerator.fst，可在bash中通过以下命令进行查看。\ngtkwave RandomGenerator.fst\n输出示例：\n··· Cycle 65529, DUT: d9ea, REF: d9ea Cycle 65530, DUT: b3d4, REF: b3d4 Cycle 65531, DUT: 67a9, REF: 67a9 Cycle 65532, DUT: cf53, REF: cf53 Cycle 65533, DUT: 9ea6, REF: 9ea6 Cycle 65534, DUT: 3d4d, REF: 3d4d Cycle 65535, DUT: 7a9a, REF: 7a9a Test Passed, destroy UT_RandomGenerator ","categories":["示例项目","教程"],"description":"基于一个16bit的LFSR随机数生成器展示工具的用法，该随机数生成器内部存在时钟信号、时序逻辑与寄存器。","excerpt":"基于一个16bit的LFSR随机数生成器展示工具的用法，该随机数生成器内部存在时钟信号、时序逻辑与寄存器。","ref":"/mlvp/docs/quick-start/eg-rmg/","tags":["examples","docs"],"title":"案例二：随机数生成器"},{"body":"参数与选项 UCAgent 的使用方式为：\nucagent \u003cworkspace\u003e \u003cdut_name\u003e {参数与选项} 输入 workspace：工作目录： workspace/\u003cDUT_DIR\u003e: 待测设计（DUT），即由 picker 导出的 DUT 对应的 Python 包 \u003cDUT_DIR\u003e，例如：Adder workspace/\u003cDUT_DIR\u003e/README.md: 以自然语言描述的该 DUT 验证需求与目标 workspace/\u003cDUT_DIR\u003e/*.md: 其他参考文件 workspace/\u003cDUT_DIR\u003e/*.v/sv/scala: 源文件，用于进行 bug 分析 其他与验证相关的文件（例如：提供的测试实例、需求说明等） dut_name: 待测设计的名称，即 \u003cDUT_DIR\u003e，例如：Adder 输出 workspace：工作目录： workspace/Guide_Doc：验证过程中所遵循的各项要求与指导文档 workspace/uc_test_report： 生成的 Toffee-test 测试报告 workspace/unity_test/tests： 自动生成的测试用例 workspace/*.md： 生成的各类文档，包括 Bug 分析、检查点记录、验证计划、验证结论等 对输出的详细解释可以参考快速开始\n位置参数 参数 必填 说明 示例 workspace 是 运行代理的工作目录 ./output dut 是 DUT 名称（工作目录下的子目录名） Adder 执行与交互 选项 简写 取值/类型 默认值 说明 --stream-output -s flag 关闭 流式输出到控制台 --human -hm flag 关闭 启动时进入人工输入/断点模式 --interaction-mode -im standard/enhanced/advanced standard 交互模式；enhanced 含规划与记忆管理，advanced 含自适应策略 --tui flag 关闭 启用终端 TUI 界面 --loop -l flag 关闭 启动后立即进入主循环（可配合 --loop-msg），适用于直接使用模式 --loop-msg str 空 进入循环时注入的首条消息 --seed int 随机 随机种子（未指定则自动随机） --sys-tips str 空 覆盖系统提示词 配置与模板 选项 简写 取值/类型 默认值 说明 --config path 无 配置文件路，如–config config.yaml径 --template-dir path 无 自定义模板目录 --template-overwrite flag 否 渲染模板到 workspace 时允许覆盖已存在内容 --output dir unity_test 输出目录名 --override A.B.C=VALUE[,X.Y=VAL2,…] 无 以“点号路径=值”覆盖配置；字符串需引号，其它按 Python 字面量解析 --gen-instruct-file -gif file 无 在 workspace 下生成外部 Agent 的引导文件（存在则覆盖） --guid-doc-path path 无 使用自定义 Guide_Doc 目录（默认使用内置拷贝） 计划与 ToDo 选项 简写 取值/类型 默认值 说明 --force-todo -fp flag 否 在 standard 模式下也启用 ToDo 工具，并在每轮提示中附带 ToDo 信息 --use-todo-tools -utt flag 否 启用 ToDo 相关工具（不限于 standard 模式） ToDo 工具概览与示例 给模型规划的，小模型关闭，大模型自行打开 说明：ToDo 工具是用于提升模型规划能力的工具，用户可以利用它来自定义模型的ToDo列表。目前该功能对模型能力要求较高，默认处于关闭状态。\n启用条件：任意模式下使用 --use-todo-tools；或在 standard 模式用 --force-todo 强制启用并在每轮提示中附带 ToDo 信息。\n约定与限制：步骤索引为 1-based；steps 数量需在 2~20；notes 与每个 step 文本长度 ≤ 100；超限会拒绝并返回错误字符串。\n工具总览\n工具类 调用名 主要功能 参数 返回 关键约束/行为 CreateToDo CreateToDo 新建当前 ToDo（覆盖旧 ToDo） task_description: str; steps: List[str] 成功提示 + 摘要字符串 校验步数与长度；成功后写入并返回摘要 CompleteToDoSteps CompleteToDoSteps 将指定步骤标记为完成，可附加备注 completed_steps: List[int]=[]; notes: str=\"\" 成功提示（完成数）+ 摘要 仅未完成步骤生效；无 ToDo 时提示先创建；索引越界忽略 UndoToDoSteps UndoToDoSteps 撤销步骤完成状态，可附加备注 steps: List[int]=[]; notes: str=\"\" 成功提示（撤销数）+ 摘要 仅已完成步骤生效；无 ToDo 时提示先创建；索引越界忽略 ResetToDo ResetToDo 重置/清空当前 ToDo 无 重置成功提示 清空步骤与备注，随后可重新创建 GetToDoSummary GetToDoSummary 获取当前 ToDo 摘要 无 摘要字符串 / 无 ToDo 提示 只读，不修改状态 ToDoState ToDoState 获取状态短语（看板/状态栏） 无 状态描述字符串 动态显示：无 ToDo/已完成/进度统计等 调用示例（以 MCP/内部工具调用为例，参数为 JSON 格式）：\n{ \"tool\": \"CreateToDo\", \"args\": { \"task_description\": \"为 Adder 核心功能完成验证闭环\", \"steps\": [ \"阅读 README 与规格，整理功能点\", \"定义检查点与通过标准\", \"生成首批单元测试\", \"运行并修复失败用例\", \"补齐覆盖率并输出报告\" ] } } { \"tool\": \"CompleteToDoSteps\", \"args\": { \"completed_steps\": [1, 2], \"notes\": \"初始问题排查完成，准备补充用例\" } } { \"tool\": \"UndoToDoSteps\", \"args\": { \"steps\": [2], \"notes\": \"第二步需要微调检查点\" } } { \"tool\": \"ResetToDo\", \"args\": {} } { \"tool\": \"GetToDoSummary\", \"args\": {} } { \"tool\": \"ToDoState\", \"args\": {} } 外部与嵌入工具 选项 简写 取值/类型 默认值 说明 --ex-tools name1[,name2…] 无 逗号分隔的外部工具类名列表（如：SqThink） --no-embed-tools flag 否 禁用内置的检索/记忆类嵌入工具 日志 选项 简写 取值/类型 默认值 说明 --log flag 否 启用日志 --log-file path 自动 日志输出文件（未指定则使用默认） --msg-file path 自动 消息日志文件（未指定则使用默认） MCP Server 选项 简写 取值/类型 默认值 说明 --mcp-server flag 否 启动 MCP Server（含文件工具） --mcp-server-no-file-tools flag 否 启动 MCP Server（无文件操作工具） --mcp-server-host host 127.0.0.1 Server 监听地址 --mcp-server-port int 5000 Server 端口 阶段控制与安全 选项 简写 取值/类型 默认值 说明 --force-stage-index int 0 强制从指定阶段索引开始 --skip int（可多次） [] 跳过指定阶段索引，可重复提供 --unskip int（可多次） [] 取消跳过指定阶段索引，可重复提供 --no-write / –nw path1 path2 … 无 限制写入目标列表；必须位于 workspace 内且存在 版本与检查 选项 简写 取值/类型 默认值 说明 --check flag 否 检查默认配置、语言目录、模板与 Guide_Doc 是否存在后退出 --version flag 输出版本并退出 示例 python3 ucagent.py ./output Adder \\ \\ -s \\ -hm \\ -im enhanced \\ --tui \\ -l \\ --loop-msg 'start verification' \\ --seed 12345 \\ --sys-tips '按规范完成Adder的验证' \\ \\ --config config.yaml \\ --template-dir ./templates \\ --template-overwrite \\ --output unity_test \\ --override 'conversation_summary.max_tokens=16384,conversation_summary.max_summary_tokens=2048,conversation_summary.use_uc_mode=True,lang=\"zh\",openai.model_name=\"gpt-4o-mini\"' \\ --gen-instruct-file GEMINI.md \\ --guid-doc-path ./output/Guide_Doc \\ \\ --use-todo-tools \\ \\ --ex-tools 'SqThink,AnotherTool' \\ --no-embed-tools \\ \\ --log \\ --log-file ./output/ucagent.log \\ --msg-file ./output/ucagent.msg \\ \\ --mcp-server-no-file-tools \\ --mcp-server-host 127.0.0.1 \\ --mcp-server-port 5000 \\ \\ --force-stage-index 2 \\ --skip 5 --skip 7 \\ --unskip 6 \\ --nw ./output/Adder ./output/unity_test 位置参数 ./output：workspace 工作目录 Adder：dut 子目录名 执行与交互 -s：流式输出 -hm：启动即人工可介入 -im enhanced：交互模式为增强（含规划与记忆） --tui：启用 TUI -l：启动后立即进入循环 --loop/–loop-msg：进入循环注入首条消息 --seed 12345：固定随机种子 --sys-tips：自定义系统提示 配置与模板 --config config.yaml：从config.yaml加载项目配置 --template-dir ./templates：指定模板目录为./templates --template-overwrite：渲染模板时允许覆盖 --output unity_test：输出目录名unity_test --override ‘…’: 覆盖配置键值（点号路径=值，多项用逗号分隔；字符串需内层引号，整体用单引号包裹以保留引号），示例里设置了会话摘要上限、启用裁剪、文档语言为“中文”、模型名为 gpt-4o-mini -gif/--gen-instruct-file GEMINI.md：在 \u003cworkspace\u003e/GEMINI.md 下生成外部协作引导文件 --guid-doc-path ./output/Guide_Doc：自定义 Guide_Doc 目录为./output/Guide_Doc 计划与 ToDo --use-todo-tools：启用 ToDo 工具及强制附带 ToDo 信息 外部与嵌入工具 --ex-tools ‘SqThink,AnotherTool’：启用外部工具SqThink,AnotherTool --no-embed-tools：禁用内置嵌入检索/记忆工具 日志 --log：开启日志文件 --log-file ./output/ucagent.log：指定日志输出文件为./output/ucagent.log --msg-file ./output/ucagent.msg：指定消息日志文件为./output/ucagent.msg MCP Server --mcp-server-no-file-tools：启动 MCP（无文件操作工具） --mcp-server-host：Server 监听地址为127.0.0.1 --mcp-server-port：Server 监听端口为5000 阶段控制与安全 --force-stage-index 2：从阶段索引 2 开始 --skip 5 --skip 7：跳过阶段5和阶段7 --unskip 7：取消跳过阶段7 --nw ./output/Adder ./output/unity_test：限制仅./output/Adder和./output/unity_test路径可写 说明 --check 与 --version 会直接退出，未与运行组合使用 --mcp-server 与 --mcp-server-no-file-tools 二选一；此处选了后者带路径参数（如 --template-dir/--guid-doc-path/–nw 的路径）需实际存在，否则会报错 --override 字符串值务必带引号，并整体用单引号包住以避免 shell 吃掉引号（示例写法已处理） ","categories":["教程"],"description":"各个选项参数说明。","excerpt":"各个选项参数说明。","ref":"/mlvp/docs/ucagent/usage/option/","tags":["docs"],"title":"参数说明"},{"body":"添加工具与 MCP Server 工具 面向可修改本仓库代码的高级用户，以下说明如何：\n添加一个新工具（供本地/Agent 内调用） 将工具暴露为 MCP Server 工具（供外部 IDE/客户端调用） 控制选择哪些工具被暴露与如何调用 涉及关键位置：\nvagent/tools/uctool.py：工具基类 UCTool、to_fastmcp（LangChain Tool → FastMCP Tool） vagent/util/functions.py：import_and_instance_tools（按名称导入实例）、create_verify_mcps（启动 FastMCP） vagent/verify_agent.py：装配工具清单，start_mcps 组合并启动 Server vagent/cli.py / vagent/verify_pdb.py：命令行与 TUI 内的 MCP 启动命令 1) 工具体系与装配 工具基类 UCTool： 继承 LangChain BaseTool，内置：call_count 计数、call_time_out 超时、流式/阻塞提示、MCP Context 注入（ctx.info）、防重入等。 推荐自定义工具继承 UCTool，获得更好的 MCP 行为与调试体验。 运行期装配（VerifyAgent 初始化）： 基础工具：RoleInfo、ReadTextFile 嵌入工具：参考检索与记忆（除非 --no-embed-tools） 文件工具：读/写/查找/路径等（可在 MCP 无文件工具模式下剔除） 阶段工具：由 StageManager 按工作流动态提供 外部工具：来自配置项 ex_tools 与 CLI --ex-tools（通过 import_and_instance_tools 零参实例化） 名称解析： 短名：类/工厂函数需在 vagent/tools/__init__.py 导出（例如 from .mytool import HelloTool），即可在 ex_tools 写 HelloTool 全路径：mypkg.mytools.HelloTool / mypkg.mytools.Factory 2) 添加一个新工具（本地/Agent 内） 规范要求：\n唯一 name、清晰 description 使用 pydantic BaseModel 定义 args_schema（MCP 转换依赖） 实现 _run（同步）或 _arun（异步）；继承 UCTool 可直接获得超时、流式与 ctx 注入 示例 1：同步工具（计数问候）\nfrom pydantic import BaseModel, Field from vagent.tools.uctool import UCTool class HelloArgs(BaseModel): who: str = Field(..., description=\"要问候的人\") class HelloTool(UCTool): name: str = \"Hello\" description: str = \"向指定对象问候，并统计调用次数\" args_schema = HelloArgs def _run(self, who: str, run_manager=None) -\u003e str: return f\"Hello, {who}! (called {self.call_count+1} times)\" 注册与使用：\n临时：--ex-tools mypkg.mytools.HelloTool 持久：项目 config.yaml ex_tools: - mypkg.mytools.HelloTool （可选）短名注册：在 vagent/tools/__init__.py 导出 HelloTool 后，可写 --ex-tools HelloTool。\n示例 2：异步流式工具（ctx.info + 超时）\nfrom pydantic import BaseModel, Field from vagent.tools.uctool import UCTool import asyncio class ProgressArgs(BaseModel): steps: int = Field(5, ge=1, le=20, description=\"进度步数\") class ProgressTool(UCTool): name: str = \"Progress\" description: str = \"演示流式输出与超时处理\" args_schema = ProgressArgs async def _arun(self, steps: int, run_manager=None): for i in range(steps): self.put_alive_data(f\"step {i+1}/{steps}\") # 供阻塞提示/日志缓冲 await asyncio.sleep(0.5) return \"done\" 说明：UCTool.ainvoke 会在 MCP 模式下注入 ctx，并启动阻塞提示线程；当 sync_block_log_to_client=True 时会周期性 ctx.info 推送日志，超时后返回错误与缓冲日志。\n3) 暴露为 MCP Server 工具 工具 → MCP 转换（vagent/tools/uctool.py::to_fastmcp）：\n必须：args_schema 继承 BaseModel；不支持“注入参数”签名。 UCTool 子类会得到 context_kwarg=“ctx” 的 FastMCP 工具，具备流式交互能力。 Server 端启动：\nVerifyAgent.start_mcps 组合工具：tool_list_base + tool_list_task + tool_list_ext + [tool_list_file] vagent/util/functions.py::create_verify_mcps 将工具序列转换为 FastMCP 工具并启动 uvicorn（mcp.streamable_http_app()）。 如何选择暴露范围：\nCLI： 启动（含文件工具）：--mcp-server 启动（无文件工具）：--mcp-server-no-file-tools 地址：--mcp-server-host，端口：--mcp-server-port TUI 命令：start_mcp_server [host] [port] / start_mcp_server_no_file_ops [host] [port] 4) 客户端调用流程 FastMCP Python 客户端（参考 tests/test_mcps.py）：\nfrom fastmcp import Client client = Client(\"http://127.0.0.1:5000/mcp\", timeout=10) print(client.list_tools()) print(client.call_tool(\"Hello\", {\"who\": \"UCAgent\"})) IDE/Agent（Claude Code、Copilot、Qwen Code 等）：将 httpUrl 指向 http://\u003chost\u003e:\u003cport\u003e/mcp，即可发现并调用工具。\n5) 生命周期、并发与超时 计数：UCTool 内置 call_count；非 UCTool 工具由 import_and_instance_tools 包装计数。 并发保护：is_in_streaming/is_alive_loop 防止重入；同一实例不允许并发执行。 超时：call_time_out（默认 20s）+ 客户端 timeout；阻塞时可用 put_alive_data + sync_block_log_to_client=True 推送心跳。 6) 配置策略与最佳实践 ex_tools 列表为“整体覆盖”，项目 config.yaml 需写出完整清单。 短名 vs 全路径：短名更便捷，全路径适用于私有包不修改本仓库时。 无参构造/工厂：装配器直接调用 (...)()，复杂配置建议在工厂内部处理（读取环境/配置文件）。 文件写权限：MCP 无文件工具模式下不要暴露写类工具；如需写入，请在本地 Agent 内使用或显式允许写目录。 通过环境变量注入外部工具（EX_TOOLS） 配置文件支持 Bash 风格环境变量占位：$(VAR: default)。你可以让 ex_tools 从环境变量注入工具类列表（支持模块全名或 vagent.tools 下的短名）。\n在项目的 config.yaml 或用户级 ~/.ucagent/setting.yaml 中写入： ex_tools: $(EX_TOOLS: []) 用环境变量提供列表（必须是可被 YAML 解析的数组字面量）： export EX_TOOLS='[\"SqThink\",\"HumanHelp\"]' # 或使用完整类路径： # export EX_TOOLS='[\"vagent.tools.extool.SqThink\",\"vagent.tools.human.HumanHelp\"]' 启动后本地对话与 MCP Server 中都会出现这些工具。短名需要在 vagent/tools/__init__.py 导出；否则请使用完整模块路径。\n与 CLI 的 --ex-tools 选项是合并关系（两边都会被装配）。\n7) 常见问题排查 工具未出现在 MCP 列表：未被装配（ex_tools 未配置/未导出）、args_schema 非 BaseModel、Server 未按预期启动。 调用报“注入参数不支持”：工具定义包含 LangChain 的 injected args；请改成显式 args_schema 参数。 超时：调大 call_time_out 或客户端 timeout；在长任务中输出进度维持心跳。 短名无效：未在 vagent/tools/__init__.py 导出；改用全路径或补导出。 完成以上步骤后：你的工具既能在本地对话中被 ReAct 自动调用，也能通过 MCP Server 暴露给外部 IDE/客户端统一调用。\n","categories":["教程"],"description":"如何自行定义参数、流程和工具。","excerpt":"如何自行定义参数、流程和工具。","ref":"/mlvp/docs/ucagent/customize/","tags":["docs"],"title":"定制功能"},{"body":" Picker 工具支持生成代码行覆盖率报告，toffee-test（https://github.com/XS-MLVP/toffee-test） 项目支持生成功能覆盖率报告。\n代码行覆盖率 目前 Picker 工具支持基于 Verilator 仿真器生成的代码行覆盖率报告。\nVerilator Verilator 仿真器提供了覆盖率支持功能。 该功能的实现方式是：\n利用 verilator_coverage 工具处理或合并覆盖率数据库，最终针对多个 DUT 生成一个 coverage.info 文件。 利用 lcov 工具的 genhtml 命令基于coverage.info和 RTL 代码源文件，生成完整的代码覆盖率报告。 使用时的流程如下：\n使用 Picker 生成 dut 时，使能 COVERAGE 功能 （添加-c选项）。 仿真器运行完成后，dut.Finish() 之后会生成覆盖率数据库文件 V{DUT_NAME}.dat。 基于 verilator_coverage 的 write-info 功能将其转换成 .info文件。 基于 lcov 的 genhtml 功能，使用.info文件和文件中指定的rtl 源文件，生成 html 报告。 注意： 文件中指定的rtl 源文件是指在生成的DUT时使用的源文件路径，需要保证这些路径在当前环境下是有效的。简单来说，需要编译时用到的所有.sv/.v文件都需要在当前环境下存在，并且目录不变。\nverilator_coverage verilator_coverage 工具用于处理 DUT 运行后生成的 .dat 的覆盖率数据。该工具可以处理并合并多个 .dat 文件，同时具有两类功能：\n基于 .dat 文件生成 .info 文件，用于后续生成网页报告。\n-annotate \u003coutput_dir\u003e：以标注的形式在源文件中呈现覆盖率情况，结果保存到output_dir中。形式如下：\n100000 input logic a; // Begins with whitespace, because // number of hits (100000) is above the limit. %000000 input logic b; // Begins with %, because // number of hits (0) is below the limit. -annotate-min \u003ccount\u003e：指定上述的 limit 为 count\n可以将 .dat 文件，结合源代码文件，将覆盖率数据以标注的形式与源代码结合在一起，并写入到指定目录。\n-write \u003cmerged-datafile\u003e -read \u003cdatafiles\u003e：将若干个.dat(datafiles)文件合并为一个.dat 文件 -write-info \u003cmerged-info\u003e -read \u003cdatafiles\u003e：将若干个.dat(datafiles)文件合并为一个.info 文件 genhtml 由 locv 包提供的 genhtml 可以由上述的.info 文件导出可读性更好的 html 报告。命令格式为：genhtml [OPTIONS] \u003cinfofiles\u003e。 建议使用-o \u003coutputdir\u003e选项将结果输出到指定目录。\n以加法器为例。\n使用示例 如果您使用 Picker 时打开了-c选项，那么在仿真结束后，会生成一个V{DUT_NAME}.dat文件。并且顶层目录会有一个 Makefile 文件，其中包含了生成覆盖率报告的命令。\n命令内容如下：\ncoverage: ... verilator_coverage -write-info coverage.info ./${TARGET}/V${PROJECT}_coverage.dat genhtml coverage.info --output-directory coverage ... 在 shell 中输入make coverage,其会根据生成的.dat 文件生成 coverage.info，再使用genhtml再 coverage 目录下生成 html 报告。\nVCS VCS 对应的文档正在完善当中。\n","categories":["示例项目","教程"],"description":"覆盖率工具","excerpt":"覆盖率工具","ref":"/mlvp/docs/env_usage/coverage/","tags":["examples","docs"],"title":"覆盖率统计"},{"body":"","categories":["示例项目","教程"],"description":"基于开放验证平台完成验证的复杂案例。","excerpt":"基于开放验证平台完成验证的复杂案例。","ref":"/mlvp/docs/advance_case/","tags":["examples","docs"],"title":"高级案例"},{"body":"Toffee 是使用 Python 语言编写的一套硬件验证框架，它依赖于多语言转换工具 Picker，该工具能够将硬件设计的 Verilog 代码转换为 Python Package，使得用户可以使用 Python 来驱动并验证硬件设计。\n其吸收了部分 UVM 验证方法学，以保证验证环境的规范性和可复用性，并重新设计了整套验证环境的搭建方式，使其更符合软件领域开发者的使用习惯，从而使软件开发者可以轻易地上手硬件验证工作。\n在 Toffee Documentation 中查看 Toffee 的详细使用说明。\n","categories":["教程"],"description":"搭建硬件验证环境所需的框架——Toffee","excerpt":"搭建硬件验证环境所需的框架——Toffee","ref":"/mlvp/docs/mlvp/","tags":["docs"],"title":"验证框架"},{"body":"UCAgent is a large-language-model based automation agent for hardware verification, focused on unit-test level verification for chip design. It analyzes designs, generates tests, runs verification, and produces reports to improve verification efficiency.\n","categories":["Example Projects","Tutorial"],"description":"LLM-powered automated unit-test verification agent","excerpt":"LLM-powered automated unit-test verification agent","ref":"/mlvp/en/docs/ucagent/","tags":["examples","docs"],"title":"(AI) Verification Agent"},{"body":"UCAgent 是一个基于大语言模型的自动化硬件验证智能体，专注于芯片设计的单元测试(Unit Test)验证工作。该项目通过 AI 技术自动分析硬件设计，生成测试用例，并执行验证任务生成测试报告，从而提高验证效率。\n","categories":["示例项目","教程"],"description":"基于大模型进行自动化UT验证智能体","excerpt":"基于大模型进行自动化UT验证智能体","ref":"/mlvp/docs/ucagent/","tags":["examples","docs"],"title":"(AI)验证智能体"},{"body":"Introduction to the Dual-Port Stack A dual-port stack is a data structure that supports simultaneous operations on two ports. Compared to a traditional single-port stack, a dual-port stack allows simultaneous read and write operations. In scenarios such as multithreaded concurrent read and write operations, the dual-port stack can provide better performance. In this example, we provide a simple dual-port stack implementation, with the source code as follows:\nmodule dual_port_stack ( input clk, input rst, // Interface 0 input in0_valid, output in0_ready, input [7:0] in0_data, input [1:0] in0_cmd, output out0_valid, input out0_ready, output [7:0] out0_data, output [1:0] out0_cmd, // Interface 1 input in1_valid, output in1_ready, input [7:0] in1_data, input [1:0] in1_cmd, output out1_valid, input out1_ready, output [7:0] out1_data, output [1:0] out1_cmd ); // Command definitions localparam CMD_PUSH = 2'b00; localparam CMD_POP = 2'b01; localparam CMD_PUSH_OKAY = 2'b10; localparam CMD_POP_OKAY = 2'b11; // Stack memory and pointer reg [7:0] stack_mem[0:255]; reg [7:0] sp; reg busy; reg [7:0] out0_data_reg, out1_data_reg; reg [1:0] out0_cmd_reg, out1_cmd_reg; reg out0_valid_reg, out1_valid_reg; assign out0_data = out0_data_reg; assign out0_cmd = out0_cmd_reg; assign out0_valid = out0_valid_reg; assign out1_data = out1_data_reg; assign out1_cmd = out1_cmd_reg; assign out1_valid = out1_valid_reg; always @(posedge clk or posedge rst) begin if (rst) begin sp \u003c= 0; busy \u003c= 0; end else begin // Interface 0 Request Handling if (!busy \u0026\u0026 in0_valid \u0026\u0026 in0_ready) begin case (in0_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out0_valid_reg \u003c= 1; stack_mem[sp] \u003c= in0_data; out0_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out0_valid_reg \u003c= 1; out0_data_reg \u003c= stack_mem[sp - 1]; out0_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out0_valid_reg \u003c= 0; end endcase end // Interface 1 Request Handling if (!busy \u0026\u0026 in1_valid \u0026\u0026 in1_ready) begin case (in1_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out1_valid_reg \u003c= 1; stack_mem[sp] \u003c= in1_data; out1_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out1_valid_reg \u003c= 1; out1_data_reg \u003c= stack_mem[sp - 1]; out1_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out1_valid_reg \u003c= 0; end endcase end // Interface 0 Response Handling if (busy \u0026\u0026 out0_ready) begin out0_valid_reg \u003c= 0; busy \u003c= 0; end // Interface 1 Response Handling if (busy \u0026\u0026 out1_ready) begin out1_valid_reg \u003c= 0; busy \u003c= 0; end end end assign in0_ready = (in0_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255 || in0_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy; assign in1_ready = (in1_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255 || in1_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy \u0026\u0026 !(in0_ready \u0026\u0026 in0_valid); endmodule In this implementation, aside from the clock signal (clk) and reset signal (rst), there are also input and output signals for the two ports, which have the same interface definition. The meaning of each signal for the ports is as follows:\nRequest Port (in)\nin_valid: Input data valid signal\nin_ready: Input data ready signal\nin_data: Input data\nin_cmd: Input command (0: PUSH, 1: POP)\nResponse Port (out)\nout_valid: Output data valid signal\nout_ready: Output data ready signal\nout_data: Output data\nout_cmd: Output command (2: PUSH_OKAY, 3: POP_OKAY)\nWhen we want to perform an operation on the stack through a port, we first need to write the required data and command to the input port, and then wait for the output port to return the result. Specifically, if we want to perform a PUSH operation on the stack, we should first write the data to be pushed into in_data, then set in_cmd to 0, indicating a PUSH operation, and set in_valid to 1, indicating that the input data is valid. Next, we need to wait for in_ready to be 1, ensuring that the data has been correctly received, at which point the PUSH request has been correctly sent.After the command is successfully sent, we need to wait for the stack’s response information on the response port. When out_valid is 1, it indicates that the stack has completed the corresponding operation. At this point, we can read the stack’s returned data from out_data (the returned data of the POP operation will be placed here) and read the stack’s returned command from out_cmd. After reading the data, we need to set out_ready to 1 to notify the stack that the returned information has been correctly received. If requests from both ports are valid simultaneously, the stack will prioritize processing requests from port 0.\nSetting Up the Driver Environment Similar to Case Study 1 and Case Study 2, before testing the dual-port stack, we first need to use the Picker tool to build the RTL code into a Python Module. After the build is complete, we will use a Python script to drive the RTL code for testing. First, create a file named dual_port_stack.v and copy the above RTL code into this file. Then, execute the following command in the same folder:\npicker export --autobuild=true dual_port_stack.v -w dual_port_stack.fst --sname dual_port_stack --tdir picker_out_dual_port_stack --lang python -e --sim verilator The generated driver environment is located in the picker_out_dual_port_stack folder. Inside, UT_dual_port_stack is the generated Python Module, and example.py is the test script. You can run the test script with the following commands:\ncd picker_out_dual_port_stack python3 example.py If no errors occur during the run, it means the environment has been set up correctly.\nDriving the DUT with Coroutines Driving the DUT with Callback Functions In this case, we need to drive a dual-port stack to test its functionality. However, you may quickly realize that the methods used in Cases 1 and 2 are insufficient for driving a dual-port stack. In the previous tests, the DUT had a single execution logic where you input data into the DUT and wait for the output.\nHowever, a dual-port stack is different because its two ports operate with independent execution logic. During the drive process, these two ports might be in entirely different states. For example, while port 0 is waiting for data from the DUT, port 1 might be sending a new request. In such situations, simple sequential execution logic will struggle to drive the DUT effectively.\nTherefore, in this case, we will use the dual-port stack as an example to introduce a callback function-based driving method to handle such DUTs.\nIntroduction to Callback Functions A callback function is a common programming technique that allows us to pass a function as an argument, which is then called when a certain condition is met. In the generated Python Module, we provide an interface StepRis for registering callback functions with the internal execution environment. Here’s how it works:\nfrom dual_port_stack import DUTdual_port_stack def callback(cycles): print(f\"The current clock cycle is {cycles}\") dut = DUTdual_port_stack() dut.StepRis(callback) dut.Step(10) You can run this code directly to see the effect of the callback function. In the above code, we define a callback function callback that takes a cycles parameter and prints the current clock cycle each time it is called. We then register this callback function to the DUT via StepRis.Once the callback function is registered, each time the Step function is run, which corresponds to each clock cycle, the callback function is invoked on the rising edge of the clock signal, with the current clock cycle passed as an argument. Using this approach, we can write different execution logics as callback functions and register multiple callback functions to the DUT, thereby achieving parallel driving of the DUT.\nDual-Port Stack Driven by Callback Functions To complete a full execution logic using callback functions, we typically write it in the form of a state machine. Each callback function invocation triggers a state change within the state machine, and multiple invocations complete a full execution logic.\nBelow is an example code for driving a dual-port stack using callback functions:\nimport random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class Status(Enum): IDLE = 0 WAIT_REQ_READY = 1 WAIT_RESP_VALID = 2 class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict self.status = self.Status.IDLE self.operation_num = 0 self.remaining_delay = 0 def push(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value self.port_dict[\"in_data\"].value = random.randint(0, 2**32-1) def pop(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.POP.value def step_callback(self, cycle): if self.status == self.Status.WAIT_REQ_READY: if self.port_dict[\"in_ready\"].value == 1: self.port_dict[\"in_valid\"].value = 0 self.port_dict[\"out_ready\"].value = 1 self.status = self.Status.WAIT_RESP_VALID if self.port_dict[\"in_cmd\"].value == self.BusCMD.PUSH.value: self.model.commit_push(self.port_dict[\"in_data\"].value) elif self.status == self.Status.WAIT_RESP_VALID: if self.port_dict[\"out_valid\"].value == 1: self.port_dict[\"out_ready\"].value = 0 self.status = self.Status.IDLE self.remaining_delay = random.randint(0, 5) if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) if self.status == self.Status.IDLE: if self.remaining_delay == 0: if self.operation_num \u003c 10: self.push() elif self.operation_num \u003c 20: self.pop() else: return self.operation_num += 1 self.status = self.Status.WAIT_REQ_READY else: self.remaining_delay -= 1 def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) dut.StepRis(port0.step_callback) dut.StepRis(port1.step_callback) dut.Step(200) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") test_stack(dut) dut.Finish() In the code above, the driving process is implemented such that each port independently drives the DUT, with a random delay added after each request is completed. Each port performs 10 PUSH operations and 10 POP operations.When a PUSH or POP request takes effect, the corresponding commit_push or commit_pop function in the StackModel is called to simulate stack behavior. After each POP operation, the data returned by the DUT is compared with the model’s data to ensure consistency.To implement the driving behavior for a single port, we created the SinglePortDriver class, which includes a method for sending and receiving data. The step_callback function handles the internal update logic.In the test_stack function, we create a SinglePortDriver instance for each port of the dual-port stack, pass the corresponding interfaces, and register the callback function to the DUT using the StepRis function. When dut.Step(200) is called, the callback function is automatically invoked each clock cycle to complete the entire driving logic.SinglePortDriver Driving Logic As mentioned earlier, callback functions typically require the execution logic to be implemented as a state machine. Therefore, in the SinglePortDriver class, the status of each port is recorded, including:\nIDLE: Idle state, waiting for the next operation.\nIn the idle state, check the remaining_delay status to determine whether the current delay has ended. If the delay has ended, proceed with the next operation; otherwise, continue waiting.\nWhen the next operation is ready, check the operation_num status (the number of operations already performed) to determine whether the next operation should be PUSH or POP. Then, call the corresponding function to assign values to the port and switch the status to WAIT_REQ_READY.\nWAIT_REQ_READY: Waiting for the request port to be ready.\nAfter the request is sent (in_valid is valid), wait for the in_ready signal to be valid to ensure the request has been correctly received.\nOnce the request is correctly received, set in_valid to 0 and out_ready to 1, indicating the request is complete and ready to receive a response.\nWAIT_RESP_VALID: Waiting for the response port to return data.\nAfter the request is correctly received, wait for the DUT’s response, i.e., wait for the out_valid signal to be valid. When the out_valid signal is valid, it indicates that the response has been generated and the request is complete. Set out_ready to 0 and switch the status to IDLE. Running the Test Copy the above code into example.py, and then run the following command:\ncd picker_out_dual_port_stack python3 example.py You can run the test code for this case directly, and you will see output similar to the following:\n... push 77 push 140 push 249 push 68 push 104 push 222 ... Pop 43 Pass: 43 == 43 Pop 211 Pass: 211 == 211 Pop 16 Pass: 16 == 16 Pop 255 Pass: 255 == 255 Pop 222 Pass: 222 == 222 Pop 104 ... In the output, you can see the data for each PUSH and POP operation, as well as the result of each POP operation. If there is no error message in the output, it indicates that the test has passed.\nPros and Cons of Callback-Driven Design By using callbacks, we can achieve parallel driving of the DUT, as demonstrated in this example. We utilized two callbacks to drive two ports with independent execution logic. In simple scenarios, callbacks offer a straightforward method for parallel driving.\nHowever, as shown in this example, even implementing a simple “request-response” flow requires maintaining a significant amount of internal state. Callbacks break down what should be a cohesive execution logic into multiple function calls, adding considerable complexity to both the code writing and debugging processes.\n","categories":["Example Projects","Tutorials"],"description":"A dual-port stack is a stack with two ports, each supporting push and pop operations. This case study uses a dual-port stack as an example to demonstrate how to use callback functions to drive the DUT.","excerpt":"A dual-port stack is a stack with two ports, each supporting push and …","ref":"/mlvp/en/docs/quick-start/eg-stack-callback/","tags":["examples","docs"],"title":"Case 3: Dual-Port Stack (Callback)"},{"body":" This section introduces the general process of verifying a DUT based on Picker.\nThe goal of the open verification platform is functional verification, which generally involves the following steps:\n1. Determine the verification object and goals Typically, the design documentation of the DUT is also delivered to the verification engineer. At this point, you need to read the documentation or source code to understand the basic functions, main structure, and expected functionalities of the verification object.\n2. Build the basic verification environment After fully understanding the design, you need to build the basic verification environment. For example, in addition to the DUT generated by Picker, you may also need to set up a reference model for comparison and a signal monitoring platform for evaluating subsequent functional points.\n3. Decompose functional points and test points Before officially starting the verification, you need to extract the functional points and further decompose them into test points. You can refer to: CSDN: Chip Verification Series - Decomposition of Testpoints\n4. Construct test cases With the test points, you need to construct test cases to cover the corresponding test points. A test case may cover multiple test points.\n5. Collect test results After running all the test cases, you need to summarize all the test results. Generally, this includes line coverage and functional coverage. The former can be obtained through the coverage function provided by the Picker tool, while the latter requires you to judge whether a function is covered by the test cases through monitoring the behavior of the DUT.\n6. Evaluate the test results Finally, you need to evaluate the obtained results, such as whether there are design errors, whether a function cannot be triggered, whether the design documentation description is consistent with the DUT behavior, and whether the design documentation is clearly described.\nNext, we will introduce the general verification process usingMMIO read and write of Nutshell Cache as an example:\n1 Determine the verification object and goals:： The MMIO read and write function of the Nutshell Cache. MMIO is a special type of IO mapping that supports accessing IO device registers by accessing memory addresses. Since the register state of IO devices can change at any time, it is not suitable to cache it. When receiving an MMIO request, the Nutshell cache will directly access the MMIO memory area to read or write data instead of querying hit/miss in the ordinary cache line.\n2 Build the basic verification environment:： We can roughly divide the verification environment into five parts: 1. Testcase Driver：Responsible for generating corresponding signals driven by test cases 2. Monitor：Monitors signals to determine whether functions are covered and correct 3. Ref Cache：A simple reference model 4. Memory/MMIO Ram：Simulates peripheral devices to simulate corresponding cache requests 5. Nutshell Cache Dut：DUT generated by Picker\nIn addition, you may need to further encapsulate the DUT interface to achieve more convenient read and write request operations. For details, refer to Nutshll cachewrapper.\n3 Decompose functional points and test points： Nutshell cache can respond to MMIO requests, further decomposing into the following test points:\nTest Point 1：MMIO requests will be forwarded to the MMIO port Test Point 2：The cache will not issue burst transfer requests when responding to MMIO requests Test Point 3：The cache will block the pipeline when responding to MMIO requests\n4 Construct test cases： The construction of test cases is simple. Knowing that the MMIO address range of the Nutshell cache obtained through Creating DUTis 0x30000000~0x7fffffff, we only need to access this memory range to obtain the expected MMIO results. Note that to trigger the test point of blocking the pipeline, you may need to initiate requests continuously. Here is a simple test case:\n# import CacheWrapper here def mmio_test(cache: CacheWrapper): mmio_lb\t= 0x30000000 mmio_rb\t= 0x30001000 print(\"\\n[MMIO Test]: Start MMIO Serial Test\") for addr in range(mmio_lb, mmio_rb, 16): addr \u0026= ~(0xf) addr1 = addr addr2 = addr + 4 addr3 = addr + 8 cache.trigger_read_req(addr1) cache.trigger_read_req(addr2) cache.trigger_read_req(addr3) cache.recv() cache.recv() cache.recv() print(\"[MMIO Test]: Finish MMIO Serial Test\") 5 Collect test results：\n''' In tb_cache.py ''' # import packages here class TestCache(): def setup_class(self): color.print_blue(\"\\nCache Test Start\") self.dut = DUTCache(\"libDPICache.so\") self.dut.init_clock(\"clock\") # Init here # ... self.testlist = [\"mmio_serial\"] def teardown_class(self): self.dut.Finish() color.print_blue(\"\\nCache Test End\") def __reset(self): # Reset cache and devices # MMIO Test def test_mmio(self): if (\"mmio_serial\" in self.testlist): # Run test from ..test.test_mmio import mmio_test mmio_test(self.cache, self.ref_cache) else: print(\"\\nmmio test is not included\") def run(self): self.setup_class() # test self.test_mmio() self.teardown_class() pass if __name__ == \"__main__\": tb = TestCache() tb.run() Run：\npython3 tb_cache.py The above is only a rough execution process, for details refer to：Nutshell Cache Verify。\n6 Evaluate the running results After the run is complete, the following data can be obtained: Line coverage: Functional coverage: It can be seen that the preset MMIO functions are all covered and correctly triggered.\n","categories":["Example Projects","Learning Materials"],"description":"Overview of the general verification process","excerpt":"Overview of the general verification process","ref":"/mlvp/en/docs/basic/test_dut/","tags":["examples","docs"],"title":"DUT Verification"},{"body":" 本节介绍基于Picker验证DUT的一般流程\n开放验证平台的目标是功能性验证，其一般有以下步骤：\n1. 确定验证对象和目标 通常来说，同时交付给验证工程师的还有DUT的设计文档。此时您需要阅读文档或者源代码，了解验证对象的基本功能、主体结构以及预期功能。\n2. 构建基本验证环境 充分了解设计之后，您需要构建验证的基本环境。例如，除了由Picker生成的DUT外，您可能还需要搭建用于比对的参考模型，也可能需要为后续功能点的评测搭建信号的监听平台。\n3. 功能点与测试点分解 在正式开始验证之前，您还需要提取功能点，并将其进一步分解成测试点。提取和分解方法可以参考：CSDN:芯片验证系列——Testpoints分解\n4. 构造测试用例 有了测试点之后，您需要构造测试用例来覆盖相应的测试点。一个用例可能覆盖多个测试点。\n5. 收集测试结果 运行完所有的测试用例之后，您需要汇总所有的测试结果。一般来说包括代码行覆盖率以及功能覆盖率。前者可以通过Picker工具提供的覆盖率功能获得，后者则需要您通过监听DUT的行为判断某功能是否被用例覆盖到。\n6. 评估测试结果 最后您需要评估得到的结果，如是否存在错误的设计、某功能是否无法被触发、设计文档表述是否与DUT行为一致、设计文档是否表述清晰等。\n接下来我们以果壳Cache的MMIO读写为例，介绍一般验证流程：\n1 确定验证对象和目标： 果壳Cache的MMIO读写功能。MMIO是一类特殊的IO映射，其支持通过访问内存地址的方式访问IO设备寄存器。由于IO设备的寄存器状态是随时可能改变的，因此不适合将其缓存在cache中。当收到MMIO请求时，果壳cache不会在普通的cache行中查询命中/缺失情况，而是会直接访问MMIO的内存区域来读取或者写入数据。\n2 构建基本验证环境： 我们可以将验证环境大致分为五个部分： 1. Testcase Driver：负责由用例产生相应的信号驱动 2. Monitor：监听信号，判断功能是否被覆盖以及功能是否正确 3. Ref Cache：一个简单的参考模型 4. Memory/MMIO Ram：外围设备的模拟，用于模拟相应cache的请求 5. Nutshell Cache Dut：由Picker生成的DUT\n此外，您可能还需要对DUT的接口做进一步封装以实现更方便的读写请求操作，具体可以参考Nutshll cachewrapper。\n3 功能点与测试点分解： 果壳cache可以响应MMIO请求，进一步分解可以得到一下测试点：\n测试点1：MMIO请求会被转发到MMIO端口上 测试点2：cache响应MMIO请求时，不会发出突发传输（Burst Transfer）的请求 测试点3：cache响应MMIO请求时，会阻塞流水线\n4 构造测试用例： 测试用例的构造是简单的，已知通过创建DUT得到的Nutshell cache的MMIO地址范围是0x30000000~0x7fffffff，则我们只需访问这段内存区间，应当就能获得MMIO的预期结果。需要注意的是，为了触发阻塞流水线的测试点，您可能需要连续地发起请求。 以下是一个简单的测试用例：\n# import CacheWrapper here def mmio_test(cache: CacheWrapper): mmio_lb\t= 0x30000000 mmio_rb\t= 0x30001000 print(\"\\n[MMIO Test]: Start MMIO Serial Test\") for addr in range(mmio_lb, mmio_rb, 16): addr \u0026= ~(0xf) addr1 = addr addr2 = addr + 4 addr3 = addr + 8 cache.trigger_read_req(addr1) cache.trigger_read_req(addr2) cache.trigger_read_req(addr3) cache.recv() cache.recv() cache.recv() print(\"[MMIO Test]: Finish MMIO Serial Test\") 5 收集测试结果：\n''' In tb_cache.py ''' # import packages here class TestCache(): def setup_class(self): color.print_blue(\"\\nCache Test Start\") self.dut = DUTCache(\"libDPICache.so\") self.dut.init_clock(\"clock\") # Init here # ... self.testlist = [\"mmio_serial\"] def teardown_class(self): self.dut.Finish() color.print_blue(\"\\nCache Test End\") def __reset(self): # Reset cache and devices # MMIO Test def test_mmio(self): if (\"mmio_serial\" in self.testlist): # Run test from ..test.test_mmio import mmio_test mmio_test(self.cache, self.ref_cache) else: print(\"\\nmmio test is not included\") def run(self): self.setup_class() # test self.test_mmio() self.teardown_class() pass if __name__ == \"__main__\": tb = TestCache() tb.run() 运行：\npython3 tb_cache.py 以上仅为大致的运行流程，具体可以参考：Nutshell Cache Verify。\n6 评估运行结果 运行结束之后可以得到以下数据： 行覆盖率： 功能覆盖率： 可以看到预设的MMIO功能均被覆盖且被正确触发。\n","categories":["示例项目","学习材料"],"description":"介绍验证的一般流程","excerpt":"介绍验证的一般流程","ref":"/mlvp/docs/basic/test_dut/","tags":["examples","docs"],"title":"DUT验证"},{"body":" In traditional chip verification practices, frameworks like UVM are widely adopted. Although they provide a comprehensive set of verification methodologies, they are typically confined to specific hardware description languages and simulation environments. Our tool breaks these limitations by converting simulation code into C++ or Python, allowing us to leverage software verification tools for more comprehensive testing. Given Python’s robust ecosystem, this project primarily uses Python as an example, briefly introducing two classic software testing frameworks: Pytest and Hypothesis. Pytest handles various testing needs with its simple syntax and rich features. Meanwhile, Hypothesis enhances the thoroughness and depth of testing by generating test cases that uncover unexpected edge cases. Our project is designed from the outset to be compatible with various modern software testing frameworks. We encourage you to explore the potential of these tools and apply them to your testing processes. Through hands-on practice, you will gain a deeper understanding of how these tools can enhance code quality and reliability. Let’s work together to improve the quality of chip development.\n","categories":["Sample Projects","Tutorials"],"description":"Available Software Testing Frameworks","excerpt":"Available Software Testing Frameworks","ref":"/mlvp/en/docs/env_usage/frameworks/","tags":["examples","docs"],"title":"Integrated Testing Framework"},{"body":"TUI（界面与操作） UCAgent 自带基于 urwid 的终端界面（TUI），用于在本地交互式观察任务进度、消息流与控制台输出，并直接输入命令（如进入/退出循环、切换模式、执行调试命令等）。\n界面组成 Mission 面板（左侧）\n阶段列表：显示当前任务的阶段（索引、标题、失败数、耗时）。颜色含义： 绿色：已完成阶段 红色：当前进行阶段 黄色：被跳过的阶段（显示“skipped”） Changed Files：近期修改文件（含修改时间与相对时间，如“3m ago”）。较新的文件以绿色显示。 Tools Call：工具调用状态与计数。忙碌中的工具会以黄色高亮（如 SqThink(2)）。 Deamon Commands：后台运行的 demo 命令列表（带开始时间与已运行时长）。 Status 面板（右上）\n显示 API 与代理状态摘要，以及当前面板尺寸参数（便于调节布局时参考）。 Messages 面板（右上中）\n实时消息流（模型回复、工具输出、系统提示）。 支持焦点与滚动控制，标题会显示“当前/总计”的消息定位。例如：Messages (123/456)。 Console（底部）\nOutput：命令与系统输出区域，支持分页浏览。 Input：命令输入行（默认提示符 “(UnityChip) ”）。提供历史、补全、忙碌提示等。 提示：界面每秒自动刷新一次（不影响输入）。当消息或输出过长时，会进入分页或手动滚动模式。\n操作与快捷键 Enter：执行当前输入命令；若输入为空会重复上一次命令；输入 q/Q/exit/quit 退出 TUI。 Esc： 若正在浏览 Messages 的历史，退出滚动并返回末尾； 若 Output 正在分页查看，退出分页； 否则聚焦到底部输入框。 Tab：命令补全；再次按 Tab 可分批显示更多可选项。 Shift+Right：清空 Console Output。 Shift+Up / Shift+Down：在 Messages 中向上/向下移动焦点（浏览历史）。 Ctrl+Up / Ctrl+Down：增/减 Console 输出区域高度。 Ctrl+Left / Ctrl+Right：减/增 Mission 面板宽度。 Shift+Up / Shift+Down（另一路径）：调整 Status 面板高度（最小 3，最大 100）。 Up / Down： 若 Output 在分页模式，Up/Down 用于翻页； 否则用于命令历史导航（将历史命令放入输入行，可编辑后回车执行）。 分页模式提示：当 Output 进入分页浏览时，底部标题会提示 “Up/Down: scroll, Esc: exit”，Esc 退出分页并返回输入状态。\n命令与用法 普通命令：直接输入并回车，例如 loop、tui、help 等（由内部调试器处理）。 历史命令：在输入行为空时按 Enter，将重复执行上一条命令。 清屏：输入 clear 并回车，仅清空 Output（不影响消息记录）。 演示/后台命令：命令末尾添加 \u0026 将在后台运行，完成后会在 Output 区域提示结束；当前后台命令可通过 list_demo_cmds 查看。 直接执行系统/危险命令：以 ! 前缀执行（例如 !loop），该模式执行后优先滚动到最新输出。 列出后台命令：list_demo_cmds 显示正在运行的 demo 命令列表与开始时间。 消息配置（message_config） 作用：在运行中查看/调整消息裁剪策略，控制历史保留与 LLM 输入 token 上限。 命令： message_config 查看当前配置 message_config 设置配置项 可配置项： max_keep_msgs：保留的历史消息条数（影响会话记忆窗口） max_token：进入模型前的消息裁剪 token 上限（影响开销/截断） 示例： message_config message_config max_keep_msgs 8 message_config max_token 4096 其他说明\n自动补全：支持命令名与部分参数的补全；候选项过多时分批显示，可多次按 Tab 查看剩余项。 忙碌提示：命令执行期间，输入框标题会轮转显示 (wait.), (wait..), (wait…)，表示正在处理。 消息焦点：当未手动滚动时，消息焦点自动跟随最新消息；进入手动滚动后，会保持当前位置，直至按 Esc 或滚动至末尾。 错误容错：若某些 UI 操作异常（如终端不支持某些控制序列），TUI 会尽量回退到安全状态继续运行。 ","categories":["教程"],"description":"tui界面的组成与操作说明。","excerpt":"tui界面的组成与操作说明。","ref":"/mlvp/docs/ucagent/usage/tui/","tags":["docs"],"title":"TUI"},{"body":"TUI (UI \u0026 Operations) UCAgent provides a urwid‑based terminal UI (TUI) for interactively observing task progress, message stream, and console output locally, and for entering commands directly (e.g., enter/exit loop, switch modes, run debug commands, etc.).\nLayout Mission panel (left)\nStage list: show current task stages (index, title, failures, elapsed). Color meanings: Green: completed stage Red: current stage Yellow: skipped stage (shows “skipped”) Changed Files: recently modified files (with mtime and relative time, e.g., “3m ago”). Newer files are highlighted in green. Tools Call: tool call status and counters. Busy tools are highlighted in yellow (e.g., SqThink(2)). Daemon Commands: demo commands running in background (with start time and elapsed). Status panel (top right)\nShows API and agent status summary, and current panel size parameters (useful when adjusting layout). Messages panel (upper middle right)\nLive message stream (model replies, tool output, system tips). Supports focus and scrolling; the title shows “current/total” position, e.g., Messages (123/456). Console (bottom)\nOutput: command and system output area with paging. Input: command input line (default prompt “(UnityChip) “). Provides history, completion, and busy hints. Tip: the UI auto‑refreshes every second (does not affect input). When messages or output are long, it enters paging or manual scrolling.\nShortcuts Enter: execute current input; if empty, repeat the last command; q/Q/exit/quit to exit TUI. Esc: If browsing Messages history, exit scrolling and return to the end; If Output is in paging view, exit paging; Otherwise focus the bottom input box. Tab: command completion; press Tab again to show more candidates in batches. Shift+Right: clear Console Output. Shift+Up / Shift+Down: move focus up/down in Messages (browse history). Ctrl+Up / Ctrl+Down: increase/decrease Console output area height. Ctrl+Left / Ctrl+Right: decrease/increase Mission panel width. Shift+Up / Shift+Down (another path): adjust Status panel height (min 3, max 100). Up / Down: If Output is in paging mode, Up/Down scrolls pages; Otherwise navigate command history (put the command into input line for editing and Enter to run). Paging mode hint: when Output enters paging, the footer shows “Up/Down: scroll, Esc: exit”; press Esc to exit paging and return to input.\nCommands and Usage Normal commands: enter and press Enter, e.g., loop, tui, help (handled by internal debugger). History commands: when input is empty, pressing Enter repeats the last command. Clear: type clear and press Enter; only clears Output (does not affect message history). Demo/background commands: append \u0026 to run in background; when finished, an end hint appears in Output; use list_demo_cmds to see current background commands. Directly run system/dangerous commands: prefix with ! (e.g., !loop); after running, it prioritizes scrolling to the latest output. List background commands: list_demo_cmds shows running demo commands and start times. Message Configuration (message_config) Purpose: view/adjust message trimming policy at runtime; control history retention and LLM input token limit. Commands: message_config to view current config message_config \u003ckey\u003e \u003cvalue\u003e to set a config item Configurable items: max_keep_msgs: number of historical messages to keep (affects conversation memory window) max_token: token limit for trimming before sending to model (affects cost/truncation) Examples: message_config message_config max_keep_msgs 8 message_config max_token 4096 Other notes\nAuto‑completion: supports command names and some parameters; if there are many candidates, they are shown in batches; press Tab multiple times to view remaining items. Busy hints: while a command is executing, the input box title cycles through (wait.), (wait..), (wait…), indicating processing. Message focus: when not manually scrolled, focus follows the latest message automatically; after entering manual scrolling, it stays until Esc or scrolled to the end. Error tolerance: if some UI operations fail (e.g., terminal doesn’t support some control sequences), the TUI tries to fall back to a safe state and continue running. ","categories":["Tutorial"],"description":"TUI layout and operations.","excerpt":"TUI layout and operations.","ref":"/mlvp/en/docs/ucagent/usage/tui/","tags":["docs"],"title":"TUI"},{"body":"双端口栈简介 双端口栈是一种数据结构，支持两个端口同时进行操作。与传统单端口栈相比，双端口栈允许同时进行数据的读写操作，在例如多线程并发读写等场景下，双端口栈能够提供更好的性能。本例中，我们提供了一个简易的双端口栈实现，其源码如下：\nmodule dual_port_stack ( input clk, input rst, // Interface 0 input in0_valid, output in0_ready, input [7:0] in0_data, input [1:0] in0_cmd, output out0_valid, input out0_ready, output [7:0] out0_data, output [1:0] out0_cmd, // Interface 1 input in1_valid, output in1_ready, input [7:0] in1_data, input [1:0] in1_cmd, output out1_valid, input out1_ready, output [7:0] out1_data, output [1:0] out1_cmd ); // Command definitions localparam CMD_PUSH = 2'b00; localparam CMD_POP = 2'b01; localparam CMD_PUSH_OKAY = 2'b10; localparam CMD_POP_OKAY = 2'b11; // Stack memory and pointer reg [7:0] stack_mem[0:255]; reg [7:0] sp; reg busy; reg [7:0] out0_data_reg, out1_data_reg; reg [1:0] out0_cmd_reg, out1_cmd_reg; reg out0_valid_reg, out1_valid_reg; assign out0_data = out0_data_reg; assign out0_cmd = out0_cmd_reg; assign out0_valid = out0_valid_reg; assign out1_data = out1_data_reg; assign out1_cmd = out1_cmd_reg; assign out1_valid = out1_valid_reg; always @(posedge clk or posedge rst) begin if (rst) begin sp \u003c= 0; busy \u003c= 0; end else begin // Interface 0 Request Handling if (!busy \u0026\u0026 in0_valid \u0026\u0026 in0_ready) begin case (in0_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out0_valid_reg \u003c= 1; stack_mem[sp] \u003c= in0_data; out0_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out0_valid_reg \u003c= 1; out0_data_reg \u003c= stack_mem[sp - 1]; out0_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out0_valid_reg \u003c= 0; end endcase end // Interface 1 Request Handling if (!busy \u0026\u0026 in1_valid \u0026\u0026 in1_ready) begin case (in1_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out1_valid_reg \u003c= 1; stack_mem[sp] \u003c= in1_data; out1_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out1_valid_reg \u003c= 1; out1_data_reg \u003c= stack_mem[sp - 1]; out1_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out1_valid_reg \u003c= 0; end endcase end // Interface 0 Response Handling if (busy \u0026\u0026 out0_ready) begin out0_valid_reg \u003c= 0; busy \u003c= 0; end // Interface 1 Response Handling if (busy \u0026\u0026 out1_ready) begin out1_valid_reg \u003c= 0; busy \u003c= 0; end end end assign in0_ready = (in0_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255|| in0_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy; assign in1_ready = (in1_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255|| in1_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy \u0026\u0026 !(in0_ready \u0026\u0026 in0_valid); endmodule 在该实现中，除了时钟信号(clk)和复位信号(rst)之外，还包含了两个端口的输入输出信号，它们拥有相同的接口定义。每个端口的信号含义如下：\n请求端口（in） in_valid 输入数据有效信号 in_ready 输入数据准备好信号 in_data 输入数据 in_cmd 输入命令 （0:PUSH, 1:POP） 响应端口（out） out_valid 输出数据有效信号 out_ready 输出数据准备好信号 out_data 输出数据 out_cmd 输出命令 （2:PUSH_OKAY, 3:POP_OKAY） 当我们想通过一个端口对栈进行一次操作时，首先需要将需要的数据和命令写入到输入端口，然后等待输出端口返回结果。\n具体地，如果我们想对栈进行一次 PUSH 操作。首先我们应该将需要 PUSH 的数据写入到 in_data 中，然后将 in_cmd 设置为 0，表示 PUSH 操作，并将 in_valid 置为 1，表示输入数据有效。接着，我们需要等待 in_ready 为 1，保证数据已经正确的被接收，此时 PUSH 请求已经被正确发送。\n命令发送成功后，我们需要在响应端口等待栈的响应信息。当 out_valid 为 1 时，表示栈已经完成了对应的操作，此时我们可以从 out_data 中读取栈的返回数据（POP 操作的返回数据将会放置于此），从 out_cmd 中读取栈的返回命令。当读取到数据后，需要将 out_ready 置为 1，以通知栈正确接收到了返回信息。\n如果两个端口的请求同时有效时，栈将会优先处理端口 0 的请求。\n构建驱动环境 与案例一和案例二类似，在对双端口栈进行测试之前，我们首先需要利用 Picker 工具将 RTL 代码构建为 Python Module。在构建完成后，我们将通过 Python 脚本驱动 RTL 代码进行测试。\n首先，创建名为 dual_port_stack.v 的文件，并将上述的 RTL 代码复制到该文件中，接着在相同文件夹下执行以下命令：\npicker export --autobuild=true dual_port_stack.v -w dual_port_stack.fst --sname dual_port_stack --tdir picker_out_dual_port_stack/ --lang python -e --sim verilator 生成好的驱动环境位于 picker_out_dual_port_stack 文件夹中, 其中 dual_port_stack 为生成的 Python Module。\n若自动编译运行过程中无错误发生，则代表环境被正确构建。\n利用回调函数驱动 DUT 在本案例中，为了测试双端口栈的功能，我们需要对其进行驱动。但你可能很快就会发现，仅仅使用案例一和案例二中的方法很难对双端口栈进行驱动。因为在此前的测试中，DUT只有一条执行逻辑，给DUT输入数据后等待DUT输出即可。\n但双端口栈却不同，它的两个端口是两个独立的执行逻辑，在驱动中，这两个端口可能处于完全不同的状态，例如端口0在等待DUT返回数据时，端口1有可能正在发送新的请求。这种情况下，使用简单的串行执行逻辑将很难对DUT进行驱动。\n因此我们在本案例中我们将以双端口栈为例，介绍一种基于回调函数的驱动方法，来完成此类DUT的驱动。\n回调函数简介 回调函数是一种常见的编程技术，它允许我们将一个函数传入，并等待某个条件满足后被调用。构建产生的 Python Module 中，我们提供了向内部执行环境注册回调函数的接口 StepRis，使用方法如下:\nfrom dual_port_stack import DUTdual_port_stack def callback(cycles): print(f\"The current clock cycle is {cycles}\") dut = DUTdual_port_stack() dut.StepRis(callback) dut.Step(10) 你可以直接运行该代码来查看回调函数的效果。\n在上述代码中，我们定义了一个回调函数 callback ，它接受一个参数 cycles ，并在每次调用时打印当前的时钟周期。接着通过 StepRis 将该回调函数注册到 DUT 中。\n注册回调函数后，每运行一次 Step 函数，即每个时钟周期，都会在时钟信号上升沿去调用该回调函数，并传入当前的时钟周期。\n通过这种方式，我们可以将不同的执行逻辑都写成回调函数的方式，并将多个回调函数注册到 DUT 中，从而实现对 DUT 的并行驱动。\n基于回调函数驱动的双端口栈 通过回调函数的形式来完成一条完整的执行逻辑，通常我们会使用状态机的模式进行编写。每调用一次回调函数，就会引起状态机内部的状态变化，多次调用回调函数，就会完成一次完整的执行逻辑。\n下面是一个基于回调函数驱动的双端口栈的示例代码：\nimport random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class Status(Enum): IDLE = 0 WAIT_REQ_READY = 1 WAIT_RESP_VALID = 2 class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict self.status = self.Status.IDLE self.operation_num = 0 self.remaining_delay = 0 def push(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value self.port_dict[\"in_data\"].value = random.randint(0, 2**32-1) def pop(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.POP.value def step_callback(self, cycle): if self.status == self.Status.WAIT_REQ_READY: if self.port_dict[\"in_ready\"].value == 1: self.port_dict[\"in_valid\"].value = 0 self.port_dict[\"out_ready\"].value = 1 self.status = self.Status.WAIT_RESP_VALID if self.port_dict[\"in_cmd\"].value == self.BusCMD.PUSH.value: self.model.commit_push(self.port_dict[\"in_data\"].value) elif self.status == self.Status.WAIT_RESP_VALID: if self.port_dict[\"out_valid\"].value == 1: self.port_dict[\"out_ready\"].value = 0 self.status = self.Status.IDLE self.remaining_delay = random.randint(0, 5) if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) if self.status == self.Status.IDLE: if self.remaining_delay == 0: if self.operation_num \u003c 10: self.push() elif self.operation_num \u003c 20: self.pop() else: return self.operation_num += 1 self.status = self.Status.WAIT_REQ_READY else: self.remaining_delay -= 1 def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) dut.StepRis(port0.step_callback) dut.StepRis(port1.step_callback) dut.Step(200) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") test_stack(dut) dut.Finish() 在上述代码中，实现了这样的驱动过程：每个端口独立对DUT进行驱动，并在一个请求完成后添加随机延迟，每个端口分别完成了 10 次 PUSH 操作与 10 次 POP 操作。\n在 PUSH 或 POP 请求生效时，会调用同一个 StackModel 中的 commit_push 或 commit_pop 函数，以模拟栈的行为，并在每次 POP 操作完成后对比 DUT 的返回数据与模型的数据是否一致。\n为了实现对单个端口的驱动行为，我们实现了 SinglePortDriver 类，其中实现了一个接口进行收发的完整过程，通过 step_callback 函数来实现内部的更新逻辑。\n在测试函数 test_stack 中，我们为双端口栈的每一个端口都创建了一个 SinglePortDriver 实例，传入了对应的接口，并通过 StepRis 函数将其对应的回到函数其注册到 DUT 中。之后调用 dut.Step(200) 时，每个时钟周期中都会自动调用一次回调函数，来完成整个驱动逻辑。\nSinglePortDriver 驱动逻辑\n上面提到，一般使用回调函数的形式需要将执行逻辑实现为状态机，因此在 SinglePortDriver 类中，需要记录包含端口所处的状态，它们分别是：\nIDLE：空闲状态，等待下一次操作 在空闲状态下，需要查看另一个状态 remaining_delay 来判断当前是否已经延时结束，如果延时结束可立即进行下一次操作，否则继续等待。 当需要执行下一次操作时，需要查看状态 operation_num （当前已经执行的操作数）来决定下一次操作时 PUSH 还是 POP。之后调用相关函数对端口进行一次赋值，并将状态切换至 WAIT_REQ_READY。 WAIT_REQ_READY：等待请求端口准备好 当请求发出后（in_valid 有效），此时需要等待 in_ready 信号有效，以确保请求已经被正确接受。 当请求被正确接受后，需要将 in_valid 置为 0，同时将 out_ready 置为 1，表示请求发送完毕，准备好接收回复。 WAIT_RESP_VALID：等待响应端口返回数据 当请求被正确接受后，需要等待 DUT 的回复，即等待 out_valid 信号有效。当 out_valid 信号有效时，表示回复已经产生，一次请求完成，于是将 out_ready 置为 0，同时将状态切换至 IDLE。 运行测试 在picker_out_dual_port_stack中创建exmaple.py文件，将上述代码复制到其中，然后执行以下命令：\ncd picker_out_dual_port_stack python3 example.py 可直接运行本案例的测试代码，你将会看到类似如下的输出：\n... push 77 push 140 push 249 push 68 push 104 push 222 ... Pop 43 Pass: 43 == 43 Pop 211 Pass: 211 == 211 Pop 16 Pass: 16 == 16 Pop 255 Pass: 255 == 255 Pop 222 Pass: 222 == 222 Pop 104 ... 在输出中，你可以看到每次 PUSH 和 POP 操作的数据，以及每次 POP 操作的结果。如果输出中没有错误信息，则表示测试通过。\n回调函数驱动的优劣 通过使用回调函数，我们能够完成对 DUT 的并行驱动，正如本例所示，我们通过两个回调函数实现了对拥有两个独立执行逻辑的端口的驱动。回调函数在简单的场景下，为我们提供了一种简单的并行驱动方法。\n但是通过本例也可以看出，仅仅实现一套简单的“请求-回复”流程，就需要维护大量的内部状态，回调函数将本应完整的执行逻辑拆分为了多次函数调用，为代码的编写和调试增加了诸多复杂性。\n","categories":["示例项目","教程"],"description":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用回调函数驱动DUT","excerpt":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用回调函数驱动DUT","ref":"/mlvp/docs/quick-start/eg-stack-callback/","tags":["examples","docs"],"title":"案例三：双端口栈（回调）"},{"body":"部分电路有多个时钟，XClock 类提供了分频功能，可以通过它实现对多时钟电路的驱动。\nXClock 中的 FreqDivWith 接口 XClock 函数提供如下分频接口\nvoid XClock::FreqDivWith(int div, // 分频数，，即绑定的XClock的频率为原时钟频率的div分之1 XClock \u0026clk, // 绑定的XClock int shift=0) // 对波形进行 shift 个半周期的移位 XClock 的一般驱动流程 创建 XClock，绑定 DUT 的驱动函数 # 假设已经创建了DUT，并将其命名为dut # 创建XClock xclock = XClock(dut.dut.simStep) 绑定关联 clk 引脚 # clk是dut的时钟引脚 xclock.Add(dut.clk) # Add方法具有别名：AddPin 通过 XPort 绑定与 clk 关联的引脚 因为在我们的工具中，对于端口的读写是通过 xclock 来驱动的，所以如果不将与 clk 关联的引脚绑定到 XClock 上，那么在驱动时，相关的引脚数值不会发生变化。\n比如，我们要进行复位操作，那么可以将 reset 绑定到 xclock 上。\n方法：\nclass XClock: def Add(xport) #将Clock和XData进行绑定 举例：\n# xclock.Add(dut.xport.Add(pin_name, XData)) xclock.Add(dut.xport.Add(\"reset\", dut.reset)) 在经过了前面的绑定之后，接下来可以使用了。\n我们根据需要来设置回调、设置分频。当然，时序电路肯定也要驱动时钟。\n这些方法都可以参照工具介绍。\n下面是举例：\n# func为回调函数，args为自定义参数 #设置上升沿回调函数 dut.StepRis(func, args=(), kwargs={}) #设置下降沿回调函数 dut.StepFal(func, args=(), kwargs={}) # 假设xclock是XClock的实例 xclock.FreqDivWith(2, half_clock) # 将xclock的频率分频为原来的一半 xclock.FreqDivWith(1, left_clock， -2) # 将xclock的频率不变，对波形进行一个周期的左移 dut.Step(10) #推进10个时钟周期 多时钟案例 例如多时钟电路有 6 个 clock，每个 clock 都有一个对应的计数器，设计代码如下：\nmodule multi_clock ( input wire clk1, input wire clk2, input wire clk3, input wire clk4, input wire clk5, input wire clk6, output reg [31:0] reg1, output reg [31:0] reg2, output reg [31:0] reg3, output reg [31:0] reg4, output reg [31:0] reg5, output reg [31:0] reg6 ); initial begin reg1 = 32'b0; reg2 = 32'b0; reg3 = 32'b0; reg4 = 32'b0; reg5 = 32'b0; reg6 = 32'b0; end always @(posedge clk1) begin reg1 \u003c= reg1 + 1; end always @(posedge clk2) begin reg2 \u003c= reg2 + 1; end always @(posedge clk3) begin reg3 \u003c= reg3 + 1; end always @(posedge clk4) begin reg4 \u003c= reg4 + 1; end always @(posedge clk5) begin reg5 \u003c= reg5 + 1; end always @(posedge clk6) begin reg6 \u003c= reg6 + 1; end endmodule 通过picker导出：\npicker export multi_clock.v -w mc.fst --tdir picker_out/MultiClock --lang python 可以通过如下 Python 进行多时钟驱动：\nfrom MultiClock import * from xspcomm import XClock def test_multi_clock(): # 创建DUT dut = DUTmulti_clock() # 创建主时钟 main_clock = XClock(dut.dut.simStep) # 创建子时钟 clk1, clk2, clk3 = XClock(lambda x: 0), XClock(lambda x: 0), XClock(lambda x: 0) clk4, clk5, clk6 = XClock(lambda x: 0), XClock(lambda x: 0), XClock(lambda x: 0) # 给子时钟添加相关的clock引脚及关联端口 clk1.Add(dut.xport.SelectPins([\"reg1\"])).AddPin(dut.clk1.xdata) clk2.Add(dut.xport.SelectPins([\"reg2\"])).AddPin(dut.clk2.xdata) clk3.Add(dut.xport.SelectPins([\"reg3\"])).AddPin(dut.clk3.xdata) clk4.Add(dut.xport.SelectPins([\"reg4\"])).AddPin(dut.clk4.xdata) clk5.Add(dut.xport.SelectPins([\"reg5\"])).AddPin(dut.clk5.xdata) clk6.Add(dut.xport.SelectPins([\"reg6\"])).AddPin(dut.clk6.xdata) # 将主时钟频率分频到子时钟 main_clock.FreqDivWith(1, clk1) main_clock.FreqDivWith(2, clk2) main_clock.FreqDivWith(3, clk3) main_clock.FreqDivWith(1, clk4, -1) main_clock.FreqDivWith(2, clk5, 1) main_clock.FreqDivWith(3, clk6, 2) # 驱动时钟 main_clock.Step(100) dut.Finish() if __name__ == \"__main__\": test_multi_clock() 上述代码输出的波形如下：\n可以看到：\nclk2 的周期是 clk1 的 2 倍 clk3 的周期是 clk1 的 3 倍， clk4 的周期和 clk1 相同，但是进行了半个周期的右移 clk5 的周期和 clk2 相同，但是进行了半个周期的左移 clk6 的周期和 clk3 相同，但是进行了一个周期的左移 ","categories":["示例项目","教程"],"description":"多时钟示例","excerpt":"多时钟示例","ref":"/mlvp/docs/env_usage/multiclock/","tags":["examples","docs"],"title":"多时钟"},{"body":"Introduction to the Dual-Port Stack and Environment Setup The dual-port stack used in this case is identical to the one implemented in Case 3. Please refer to the Introduction to the Dual-Port Stack and Driver Environment Setup in Case 3 for more details.\nDriving the DUT Using Coroutines In Case 3, we used callbacks to drive the DUT. While callbacks offer a way to perform parallel operations, they break the execution flow into multiple function calls and require maintaining a large amount of intermediate state, making the code more complex to write and debug.\nIn this case, we will introduce a method of driving the DUT using coroutines. This method not only allows for parallel operations but also avoids the issues associated with callbacks.\nIntroduction to Coroutines Coroutines are a form of “lightweight” threading that enables behavior similar to concurrent execution without the overhead of traditional threads. Coroutines operate on a single-threaded event loop, where multiple coroutines can be defined and added to the event loop, with the event loop managing their scheduling.\nTypically, a defined coroutine will continue to execute until it encounters an event that requires waiting. At this point, the event loop pauses the coroutine and schedules other coroutines to run. Once the event occurs, the event loop resumes the paused coroutine to continue execution.\nFor parallel execution in hardware verification, this behavior is precisely what we need. We can create multiple coroutines to handle various verification tasks. We can treat the clock execution as an event, and within each coroutine, wait for this event. When the clock signal arrives, the event loop wakes up all the waiting coroutines, allowing them to continue executing until they wait for the next clock signal. We use Python’s asyncio to implement coroutine support:\nimport asyncio from dual_port_stack import * async def my_coro(dut, name): for i in range(10): print(f\"{name}: {i}\") await dut.AStep(1) async def test_dut(dut): asyncio.create_task(my_coro(dut, \"coroutine 1\")) asyncio.create_task(my_coro(dut, \"coroutine 2\")) await asyncio.create_task(dut.RunStep(10)) dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_dut(dut)) dut.Finish() You can run the above code directly to observe the execution of coroutines. In the code, we use create_task to create two coroutine tasks and add them to the event loop. Each coroutine task continuously prints a number and waits for the next clock signal.We use dut.RunStep(10) to create a background clock, which continuously generates clock synchronization signals, allowing other coroutines to continue execution when the clock signal arrives.\nDriving the Dual-Port Stack with Coroutines Using coroutines, we can write the logic for driving each port of the dual-port stack as an independent execution flow without needing to maintain a large amount of intermediate state.\nBelow is a simple verification code using coroutines:\nimport asyncio import random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"Push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict async def send_req(self, is_push): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value if is_push else self.BusCMD.POP.value self.port_dict[\"in_data\"].value = random.randint(0, 2**8-1) await self.dut.AStep(1) await self.dut.ACondition(lambda: self.port_dict[\"in_ready\"].value == 1) self.port_dict[\"in_valid\"].value = 0 if is_push: self.model.commit_push(self.port_dict[\"in_data\"].value) async def receive_resp(self): self.port_dict[\"out_ready\"].value = 1 await self.dut.AStep(1) await self.dut.ACondition(lambda: self.port_dict[\"out_valid\"].value == 1) self.port_dict[\"out_ready\"].value = 0 if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) async def exec_once(self, is_push): await self.send_req(is_push) await self.receive_resp() for _ in range(random.randint(0, 5)): await self.dut.AStep(1) async def main(self): for _ in range(10): await self.exec_once(is_push=True) for _ in range(10): await self.exec_once(is_push=False) async def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) asyncio.create_task(port0.main()) asyncio.create_task(port1.main()) await asyncio.create_task(dut.RunStep(200)) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_stack(dut)) dut.Finish() Similar to Case 3, we define a SinglePortDriver class to handle the logic for driving a single port. In the main function, we create two instances of SinglePortDriver, each responsible for driving one of the two ports. We place the driving processes for both ports in the main function and add them to the event loop using asyncio.create_task. Finally, we use dut.RunStep(200) to create a background clock to drive the test. This code implements the same test logic as in Case 3, where each port performs 10 PUSH and 10 POP operations, followed by a random delay after each operation. As you can see, using coroutines eliminates the need to maintain any intermediate state. SinglePortDriver Logic In the SinglePortDriver class, we encapsulate a single operation into the exec_once function. In the main function, we first call exec_once(is_push=True) 10 times to complete the PUSH operations, and then call exec_once(is_push=False) 10 times to complete the POP operations.In the exec_once function, we first call send_req to send a request, then call receive_resp to receive the response, and finally wait for a random number of clock signals to simulate a delay.The send_req and receive_resp functions have similar logic; they set the corresponding input/output signals to the appropriate values and wait for the corresponding signals to become valid. The implementation can be written according to the execution sequence of the ports.Similarly, we use the StackModel class to simulate stack behavior. The commit_push and commit_pop functions simulate the PUSH and POP operations, respectively, with the POP operation comparing the data.\nRunning the Test Copy the above code into example.py and then execute the following commands:\ncd picker_out_dual_port_stack python3 example.py You can run the test code for this case directly, and you will see output similar to the following:\n... Push 141 Push 102 Push 63 Push 172 Push 208 Push 130 Push 151 ... Pop 102 Pass: 102 == 102 Pop 138 Pass: 138 == 138 Pop 56 Pass: 56 == 56 Pop 153 Pass: 153 == 153 Pop 129 Pass: 129 == 129 Pop 235 Pass: 235 == 235 Pop 151 ... In the output, you can see the data for each PUSH and POP operation, as well as the result of each POP operation. If there are no error messages in the output, it indicates that the test passed.\nPros and Cons of Coroutine-Driven Design Using coroutine functions, we can effectively achieve parallel operations while avoiding the issues that come with callback functions. Each independent execution flow can be fully retained as a coroutine, which greatly simplifies code writing.\nHowever, in more complex scenarios, you may find that having many coroutines can make synchronization and timing management between them more complicated. This is especially true when you need to synchronize between two coroutines that do not directly interact with the DUT. At this point, you’ll need a set of coroutine writing standards and design patterns for verification code to help you write coroutine-based verification code more effectively. Therefore, we provide the mlvp library, which offers a set of design patterns for coroutine-based verification code. You can learn more about mlvp and how it can help you write better verification code by visiting here .\n","categories":["Example Projects","Tutorials"],"description":"The dual-port stack is a stack with two ports, each supporting push and pop operations. This case study uses the dual-port stack as an example to demonstrate how to drive a DUT using coroutines.","excerpt":"The dual-port stack is a stack with two ports, each supporting push …","ref":"/mlvp/en/docs/quick-start/eg-stack-async/","tags":["examples","docs"],"title":"Case 4: Dual-Port Stack (Coroutines)"},{"body":"FAQ 模型切换：在 config.yaml 改 openai.model_name 验证过程中出现错误怎么办：使用 Ctrl+C 进入交互模式，通过 status 查看当前状态，使用 help 获取调试命令。 Check 失败：先 ReadTextFile 阅读 reference_files；再按返回信息修复，循环 RunTestCases → Check 自定义阶段：修改 vagent/lang/zh/config/default.yaml 的 stage；或用 --override 临时覆盖 添加工具：vagent/tools/ 下新建类，继承 UCTool，运行时 --ex-tools YourTool MCP 连接失败：检查端口/防火墙，改 --mcp-server-port；无嵌入可加 --no-embed-tools 只读保护：通过 --no-write/--nw 指定路径限制写入（必须位于 workspace 内） 为什么快速启动找不到config.yaml/定制流程时找不到config.yaml? 使用pip 安装后并没有config.yaml那个文件，所以在快速启动的启动 MCP Server没有加--config config.yaml这个选项。 可以通过在工作目录添加config.yaml文件并且加上--config config.yaml参数来启动；也可以使用克隆仓库来使用UCAgent的方式来解决。 运行中如何调整消息窗口与 token 上限？ 在 TUI 输入：message_config 查看当前配置； 设置：message_config max_keep_msgs 8 或 message_config max_token 4096； 作用范围：影响会话历史裁剪与送入 LLM 的最大 token 上限（通过 Summarization/Trim 节点生效）。 文档中的 “CK bug” 要改吗？ 是。术语统一为 “TC bug”。同时确保 bug 文档里的 \u003cTC-*\u003e 能匹配失败用例（文件/类/用例名）。 为什么找不到 WriteTextFile 工具？ 该工具已移除。请改用 EditTextFile（支持 overwrite/append/replace 三种模式）或其他文件工具（Copy/Move/Delete 等）。 ","categories":["教程"],"description":"常见问题与解答。","excerpt":"常见问题与解答。","ref":"/mlvp/docs/ucagent/usage/faq/","tags":["docs"],"title":"FAQ"},{"body":"FAQ Switch model: set openai.model_name in config.yaml. Errors during verification: press Ctrl+C to enter interactive mode; run status and help. Check failed: read reference_files via ReadTextFile; fix per hints; iterate RunTestCases → Check. Custom stages: edit vagent/lang/zh/config/default.yaml or use --override. Add tools: create class under vagent/tools/, inherit UCTool, and load with --ex-tools YourTool. MCP connection: check port/firewall; change --mcp-server-port; add --no-embed-tools if no embedding. Read-only protection: limit writes with --no-write/--nw (paths must be under workspace). Why is there no default config.yaml in Quick Start? When installed via pip, there is no repo config.yaml, so Quick Start Start MCP Server doesn’t pass --config config.yaml. You can add a config.yaml in your workspace and start with --config config.yaml, or clone the repo to use the built-in configs. Adjust message window and token limit? In TUI: message_config to view; set message_config max_keep_msgs 8 or message_config max_token 4096. Scope: affects conversation history trimming and the maximum token limit sent to the LLM (effective via the Summarization/Trim node). “CK bug” vs “TC bug”? Use the unified term “TC bug”. Ensure \u003cTC-*\u003e in the bug doc maps to failing tests. Where is WriteTextFile? Removed. Use EditTextFile (overwrite/append/replace) or other file tools. ","categories":["Tutorial"],"description":"Common questions and answers.","excerpt":"Common questions and answers.","ref":"/mlvp/en/docs/ucagent/usage/faq/","tags":["docs"],"title":"FAQ"},{"body":"Below is an overview of the built‑in tools (UCTool family) in this repository, grouped by function: name (call name), purpose, and parameter description (field: type — meaning).\nTips:\nTools with “file write” capability are only available locally/in allowed‑write mode; in MCP no‑file‑tools mode, write‑type tools are not exposed. All tools validate parameters via args_schema; MCP clients will render parameter forms from the schema. Basics / Info RoleInfo (RoleInfo)\nPurpose: return the current agent’s role information (can customize role_info at startup). Parameters: none HumanHelp (HumanHelp)\nPurpose: ask a human for help (use only when truly stuck). Parameters: message: str — help message Planning / ToDo CreateToDo Purpose: create a ToDo (overwrites any existing ToDo). Parameters: task_description: str — task description steps: List[str] — steps (1–20 steps) CompleteToDoSteps Purpose: mark specified steps as completed, with optional notes. Parameters: completed_steps: List[int] — step indices to mark done (1‑based) notes: str — notes UndoToDoSteps Purpose: undo step completion status, with optional notes. Parameters: steps: List[int] — step indices to undo (1‑based) notes: str — notes ResetToDo Purpose: reset/clear the current ToDo. Parameters: none GetToDoSummary / ToDoState Purpose: get ToDo summary / short kanban‑style status phrase. Parameters: none Memory / Retrieval SemanticSearchInGuidDoc (SemanticSearchInGuidDoc)\nPurpose: semantic search within Guide_Doc / project docs, returning the most relevant fragments. Parameters: query: str — query text limit: int — number of results (1–100, default 3) MemoryPut\nPurpose: write long‑term memory by scope. Parameters: scope: str — namespace/scope (e.g. general / task‑specific) data: str — content (can be JSON text) MemoryGet\nPurpose: retrieve memory by scope. Parameters: scope: str — namespace/scope query: str — query text limit: int — number of results (1–100, default 3) Test / Execution RunPyTest (RunPyTest)\nPurpose: run pytest under a directory/file; can return stdout/stderr. Parameters: test_dir_or_file: str — test directory or file pytest_ex_args: str — extra pytest args (e.g. “-v –capture=no”) return_stdout: bool — whether to return stdout return_stderr: bool — whether to return stderr timeout: int — timeout in seconds (default 15) RunUnityChipTest (RunUnityChipTest)\nPurpose: UnityChip‑oriented test runner wrapper producing toffee_report.json etc. Parameters: same as RunPyTest; additionally internal fields (workspace / result_dir / result_json_path). File / Path / Text SearchText (SearchText)\nPurpose: text search within workspace; supports glob and regex. Parameters: pattern: str — search pattern (plain/glob/regex) directory: str — relative directory (empty for repo‑wide; if a file path, only search that file) max_match_lines: int — max matched lines per file (default 20) max_match_files: int — max files to return (default 10) use_regex: bool — use regex or not case_sensitive: bool — case sensitive or not include_line_numbers: bool — whether to include line numbers FindFiles (FindFiles)\nPurpose: find files by glob. Parameters: pattern: str — filename pattern (fnmatch glob) directory: str — relative directory (empty for repo‑wide) max_match_files: int — max files to return (default 10) PathList (PathList)\nPurpose: list directory structure (depth‑limited). Parameters: path: str — directory (relative to workspace) depth: int — depth (‑1 all, 0 current) ReadBinFile (ReadBinFile)\nPurpose: read binary file (returns [BIN_DATA]). Parameters: path: str — file path (relative to workspace) start: int — start byte (default 0) end: int — end byte (default ‑1 means EOF) ReadTextFile (ReadTextFile)\nPurpose: read text file (with line numbers, returns [TXT_DATA]). Parameters: path: str — file path (relative to workspace) start: int — start line (1‑based, default 1) count: int — number of lines (‑1 to end of file) EditTextFile (EditTextFile)\nPurpose: edit/create text file; modes: replace/overwrite/append. Parameters: path: str — file path (relative to workspace; created if not exists) data: str — text to write (None to clear) mode: str — edit mode (replace/overwrite/append; default replace) start: int — start line for replace mode (1‑based) count: int — number of lines to replace in replace mode (‑1 to end, 0 insert) preserve_indent: bool — whether to preserve indentation in replace mode CopyFile (CopyFile)\nPurpose: copy file; optional overwrite. Parameters: source_path: str — source file dest_path: str — destination file overwrite: bool — whether to overwrite if destination exists MoveFile (MoveFile)\nPurpose: move/rename file; optional overwrite. Parameters: source_path: str — source file dest_path: str — destination file overwrite: bool — whether to overwrite if destination exists DeleteFile (DeleteFile)\nPurpose: delete file. Parameters: path: str — file path CreateDirectory (CreateDirectory)\nPurpose: create directory (recursive). Parameters: path: str — directory path parents: bool — create parents recursively exist_ok: bool — ignore if already exists ReplaceStringInFile (ReplaceStringInFile)\nPurpose: exact string replacement (strict matching; can create file). Parameters: path: str — target file old_string: str — full original text to replace (with context, exact match) new_string: str — new content GetFileInfo (GetFileInfo)\nPurpose: get file info (size, mtime, human‑readable size etc.). Parameters: path: str — file path Extension Example SimpleReflectionTool (SimpleReflectionTool) Purpose: example “self‑reflection” tool (from extool.py), as an extension reference. Parameters: message: str — self‑reflection text Notes:\nTool call timeout defaults to 20s (individual tools may override); for long tasks, periodically output progress to avoid timeout. In MCP no‑file‑tools mode, write‑type tools are not exposed by default; if writing is required, prefer the local Agent mode or restrict writable directories explicitly. ","categories":["Reference"],"description":"UCAgent built-in tool catalog (by category).","excerpt":"UCAgent built-in tool catalog (by category).","ref":"/mlvp/en/docs/ucagent/tool_list/","tags":["docs"],"title":"Tool List"},{"body":" After we complete the DUT verification, writing a verification report is a crucial step. This section will provide an overview of the structure of the verification report and the content that needs to be covered.\nThe verification report is a review of the entire verification process and an important supporting document for determining the reasonableness of the verification. Generally, the verification report should include the following content:\nBasic document information (author, log, version, etc.) Verification object (verification target) Introduction to functional points Verification plan Breakdown of test points Test cases Test environment Result analysis Defect analysis Verification conclusion The following content provides further explanation of the list, with specific examples available innutshell_cache_report_demo.pdf\n1. Basic Information Including author, log, version, date, etc.\n2. Verification object (verification target) A necessary introduction to your verification object, which may include its structure, basic functions, interface information, etc.\n3. Introduction to functional points By reading the design documents or source code, you need to summarize the target functions of the DUT and break them down into various functional points.\n4. Verification plan Including your planned verification process and verification framework. Additionally, you should explain how each part of your framework works together.\n5. Breakdown of test points Proposed testing methods for the functional points. Specifically, it can include what signal output should be observed under certain signal inputs.\n6. Test cases The specific implementation of the test points. A test case can include multiple test points.\n7. Test environment Including hardware information, software version information, etc.\n8. Result analysis Result analysis generally refers to coverage analysis. Typically, two types of coverage should be considered: 1. Line Coverage： How many RTL lines of code are executed in the test cases. Generally, we require line coverage to be above 98%.\n2. Functional Coverage：Determine whether the extracted functional points are covered and correctly triggered based on the relevant signals. We generally require test cases to cover each functional point.\n9. Defect analysis Analyze the defects present in the DUT. This can include the specification and detail of the design documents, the correctness of the DUT functions (whether there are bugs), and whether the DUT functions can be triggered.\n10. Verification conclusion The final conclusion drawn after completing the chip verification process, summarizing the above content.\n","categories":["Example Projects","Learning Materials"],"description":"An overview of the structure and content of the verification report.","excerpt":"An overview of the structure and content of the verification report.","ref":"/mlvp/en/docs/basic/report/","tags":["examples","docs"],"title":"Verification Report"},{"body":"双端口栈简介与环境构建 本案例中使用的双端口栈与案例三中的实现完全相同，请查看案例三中的双端口栈简介及构建驱动环境。\n利用协程驱动 DUT 在案例三中，我们使用了回调函数的方式来驱动DUT，回调函数虽然给我们提供了一种能够完成并行操作的方式，然而其却把完成的执行流程割裂为多次函数调用，并需要维护大量中间状态，导致代码的编写及调试变得较为复杂。\n在本案例中，我们将会介绍一种通过协程驱动的方法，这种方法不仅能够做到并行操作，同时能够很好地避免回调函数所带来的问题。\n协程简介 协程是一种“轻量级”的线程，通过协程，你可以实现与线程相似的并发执行的行为，但其开销却远小于线程。其实现原理是，协程库实现了一个运行于单线程之上的事件循环（EventLoop），程序员可以定义若干协程并且加入到事件循环，由事件循环负责这些协程的调度。\n一般来说，我们定义的协程在执行过程中会持续执行，直到遇到一个需要等待的“事件”，此时事件循环就会暂停执行该协程，并调度其他协程运行。当事件发生后，事件循环会再次唤醒该协程，继续执行。\n对于硬件验证中的并行执行来说，这种特性正是我们所需要的，我们可以创建多个协程，来完成验证中的多个驱动任务。我们可以将时钟的执行当做事件，在每个协程中等待这个事件，当时钟信号到来时，事件循环会唤醒所有等待的协程，使其继续执行，直到他们等待下一个时钟信号。\n我们用 Python 中的 asyncio 来实现对协程的支持：\nimport asyncio from dual_port_stack import * async def my_coro(dut, name): for i in range(10): print(f\"{name}: {i}\") await dut.AStep(1) async def test_dut(dut): asyncio.create_task(my_coro(dut, \"coroutine 1\")) asyncio.create_task(my_coro(dut, \"coroutine 2\")) await asyncio.create_task(dut.RunStep(10)) dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_dut(dut)) dut.Finish() 你可以直接运行上述代码来观察协程的执行过程。在上述代码中我们用 create_task 创建了两个协程任务并加入到事件循环中，每个协程任务中，会不断打印一个数字并等待下一个时钟信号到来。\n我们使用 dut.RunStep(10) 来创建一个后台时钟，它会不断产生时钟同步信号，使得其他协程能够在时钟信号到来时继续执行。\n基于协程驱动的双端口栈 利用协程，我们就可以将驱动双端口栈中单个端口逻辑写成一个独立的执行流，不需要再去维护大量的中间状态。\n下面是我们提供的一个简单的使用协程驱动的验证代码：\nimport asyncio import random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"Push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict async def send_req(self, is_push): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value if is_push else self.BusCMD.POP.value self.port_dict[\"in_data\"].value = random.randint(0, 2**8-1) await self.dut.AStep(1) await self.dut.ACondition(lambda: self.port_dict[\"in_ready\"].value == 1) self.port_dict[\"in_valid\"].value = 0 if is_push: self.model.commit_push(self.port_dict[\"in_data\"].value) async def receive_resp(self): self.port_dict[\"out_ready\"].value = 1 await self.dut.AStep(1) await self.dut.ACondition(lambda: self.port_dict[\"out_valid\"].value == 1) self.port_dict[\"out_ready\"].value = 0 if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) async def exec_once(self, is_push): await self.send_req(is_push) await self.receive_resp() for _ in range(random.randint(0, 5)): await self.dut.AStep(1) async def main(self): for _ in range(10): await self.exec_once(is_push=True) for _ in range(10): await self.exec_once(is_push=False) async def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) asyncio.create_task(port0.main()) asyncio.create_task(port1.main()) await asyncio.create_task(dut.RunStep(200)) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_stack(dut)) dut.Finish() 与案例三类似，我们定义了一个 SinglePortDriver 类，用于驱动单个端口的逻辑。在 main 函数中，我们创建了两个 SinglePortDriver 实例，分别用于驱动两个端口。我们将两个端口的驱动过程放在了入口函数 main 中，并通过 asyncio.create_task 将其加入到事件循环中，在最后我们通过 dut.RunStep(200) 来创建了后台时钟，以推动测试的进行。\n该代码实现了与案例三一致的测试逻辑，即在每个端口中对栈进行 10 次 PUSH 和 10 次 POP 操作，并在操作完成后添加随机延迟。但你可以清晰的看到，利用协程进行编写，不需要维护任何的中间状态。\nSinglePortDriver 逻辑\n在 SinglePortDriver 类中，我们将一次操作封装为 exec_once 这一个函数，在 main 函数中只需要首先调用 10 次 exec_once(is_push=True) 来完成 PUSH 操作，再调用 10 次 exec_once(is_push=False) 来完成 POP 操作即可。\n在 exec_once 函数中，我们首先调用 send_req 函数来发送请求，然后调用 receive_resp 函数来接收响应，最后通过等待随机次数的时钟信号来模拟延迟。\nsend_req 和 receive_resp 函数的实现逻辑类似，只需要将对应的输入输出信号设置为对应的值，然后等待对应的信号变为有效即可，可以完全根据端口的执行顺序来编写。\n类似地，我们使用 StackModel 类来模拟栈的行为，在 commit_push 和 commit_pop 函数中分别模拟了 PUSH 和 POP 操作，并在 POP 操作中进行了数据的比较。\n运行测试 在picker_out_dual_port_stack文件夹中创建example.py将上述代码复制到其中，然后执行以下命令：\ncd picker_out_dual_port_stack python3 example.py 可直接运行本案例的测试代码，你将会看到类似如下的输出：\n... Push 141 Push 102 Push 63 Push 172 Push 208 Push 130 Push 151 ... Pop 102 Pass: 102 == 102 Pop 138 Pass: 138 == 138 Pop 56 Pass: 56 == 56 Pop 153 Pass: 153 == 153 Pop 129 Pass: 129 == 129 Pop 235 Pass: 235 == 235 Pop 151 ... 在输出中，你可以看到每次 PUSH 和 POP 操作的数据，以及每次 POP 操作的结果。如果输出中没有错误信息，则表示测试通过。\n协程驱动的优劣 通过协程函数，我们可以很好地实现并行操作，同时避免了回调函数所带来的问题。每个独立的执行流都能被完整保留，实现为一个协程，大大方便了代码的编写。\n然而，在更为复杂的场景下你会发现，实现了众多协程，会使得协程之间的同步和时序管理变得复杂。尤其是你需要在两个不与DUT直接交互的协程之间进行同步时，这种现象会尤为明显。\n在这时候，你就需要一套协程编写的规范以及验证代码的设计模式，来帮助你更好地编写基于协程的验证代码。因此，我们提供了 toffee 库，它提供了一套基于协程的验证代码设计模式，你可以通过使用 toffee 来更好地编写验证代码，你可以在这里去进一步了解 toffee。\n","categories":["示例项目","教程"],"description":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用协程驱动DUT","excerpt":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用协程驱动DUT","ref":"/mlvp/docs/quick-start/eg-stack-async/","tags":["examples","docs"],"title":"案例四：双端口栈（协程）"},{"body":"在Verilog中，一个module只有一个实例，但很多测试场景下需要实现多个module，为此picker提供了动态多实例和静态多实例的支持。\n动态多实例 动态多实例相当于类的实例化，在创建dut的同时实例化对应的module，所以用户无感知。支持最大16个实例同时运行。\n例子：\n以Adder为例，我们可以在测试时根据需要在合适的位置创建多个dut，来动态创建多个Adder实例。 当需要销毁一个dut时，也不会影响后续创建新的dut。\n创建一个名为 picker_out_adder 的文件夹，其中包含一个 Adder.v 文件。该文件的源码参考案例一：简单加法器。\n运行下述命令将RTL导出为 Python Module：\npicker export Adder.v --autobuild true -w Adder.fst --sname Adder 在picker_out_adder中添加 example.py，动态创建多个Adder实例：\nfrom Adder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): dut=[] # 可以通过创建多个dut，实例化多个Adder，理论上支持最大16个实例同时运行 for i in range(7): # 这里通过循环创建了7个dut dut.append(DUTAdder(waveform_filename=f\"{i}.fst\")) for d in dut: d.a.value = random_int() d.b.value = random_int() d.cin.value = random_int() \u0026 1 d.Step(1) print(f\"DUT: sum={d.sum.value}, cout={d.cout.value}\") # 通过Finish()函数在合适的时机撤销某个dut，也即销毁某个实例 d.Finish() # 可以根据需要在合适的时机创建新的Adder实例 # 下面创建了一个新的dut，旨在说明可以在程序结束前的任何时机创建新的dut dut_new = DUTAdder(waveform_filename=f\"new.fst\") dut_new.a.value = random_int() dut_new.b.value = random_int() dut_new.cin.value = random_int() \u0026 1 dut_new.Step(1) print(f\"DUT: sum={dut_new.sum.value}, cout={dut_new.cout.value}\") dut_new.Finish() if __name__ == \"__main__\": main() 注：目前仅支持 verilator模拟器\n静态多实例 静态多实例的使用不如动态多实例灵活，相当于在进行dut封装时就创建了n个目标模块。 需要在使用 picker 生成 dut_top.sv/v 的封装时，通过–sname参数指定多个模块名称和对应的数量。\n单个模块需要多实例 同样以Adder为例，在使用picker对dut进行封装时执行如下命令：\npicker export Adder.v --autobuild true -w Adder.fst --sname Adder,3 通过–sname参数指定在dut中创建3个Adder，封装后dut的引脚定义为：\n# init.py # 这里仅放置了部分代码 class DUTAdder(object): ... # all Pins # 静态多实例 self.Adder_0_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_0_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_0_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_1_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_1_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_1_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_2_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_2_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_2_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_2_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_2_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) ... 可以看到在 picker 生成 dut 时，就在 DUTAdder 内创建了多个Adder实例。\n下面是简单的多实例代码举例：\nfrom Adder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): # 在dut内部实例化了多个Adder dut = DUTAdder(waveform_filename = \"1.fst\") dut.Adder_0_a.value = random_int() dut.Adder_0_b.value = random_int() dut.Adder_0_cin.value = random_int() \u0026 1 dut.Adder_1_a.value = random_int() dut.Adder_1_b.value = random_int() dut.Adder_1_cin.value = random_int() \u0026 1 dut.Adder_2_a.value = random_int() dut.Adder_2_b.value = random_int() dut.Adder_2_cin.value = random_int() \u0026 1 dut.Step(1) print(f\"Adder_0: sum={dut.Adder_0_sum.value}, cout={dut.Adder_0_cout.value}\") print(f\"Adder_1: sum={dut.Adder_1_sum.value}, cout={dut.Adder_1_cout.value}\") print(f\"Adder_2: sum={dut.Adder_2_sum.value}, cout={dut.Adder_2_cout.value}\") # 静态多实例不可以根据需要动态的创建新的Adder实例，三个Adder实例的周期与dut的生存周期相同 dut.Finish() if __name__ == \"__main__\": main() 多个模块需要多实例 例如在 Adder.v 和 RandomGenerator.v 设计文件中分别有模块 Adder 和 RandomGenerator，RandomGenerator.v文件的源码为：\nmodule RandomGenerator ( input wire clk, input wire reset, input [127:0] seed, output [127:0] random_number ); reg [127:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[126:0], lfsr[127] ^ lfsr[126]}; end end assign random_number = lfsr; endmodule 需要 DUT 中有 2 个 Adder，3 个 RandomGenerator，生成的模块名称为 RandomAdder（若不指定，默认名称为 Adder_Random），则可执行如下命令：\npicker export Adder.v,RandomGenerator.v --sname Adder,2,RandomGenerator,3 --tname RandomAdder -w randomadder.fst 得到封装后的dut为DUTRandomAdder，包含2个Adder实例和3个RandomGenerator实例。\n封装后dut的引脚定义为：\n# init.py # 这里仅放置了部分代码 class DUTRandomAdder(object): ... # all Pins # 静态多实例 self.Adder_0_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_0_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_0_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_1_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_1_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_1_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.RandomGenerator_0_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_0_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_0_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_0_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.RandomGenerator_1_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_1_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_1_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_1_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.RandomGenerator_2_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_2_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_2_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_2_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) ... 可以看到在 picker 生成 dut 时，就在 DUTAdder 内创建了多个Adder实例。\n对应的测试代码举例为：\nfrom RandomAdder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): # 在dut内部实例化了多个Adder dut = DUTRandomAdder() dut.InitClock(\"RandomGenerator_0_clk\") dut.InitClock(\"RandomGenerator_1_clk\") dut.InitClock(\"RandomGenerator_2_clk\") dut.Adder_0_a.value = random_int() dut.Adder_0_b.value = random_int() dut.Adder_0_cin.value = random_int() \u0026 1 dut.Adder_1_a.value = random_int() dut.Adder_1_b.value = random_int() dut.Adder_1_cin.value = random_int() \u0026 1 # 在dut内部实例化了多个RandomGenerator seed = random.randint(0, 2**128 - 1) dut.RandomGenerator_0_seed.value = seed dut.RandomGenerator_0_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_0_random_number.value:x}\") dut.Step(1) dut.RandomGenerator_1_seed.value = seed dut.RandomGenerator_1_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_1_random_number.value:x}\") dut.Step(1) dut.RandomGenerator_2_seed.value = seed dut.RandomGenerator_2_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_2_random_number.value:x}\") dut.Step(1) print(f\"Adder_0: sum={dut.Adder_0_sum.value}, cout={dut.Adder_0_cout.value}\") print(f\"Adder_1: sum={dut.Adder_1_sum.value}, cout={dut.Adder_1_cout.value}\") # 静态多实例各个模块多个实例的生命周期与dut的生命周期相同 dut.Finish() if __name__ == \"__main__\": main() ","categories":["示例项目","教程"],"description":"多实例示例","excerpt":"多实例示例","ref":"/mlvp/docs/env_usage/multiinstance/","tags":["examples","docs"],"title":"多实例"},{"body":"以下为当前仓库内内置工具（UCTool 家族）的概览，按功能类别归纳：名称（调用名）、用途与参数说明（字段: 类型 — 含义）。\n提示：\n带有“文件写”能力的工具仅在本地/允许写模式下可用；MCP 无文件工具模式不会暴露写类工具。 各工具均基于 args_schema 校验参数，MCP 客户端将根据 schema 生成参数表单。 基础/信息类 RoleInfo（RoleInfo）\n用途：返回当前代理的角色信息（可在启动时自定义 role_info）。 参数：无 HumanHelp（HumanHelp）\n用途：向人类请求帮助（仅在确实卡住时使用）。 参数： message: str — 求助信息 规划/ToDo 类 CreateToDo 用途：创建 ToDo（覆盖旧 ToDo）。 参数： task_description: str — 任务描述 steps: List[str] — 步骤（1–20 步） CompleteToDoSteps 用途：将指定步骤标记为完成，可附加备注。 参数： completed_steps: List[int] — 完成的步骤序号（1-based） notes: str — 备注 UndoToDoSteps 用途：撤销步骤完成状态，可附加备注。 参数： steps: List[int] — 撤销的步骤序号（1-based） notes: str — 备注 ResetToDo 用途：重置/清空当前 ToDo。 参数：无 GetToDoSummary / ToDoState 用途：获取 ToDo 摘要 / 看板状态短语。 参数：无 记忆/检索类 SemanticSearchInGuidDoc（SemanticSearchInGuidDoc）\n用途：在 Guide_Doc/项目文档中做语义检索，返回最相关片段。 参数： query: str — 查询语句 limit: int — 返回条数（1–100，默认 3） MemoryPut\n用途：按 scope 写入长时记忆。 参数： scope: str — 命名空间/范围（如 general/task-specific） data: str — 内容（可为 JSON 文本） MemoryGet\n用途：按 scope 检索记忆。 参数： scope: str — 命名空间/范围 query: str — 查询语句 limit: int — 返回条数（1–100，默认 3） 测试/执行类 RunPyTest（RunPyTest）\n用途：在指定目录/文件下运行 pytest，支持返回 stdout/stderr。 参数： test_dir_or_file: str — 测试目录或文件 pytest_ex_args: str — 额外 pytest 参数（如 “-v --capture=no”） return_stdout: bool — 是否返回标准输出 return_stderr: bool — 是否返回标准错误 timeout: int — 超时秒数（默认 15） RunUnityChipTest（RunUnityChipTest）\n用途：面向 UnityChip 项目封装的测试执行，产生 toffee_report.json 等结果。 参数：同 RunPyTest；另含内部字段（workspace/result_dir/result_json_path）。 文件/路径/文本类 SearchText（SearchText）\n用途：在工作区内按文本搜索，支持通配与正则。 参数： pattern: str — 搜索模式（明文/通配/正则） directory: str — 相对目录（为空则全仓；填文件则仅搜该文件） max_match_lines: int — 每个文件返回的最大匹配行数（默认 20） max_match_files: int — 返回的最大文件数（默认 10） use_regex: bool — 是否使用正则 case_sensitive: bool — 区分大小写 include_line_numbers: bool — 返回是否带行号 FindFiles（FindFiles）\n用途：按通配符查找文件。 参数： pattern: str — 文件名模式（fnmatch 通配） directory: str — 相对目录（为空则全仓） max_match_files: int — 返回最大文件数（默认 10） PathList（PathList）\n用途：列出目录结构（可限制深度）。 参数： path: str — 目录（相对 workspace） depth: int — 深度（-1 全部，0 当前） ReadBinFile（ReadBinFile）\n用途：读取二进制文件（返回 [BIN_DATA]）。 参数： path: str — 文件路径（相对 workspace） start: int — 起始字节（默认 0） end: int — 结束字节（默认 -1 表示 EOF） ReadTextFile（ReadTextFile）\n用途：读取文本文件（带行号，返回 [TXT_DATA]）。 参数： path: str — 文件路径（相对 workspace） start: int — 起始行（1-based，默认 1） count: int — 行数（-1 到文件末尾） EditTextFile（EditTextFile）\n用途：编辑/创建文本文件，模式：replace/overwrite/append。 参数： path: str — 文件路径（相对 workspace，不存在则创建） data: str — 写入的文本（None 表示清空） mode: str — 编辑模式（replace/overwrite/append，默认 replace） start: int — replace 模式的起始行（1-based） count: int — replace 模式替换行数（-1 到末尾，0 插入） preserve_indent: bool — replace 时是否保留缩进 CopyFile（CopyFile）\n用途：复制文件；可选覆盖。 参数： source_path: str — 源文件 dest_path: str — 目标文件 overwrite: bool — 目标存在时是否覆盖 MoveFile（MoveFile）\n用途：移动/重命名文件；可选覆盖。 参数： source_path: str — 源文件 dest_path: str — 目标文件 overwrite: bool — 目标存在时是否覆盖 DeleteFile（DeleteFile）\n用途：删除文件。 参数： path: str — 文件路径 CreateDirectory（CreateDirectory）\n用途：创建目录（递归）。 参数： path: str — 目录路径 parents: bool — 递归创建父目录 exist_ok: bool — 已存在是否忽略 ReplaceStringInFile（ReplaceStringInFile）\n用途：精确字符串替换（强约束匹配；可新建文件）。 参数： path: str — 目标文件 old_string: str — 需要被替换的完整原文（含上下文，精确匹配） new_string: str — 新内容 GetFileInfo（GetFileInfo）\n用途：获取文件信息（大小、修改时间、人类可读尺寸等）。 参数： path: str — 文件路径 扩展示例 SimpleReflectionTool（SimpleReflectionTool） 用途：示例型“自我反思”工具（来自 extool.py），可作为扩展参考。 参数： message: str — 自我反思文本 备注：\n工具调用超时默认 20s（具体工具可重写）；长任务请周期性输出进度避免超时。 MCP 无文件工具模式下默认不暴露写类工具；如需写入，建议在本地 Agent 模式或按需限制可写目录。 ","categories":["参考"],"description":"UCAgent 可用工具清单（按类别归纳）。","excerpt":"UCAgent 可用工具清单（按类别归纳）。","ref":"/mlvp/docs/ucagent/tool_list/","tags":["docs"],"title":"工具列表"},{"body":" 在我们完成DUT验证后，编写验证报告是至关重要的一环。本节将从整体角度概述验证报告的结构以及报告所需覆盖的内容。\n验证报告是对整个验证过程的回顾，是验证合理与否的重要支持文件。一般情况下，验证报告需要包含以下内容：\n文档基本信息（作者、日志、版本等） 验证对象（验证目标） 功能点介绍 验证方案 测试点分解 测试用例 测试环境 结果分析 缺陷分析 测试结论 以下内容对列表进行进一步解释，具体示例可以参考nutshell_cache_report_demo.pdf\n1. 基本信息 应当包括作者、日志、版本、日期等。\n2. 验证对象（验证目标） 需要对您的验证对象做必要的介绍，可以包括其结构、基本功能、接口信息等。\n3. 功能点介绍 通过阅读设计文档或者源码，您需要总结DUT的目标功能，并将其细化为各功能点。\n4. 验证方案 应当包括您计划的验证流程以及验证框架。同时，您也应当接受您的框架各部分是如何协同工作的。\n5. 测试点分解 针对功能点提出的测试方法。具体可以包括在怎样的信号输入下应当观测到怎样的信号输出。\n6. 测试用例 测试点的具体实现。一个测试用例可以包括多个测试点。\n7. 测试环境 包括硬件信息、软件版本信息等。\n8. 结果分析 结果分析一般指覆盖率分析。通常来说应当考虑两类覆盖率：\n1. 行覆盖率： 在测试用例中有多少RTL行代码被执行。一般来说我们要求行覆盖率在98%以上。\n2. 功能覆盖率：根据相应的信号判断您提取的功能点是否被覆盖且被正确触发。一般我们要求测试用例覆盖每个功能点。\n9. 缺陷分析 对DUT存在的缺陷进行分析。可以包括设计文档的规范性和详细性、DUT功能的正确性（是否存在bug）、DUT功能是否能被触发等方面。\n10. 验证结论 验证结论是在完成芯片验证过程后得出的最终结论，是对以上内容的总结。\n","categories":["示例项目","学习材料"],"description":"概述验证报告的结构与内容。","excerpt":"概述验证报告的结构与内容。","ref":"/mlvp/docs/basic/report/","tags":["examples","docs"],"title":"验证报告"},{"body":"Internal signals refer to those not exposed at the module’s IO ports, but which play roles in control, data transfer, or state tracking within the module. Typically, when picker converts RTL to DUT, only IO ports are automatically exposed, and internal signals are not exported by default.\nHowever, when more detailed verification of internal module logic is needed, or when debugging known issues, verification engineers often need access to these internal signals. In addition to traditional tools like Verilator and VCS, picker also provides an internal signal extraction mechanism as an auxiliary tool.\nMotivation Take the up-counter as an example:\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); wire upper; assign upper = (count == 4'b1111); always @(posedge clk) begin if (reset) begin count = 4'b0000; end else if (!upper) begin count = count + 1; end end endmodule The IO signals of the module are those defined directly in the module declaration, such as:\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); Here, clk, reset, and count are IO signals and can be directly accessed. The following wire upper; is an internal signal, whose value is determined by both module inputs and internal logic. While the counter logic in this example is simple, larger hardware modules often present the following challenges:\nWhen the module output does not match expectations, the problematic code range is large, making it hard to quickly narrow down the issue. Complex internal logic makes understanding and analysis difficult; internal signals can serve as key markers to clarify the module’s operation. For these issues, accessing and analyzing internal signals is very effective. Traditionally, tools like Verilator and VCS are used to observe internal signals. To further lower the verification barrier, picker provides three internal signal access methods: DPI direct export, VPI dynamic access, and direct memory read/write.\nDPI Direct Export DPI (Direct Programming Interface) is an interface for interaction between Verilog and other languages. In picker’s default implementation, DPI is provided for the IO ports of the DUT. When running picker, if the --internal option is added, DPI can also be provided for internal signals. Picker will use a predefined internal signal file to extract both internal signals and IO ports from the RTL when converting to DUT.\nWriting the Signal File The signal file specifies to picker which internal signals to extract, listing the module and the internal signals to extract.\nExample internal.yaml:\nUpperCounter: - \"wire upper\" The first line is the module name (e.g., UpperCounter). From the second line, list the internal signals to extract in the format “type signal_name”. For example, if upper is a wire, write \"wire upper\". (In theory, as long as the signal name matches the variable name in the Verilog code, it can be matched. The type can be arbitrary, but it’s recommended to use Verilog-supported types like wire, reg, logic, etc.)\nThe ability to extract internal signals depends on the simulator. For example, Verilator cannot extract signals starting with an underscore _.\nNote: For multi-bit internal signals, you need to explicitly specify the width, so the actual format is “type [width] signal_name”.\nUpperCounter: - \"wire upper\" - \"reg [3:0] another_multiples\" # This signal does not exist in this example, just to illustrate the yaml format Export Parameter After writing the signal file, you need to specify it explicitly when running picker using the --internal option:\n--internal=[internal_signal_file] Full command example:\npicker export --autobuild=true upper_counter.sv -w upper_counter.fst --sname UpperCounter \\ --tdir picker_out_upper_counter/ --lang python -e --sim verilator --internal=internal.yaml You can find the signals.json file generated by picker for the DUT:\n{ \"UpperCounter_upper\": { \"High\": -1, \"Low\": 0, \"Pin\": \"wire\", \"_\": true }, \"clk\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true }, \"count\": { \"High\": 3, \"Low\": 0, \"Pin\": \"output\", \"_\": true }, \"reset\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true } } This file shows the signal interfaces generated by picker. The first signal, UpperCounter_upper, is the internal signal we wanted to extract. The part before the first underscore is the module name defined in internal.yaml, and the rest is the internal signal name.\nSignal Access After extraction, accessing internal signals is no different from accessing IO signals—they are all XData objects on the DUT and can be accessed as dut.signal_name.\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() print(dut.UpperCounter_upper.value) VPI Dynamic Access VPI (Verilog Procedural Interface) is a standard interface in Verilog that allows external programs (like C) to interact with the Verilog simulator during simulation. With VPI, users can access, read, and modify signals, variables, and module instances in the Verilog simulation, as well as register callbacks to control and extend the simulation process. VPI is often used for custom system tasks, advanced verification, dynamic signal access, and waveform processing. VPI is part of the IEEE 1364 standard.\nExport Parameter picker export --help ... --vpi Enable VPI, for flexible internal signal access default is OFF Enable VPI support with the --vpi parameter, for example:\npicker export upper_counter.sv --sname UpperCounter --tdir picker_out_upper_counter/ --lang python --vpi Signal Access After enabling --vpi, you can use the DUT interface dut.GetInternalSignalList(use_vpi=True) to list all accessible internal signals, and dut.GetInternalSignal(name, use_vpi=True) to dynamically construct XData for data access.\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() # List all internal signals # Or use dut.VPIInternalSignalList() dut.GetInternalSignalList(use_vpi=True) # Dynamically construct XData internal_upper = dut.GetInternalSignal(\"UpperCounter.upper\", use_vpi=True) # Read access print(internal_upper.value) # Write access (writing is possible, but the value will be overwritten after dut.step; not recommended for non-reg types) internal_upper.value = 0x1 Direct Memory Read/Write Both DPI and VPI-based internal signal access have some performance overhead. For maximum performance, picker implements direct internal signal access for Verilator/GSIM simulators.\nExport Parameter picker export --help ... --rw,--access-mode ENUM:value in {dpi-\u003e0,mem_direct-\u003e1} OR {0,1} Enable direct memory read/write for Verilator by using --rw 1, for example:\npicker export upper_counter.sv --sname UpperCounter --tdir picker_out_upper_counter/ --lang python --rw 1 Signal Access After enabling direct memory read/write, you can use dut.GetInternalSignalList(use_vpi=False) to list all internal signals, and dut.GetInternalSignal(name, use_vpi=False) to dynamically construct XData for signal read/write.\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() # List all internal signals dut.GetInternalSignalList(use_vpi=False) # Dynamically construct XData internal_upper = dut.GetInternalSignal(\"UpperCounter_top.UpperCounter.upper\", use_vpi=False) # Read access print(internal_upper.value) # Write access (writing is possible, but the value will be overwritten after dut.step; not recommended for non-reg types) internal_upper.value = 0x1 Comparison of Internal Signal Access Methods Each internal signal access method provided by picker has its own advantages and disadvantages. Choose according to your needs.\nMethod Name Enable Option Advantages Disadvantages Access Interface Supported Simulators Suitable Scenarios DPI Direct Export –internal=cfg.yaml Fast Must specify signals in advance\nRead-only\nRecompile after changes Same as normal pins verilator, VCS Few signals, no write needed VPI Dynamic Access –vpi Flexible, all signals\nNo need to specify in advance Slow GetInternalSignalList\nGetInternalSignal verilator, VCS Small designs or not speed-critical Direct Mem R/W –rw 1 Fast\nFlexible\nNo need to specify in advance Some signals may be optimized away GetInternalSignalList\nGetInternalSignal verilator, GSIM Large designs, e.g., Xiangshan CPU *Note: These methods are independent and can be used together.\n","categories":["Example Project","Tutorial"],"description":"Internal Signal Example","excerpt":"Internal Signal Example","ref":"/mlvp/en/docs/env_usage/internalsignal/","tags":["examples","docs"],"title":"Internal Signals"},{"body":"YunSuan介绍 YunSuan模块是开源高性能RISC-V处理器项目XiangShan（香山）的核心组成部分，专门负责实现处理器的各种算术和逻辑运算功能，包括整数运算单元、浮点运算单元、向量处理单元等。YunSuan模块与XiangShan处理器的流水线紧密集成，为RISC-V指令集提供完整的运算支持。 赛题简介 本次比赛分为两个排名赛道，找bug赛道和token效率赛道，本次赛题均选自该模块的向量处理单元。每个模块含有五道题，1、2题为简单难度；3题为中等难度；4、5题困难难度。\n找bug 你需要将选题中的bug找出，确认后在网站上提交； 发现注入bug记100-500分； 若发现全新bug（即在未注入bug的版本中找到新bug，第一个提交队伍活动额外奖金）记 1000-2000分； 最后将按积分进行排名，前3名发奖金\ntoken效率 利用UCAgent工具的API模式寻找bug，计算token效率：E = bug数/消耗总token （发现人工注入bug必须 \u003e 80%）； 结果按E进行队伍排序，前3名发奖金；\n备注：token将由UCAgent统计；你需要修改UCAgent以达到更高的token效率 UCAgent链接：https://github.com/XS-MLVP/UCAgent\n本次给出每个模块第一道题目，剩余题目将于活动当天一并给出\n赛题\u0026模块： VectorFloatFMA VectorFloatFMA是XiangShan处理器中YunSuan模块的关键组件，专门执行RISC-V V扩展的向量浮点乘加指令，支持FP16、FP32和FP64格式的并行计算，包括乘法、乘加、乘减等操作，并处理舍入模式、异常标志及特殊情况如NaN和无穷大，以实现高性能向量浮点运算。 无bug版本 题目1（简单） Spec：向量浮点指令\nVectorFloatAdder模块 VectorFloatAdder是一个支持多种浮点运算的向量加法器模块，能够处理半精度（f16）、单精度（f32）和双精度（f64）浮点数，并支持扩展操作（如扩展输入和扩展结果）。它支持包括加法、减法、最小值、最大值、比较、符号注入、分类和归约操作在内的多种操作。该模块通过多个子模块并行处理不同精度的数据，并根据操作码和格式选择相应的结果和异常标志。 无bug版本 题目1（简单） Spec：向量浮点指令\nVectorIdiv模块 VectorIdiv 是一个支持多种数据位宽的向量整数除法器模块。它通过并行实例化多个不同位宽的除法子模块来处理128位向量数据，采用状态机控制除法流程，最终输出商、余数向量和除零标志。该设计实现了高效的向量化整数除法运算。 无bug版本 题目1（简单） Spec：向量整数除法指令\n","categories":["参考"],"description":"黑客马拉松赛题","excerpt":"黑客马拉松赛题","ref":"/mlvp/docs/ucagent/hackathon/","tags":["docs"],"title":"黑客马拉松"},{"body":"内部信号是指未在模块IO端口中暴露，但在模块内部承担控制、数据传输或状态跟踪等功能的信号。通常，picker在将RTL转换为DUT时只会自动暴露IO端口，内部信号不会被主动导出。\n但在需要对模块内部逻辑进行更细致验证，或根据已知bug进一步定位问题时，验证人员往往需要访问这些内部信号。除了传统的verilator和VCS等工具，picker还提供了内部信号提取机制，可作为辅助手段。\n动机 以上限计数器为例：\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); wire upper; assign upper = (count == 4'b1111); always @(posedge clk) begin if (reset) begin count = 4'b0000; end else if (!upper) begin count = count + 1; end end endmodule 模块的IO信号指的是直接写在模块定义中的信号，也就是：\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); 该部分中的 clk、reset 和 count 是 IO 信号，可以直接暴露访问。而紧接着的 wire upper; 则属于内部信号，其取值由模块输入和模块内部逻辑共同决定。本案例中的计数器逻辑较为简单，但对于更大规模的硬件模块，常常会遇到以下难题：\n当模块输出与预期不符时，问题范围较大，难以及时定位，需要有效手段快速缩小排查范围； 模块内部逻辑复杂，理解和分析存在困难，此时也需要借助内部信号作为关键标记，理清模块运行机制。 针对上述问题，访问和分析内部信号是非常有效的手段。传统上，通常借助如 Verilator、VCS 等仿真工具来查看内部信号。为进一步降低验证门槛，picker 还提供了三种内部信号访问方式：DPI 直接导出、VPI 动态访问和直接内存读写。\nDPI 直接导出 DPI即Direct Programming Interface，是verilog与其他语言交互的接口，在picker的默认实现中，支持了为待测硬件模块的IO端口提供DPI。在执行picker时，如果添加了--internal 选项，则可同样为待测模块的内部信号提供DPI。此时，picker将会基于预定义的内部信号文件，在将verilog转化为DUT时，同步抽取rtl中的内部信号和IO端口一并暴露出来。\n编写信号文件 信号文件是我们向picker指定需要提取的内部信号的媒介，它规定了需提取内部信号的模块和该模块需要提取的内部信号。\n示例internal.yaml，内容如下：\nUpperCounter: - \"wire upper\" 第一行是模块名称，如UpperCounter，第二行开始是需要提取的模块内部信号，以“类型 信号名”的格式写出。比如，upper的类型为wire，我们就写成“wire upper” （理论上只要信号名符合verilog代码中的变量名就可以匹配到对应的信号，类型随便写都没问题，但还是建议写verilog语法支持的类型，比如wire、log、logic等）\n内部信号提取的能力取决于模拟器，譬如，verilator就无法提取下划线_开头的信号。\n注：多位宽的内部信号需要显式写出位宽，所以实际的格式是“类型 [宽度] 信号名”\nUpperCounter: - \"wire upper\" - \"reg [3:0] another_multiples\" # 本案例中这个信号不存在，只是用于说明yaml的格式 选项支持 写好信号文件之后，需要在运行picker时显式指定内部文件，这通过internal选项完成：\n--internal=[internal_signal_file] 完整命令如下：\npicker export --autobuild=true upper_counter.sv -w upper_counter.fst --sname UpperCounter \\ --tdir picker_out_upper_counter/ --lang python -e --sim verilator --internal=internal.yaml 我们可以找到picker为DUT配套生成的signals.json文件：\n{ \"UpperCounter_upper\": { \"High\": -1, \"Low\": 0, \"Pin\": \"wire\", \"_\": true }, \"clk\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true }, \"count\": { \"High\": 3, \"Low\": 0, \"Pin\": \"output\", \"_\": true }, \"reset\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true } } 这个文件展示了picker生成的信号接口，可以看到，第一个信号UpperCounter_upper就是我们需要提取的内部信号， 其中第一个下划线之前的部分是我们在internal.yaml中的第一行定义的模块名UpperCounter，后面的部分则是内部信号名。\n信号访问 picker完成提取之后，内部信号的访问和io信号的访问就没有什么区别了，本质上他们都是dut上的一个XData，使用“dut.信号名”的方式访问即可。\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() print(dut.UpperCounter_upper.value) VPI动态访问 VPI（Verilog Procedural Interface）是Verilog语言的一种标准接口，用于在仿真时让C语言等外部程序与Verilog仿真器进行交互。通过VPI，用户可以在C程序中访问、读取、修改Verilog仿真中的信号、变量、模块实例等信息，还可以注册回调函数，实现对仿真过程的控制和扩展。VPI常用于开发自定义系统任务、实现高级验证功能、动态信号访问和波形处理等。VPI 是 IEEE 1364 标准的一部分。\n选项支持 picker export --help ... --vpi Enable VPI, for flexible internal signal access default is OFF 可通过参数--vpi开启VPI支持，例如：\npicker export upper_counter.sv --sname UpperCounter --tdir picker_out_upper_counter/ --lang python --vpi 信号访问 开启--vpi后，可通过DUT的接口dut.GetInternalSignalList(use_vpi=True)列出所有内部可访问信号，通过dut.GetInternalSignal(name, use_vpi=True)动态构建XData进行数据访问。\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() # 列出所有内部信号 # 或者通过 dut.VPIInternalSignalList() dut.GetInternalSignalList(use_vpi=True) # 动态构建 XData internal_upper = dut.GetInternalSignal(\"UpperCounter.upper\", use_vpi=True) # 读访问 print(internal_upper.value) # 写访问 (虽然能写入，但是dut step后值会被覆盖，不建议对非reg类型进行写操作) internal_upper.value = 0x1 直接内存读写 无论是基于DPI还是VPI进行内部信号访问都有一定的性能开销，为了实现极致性能体验，picker针对verilator/GSIM仿真器实现了内部信号直接访问。\n选项支持 picker export --help ... --rw,--access-mode ENUM:value in {dpi-\u003e0,mem_direct-\u003e1} OR {0,1} 可通过参数--rw 1开启针对verilator仿真器内部信号直接读写功能，例如：\npicker export upper_counter.sv --sname UpperCounter --tdir picker_out_upper_counter/ --lang python --rw 1 信号访问 开启直接内存读写后，可通过dut.GetInternalSignalList(use_vpi=False)列出所有内部信号，通过dut.GetInternalSignal(name, use_vpi=False)动态构建XData实现信号读写。\nfrom UpperCounter import * def test(): dut = DUTUpperCounter() # 列出所有内部信号 dut.GetInternalSignalList(use_vpi=False) # 动态构建 XData internal_upper = dut.GetInternalSignal(\"UpperCounter_top.UpperCounter.upper\", use_vpi=False) # 读访问 print(internal_upper.value) # 写访问 (虽然能写入，但是dut step后值会被覆盖，不建议对非reg类型进行写操作) internal_upper.value = 0x1 内部信号访问方法对比 picker提供的每种内部信号访问方法都有各自的优缺点，需要按需要进行选择。\n方法名称 开启参数 优点 缺点 访问接口 支持仿真器 适用场景 DPI 直接导出 –internal=cfg.yaml 速度快 需要提前指定信号\n信号只读\n修改后需要重新编译 无（同普通引脚） verilator、VCS 信号少，不需要写操作 VPI动态访问 –vpi 灵活，信号全\n不需要提前指定信号 速度慢 GetInternalSignalList\nGetInternalSignal verilator、VCS 小规模电路或不在意仿真速度 直接内存读写 –rw 1 速度快\n灵活\n不需要提前指定信号 部分信号可能被优化掉 GetInternalSignalList\nGetInternalSignal verilator、GSIM 大规模电路，例如整个香山核 *注： 上述方法彼此独立，可以混用\n","categories":["示例项目","教程"],"description":"内部信号示例","excerpt":"内部信号示例","ref":"/mlvp/docs/env_usage/internalsignal/","tags":["examples","docs"],"title":"内部信号"},{"body":"","categories":["Sample Projects","Tutorials"],"description":"The Open Verification Platform supports multiple languages.","excerpt":"The Open Verification Platform supports multiple languages.","ref":"/mlvp/en/docs/multi-lang/","tags":["examples","docs"],"title":"Multi-language Support"},{"body":"","categories":["示例项目","教程"],"description":"开放验证平台支持多种语言","excerpt":"开放验证平台支持多种语言","ref":"/mlvp/docs/multi-lang/","tags":["examples","docs"],"title":"多语言支持"},{"body":" This learning resource introduces the basic concepts and techniques related to verification, as well as how to use the open-source tools provided by this project for chip verification.\nBefore studying this material, it is assumed that you already have basic knowledge of Linux, Python, etc.\nRelevant learning materials:\n《Linux 101》Online Lecture Notes Python Official Python Tutorial Official Python Tutorial If you plan to participate in the “Open Source Verification Projects” published on this platform, it is recommended to complete the study of this material in advance.\n","categories":"","description":"","excerpt":" This learning resource introduces the basic concepts and techniques …","ref":"/mlvp/en/docs/","tags":"","title":"Learning Resources"},{"body":" 本学习资源介绍验证相关的基本概念、技术，以及如何使用本项目提供的开源工具进行芯片验证\n学习本材料前，假定您已经拥有linux、python等相关基础知识。\n相关学习材料：\n《Linux 101》在线讲义 Python官方教程 Javatpoint上的Git基础 若计划参与本平台上发布的“开源开放验证项目”，建议提前完本材料的学习。\n","categories":"","description":"","excerpt":" 本学习资源介绍验证相关的基本概念、技术，以及如何使用本项目提供的开源工具进行芯片验证\n学习本材料前，假定您已经拥有linux、python …","ref":"/mlvp/docs/","tags":"","title":"学习资源"},{"body":"软件测试 在正式开始pytest 之间我们先了解一下软件的测试，软件测试一般分为如下四个方面\n单元测试：称模块测试，针对软件设计中的最小单位——程序模块，进行正确性检查的测试工作 集成测试：称组装测试，通常在单元测试的基础上，将所有程序模块进行有序的、递增测试，重点测试不同模块的接口部分 系统测试：将整个软件系统看成一个整体进行测试，包括对功能、性能以及软件所运行的软硬件环境进行测试 验收测试：指按照项目任务书或合同、供需双方约定的验收依据文档进行的对整个系统的测试与评审，决定是否接收或拒收系统 pytest最初是作为一个单元测试框架而设计的，但它也提供了许多功能，使其能够进行更广泛的测试，包括集成测试，系统测试，他是一个非常成熟的全功能的python 测试框架。 它通过收集测试函数和模块，并提供丰富的断言库来简化测试的编写和运行，是一个非常成熟且功能强大的 Python 测试框架，具有以下几个特点：\n简单灵活：Pytest 容易上手，且具有灵活性。 支持参数化：您可以轻松地为测试用例提供不同的参数。 全功能：Pytest 不仅支持简单的单元测试，还可以处理复杂的功能测试。您甚至可以使用它来进行自动化测试，如 Selenium 或 Appium 测试，以及接口自动化测试（结合 Pytest 和 Requests 库）。 丰富的插件生态：Pytest 有许多第三方插件，您还可以自定义扩展。一些常用的插件包括： pytest-selenium：集成 Selenium。 pytest-html：生成HTML测试报告。 pytest-rerunfailures：在失败的情况下重复执行测试用例。 pytest-xdist：支持多 CPU 分发。 与 Jenkins 集成良好。 支持 Allure 报告框架。 本文将基于测试需求简单介绍pytest的用法，其完整手册在这里，供同学们进行深入学习。\nPytest安装 # 安装pytest： pip install pytest # 升级pytest pip install -U pytest # 查看pytest版本 pytest --version # 查看已安装包列表 pip list # 查看pytest帮助文档 pytest -h # 安装第三方插件 pip install pytest-sugar pip install pytest-rerunfailures pip install pytest-xdist pip install pytest-assume pip install pytest-html Pytest使用 命名规则 # 首先在使用pytest 时我们的模块名通常是以test开头或者test结尾，也可以修改配置文件，自定义命名规则 # test_*.py 或 *_test.py test_demo1 demo2_test # 模块中的类名要以Test 开始且不能有init 方法 class TestDemo1: class TestLogin: # 类中定义的测试方法名要以test_开头 test_demo1(self) test_demo2(self) # 测试用例 class test_one: def test_demo1(self): print(\"测试用例1\") def test_demo2(self): print(\"测试用例2\") Pytest 参数 pytest支持很多参数，可以通过help命令查看\npytest -help 我们在这里列出来常用的几个：\n-m: 用表达式指定多个标记名。 pytest 提供了一个装饰器 @pytest.mark.xxx，用于标记测试并分组（xxx是你定义的分组名），以便你快速选中并运行，各个分组直接用 and、or 来分割。\n-v: 运行时输出更详细的用例执行信息 不使用-v参数，运行时不会显示运行的具体测试用例名称；使用-v参数，会在 console 里打印出具体哪条测试用例被运行。\n-q: 类似 unittest 里的 verbosity，用来简化运行输出信息。 使用 -q 运行测试用例，仅仅显示很简单的运行信息， 例如：\n.s.. [100%] 3 passed, 1 skipped in 9.60s -k: 可以通过表达式运行指定的测试用例。 它是一种模糊匹配，用 and 或 or 区分各个关键字，匹配范围有文件名、类名、函数名。\n-x: 出现一条测试用例失败就退出测试。 在调试时，这个功能非常有用。当出现测试失败时，停止运行后续的测试。\n-s: 显示print内容 在运行测试脚本时，为了调试或打印一些内容，我们会在代码中加一些print内容，但是在运行pytest时，这些内容不会显示出来。如果带上-s，就可以显示了。\npytest test_se.py -s Pytest 选择测试用例执行 在 Pytest 中，您可以按照测试文件夹、测试文件、测试类和测试方法的不同维度来选择执行测试用例。\n按照测试文件夹执行 # 执行所有当前文件夹及子文件夹下的所有测试用例 pytest . # 执行跟当前文件夹同级的tests文件夹及子文件夹下的所有测试用例 pytest ../tests # 按照测试文件执行 # 运行test_se.py下的所有的测试用例 pytest test_se.py # 按照测试类执行，必须以如下格式： pytest 文件名 .py:: 测试类，其中“::”是分隔符，用于分割测试module和测试类。 # 运行test_se.py文件下的，类名是TestSE下的所有测试用例 pytest test_se.py::TestSE # 测试方法执行，必须以如下格式： pytest 文件名 .py:: 测试类 :: 测试方法，其中 “::” 是分隔符，用于分割测试module、测试类，以及测试方法。 # 运行test_se.py文件下的，类名是TestSE下的，名字为test_get_new_message的测试用例 pytest test_se.py::TestSE::test_get_new_message # 以上选择测试用例的方法均是在**命令行**，如果您想直接在测试程序里执行可以直接在main函数中**调用pytest.main()**,其格式为： pytest.main([模块.py::类::方法]) 此外，Pytest 还支持控制测试用例执行的多种方式，例如过滤执行、多进程运行、重试运行等。\n使用Pytest编写验证 在测试过程中，我们使用之前验证过的加法器，进入Adder文件夹，在picker_out_adder目录下新建一个test_adder.py文件，内容如下： # 导入测试模块和所需的库 from Adder import * import pytest import ctypes import random # 使用 pytest fixture 来初始化和清理资源 @pytest.fixture def adder(): # 创建 DUTAdder 实例，加载动态链接库 dut = DUTAdder() # 执行一次时钟步进，准备 DUT dut.Step(1) # yield 语句之后的代码会在测试结束后执行，用于清理资源 yield dut # 清理DUT资源，并生成测试覆盖率报告和波形 dut.Finish() class TestFullAdder: # 将 full_adder 定义为静态方法，因为它不依赖于类实例 @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # 使用 pytest.mark.usefixtures 装饰器指定使用的 fixture @pytest.mark.usefixtures(\"adder\") # 定义测试方法，adder 参数由 pytest 通过 fixture 注入 def test_adder(self, adder): # 进行多次随机测试 for _ in range(114514): # 随机生成 64 位的 a 和 b，以及 1 位的进位 cin a = random.getrandbits(64) b = random.getrandbits(64) cin = random.getrandbits(1) # 设置 DUT 的输入 adder.a.value = a adder.b.value = b adder.cin.value = cin # 执行一次时钟步进 adder.Step(1) # 使用静态方法计算预期结果 sum, cout = self.full_adder(a, b, cin) # 断言 DUT 的输出与预期结果相同 assert sum == adder.sum.value assert cout == adder.cout.value if __name__ == \"__main__\": pytest.main(['-v', 'test_adder.py::TestFullAdder']) 运行测试之后输出如下： collected 1 item test_adder.py ✓ 100% ██████████ Results (4.33s): 测试成功表明，在经过114514次循环之后，我们的设备暂时没有发现bug。然而，使用多次循环的随机数生成测试用例会消耗大量资源，并且这些随机生成的测试用例可能无法有效覆盖所有边界条件。在下一部分，我们将介绍一种更有效的测试用例生成方法。\n","categories":["示例项目","教程"],"description":"可用来管理测试，生成测试报告","excerpt":"可用来管理测试，生成测试报告","ref":"/mlvp/docs/env_usage/frameworks/pytest/","tags":["examples","docs"],"title":"PyTest"},{"body":"Software Testing Before we start with pytest, let’s understand software testing. Software testing generally involves the following four aspects:\nUnit Testing: Also known as module testing, it involves checking the correctness of program modules, which are the smallest units in software design. Integration Testing: Also known as assembly testing, it usually builds on unit testing by sequentially and incrementally testing all program modules, focusing on the interface parts of different modules. System Testing: It treats the entire software system as a whole for testing, including testing the functionality, performance, and the software’s running environment. Acceptance Testing: Refers to testing the entire system according to the project task book, contract, and acceptance criteria agreed upon by both the supply and demand sides, to determine whether to accept or reject the system. pytest was initially designed as a unit testing framework, but it also provides many features that allow it to be used for a wider range of testing, including integration testing and system testing. It is a very mature full-featured Python testing framework. It simplifies test writing and execution by collecting test functions and modules and providing a rich assertion library. It is a very mature and powerful Python testing framework with the following key features:\nSimple and Flexible: Pytest is easy to get started with and is flexible. Supports Parameterization: You can easily provide different parameters for test cases. Full-featured: Pytest not only supports simple unit testing but can also handle complex functional testing. You can even use it for automation testing, such as Selenium or Appium testing, as well as interface automation testing (combining Pytest with the Requests library). Rich Plugin Ecosystem: Pytest has many third-party plugins, and you can also customize extensions. Some commonly used plugins include: pytest-selenium: Integrates Selenium. pytest-html: Generates HTML test reports. pytest-rerunfailures: Repeats test cases in case of failure. pytest-xdist: Supports multi-CPU distribution. Well Integrated with Jenkins. Supports Allure Report Framework. This article will briefly introduce the usage of pytest based on testing requirements. The complete manual is available here for students to study in depth.\nInstalling Pytest # Install pytest: pip install pytest # Upgrade pytest pip install -U pytest # Check pytest version pytest --version # Check installed package list pip list # Check pytest help documentation pytest -h # Install third-party plugins pip install pytest-sugar pip install pytest-rerunfailures pip install pytest-xdist pip install pytest-assume pip install pytest-html Using Pytest Naming Convention # When using pytest, our module names are usually prefixed with test or end with test. You can also modify the configuration file to customize the naming convention. # test_*.py or *_test.py test_demo1 demo2_test # The class name in the module must start with Test and cannot have an init method. class TestDemo1: class TestLogin: # The test methods defined in the class must start with test_ test_demo1(self) test_demo2(self) # Test Case class test_one: def test_demo1(self): print(\"Test Case 1\") def test_demo2(self): print(\"Test Case 2\") Pytest Parameters pytest supports many parameters, which can be viewed using the help command.\npytest -help Here are some commonly used ones:\n-m: Specify multiple tag names with an expression. pytest provides a decorator @pytest.mark.xxx for marking tests and grouping them (xxx is the group name you defined), so you can quickly select and run them, with different groups separated by and or or.\n-v: Outputs more detailed information during runtime. Without -v, the runtime does not display the specific test case names being run; with -v, it prints out the specific test cases in the console.\n-q: Similar to the verbosity in unittest, used to simplify the runtime output. When running tests with -q, only simple runtime information is displayed, for example:\n.s.. [100%] 3 passed, 1 skipped in 9.60s -k: You can run specified test cases using an expression. It is a fuzzy match, with and or or separating keywords, and the matching range includes file names, class names, and function names.\n-x: Exit the test if one test case fails. This is very useful for debugging. When a test fails, stop running the subsequent tests.\n-s: Display print content. When running test scripts, we often add some print content for debugging or printing some content. However, when running pytest, this content is not displayed. If you add -s, it will be displayed.\npytest test_se.py -s Selecting Test Cases to Execute with Pytest In Pytest, you can select and execute test cases based on different dimensions such as test folders, test files, test classes, and test methods.\nExecute by test folder # Execute all test cases in the current folder and subfolders pytest . # Execute all test cases in the tests folder and its subfolders, which are at the same level as the current folder pytest ../tests # Execute by test file # Run all test cases in test_se.py pytest test_se.py # Execute by test class, must be in the following format: pytest file_name.py::TestClass, where \"::\" is the separator used to separate the test module and test class. # Run all test cases under the class named TestSE in the test_se.py file pytest test_se.py::TestSE # Execute by test method, must be in the following format: pytest file_name.py::TestClass::TestMethod, where \"::\" is the separator used to separate the test module, test class, and test method. # Run the test case named test_get_new_message under the class named TestSE in the test_se.py file pytest test_se.py::TestSE::test_get_new_message # The above methods of selecting test cases are all on the **command line**. If you want to execute directly in the test program, you can directly call pytest.main(), the format is: pytest.main([module.py::class::method]) In addition, Pytest also supports multiple ways to control the execution of test cases, such as filtering execution, running in multiple processes, retrying execution, etc.\nWriting Validation with Pytest During testing, we use the previously validated adder. Go to the Adder folder, create a new test_adder.py file in the picker_out_adder directory, with the following content: # Import test modules and required libraries from Adder import * import pytest import ctypes import random # Use pytest fixture to initialize and clean up resources @pytest.fixture def adder(): # Create an instance of DUTAdder, load the dynamic link library dut = DUTAdder() # Execute one clock step to prepare the DUT dut.Step(1) # The code after the yield statement will be executed after the test ends, used to clean up resources yield dut # Clean up DUT resources and generate test coverage reports and waveforms dut.Finish() class TestFullAdder: # Define full_adder as a static method, as it does not depend on class instances @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # Use the pytest.mark.usefixtures decorator to specify the fixture to use @pytest.mark.usefixtures(\"adder\") # Define the test method, where adder is injected by pytest through the fixture def test_adder(self, adder): # Perform multiple random tests for _ in range(114514): # Generate random 64-bit a, b, and 1-bit cin a = random.getrandbits(64) b = random.getrandbits(64) cin = random.getrandbits(1) # Set the input of the DUT adder.a.value = a adder.b.value = b adder.cin.value = cin # Execute one clock step adder.Step(1) # Calculate the expected result using a static method sum, cout = self.full_adder(a, b, cin) # Assert that the output of the DUT is the same as the expected result assert sum == adder.sum.value assert cout == adder.cout.value if __name__ == \"__main__\": pytest.main(['-v', 'test_adder.py::TestFullAdder']) After running the test, the output is as follows: collected 1 item test_adder.py ✓ 100% ██████████ Results (4.33s): The successful test indicates that after 114514 loops, our device has not found any bugs for now. However, using randomly generated test cases with multiple loops consumes a considerable amount of resources, and these randomly generated test cases may not effectively cover all boundary conditions. In the next section, we will introduce a more efficient method for generating test cases.\n","categories":["Sample Projects","Tutorials"],"description":"Used for managing tests and generating test reports.","excerpt":"Used for managing tests and generating test reports.","ref":"/mlvp/en/docs/env_usage/frameworks/pytest/","tags":["examples","docs"],"title":"PyTest"},{"body":"Hypothesis 在上一节中，我们通过手动编写测试用例，并为每个用例指定输入和预期输出。这种方式存在一些问题，例如测试用例覆盖不全面、边界条件 容易被忽略等。它是一个用于属性基于断言的软件测试的 Python 库。Hypothesis 的主要目标是使测试更简单、更快速、更可靠。它使用了一种称为属性基于断言的测试方法，即你可以为你的代码编写一些假（hypotheses），然后 Hypothesis 将会自动生成测试用例并验证这些假设。这使得编写全面且高效的测试变得更加容易。Hypothesis 可以自动生成各种类型的输入数据，包括基本类型（例如整数、浮点数、字符串等）、容器类型（例如列表、集合、字典等）、自定义类型等。然后，它会根据你提供的属性（即断言）进行测试，如果发现测试失败，它将尝试缩小输入数据的范围以找出最小的失败案例。通过 Hypothesis，你可以更好地覆盖代码的边界条件，并发现那些你可能没有考虑到的错误情况。这有助于提高代码的质量和可靠性。\n基本概念 测试函数：即待测试的函数或方法，我们需要对其进行测试。 属性：定义了测试函数应该满足的条件。属性是以装饰器的形式应用于测试函数上的。 策略：用于生成测试数据的生成器。Hypothesis 提供了一系列内置的策略，如整数、字符串、列表等。我们也可以自定义策略。 测试生成器：基于策略生成测试数据的函数。Hypothesis 会自动为我们生成测试数据，并将其作为参数传递给测试函数。 本文将基于测试需求简单介绍Hypothesis的用法，其完整手册在这里，供同学们进行深入学习。\n安装 使用pip安装，在python中导入即可使用\npip install hypothesis import hypothesis 基本用法 属性和策略 Hypothesis 使用属性装饰器来定义测试函数的属性。最常用的装饰器是 @given，它指定了测试函数应该满足的属性。 我们可以通过@given 装饰器定义了一个测试函数 test_addition。并给x 添加对应的属性，测试生成器会自动为测试函数生成测试数据，并将其作为参数传递给函数，例如\ndef addition(number: int) -\u003e int: return number + 1 @given(x=integers(), y=integers())　def test_addition(x, y):　assert x + 1 == addition（1） 其中integers () 是一个内置的策略，用于生成整数类型的测试数据。Hypothesis 提供了丰富的内置策略，用于生成各种类型的测试数据。除了integers ()之外，还有字符串、布尔值、列表、字典等策略。例如使用 text () 策略生成字符串类型的测试数据，使用 lists (text ()) 策略生成字符串列表类型的测试数据\n@given(s=text(), l=lists(text())) def test_string_concatenation(s, l):　result = s + \"\".join(l)　assert len(result) == len(s) + sum(len(x) for x in l) 除了可以使用内置的策略以外，还可以使用自定义策略来生成特定类型的测试数据，例如我们可以生产一个非负整形的策略\ndef non_negative_integers(): return integers(min_value=0) @given(x=non_negative_integers()) def test_positive_addition(x): assert x + 1 \u003e x 期望 我们可以通过expect 来指明需要的函数期待得到的结果\n@given(x=integers()) def test_addition(x): expected = x + 1 actual = addition(x) 假设和断言 在使用 Hypothesis 进行测试时，我们可以使用标准的 Python 断言来验证测试函数的属性。Hypothesis 会自动为我们生成测试数据，并根据属性装饰器中定义的属性来运行测试函数。如果断言失败，Hypothesis 会尝试缩小测试数据的范围，以找出导致失败的最小样例。\n假如我们有一个字符串反转函数，我们可以通过assert 来判断翻转两次后他是不是等于自身\ndef test_reverse_string(s): expected = x + 1 actual = addition(x) assert actual == expected 编写测试 Hypothesis 中的测试由两部分组成：一个看起来像您选择的测试框架中的常规测试但带有一些附加参数的函数，以及一个@given指定如何提供这些参数的装饰器。以下是如何使用它来验证我们之前验证过的全加器的示例：\n在上一节的代码基础上，我们进行一些修改，将生成测试用例的方法从随机数修改为integers ()方法，修改后的代码如下：\nfrom Adder import * import pytest from hypothesis import given, strategies as st # 使用 pytest fixture 来初始化和清理资源 @pytest.fixture(scope=\"class\") def adder(): # 创建 DUTAdder 实例，加载动态链接库 dut = DUTAdder() # yield 语句之后的代码会在测试结束后执行，用于清理资源 yield dut # 清理DUT资源，并生成测试覆盖率报告和波形 dut.Finish() class TestFullAdder: # 将 full_adder 定义为静态方法，因为它不依赖于类实例 @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = a Sum += b + cin Cout = (Sum \u003e\u003e 128) \u0026 0b1 Sum \u0026= 0xffffffffffffffffffffffffffffffff return Sum, Cout # 使用 hypothesis 自动生成测试用例 @given( a=st.integers(min_value=0, max_value=0xffffffffffffffff), b=st.integers(min_value=0, max_value=0xffffffffffffffff), cin=st.integers(min_value=0, max_value=1) ) # 定义测试方法，adder 参数由 pytest 通过 fixture 注入 def test_full_adder_with_hypothesis(self, adder, a, b, cin): # 计算预期的和与进位 sum_expected, cout_expected = self.full_adder(a, b, cin) # 设置 DUT 的输入 adder.a.value = a adder.b.value = b adder.cin.value = cin # 执行一次时钟步进 adder.Step(1) # 断言 DUT 的输出与预期结果相同 assert sum_expected == adder.sum.value assert cout_expected == adder.cout.value if __name__ == \"__main__\": # 以详细模式运行指定的测试 pytest.main(['-v', 'test_adder.py::TestFullAdder']) 这个例子中，@given 装饰器和 strategies 用于生成符合条件的随机数据。st.integers() 是生成指定范围整数的策略，用于为 a 和 b 生成 0 到 0xffffffffffffffff 之间的数，以及为 cin 生成 0 或 1。Hypothesis会自动重复运行这个测试，每次都使用不同的随机输入，这有助于揭示潜在的边界条件或异常情况。\n运行测试，输出结果如下： collected 1 item test_adder.py ✓ 100% ██████████ Results (0.42s): 1 passed 可以看到在很短的时间里我们已经完成了测试\n","categories":["示例项目","教程"],"description":"可用来生成激励","excerpt":"可用来生成激励","ref":"/mlvp/docs/env_usage/frameworks/hypothesis/","tags":["examples","docs"],"title":"Hypothesis"},{"body":"Hypothesis In the previous section, we manually wrote test cases and specified inputs and expected outputs for each case. This method has some issues, such as incomplete test case coverage and the tendency to overlook boundary conditions. Hypothesis is a Python library for property-based testing. Its main goal is to make testing simpler, faster, and more reliable. It uses a method called property-based testing, where you can write some hypotheses for your code, and Hypothesis will automatically generate test cases to verify these hypotheses. This makes it easier to write comprehensive and efficient tests. Hypothesis can automatically generate various types of input data, including basic types (e.g., integers, floats, strings), container types (e.g., lists, sets, dictionaries), and custom types. It tests based on the properties (assertions) you provide. If a test fails, it will try to narrow down the input data to find the smallest failing case. With Hypothesis, you can better cover the boundary conditions of your code and uncover errors you might not have considered. This helps improve the quality and reliability of your code.\nBasic Concepts Test Function: The function or method to be tested. Properties: Conditions that the test function should satisfy. Properties are applied to the test function as decorators. Strategy: A generator for test data. Hypothesis provides a range of built-in strategies, such as integers, strings, lists, etc. You can also define custom strategies. Test Generator: A function that generates test data based on strategies. Hypothesis automatically generates test data and passes it as parameters to the test function. This article will briefly introduce Hypothesis based on testing requirements. The complete manual is available for in-depth study.\nInstallation Install with pip and import in Python to use:\npip install hypothesis import hypothesis Basic Usage Properties and Strategies Hypothesis uses property decorators to define the properties of test functions. The most common decorator is @given, which specifies the properties the test function should satisfy. We can define a test function test_addition using the @given decorator and add properties to x. The test generator will automatically generate test data for the function and pass it as parameters, for example:\ndef addition(number: int) -\u003e int: return number + 1 @given(x=integers(), y=integers())　def test_addition(x, y):　assert x + 1 == addition（1） In this example, integers() is a built-in strategy for generating integer test data. Hypothesis offers a variety of built-in strategies for generating different types of test data. Besides integers(), there are strategies for strings, booleans, lists, dictionaries, etc. For instance, using the text() strategy to generate string test data and using lists(text()) to generate lists of strings:\n@given(s=text(), l=lists(text())) def test_string_concatenation(s, l):　result = s + \"\".join(l)　assert len(result) == len(s) + sum(len(x) for x in l) You can also define custom strategies to generate specific types of test data, for example, a strategy for non-negative integers:\ndef non_negative_integers(): return integers(min_value=0) @given(x=non_negative_integers()) def test_positive_addition(x): assert x + 1 \u003e x Expectations We can use expect to specify the expected result of a function:\n@given(x=integers()) def test_addition(x): expected = x + 1 actual = addition(x) Hypotheses and Assertions When using Hypothesis for testing, we can use standard Python assertions to verify the properties of the test function. Hypothesis will automatically generate test data and run the test function based on the properties defined in the decorator. If an assertion fails, Hypothesis will try to narrow down the test data to find the smallest failing case.\nSuppose we have a string reversal function. We can use an assert statement to check if reversing a string twice equals itself:\ndef test_reverse_string(s): expected = x + 1 actual = addition(x) assert actual == expected Writing Tests Tests in Hypothesis consist of two parts: a function that looks like a regular test in your chosen framework but with some extra parameters, and a @given decorator specifying how to provide those parameters. Here’s an example of how to use it to verify a full adder, which we tested previously:\nBased on the previous section’s code, we modify the method of generating test cases from random numbers to the integers() method. The modified code is as follows:\nfrom Adder import * import pytest import ctypes import random from hypothesis import given, strategies as st # Initializing and Cleaning Up Resources Using pytest Fixture from Adder import * import pytest import ctypes from hypothesis import given, strategies as st # Using pytest fixture to initialize and clean up resources @pytest.fixture(scope=\"class\") def adder(): # Create DUTAdder instance and load dynamic library dut = DUTAdder() # Perform a clock step to prepare the DUT dut.Step(1) # Code after yield executes after tests finish, for cleanup yield dut # Clean up DUT resources and generate coverage report and waveform dut.Finish() class TestFullAdder: # Define full_adder as a static method, as it doesn't depend on class instance @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # Use Hypothesis to automatically generate test cases @given( a=st.integers(min_value=0, max_value=0xffffffffffffffff), b=st.integers(min_value=0, max_value=0xffffffffffffffff), cin=st.integers(min_value=0, max_value=1) ) # Define test method, adder parameter injected by pytest via fixture def test_full_adder_with_hypothesis(self, adder, a, b, cin): # Calculate expected sum and carry sum_expected, cout_expected = self.full_adder(a, b, cin) # Set DUT inputs adder.a.value = a adder.b.value = b adder.cin.value = cin # Perform a clock step adder.Step(1) # Assert DUT outputs match expected results assert sum_expected == adder.sum.value assert cout_expected == adder.cout.value if __name__ == \"__main__\": # Run specified tests in verbose mode pytest.main(['-v', 'test_adder.py::TestFullAdder']) In this example, the @given decorator and strategies are used to generate random data that meets specified conditions. st.integers() is a strategy for generating integers within a specified range, used to generate numbers between 0 and 0xffffffffffffffff for a and b, and between 0 and 1 for cin. Hypothesis will automatically rerun this test multiple times, each time using different random inputs, helping reveal potential boundary conditions or edge cases.\nRun the tests, and the output will be as follows: collected 1 item test_adder.py ✓ 100% ██████████ Results (0.42s): 1 passed As we can see, the tests were completed in a short amount of time.\n","categories":["Sample Projects","Tutorials"],"description":"Can Be Used to Generate Stimuli","excerpt":"Can Be Used to Generate Stimuli","ref":"/mlvp/en/docs/env_usage/frameworks/hypothesis/","tags":["examples","docs"],"title":"Hypothesis"},{"body":" 在芯片验证的传统实践中，UVM等框架被广泛采用。尽管它们提供了一整套验证方法，但通常只适用于特定的硬件描述语言和仿真环境。本工具突破了这些限制，能够将仿真代码转换成C++或Python，使得我们可以利用软件验证工具来进行更全面的测试。\n因为Python具有强大的生态系统，所以本项目主要以Python作为示例，简单介绍Pytest和Hypothesis两个经典软件测试框架。Pytest以其简洁的语法和丰富的功能，轻松应对各种测试需求。而Hypothesis则通过生成测试用例，揭示出意料之外的边缘情况，提高了测试的全面性和深度。\n我们的项目从一开始就设计为与多种现代软件测试框架兼容。我们鼓励您探索这些工具的潜力，并将其应用于您的测试流程中。通过亲身实践，您将更深刻地理解这些工具如何提升代码的质量和可靠性。让我们一起努力，提高芯片开发的质量。\n","categories":["示例项目","教程"],"description":"可用软件测试框架","excerpt":"可用软件测试框架","ref":"/mlvp/docs/env_usage/frameworks/","tags":["examples","docs"],"title":"集成测试框架"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/example-projects/","tags":"","title":"Example Projects"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/tutorials/","tags":"","title":"Tutorials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E6%95%99%E7%A8%8B/","tags":"","title":"教程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E7%A4%BA%E4%BE%8B%E9%A1%B9%E7%9B%AE/","tags":"","title":"示例项目"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/example-project/","tags":"","title":"Example Project"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/mlvp/","tags":"","title":"Goldydocs"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/mlvp/en/","tags":"","title":"Goldydocs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/learning-materials/","tags":"","title":"Learning Materials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/reference/","tags":"","title":"Reference"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/sample-projects/","tags":"","title":"Sample Projects"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/tutorial/","tags":"","title":"Tutorial"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E5%8F%82%E8%80%83/","tags":"","title":"参考"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/search/","tags":"","title":"搜索结果"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E5%AD%A6%E4%B9%A0%E6%9D%90%E6%96%99/","tags":"","title":"学习材料"}]