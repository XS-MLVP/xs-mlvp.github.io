[{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/docs/mlvp/api/bundle/","tags":"","title":"Bundle API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/docs/mlvp/api/bundle/","tags":"","title":"Bundle API"},{"body":" This page provides a brief introduction to chip verification, including concepts used in examples such as DUT (Design Under Test) and RM (Reference Model).\nThe chip verification process needs to align with the actual situation of the company or team. There is no absolute standard that meets all requirements and must be referenced.\nWhat is Chip Verification? The chip design-to-production process involves three main stages: chip design, chip manufacturing, and chip packaging/testing. Chip design is further divided into front-end and back-end design. Front-end design, also known as logic design, aims to achieve the desired circuit logic functionality. Back-end design, or physical design, focuses on optimizing layout and routing to reduce chip area, lower power consumption, and increase frequency. Chip verification is a critical step in the chip design process. Its goal is to ensure that the designed chip meets the specified requirements in terms of functionality, performance, and power consumption. The verification process typically includes functional verification, timing verification, and power verification, using methods and tools such as simulation, formal verification, hardware acceleration, and prototyping. For this tutorial, chip verification refers only to the verification of the front-end design to ensure that the circuit logic meets the specified requirements (“Does this proposed design do what is intended?”), commonly known as functional verification. This does not include back-end design aspects like power and frequency.\nFor chip products, design errors that make it to production can be extremely costly to fix, as it might require recalling products and remanufacturing chips, incurring significant financial and time costs. Here are some classic examples of failures due to inadequate chip verification: Intel Pentium FDIV Bug：In 1994, Intel’s Pentium processor was found to have a severe division error known as the FDIV bug. This error was due to incorrect entries in a lookup table within the chip’s floating-point unit. Although it rarely affected most applications, it caused incorrect results in specific calculations. Intel had to recall a large number of processors, leading to significant financial losses.\nAriane 5 Rocket Failure：Though not a chip example, this highlights the importance of hardware verification. In 1996, the European Space Agency’s Ariane 5 rocket exploded shortly after launch due to an overflow when converting a 64-bit floating-point number to a 16-bit integer in the navigation system, causing the system to crash. This error went undetected during design and led to the rocket’s failure.\nAMD Barcelona Bug：In 2007, AMD’s Barcelona processor had a severe Translation Lookaside Buffer (TLB) error that could cause system crashes or reboots. AMD had to mitigate this by lowering the processor’s frequency and releasing BIOS updates, which negatively impacted their reputation and financial status.\nThese cases emphasize the importance of chip verification. Errors detected and fixed during the design phase can prevent these costly failures. Insufficient verification continues to cause issues today, such as a new entrant in the ASIC chip market rushing a 55nm chip without proper verification, leading to three failed tape-outs and approximately $500,000 in losses per failure.\nChip Verification Process The coupling relationship between chip design and verification is shown in the diagram above. Both design and verification have the same input: the specification document. Based on this document, both design and verification teams independently code according to their understanding and requirements. The design team needs to ensure that the RTL code is “synthesizable,” considering circuit characteristics, while the verification team mainly focuses on whether the functionality meets the requirements, with fewer coding constraints. After both teams complete module development, a sanity test is conducted to check if the functionality matches. If there are discrepancies, collaborative debugging is done to identify and fix issues before retesting. Due to the high coupling between chip design and verification, some companies directly couple their design and verification teams, assigning verification teams to each design submodule. The coupling process in the diagram is coarse-grained, with specific chips (e.g., SoC, DDR) and companies having their cooperation models.\nIn the above comparison test, the module produced by the design team is usually called DUT (Design Under Test), while the model developed by the verification team is called RM (Reference Model). The verification process includes: writing a verification plan, creating a verification platform, organizing functional points, constructing test cases, running and debugging, collecting bugs/coverage, regression testing, and writing test reports.\nVerification Plan： The verification plan describes how verification will be carried out and how verification quality will be ensured to meet functional verification requirements. It typically includes verification goals, strategies, environment, items, process, risk mitigation, resources, schedule, results, and reports. Verification goals specify the functions or performance metrics to be verified, directly extracted from the chip specification. Verification strategy outlines the methods to be used, such as simulation, formal verification, FPGA acceleration, etc., and how to organize the verification tasks. The verification environment details the specific testing environment, including verification tools and versions. The verification item library lists specific items to be verified and expected results. Verification plans can be general or specific to sub-tasks.\nPlatform Setup： The verification platform is the execution environment for specific verification tasks. Similar verification tasks can use the same platform. Setting up the platform is a key step, including choosing verification tools (e.g., software simulation, formal verification, hardware acceleration), configuring the environment (e.g., server, FPGA), creating the test environment, and basic test cases. Initial basic test cases are often called “smoke tests.” Subsequent test codes are based on this platform, so it must be reusable. The platform includes the test framework, the code being tested, and basic signal stimuli.\nOrganizing Functional Points： This involves listing the DUT’s basic functions based on the specification manual and detailing how to test each function. Functional points are prioritized based on importance, risk, and complexity. They also need to be tracked for status, with updates synchronized to the plan if changes occur.\nTest Cases These are conditions or variables used to determine if the DUT meets specific requirements and operates correctly. Each case includes test conditions, input data, expected results, actual results, and test outcomes. Running test cases and comparing expected vs. actual results help verify the system or application’s correct implementation of functions or requirements. Test cases are crucial tools for verifying chip design against specifications.\nCoding Implementation： This is the execution of test cases, including generating test data, selecting the test framework, programming language, and writing the reference model. This phase requires a deep understanding of functional points and test cases. Misunderstandings can lead to the DUT being undrivable or undetected bugs.\nCollecting Bugs/Coverage： The goal of verification is to find design bugs early, so collected bugs need unique identifiers, severity ratings, and status tracking with design engineers. Discovering bugs is ideal, but since not every test finds bugs, coverage is another metric to evaluate verification thoroughness. Sufficient verification is indicated when coverage (e.g., code coverage \u003e90%) exceeds a threshold.\nRegression Testing： As verification and design are iterative, regression tests ensure the modified DUT still functions correctly after bug fixes. This catches new errors or reactivates old ones due to changes. Regression tests can be comprehensive or selective, covering all functions or specific parts.\nTest Report： This summarizes the entire verification process, providing a comprehensive view of the testing activities, including objectives, executed test cases, discovered issues, coverage, and efficiency.\nLevels of Chip Verification Chip verification typically includes four levels based on the object size: UT, BT, IT, and ST.\nUnit Testing（UT）： The lowest verification level, focusing on single modules or components to ensure their functionality is correct.\nBlock Testing (BT) ： Modules often have tight coupling, making isolated UT testing complex. BT merges several coupled modules into one DUT block for testing.\nIntegration Testing (IT) ： Builds on UT by combining multiple modules or components to verify their collaborative functionality, usually testing subsystem functionality.\nSystem Testing (ST) ： Also called Top verification, ST combines all modules or components into a complete system to verify overall functionality and performance requirements.\nIn theory, these levels follow a bottom-up order, each building on the previous level. However, practical verification activities depend on the scale, expertise, and functional needs of the enterprise, so not all levels are always involved. At each level, relevant test cases are written, tests run, and results analyzed to ensure the chip design’s correctness and quality.\nChip Verification Metrics Verification metrics typically include functional correctness, test coverage, defect density, verification efficiency, and verification cost. Functional correctness is the fundamental metric, ensuring the chip executes its designed functions correctly. This is validated through functional test cases, including normal and robustness tests. Test coverage indicates the extent to which test cases cover design functionality, with higher coverage implying higher verification quality. Coverage can be further divided into code coverage, functional coverage, condition coverage, etc. Defect density measures the number of defects found in a given design scale or code volume, with lower density indicating higher design quality. Verification efficiency measures the amount of verification work completed within a given time and resource frame, with higher efficiency indicating higher productivity. Verification cost encompasses all resources required for verification, including manpower, equipment, and time, with lower costs indicating higher cost-effectiveness.\nFunctional correctness is the absolute benchmark for verification. However, in practice, it is often impossible to determine if the test plan is comprehensive and if all test spaces have been adequately covered. Therefore, a quantifiable metric is needed to guide whether verification is sufficient and when it can be concluded. This metric is commonly referred to as “test coverage.” Test coverage typically includes code coverage (lines, functions, branches) and functional coverage.\nCode Line Coverage： This indicates how many lines of the DUT design code were executed during testing.\nFunction Coverage： This indicates how many functions of the DUT design code were executed during testing.\nBranch Coverage： This indicates how many branches (if-else) of the DUT design code were executed during testing.\nFunctional Coverage： This indicates how many predefined functions were triggered during testing.\nHigh code coverage can improve the quality and reliability of verification but does not guarantee complete correctness since it cannot cover all input and state combinations. Therefore, in addition to pursuing high code coverage, other testing methods and metrics, such as functional testing, performance testing, and defect density, should be combined.\nChip Verification Management Chip verification management is a comprehensive process that encompasses all activities in the chip verification process, including the development of verification strategies, the setup of the verification environment, the writing and execution of test cases, the collection and analysis of results, and the tracking and resolution of issues and defects. The goal of chip verification management is to ensure that the chip design meets all functional and performance requirements, as well as specifications and standards.\nIn chip verification management, the first step is to formulate a detailed verification strategy, including objectives, scope, methods, and schedules. Then, a suitable verification environment must be set up, including hardware, software tools, and test data. Next, a series of test cases covering all functional and performance points must be written and executed, with results collected and analyzed to identify problems and defects. Finally, these issues and defects need to be tracked and fixed until all test cases pass.\nChip verification management is a complex process requiring a variety of skills and knowledge, including chip design, testing methods, and project management. It requires close collaboration with other activities, such as chip design, production, and sales, to ensure the quality and performance of the chip. The effectiveness of chip verification management directly impacts the success of the chip and the company’s competitiveness. Therefore, chip verification management is a crucial part of the chip development process.\nThe chip verification management process can be based on a “project management platform” and a “bug management platform,” with platform-based management typically being significantly more efficient than manual management.\nCurrent State of Chip Verification Currently, chip verification is typically completed within chip design companies. This process is not only technically complex but also entails significant costs. Given the close relationship between acceptance and design, chip verification inevitably involves the source code of the chip design. However, chip design companies usually consider the source code as a trade secret, necessitating internal personnel to perform the verification, making outsourcing difficult.\nThe importance of chip verification lies in ensuring that the designed chip operates reliably under various conditions. Verification is not only for meeting technical specifications but also for addressing the growing complexity and emerging technology demands. As the semiconductor industry evolves, the workload of chip verification has been continuously increasing, especially for complex chips, where verification work has exceeded design work, accounting for more than 70%. This means that in terms of engineer personnel ratio, verification engineers are usually twice the number of design engineers (e.g., in a team of three thousand at Zeku, there are about one thousand design engineers and two thousand verification engineers. Similar or higher ratios apply to other large chip design companies).\nDue to the specificity of verification work, which requires access to the chip design source code, it significantly limits the possibility of outsourcing chip verification. The source code is considered the company’s core trade secret, involving technical details and innovations, thus making it legally and securely unfeasible to share with external parties. Consequently, internal personnel must shoulder the verification work, increasing the internal workload and costs.\nGiven the current situation, the demand for chip verification engineers continues to grow. They need a solid technical background, familiarity with various verification tools and methods, and keen insight into emerging technologies. Due to the complexity of verification work, verification teams typically need a large scale, contrasting sharply with the design team size.\nTo meet this challenge, the industry may need to continuously explore innovative verification methods and tools to improve efficiency and reduce costs.\nSummary: Complex Chip Verification Costs High Verification Workload： For complex chips, verification work accounts for over 70% of the entire chip design work.\nHigh Labor Costs： The number of verification engineers is twice that of design engineers, with complex tasks requiring thousands of engineers.\nInternal Verification： To ensure trade secrets (chip design code) are not leaked, chip design companies can only hire a large number of verification engineers to perform verification work internally.\nCrowdsourcing Chip Verification In contrast to hardware, the software field has already made testing outsourcing (subcontracting) a norm to reduce testing costs. This business is highly mature, with a market size in the billions of yuan, advancing towards the trillion-yuan scale. From the content perspective, software testing and hardware verification share significant similarities (different targets with the same system objective). Is it feasible to subcontract hardware verification in the same way as software?\nCrowdsourcing chip verification faces many challenges, such as: Small Number of Practitioners： Compared to the software field, the number of hardware developers is several orders of magnitude smaller. For instance, according to GitHub statistics (https://madnight.github.io/githut/#/pull_requests/2023/2), traditional software programming languages (Python, Java, C++, Go) account for nearly 50%, whereas hardware description languages like Verilog account for only 0.076%, reflecting the disparity in developer numbers.\nCommercial Verification Tools： The verification tools used in enterprises (simulators, formal verification, data analysis) are almost all commercial tools, which are nearly invisible to ordinary people and difficult to self-learn.\nLack of Open Learning Materials： Chip verification involves accessing the chip design source code, which is typically regarded as the company’s trade secrets and proprietary technology. Chip design companies may be unwilling to disclose detailed verification processes and techniques, limiting the availability of learning materials.\nFeasibility Analysis Although the chip verification field has been relatively closed, from a technical perspective, adopting a subcontracting approach for verification is a feasible option due to several factors:\nFirstly, with the gradual increase of open-source chip projects, the source code involved in verification has become more open and transparent. These open-source projects do not have concerns about trade secrets in their design and verification process, providing more possibilities for learning and research. Even if some projects involve trade secrets, encryption and other methods can be used to hide design codes, addressing trade secret issues to a certain extent and making verification easier to achieve.\nSecondly, many fundamental verification tools have emerged in the chip verification field, such as Verilator and SystemC. These tools provide robust support for verification engineers, helping them perform verification work more efficiently. These tools alleviate some of the complexity and difficulty of the verification process, providing a more feasible technical foundation for adopting subcontracted verification methods.\nIn the open-source software field, some successful cases can be referenced. For example, the Linux kernel verification process adopts a subcontracting approach, with different developers and teams responsible for verifying different modules, ultimately forming a complete system. Similarly, in the machine learning field, the ImageNet project adopted a crowdsourced annotation strategy, completing large-scale image annotation tasks through crowdsourcing. These cases provide successful experiences for the chip verification field, proving the potential of subcontracted verification to improve efficiency and reduce costs.\nTherefore, despite the chip verification field being relatively closed compared to other technical fields, technological advances and the increase of open-source projects offer new possibilities for adopting subcontracted verification. By drawing on successful experiences from other fields and utilizing existing verification tools, we can promote the application of more open and efficient verification methods in chip verification, further advancing the industry. This openness and flexibility in technology will provide more choices for verification engineers, promoting innovative and diverse development in the chip verification field.\nTechnical Route To overcome challenges and engage more people in chip verification, this project continuously attempts the following technical directions:\nProvide Multi-language Verification Tools： Traditional chip verification is based on the System Verilog programming language, which has a small user base. To allow other software development/testing professionals to participate in chip verification, this project provides multi-language verification conversion tools Picker, enabling verifiers to use familiar programming languages (e.g., C++, Python, Java, Go) with open-source verification tools.\nProvide Verification Learning Materials： The scarcity of chip verification learning materials is mainly due to the improbability of commercial companies disclosing internal data. Therefore, this project will continuously update learning materials, allowing verifiers to learn the necessary skills online for free.\nProvide Real Chip Verification Cases： To make the learning materials more practical, this project uses the “Xiangshan Kunming Lake (an industrial-grade high-performance RISC-V processor) IP core” as a basis, continuously updating verification cases by extracting modules from it.\nOrganize Chip Design Subcontracted Verification： Applying what is learned is the goal of every learner. Therefore, this project periodically organizes subcontracted chip design verification, allowing everyone (whether you are a university student, verification expert, software developer, tester, or high school student) to participate in real chip design work.\nThe goal of this project is to achieve the following vision: “Open the black box of traditional verification modes, allowing anyone interested to participate in chip verification anytime, anywhere, using their preferred programming language.”\n","categories":"","description":"Basic concepts of chip verification\n","excerpt":"Basic concepts of chip verification\n","ref":"/mlvp/en/docs/basic/ic_verify/","tags":"","title":"Chip Verification"},{"body":"Verification Report Chinese version:\nhttps://github.com/XS-MLVP/Example-NutShellCache/blob/master/nutshell_cache_report_demo.pdf\nEnglish verision:\nTBD\nVerification Environment \u0026 Test Case Code https://github.com/XS-MLVP/Example-NutShellCache\n","categories":["Example Projects","Tutorials"],"description":"Verification of Nutshell Cache using Python.","excerpt":"Verification of Nutshell Cache using Python.","ref":"/mlvp/en/docs/advance_case/nutshellcache/","tags":["examples","docs"],"title":"Complete Verification of Nutshell Cache"},{"body":"How to Simultaneously Call Multiple Driver Functions Once the verification environment is set up, you can write test cases using the interfaces provided by the verification environment. However, it is often difficult to call two driver functions simultaneously using conventional serial code. This becomes especially important when multiple interfaces need to be driven at the same time, and mlvp provides a simple way to handle such scenarios.\nSimultaneously Calling Multiple Driver Functions of Different Categories For example, suppose the current Env structure is as follows:\nDualPortStackEnv - port1_agent - @driver_method push - @driver_method pop - port2_agent - @driver_method push - @driver_method pop We want to call the push functions of both port1_agent and port2_agent simultaneously in a test case, to drive both interfaces at the same time.In mlvp, this can be achieved using the Executor.\nfrom mlvp import Executor def test_push(env): async with Executor() as exec: exec(env.port1_agent.push(1)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) We use async with to create an Executor object and establish an execution block. By directly calling exec, you can add the driver functions that need to be executed. When the Executor object exits the scope, all added driver functions will be executed simultaneously. The Executor will automatically wait for all the driver functions to complete.If you need to retrieve the return values of the driver functions, you can use the get_results method. get_results returns a dictionary where the keys are the names of the driver functions, and the values are lists containing the return values of the respective driver functions.\nMultiple Calls to the Same Driver Function If the same driver function is called multiple times in the execution block, Executor will automatically serialize these calls.\nfrom mlvp import Executor def test_push(env): async with Executor() as exec: for i in range(5): exec(env.port1_agent.push(1)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) In the code above, port1_agent.push will be called 5 times, and port2_agent.push will be called once. Since port1_agent.push is the same driver function, Executor will automatically serialize these 5 calls, and the return values will be stored sequentially in the result list. Meanwhile, port2_agent.push will execute in parallel with the serialized port1_agent.push calls. In this process, we created a scheduling process like this:\n------------------ current time -------------------- +---------------------+ +---------------------+ | group \"agent1.push\" | | group \"agent2.push\" | | +-----------------+ | | +-----------------+ | | | agent1.push | | | | agent2.push | | | +-----------------+ | | +-----------------+ | | +-----------------+ | +---------------------+ | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | +---------------------+ ------------------- Executor exit ------------------- Executor automatically created two scheduling groups based on the function names of the driver functions, and the driver functions were added to their respective groups in the order they were called. Inside the scheduling group, the driver functions are executed sequentially. Across groups, driver functions are executed in parallel.The default name for the scheduling group is the driver function’s path name, separated by periods (.).Using the sche_group parameter, you can manually specify which scheduling group a driver function call belongs to. For example:\nfrom mlvp import Executor def test_push(env): async with Executor() as exec: for i in range(5): exec(env.port1_agent.push(1), sche_group=\"group1\") exec(env.port2_agent.push(2), sche_group=\"group1\") print(\"result\", exec.get_results()) In this case, port1_agent.push and port2_agent.push will be added sequentially to the same scheduling group, group1, and they will execute in series. In the dictionary returned by get_results, group1 will be the key, and its value will be a list of the return values for all the driver functions in group1.\nAdding Custom Functions to the Executor If we call driver functions or other functions from a custom function and wish to schedule the custom function through the Executor, we can add the custom function in the same way as we add driver functions.\nfrom mlvp import Executor async def multi_push_port1(env, times): for i in range(times): await env.port1_agent.push(1) async def test_push(env): async with Executor() as exec: for i in range(2): exec(multi_push_port1(env, 5)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) Here, multi_push_port1 will be added to the Executor, creating a scheduling group named multi_push_port1 and adding two calls to it. This will execute in parallel with the port2_agent.push group.We can also use Executor within custom functions, or call other custom functions, allowing us to create arbitrarily complex scheduling scenarios with Executor.\nExample Scenarios: Scenario 1 : The environment interface is as follows:\nEnv - agent1 - @driver_method send - agent2 - @driver_method send The send function in both agents needs to be called 5 times in parallel, sending the result of the previous call each time, with the first call sending 0. The two function calls are independent of each other.\nfrom mlvp import Executor async def send(agent): result = 0 for i in range(5): result = await agent.send(result) async def test_send(env): async with Executor() as exec: exec(send(env.agent1), sche_group=\"agent1\") exec(send(env.agent2), sche_group=\"agent2\") print(\"result\", exec.get_results()) Scenario 2 : The environment interface is as follows:\nenv - agent1 - @driver_method long_task - agent2 - @driver_method task1 - @driver_method task2 task1 and task2 need to be executed in parallel, with synchronization after each call. Both need to be called 5 times, and long_task needs to execute in parallel with task1 and task2.\nfrom mlvp import Executor async def exec_once(env): async with Executor() as exec: exec(env.agent2.task1()) exec(env.agent2.task2()) async def test_case(env): async with Executor() as exec: for i in range(5): exec(exec_once(env)) exec(env.agent1.long_task()) print(\"result\", exec.get_results()) Setting Executor Exit Conditions The Executor will wait for all driver functions to complete before exiting, but sometimes it’s unnecessary to wait for all functions. You can set the exit condition using the exit parameter when creating the Executor.The exit parameter can be set to all, any, or none, which correspond to exiting after all groups finish, after any group finishes, or immediately without waiting.\nfrom mlvp import Executor async def send_forever(agent): result = 0 while True: result = await agent.send(result) async def test_send(env): async with Executor(exit=\"any\") as exec: exec(send_forever(env.agent1)) exec(env.agent2.send(1)) print(\"result\", exec.get_results()) In this code, the send_forever function runs in an infinite loop. By setting exit=\"any\", the Executor will exit after env.agent2.send completes, without waiting for send_forever.If needed later, you can wait for all tasks to complete by calling exec.wait_all.\n","categories":"","description":"","excerpt":"How to Simultaneously Call Multiple Driver Functions Once the …","ref":"/mlvp/en/docs/mlvp/cases/executor/","tags":"","title":"How to Drive Using Test Environment Interfaces"},{"body":"Starting the Event Loop In the previously described verification environment, we designed a standardized setup. However, if we attempt to write it as a simple single-threaded program, we may encounter complex implementation issues.\nFor instance, consider having two driver methods that drive two different interfaces. Inside each driver method, we need to wait for several clock cycles of the DUT (Device Under Test), and both methods must run simultaneously. In a basic single-threaded program, running both driver methods concurrently can be quite challenging. Even if we force concurrency using multithreading, there is still no built-in mechanism to wait for the DUT to advance through multiple clock cycles. This limitation exists because the interfaces provided by Picker allow us to push the DUT forward by one cycle at a time but not to wait for it. Moreover, in cases where multiple components of the environment need to run concurrently, we require an environment that supports asynchronous execution. mlvp uses Python’s coroutines to manage asynchronous programs. It builds an event loop on top of a single thread to manage multiple concurrently running coroutines. These coroutines can wait on each other and switch between tasks via the event loop.Before starting the event loop, we need to understand two keywords, async and await, to grasp how Python manages coroutines.By adding the async keyword before a function, we define it as a coroutine, for example:\nasync def my_coro(): ... Inside the coroutine, we use the await keyword to run another coroutine and wait for it to complete, for example:\nasync def my_coro(): return \"my_coro\" async def my_coro2(): result = await my_coro() print(result) If you don’t want to wait for a coroutine to finish but simply run it in the background, you can use mlvp ’s create_task method, like so:\nimport mlvp async def my_coro(): return \"my_coro\" async def my_coro2(): mlvp.create_task(my_coro()) How do we start the event loop and run my_coro2? In mlvp , we use mlvp.run to start the event loop and run the asynchronous program:\nimport mlvp mlvp.run(my_coro2()) Since all environment components in mlvp need to run within the event loop, when starting the mlvp verification environment, you must first initiate the event loop via mlvp.run and then create the mlvp verification environment inside the loop. Thus, the test environment should be set up as follows:\nimport mlvp async def start_test(): # Create verification environment env = MyEnv() ... mlvp.run(start_test()) How to Manage DUT Clock As mentioned earlier, if we need two driver methods to run simultaneously and each one to wait for several DUT clock cycles, asynchronous environments allow us to wait for specific events. However, Picker only provides the ability to push the DUT forward by one cycle and does not provide an event to wait on. mlvp addresses this by creating a background clock to automatically push the DUT forward one cycle at a time. After each cycle, the background clock sends a clock signal to other coroutines, allowing them to resume execution. The actual clock cycles of the DUT are driven by the background clock, while other coroutines only need to wait for the clock signal.In mlvp , the background clock is created using start_clock:\nimport mlvp async def start_test(): dut = MyDUT() mlvp.start_clock(dut) mlvp.run(start_test()) Simply call start_clock within the event loop to create the background clock. It requires a DUT object to drive the DUT’s execution and bind the clock signal to the DUT and its pins.In other coroutines, you can use ClockCycles to wait for the clock signal. The ClockCycles parameter can be the DUT itself or any of its pins. For example:\nimport mlvp from mlvp.triggers import * async def my_coro(dut): await ClockCycles(dut, 10) print(\"10 cycles passed\") async def start_test(): dut = MyDUT() mlvp.start_clock(dut) await my_coro(dut) mlvp.run(start_test()) In my_coro, ClockCycles is used to wait for 10 clock cycles of the DUT. After 10 cycles, my_coro continues executing and prints “10 cycles passed.”mlvp provides several methods for waiting on clock signals, such as:\nClockCycles: Wait for a specified number of DUT clock cycles.\nValue: Wait for a DUT pin to equal a specific value.\nAllValid: Wait for all DUT pins to be valid simultaneously.\nCondition: Wait for a condition to be met.\nChange: Wait for a change in the value of a DUT pin.\nRisingEdge: Wait for the rising edge of a DUT pin.\nFallingEdge: Wait for the falling edge of a DUT pin.\nFor more methods of waiting on clock signals, refer to the API documentation.\n","categories":"","description":"","excerpt":"Starting the Event Loop In the previously described verification …","ref":"/mlvp/en/docs/mlvp/env/start_test/","tags":"","title":"How to Use an Asynchronous Environment"},{"body":"Installation toffee Toffee is a Python-based hardware verification framework designed to help users build hardware verification environments more conveniently and systematically using Python. It leverages the multi-language conversion tool picker, which converts Verilog code of hardware designs into Python Packages, enabling users to drive and verify hardware designs in Python.\nToffee requires the following dependencies:\nPython 3.6.8+\nPicker 0.9.0+\nOnce these dependencies are installed, you can install Toffee via pip:\npip install pytoffee Or install the latest version of Toffee with the following command:\npip install pytoffee@git+https://github.com/XS-MLVP/toffee@master For a local installation:\ngit clone https://github.com/XS-MLVP/toffee.git cd toffee pip install . toffee-test toffee-test is a pytest plugin that provides testing support for the toffee framework. It includes identifying test functions as toffee test case objects, making them recognizable and executable by the toffee framework, offering resource management for test cases and providing test report generationto assist users in writing test cases for toffee\nTo install toffee-test via pip:\npip install toffee-test Or install the development version:\npip install toffee-test@git+https://github.com/XS-MLVP/toffee-test@master Or install from source:\ngit clone https://github.com/XS-MLVP/toffee-test.git cd toffee-test pip install . Setting Up a Simple Verification Environment We will demonstrate how to use mlvp with a simple adder example located in the example/adder directory. The adder design is as follows:\nmodule Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] io_a, input [WIDTH-1:0] io_b, input io_cin, output [WIDTH-1:0] io_sum, output io_cout ); assign {io_cout, io_sum} = io_a + io_b + io_cin; endmodule First, use picker to convert it into a Python package, and then use mlvp to set up the verification environment. After installing the dependencies, run the following command in the example/adder directory to complete the conversion:\nmake dut To verify the adder’s functionality, we will use mlvp to set up a verification environment. First, we create a driver method for the adder interface using Bundle to describe the interface and Agent to define the driving methods, as shown below:\nclass AdderBundle(Bundle): a, b, cin, sum, cout = Signals(5) class AdderAgent(Agent): @driver_method() async def exec_add(self, a, b, cin): self.bundle.a.value = a self.bundle.b.value = b self.bundle.cin.value = cin await self.bundle.step() return self.bundle.sum.value, self.bundle.cout.value We use the driver_method decorator to mark the exec_add method, which drives the adder. Each time the method is called, it assigns the input signals a, b, and cin to the adder’s input ports, then reads the output signals sum and cout after the next clock cycle and returns them.The Bundle describes the interface the Agent needs to drive. It provides connection methods to the DUT’s input and output ports, allowing the Agent to drive any DUT with the same interface.Next, we create a reference model to verify the correctness of the adder’s output. In mlvp, we use the Model class for this, as shown below:\nclass AdderModel(Model): @driver_hook(agent_name=\"add_agent\") def exec_add(self, a, b, cin): result = a + b + cin sum = result \u0026 ((1 \u003c\u003c 64) - 1) cout = result \u003e\u003e 64 return sum, cout In the reference model, we define the exec_add method, which shares the same input parameters as the exec_add method in the Agent. The method calculates the expected output for the adder. We use the driver_hook decorator to associate this method with the Agent’s exec_add method. Next, we create a top-level test environment to link the driving methods and the reference model, as shown below:\nclass AdderEnv(Env): def __init__(self, adder_bundle): super().__init__() self.add_agent = AdderAgent(adder_bundle) self.attach(AdderModel()) At this point, the verification environment is set up. toffee will automatically drive the reference model, collect results, and compare them with the adder’s output.\nAfter that, we can now write several test cases to verify the adder’s functionality by toffee-test, as shown below:\n@pytest.mark.mlvp_async async def test_random(mlvp_request): env = mlvp_request() for _ in range(1000): a = random.randint(0, 2**64-1) b = random.randint(0, 2**64-1) cin = random.randint(0, 1) await env.add_agent.exec_add(a, b, cin) @pytest.mark.mlvp_async async def test_boundary(mlvp_request): env = mlvp_request() for cin in [0, 1]: for a in [0, 2**64-1]: for b in [0, 2**64-1]: await env.add_agent.exec_add(a, b, cin) You can run the example in the example/adder directory with the following command:\nmake run After running, the report will be automatically generated in the reports directory.\n","categories":"","description":"","excerpt":"Installation toffee Toffee is a Python-based hardware verification …","ref":"/mlvp/en/docs/mlvp/quick-start/","tags":"","title":"Quick Start"},{"body":" This page will briefly introduce what verification is, as well as concepts used in the examples, such as DUT (Design Under Test) and RM (Reference Model).\nChip Verification Chip verification is a crucial step to ensure the correctness and reliability of chip designs, including functional verification, formal verification, and physical verification. This material only covers functional verification, focusing on simulation-based chip functional verification. The processes and methods of chip functional verification have many similarities with software testing, such as unit testing, system testing, black-box testing, and white-box testing. They also share similar metrics, such as functional coverage and code coverage. In essence, apart from the different tools and programming languages used, their goals and processes are almost identical. Thus, software test engineers should be able to perform chip verification without considering the tools and programming languages. However, in practice, software testing and chip verification are two completely separate fields, primarily due to the different verification tools and languages, making it difficult for software test engineers to crossover. In chip verification, hardware description languages (e.g., Verilog or SystemVerilog) and specialized commercial tools for circuit simulation are commonly used. Hardware description languages differ from high-level software programming languages like C++/Python, featuring a unique “clock” characteristic, which poses a high learning curve for software engineers.\nTo bridge the gap between chip verification and traditional software testing, allowing more people to participate in chip verification, this project provides the following content:\nMulti-language verification tools (Picker), allowing users to use their preferred programming language for chip verification. Verification framework (MLVP), enabling functional verification without worrying about the clock.\nIntroduction to basic circuits and verification knowledge, helping software enthusiasts understand circuit characteristics more easily.\nBasic learning materials for fundamental verification knowledge.\nReal high-performance chip verification cases, allowing enthusiasts to participate in verification work remotely.\nBasic Terms DUT: Design Under Test, usually referring to the designed RTL code.\nRM: Reference Model, a standard error-free model corresponding to the unit under test.\nRTL: Register Transfer Level, typically referring to the Verilog or VHDL code corresponding to the chip design.\nCoverage: The percentage of the test range relative to the entire requirement range. In chip verification, this typically includes line coverage, function coverage, and functional coverage.\nDV: Design Verification, referring to the collaboration of design and verification.\nDifferential Testing (difftest): Selecting two (or more) functionally identical units under test, submitting the same test cases that meet the unit’s requirements to observe whether there are differences in the execution results.\nTool Introduction The core tool used in this material is Picker（https://github.com/XS-MLVP/picker）. Its purpose is to automatically provide high-level programming language interfaces (Python/C++) for RTL-written design modules. Based on this tool, verification personnel with a software development (testing) background can perform chip verification without learning hardware description languages like Verilog/VHDL.\nSystem Requirements Recommended operating system: Ubuntu 22.04 LTS\nIn the development and research of system architecture, Linux is the most commonly used platform, mainly because Linux has a rich set of software and tool resources. Due to its open-source nature, important tools and software (such as Verilator) can be easily developed for Linux. In this course, multi-language verification tools like Picker and Swig can run stably on Linux. ","categories":["Sample Projects","Tutorials"],"description":"How to use the open verification platform to participate in hardware verification.","excerpt":"How to use the open verification platform to participate in hardware …","ref":"/mlvp/en/docs/quick-start/","tags":["examples","docs"],"title":"Quick Start"},{"body":"Installing the Picker Tool from Source Installing Dependencies cmake ( \u003e=3.11 )\ngcc ( Supports C++20, at least GCC version 10, recommended 11 or higher )\npython3 ( \u003e=3.8 )\nverilator ( ==4.218 )\nverible-verilog-format ( \u003e=0.0-3428-gcfcbb82b )\nswig ( \u003e=4.2.0 , for multi-language support )\nPlease ensure that the tools like verible-verilog-format have been added to the environment variable $PATH, so they can be called directly from the command line.\nSource Code Download git clone https://github.com/XS-MLVP/picker.git --depth=1 cd picker make init Build and Install cd picker make # You can enable support for other languages by # using `make BUILD_XSPCOMM_SWIG=python,java,scala,golang`. # Each language requires its own development environment, # which needs to be configured separately, such as `javac` for Java. sudo -E make install The default installation path is /usr/local, with binary files placed in /usr/local/bin and template files in /usr/local/share/picker. If you need to change the installation directory, you can pass arguments to cmake by specifying ARGS, for example: make ARGS=\"-DCMAKE_INSTALL_PREFIX=your_install_dir\" The installation will automatically install the xspcomm base library (https://github.com/XS-MLVP/xcomm), which is used to encapsulate the basic types of RTL modules, located at /usr/local/lib/libxspcomm.so. You may need to manually set the link directory parameters (-L) during compilation.\nIf support for languages such as Java is enabled, the corresponding xspcomm multi-language packages will also be installed.\npicker can also be compiled into a wheel file and installed via pip\nTo package picker into a wheel installation package, use the following command:\nmake wheel # or BUILD_XSPCOMM_SWIG=python,java,scala,golang make wheel After compilation, the wheel file will be located in the dist directory. You can then install it via pip, for example:\npip install dist/xspcomm-0.0.1-cp311-cp311-linux_x86_64.whl pip install dist/picker-0.0.1-cp311-cp311-linux_x86_64.whl After installation, execute the picker command to except the flow output:\nXDut Generate. Convert DUT(*.v/*.sv) to C++ DUT libs. Usage: ./build/bin/picker [OPTIONS] [SUBCOMMAND] Options: -h,--help Print this help message and exit -v,--version Print version --show_default_template_path Print default template path --show_xcom_lib_location_cpp Print xspcomm lib and include location --show_xcom_lib_location_java Print xspcomm-java.jar location --show_xcom_lib_location_scala Print xspcomm-scala.jar location --show_xcom_lib_location_python Print python module xspcomm location --show_xcom_lib_location_golang Print golang module xspcomm location --check check install location and supproted languages Subcommands: export Export RTL Projects Sources as Software libraries such as C++/Python pack Pack UVM transaction as a UVM agent and Python class Installation Test picker currently has two subcommands: export and pack.\nThe export subcommand is used to convert RTL designs into “libraries” corresponding to other high-level programming languages, which can be driven through software.\n$picker export –help\nExport RTL Projects Sources as Software libraries such as C++/Python Usage: picker export [OPTIONS] file... Positionals: file TEXT ... REQUIRED DUT .v/.sv source file, contain the top module Options: -h,--help Print this help message and exit --fs,--filelist TEXT ... DUT .v/.sv source files, contain the top module, split by comma. Or use '*.txt' file with one RTL file path per line to specify the file list --sim TEXT [verilator] vcs or verilator as simulator, default is verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python] Build example project, default is python, choose cpp, java or python --sdir,--source_dir TEXT [/home/yaozhicheng/workspace/picker/template] Template Files Dir, default is ${picker_install_path}/../picker/template --tdir,--target_dir TEXT [./picker_out] Codegen render files to target dir, default is ./picker_out --sname,--source_module_name TEXT ... Pick the module in DUT .v file, default is the last module in the -f marked file --tname,--target_module_name TEXT Set the module name and file name of target DUT, default is the same as source. For example, -T top, will generate UTtop.cpp and UTtop.hpp with UTtop class --internal TEXT Exported internal signal config file, default is empty, means no internal pin -F,--frequency TEXT [100MHz] Set the frequency of the **only VCS** DUT, default is 100MHz, use Hz, KHz, MHz, GHz as unit -w,--wave_file_name TEXT Wave file name, emtpy mean don't dump wave -c,--coverage Enable coverage, default is not selected as OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1] Copy xspcomm lib to generated DUT dir, default is true -V,--vflag TEXT User defined simulator compile args, passthrough. Eg: '-v -x-assign=fast -Wall --trace' || '-C vcs -cc -f filelist.f' -C,--cflag TEXT User defined gcc/clang compile command, passthrough. Eg:'-O3 -std=c++17 -I./include' --verbose Verbose mode -e,--example Build example project, default is OFF --autobuild BOOLEAN [1] Auto build the generated project, default is true Static Multi-Module Support:\nWhen generating the wrapper for dut_top.sv/v, picker allows specifying multiple module names and their corresponding quantities using the --sname parameter. For example, if there are modules A and B in the design files a.v and b.v respectively, and you need 2 instances of A and 3 instances of B in the generated DUT, and the combined module name is C (if not specified, the default name will be A_B). This can be achieved using the following command:\npicker path/a.v,path/b.v --sname A,2,B,3 --tname C Environment Variables:\nDUMPVARS_OPTION: Sets the option parameter for $dumpvars. For example, DUMPVARS_OPTION=\"+mda\" picker .... enables array waveform support in VCS. SIMULATOR_FLAGS: Parameters passed to the backend simulator. Refer to the documentation of the specific backend simulator for details. CFLAGS: Sets the -cflags parameter for the backend simulator. The pack subcommand is used to convert UVM sequence_item into other languages and then communicate through TLM (currently supports Python, other languages are under development).\n$picker pack –help\nPack uvm transaction as a uvm agent and python class Usage: picker pack [OPTIONS] file... Positionals: file TEXT ... REQUIRED Sv source file, contain the transaction define Options: -h,--help Print this help message and exit -e,--example Generate example project based on transaction, default is OFF -c,--force Force delete folder when the code has already generated by picker -r,--rename TEXT ... Rename transaction name in picker generate code Test Examples After picker compilation, execute the following commands in the picker directory to test the examples:\nbash example/Adder/release-verilator.sh --lang cpp bash example/Adder/release-verilator.sh --lang python # Default enable cpp and python # for other languages support：make BUILD_XSPCOMM_SWIG=python,java,scala,golang bash example/Adder/release-verilator.sh --lang java bash example/Adder/release-verilator.sh --lang scala bash example/Adder/release-verilator.sh --lang golang bash example/RandomGenerator/release-verilator.sh --lang cpp bash example/RandomGenerator/release-verilator.sh --lang python bash example/RandomGenerator/release-verilator.sh --lang java More Documents For guidance on chip verification with picker, please refer to: https://open-verify.cc/mlvp/en/docs/\n","categories":["Tutorials"],"description":"Install the necessary dependencies, **download, build, and install**  the required tools.","excerpt":"Install the necessary dependencies, **download, build, and install** …","ref":"/mlvp/en/docs/quick-start/installer/","tags":["docs"],"title":"Setting Up the Verification Environment"},{"body":"To meet the requirements of an open verification environment, we have developed the Picker tool, which is used to convert RTL designs into multi-language interfaces for verification. We will use the environment generated by the Picker tool as the basic verification environment. Next, we will introduce the Picker tool and its basic usage.\nIntroduction to Picker Picker is an auxiliary tool for chip verification with two main functions:\nPackaging RTL Design Verification Modules: Picker can package RTL design verification modules (.v/.scala/.sv) into dynamic libraries and provide programming interfaces in various high-level languages (currently supporting C++, Python, Java, Scala, Golang) to drive the circuit.\nAutomatic UVM-TLM Code Generation: Picker can automate TLM code encapsulation based on the UVM sequence_item provided by the user, providing a communication interface between UVM and other high-level languages such as Python.\nThis tool allows users to perform chip unit testing based on existing software testing frameworks such as pytest, junit, TestNG, go test, etc. Advantages of Verification Using Picker:\nNo RTL Design Leakage: After conversion by Picker, the original design files (.v) are transformed into binary files (.so). Verification can still be performed without the original design files, and the verifier cannot access the RTL source code.\nReduced Compilation Time: When the DUT (Design Under Test) is stable, it only needs to be compiled once (packaged into a .so file).\nWide User Base: With support for multiple programming interfaces, it caters to developers of various languages.\nUtilization of a Rich Software Ecosystem: Supports ecosystems such as Python3, Java, Golang, etc.\nAutomated UVM Transaction Encapsulation: Enables communication between UVM and Python through automated UVM transaction encapsulation.\nRTL Simulators Currently Supported by Picker:\nVerilator\nSynopsys VCS Working Principle of Picker The main function of Picker is to convert Verilog code into C++ or Python code. For example, using a processor developed with Chisel: first, it is converted into Verilog code through Chisel’s built-in tools, and then Picker provides high-level programming language interfaces.\nPython Module Generation Process of Module Generation Picker exports Python modules based on C++.\nPicker is a code generation tool. It first generates project files and then uses make to compile them into binary files.\nPicker first uses a simulator to compile the RTL code into a C++ class and then compiles it into a dynamic library (see the C++ steps for details).\nUsing the Swig tool, Picker then exports the dynamic library as a Python module based on the C++ header file definitions generated in the previous step.\nFinally, the generated module is exported to a directory, with other intermediate files being either cleaned up or retained as needed.\nSwig is a tool used to export C/C++ code to other high-level languages. It parses C++ header files and generates corresponding intermediate code. For detailed information on the generation process, please refer to the Swig official documentation . For information on how Picker generates C++ classes, please refer to C++ .\nThe generated module can be imported and used by other Python programs, with a file structure similar to that of standard Python modules. Using the Python Module The --language python or --lang python parameter specifies the generation of the Python base library.\nThe --example, -e parameter generates an executable file containing an example project.\nThe --verbose, -v parameter preserves intermediate files generated during project creation.\nUsing the Tool to Generate Python’s DUT Class Using the simple adder example from Case One:\nPicker automatically generates a base class in Python, referred to as the DUT class. For the adder example, the user needs to write test cases, importing the Python module generated in the previous section and calling its methods to operate on the hardware module. The directory structure is as follows: picker_out_adder |-- UT_Adder # Project generated by Picker tool | |-- Adder.fst.hier | |-- _UT_Adder.so | |-- __init__.py | |-- libDPIAdder.a | |-- libUTAdder.so | `-- libUT_Adder.py `-- example.py # User-written code The DUTAdder class has a total of eight methods, as shown below: class DUTAdder: def InitClock(name: str) # Initialize clock, with the clock pin name as a parameter, e.g., clk def Step(i: int = 1) # Advance the circuit by i cycles def StepRis(callback: Callable, args=None, args=(), kwargs={}) # Set rising edge callback function def StepFal(callback: Callable, args=None, args=(), kwargs={}) # Set falling edge callback function def SetWaveform(filename) # Set waveform file def SetCoverage(filename) # Set code coverage file def RefreshComb() # Advance combinational circuit def Finish() # Destroy the circuit Pins corresponding to the DUT, such as reset and clock, are represented as member variables in the DUTAdder class. As shown below, pin values can be read and written via the value attribute. from UT_Adder import * dut = DUTAdder() dut.a.value = 1 # Assign value to the pin by setting the .value attribute dut.a[12] = 1 # Assign value to the 12th bit of the input pin a x = dut.a.value # Read the value of pin a y = dut.a[12] # Read the 12th bit of pin a General Flow for Driving DUT Create DUT and Set Pin Modes: By default, pins are assigned values on the rising edge of the next cycle. For combinational logic, you need to set the assignment mode to immediate assignment.\nInitialize the Clock: This binds the clock pin to the internal xclock of the DUT. Combinational logic does not require a clock and can be ignored.\nReset the Circuit: Most sequential circuits need to be reset.\nWrite Data to DUT Input Pins: Use the pin.Set(x) interface or pin.value = x for assignment.\nDrive the Circuit: Use Step for sequential circuits and RefreshComb for combinational circuits.\nObtain and Check Outputs of DUT Pins: For example, compare the results with a reference model using assertions.\nComplete Verification and Destroy DUT: Calling Finish() will write waveform, coverage, and other information to files.\nThe corresponding pseudocode is as follows:\nfrom UT_DUT import * # 1 Create dut = DUT() # 2 Initialize dut.SetWaveform(\"test.fst\") dut.InitClock(\"clock\") # 3 Reset dut.reset = 1 dut.Step(1) dut.reset = 0 dut.Step(1) # 4 Input Data dut.input_pin1.value = 0x123123 dut.input_pin3.value = \"0b1011\" # 5 Drive the Circuit dut.Step(1) # 6 Get Results x = dut.output_pin.value print(\"result:\", x) # 7 Destroy dut.Finish() Other Data Types In general, most DUT verification tasks can be accomplished using the interfaces provided by the DUT class. However, for special cases, additional interfaces are needed, such as custom clocks, asynchronous operations, advancing combinational circuits and writing waveforms, and modifying pin properties. In the DUT class generated by Picker, in addition to XData type pin member variables , there are also XClock type xclock and XPort type xport .\nclass DUTAdder(object): xport: XPort # Member variable xport for managing all pins in the DUT xclock: XClock # Member variable xclock for managing the clock # DUT Pins a: XData b: XData cin: XData cout: XData XData Class Data in DUT pins usually have an uncertain bit width and can be in one of four states: 0, 1, Z, and X. Picker provides XData to represent pin data in the circuit. Main Methods class XData: # Split XData, for example, create a separate XData for bits 7-10 of a 32-bit XData # name: Name, start: Start bit, width: Bit width, e.g., auto sub = a.SubDataRef(\"sub_pin\", 0, 4) def SubDataRef(name, start, width): XData def GetWriteMode(): WriteMode # Get the write mode of XData: Imme (immediate), Rise (rising edge), Fall (falling edge) def SetWriteMode(mode: WriteMode) # Set the write mode of XData, e.g., a.SetWriteMode(WriteMode::Imme) def DataValid(): bool # Check if the data is valid (returns false if value contains X or Z states, otherwise true) def W(): int # Get the bit width of XData (0 indicates XData is of Verilog's logic type, otherwise it's the width of Vec type) def U(): int # Get the unsigned value of XData (e.g., x = a.value) def S(): int # Get the signed value of XData def String(): str # Convert XData to a hexadecimal string, e.g., \"0x123ff\", if ? appears, it means X or Z state in the corresponding 4 bits def Equal(xdata): bool # Compare two XData instances for equality def Set(value) # Assign value to XData, value can be XData, string, int, bytes, etc. def GetBytes(): bytes # Get the value of XData in bytes format def Connect(xdata): bool # Connect two XData instances; only In and Out types can be connected. When Out data changes, In type XData will be automatically updated. def IsInIO(): bool # Check if XData is of In type, which can be read and written def IsOutIO(): bool # Check if XData is of Out type, which is read-only def IsBiIO(): bool # Check if XData is of Bi type, which can be read and written def IsImmWrite(): bool # Check if XData is in Imm write mode def IsRiseWrite(): bool # Check if XData is in Rise write mode def IsFallWrite(): bool # Check if XData is in Fall write mode def AsImmWrite() # Change XData's write mode to Imm def AsRiseWrite() # Change XData's write mode to Rise def AsFallWrite() # Change XData's write mode to Fall def AsBiIO() # Change XData to Bi type def AsInIO() # Change XData to In type def AsOutIO() # Change XData to Out type def FlipIOType() # Invert the IO type of XData, e.g., In to Out or Out to In def Invert() # Invert the data in XData def At(index): PinBind # Get the pin at index, e.g., x = a.At(12).Get() or a.At(12).Set(1) def AsBinaryString() # Convert XData's data to a binary string, e.g., \"1001011\" To simplify assignment operations, XData has overloaded property assignment for Set(value) and U() methods, allowing assignments and retrievals with pin.value = x and x = pin.value.\n# Access with .value # a is of XData type a.value = 12345 # Decimal assignment a.value = 0b11011 # Binary assignment a.value = 0o12345 # Octal assignment a.value = 0x12345 # Hexadecimal assignment a.value = -1 # Assign all bits to 1, a.value = x is equivalent to a.Set(x) a[31] = 0 # Assign value to bit 31 a.value = \"x\" # Assign high impedance state a.value = \"z\" # Assign unknown state x = a.value # Retrieve value, equivalent to x = a.U() XPort Class Directly operating on XData is clear and intuitive when dealing with a few pins. However, managing multiple XData instances can be cumbersome. XPort is a wrapper around XData that allows centralized management of multiple XData instances. It also provides methods for convenient batch management. Initialization and Adding Pins port = XPort(\"p\") # Create an XPort instance with prefix p Main Methods\nclass XPort: def XPort(prefix = \"\") # Create a port with prefix prefix, e.g., p = XPort(\"tile_link_\") def PortCount(): int # Get the number of pins in the port (i.e., number of bound XData instances) def Add(pin_name, XData) # Add a pin, e.g., p.Add(\"reset\", dut.reset) def Del(pin_name) # Delete a pin def Connect(xport2) # Connect two ports def NewSubPort(std::string subprefix): XPort # Create a sub-port with all pins starting with subprefix def Get(key, raw_key = False): XData # Get XData def SetZero() # Set all XData in the port to 0 XClock Class XClock is a wrapper for the circuit clock used to drive the circuit. In traditional simulation tools (e.g., Verilator), you need to manually assign values to clk and update the state using functions like step_eval. Our tool provides methods to bind the clock directly to XClock, allowing the Step() method to simultaneously update the clk and circuit state. Initialization and Adding Pins # Initialization clk = XClock(stepfunc) # Parameter stepfunc is the circuit advancement method provided by DUT backend, e.g., Verilator's step_eval Main Methods\nclass XClock: def Add(xdata) # Bind Clock with xdata, e.g., clock.Add(dut.clk) def Add(xport) # Bind Clock with XData def RefreshComb() # Advance circuit state without advancing time or dumping waveform def RefreshCombT() # Advance circuit state (advance time and dump waveform) def Step(int s = 1) # Advance the circuit by s clock cycles, DUT.Step = DUT.xclock.Step def StepRis(func, args=(), kwargs={}) ","categories":["Tutorial"],"description":"Basic usage of the verification tool.","excerpt":"Basic usage of the verification tool.","ref":"/mlvp/en/docs/env_usage/picker_usage/","tags":["docs"],"title":"Tool Introduction"},{"body":"Principle Introduction Basic Library In this chapter, we will introduce how to use Picker to compile RTL code into a C++ class and compile it into a dynamic library.\nFirst, the Picker tool parses the RTL code, creates a new module based on the specified Top Module, encapsulates the module’s input and output ports, and exports DPI/API to operate the input ports and read the output ports.\nThe tool determines the module to be encapsulated by specifying the file and Module Name of the Top Module. At this point, you can understand Top as the main function in software programming.\nNext, the Picker tool uses the specified simulator to compile the RTL code and generate a DPI library file. This library file contains the logic required to simulate running the RTL code (i.e., the hardware simulator).\nFor VCS, this library file is a .so (dynamic library) file, and for Verilator, it is a .a (static library) file. DPI stands for Direct Programming Interface，which can be understood as an API specification.\nThen, the Picker tool renders the base class defined in the source code according to the configuration parameters, generates a base class (wrapper) for interfacing with the simulator and hides simulator details, and links the base class with the DPI library file to generate a UT dynamic library file.\nAt this point, the UT library file uses the unified API provided by the Picker tool template. Compared with the simulator-specific API in the DPI library file, the UT library file provides a unified API interface for the hardware simulator generated by the simulator. The generated UT library file is common across different languages! Unless otherwise specified, other high-level languages will operate the hardware simulator by calling the UT dynamic library. Finally, based on the configuration parameters and parsed RTL code, the Picker tool generates a C++ class source code. This source code is the definition (.hpp) and implementation (.cpp) of the RTL hardware module in the software. Instantiating this class is equivalent to creating a hardware module.\nThis class inherits from the base class and implements the pure virtual functions in the base class to instantiate the hardware in software. There are two reasons for not encapsulating this class implementation into the dynamic library:\nSince the UT library file needs to be common across different languages, and different languages have different ways to implement classes, for universality, the class implementation is not encapsulated into the dynamic library. To facilitate debugging, enhance code readability, and make it easier for users to repackage and modify. Generating Executable Files In this chapter, we will introduce how to write test cases and generate executable files based on the basic library generated in the previous chapter (including dynamic libraries, class declarations, and definitions).\nFirst, users need to write test cases, which means instantiating the class generated in the previous chapter and calling the methods in the class to operate the hardware module. Details can be found in [Random Number Generator Verification - Configure Test Code](docs/quick-start/examples/rmg/#Configure Test Code) for instantiation and initialization process.\nSecond, users need to apply different linking parameters to generate executable files based on the different simulators applied in the basic library. The corresponding parameters are defined in template/cpp/cmake/*.cmake.\nFinally, according to the configured linking parameters, the compiler will link the basic library and generate an executable file.\nTaking Adder Verification as an example, picker_out_adder/cpp/cmake/*.cmake is a copy of the template described in item 2 above. vcs.cmake defines the linking parameters of the basic library generated using the VCS simulator, and verilator.cmake defines the linking parameters of the basic library generated using the Verilator simulator.\nUsage The parameter --language cpp or -l cpp is used to specify the generation of the C++ basic library. The parameter -e is used to generate an executable file containing an example project. The parameter -v is used to retain intermediate files when generating the project. #include \"UT_Adder.hpp\" int64_t random_int64() { static std::random_device rd; static std::mt19937_64 generator(rd()); static std::uniform_int_distribution\u003cint64_t\u003e distribution(INT64_MIN, INT64_MAX); return distribution(generator); } int main() { #if defined(USE_VCS) UTAdder *dut = new UTAdder(\"libDPIAdder.so\"); #elif defined(USE_VERILATOR) UTAdder *dut = new UTAdder(); #endif // dut-\u003einitClock(dut-\u003eclock); dut-\u003exclk.Step(1); printf(\"Initialized UTAdder\\n\"); struct input_t { uint64_t a; uint64_t b; uint64_t cin; }; struct output_t { uint64_t sum; uint64_t cout; }; for (int c = 0; c \u003c 114514; c++) { input_t i; output_t o_dut, o_ref; i.a = random_int64(); i.b = random_int64(); i.cin = random_int64() \u0026 1; auto dut_cal = [\u0026]() { dut-\u003ea = i.a; dut-\u003eb = i.b; dut-\u003ecin = i.cin; dut-\u003exclk.Step(1); o_dut.sum = (uint64_t)dut-\u003esum; o_dut.cout = (uint64_t)dut-\u003ecout; }; auto ref_cal = [\u0026]() { uint64_t sum = i.a + i.b; bool carry = sum \u003c i.a; sum += i.cin; carry = carry || sum \u003c i.cin; o_ref.sum = sum; o_ref.cout = carry ; }; dut_cal(); ref_cal(); printf(\"[cycle %llu] a=0x%lx, b=0x%lx, cin=0x%lx\\n\", dut-\u003exclk.clk, i.a, i.b, i.cin); printf(\"DUT: sum=0x%lx, cout=0x%lx\\n\", o_dut.sum, o_dut.cout); printf(\"REF: sum=0x%lx, cout=0x%lx\\n\", o_ref.sum, o_ref.cout); Assert(o_dut.sum == o_ref.sum, \"sum mismatch\"); } delete dut; printf(\"Test Passed, destory UTAdder\\n\"); return 0; } Generating Waveforms In C++, the destructor of the DUT automatically calls dut.finalize(), so you only need to delete dut after the test ends to perform post-processing (write waveform, coverage files, etc.).\n#include \"UT_Adder.hpp\" int main() { UTAdder *dut = new UTAdder(\"libDPIAdder.so\"); printf(\"Initialized UTAdder\\n\"); for (int c = 0; c \u003c 114514; c++) { auto dut_cal = [\u0026]() { dut-\u003ea = c * 2; dut-\u003eb = c / 2; dut-\u003ecin = i.cin; dut-\u003exclk.Step(1); o_dut.sum = (uint64_t)dut-\u003esum; o_dut.cout = (uint64_t)dut-\u003ecout; }; dut_cal(); printf(\"[cycle %llu] a=0x%lx, b=0x%lx, cin=0x%lx\\n\", dut-\u003exclk.clk, i.a, i.b, i.cin); printf(\"DUT: sum=0x%lx, cout=0x%lx\\n\", o_dut.sum, o_dut.cout); } delete dut; // automatically call dut.finalize() in ~UTAdder() printf(\"Simulation finished\\n\"); return 0; } ","categories":["Tutorials"],"description":"Encapsulate the DUT hardware runtime environment with C++ and compile it into a dynamic library.","excerpt":"Encapsulate the DUT hardware runtime environment with C++ and compile …","ref":"/mlvp/en/docs/multi-lang/cpp/","tags":["docs"],"title":"Using C++"},{"body":"源码安装Picker工具 依赖安装 cmake ( \u003e=3.11 ) gcc ( 支持c++20,至少为gcc版本10, 建议11及以上 ) python3 ( \u003e=3.8 ) verilator ( \u003e=4.218 ) verible-verilog-format ( \u003e=0.0-3428-gcfcbb82b ) swig ( \u003e=4.2.0, 用于多语言支持 ) 请注意，请确保verible-verilog-format等工具的路径已经添加到环境变量$PATH中，可以直接命令行调用。\n下载源码 git clone https://github.com/XS-MLVP/picker.git --depth=1 cd picker make init 构建并安装 cd picker make # 可通过 make BUILD_XSPCOMM_SWIG=python,java,scala,golang 开启其他语言支持。 # 各语言需要自己的开发环境，需要自行配置，例如javac等 sudo -E make install 默认的安装的目标路径是 /usr/local， 二进制文件被置于 /usr/local/bin，模板文件被置于 /usr/local/share/picker。 如果需要修改安装目录，可以通过指定ARGS给cmake传递参数，例如make ARGS=\"-DCMAKE_INSTALL_PREFIX=your_instal_dir\" 安装时会自动安装 xspcomm基础库（https://github.com/XS-MLVP/xcomm），该基础库是用于封装 RTL 模块的基础类型，位于 /usr/local/lib/libxspcomm.so。 可能需要手动设置编译时的链接目录参数(-L)\n如果开启了Java等语言支持，还会安装 xspcomm 对应的多语言软件包。\npicker也可以编译为wheel文件，通过pip安装\n通过以下命令把picker打包成wheel安装包：\nmake wheel # or BUILD_XSPCOMM_SWIG=python,java,scala,golang make wheel 编译完成后，wheel文件位于dist目录，然后通过pip安装，例如：\npip install dist/xspcomm-0.0.1-cp311-cp311-linux_x86_64.whl pip install dist/picker-0.0.1-cp311-cp311-linux_x86_64.whl 安装完成后，执行picker命令可以得到以下输出:\nXDut Generate. Convert DUT(*.v/*.sv) to C++ DUT libs. Usage: ./build/bin/picker [OPTIONS] [SUBCOMMAND] Options: -h,--help Print this help message and exit -v,--version Print version --show_default_template_path Print default template path --show_xcom_lib_location_cpp Print xspcomm lib and include location --show_xcom_lib_location_java Print xspcomm-java.jar location --show_xcom_lib_location_scala Print xspcomm-scala.jar location --show_xcom_lib_location_python Print python module xspcomm location --show_xcom_lib_location_golang Print golang module xspcomm location --check check install location and supproted languages Subcommands: export Export RTL Projects Sources as Software libraries such as C++/Python pack Pack UVM transaction as a UVM agent and Python class 安装测试 当前picker有export和pack两个子命令。\nexport 子命令用于将RTL设计转换成其他高级编程语言对应的“库”，可以通过软件的方式进行驱动。\npicker export –help\nExport RTL Projects Sources as Software libraries such as C++/Python Usage: picker export [OPTIONS] file... Positionals: file TEXT ... REQUIRED DUT .v/.sv source file, contain the top module Options: -h,--help Print this help message and exit --fs,--filelist TEXT ... DUT .v/.sv source files, contain the top module, split by comma. Or use '*.txt' file with one RTL file path per line to specify the file list --sim TEXT [verilator] vcs or verilator as simulator, default is verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python] Build example project, default is python, choose cpp, java or python --sdir,--source_dir TEXT Template Files Dir, default is ${picker_install_path}/../picker/template --sname,--source_module_name TEXT ... Pick the module in DUT .v file, default is the last module in the -f marked file --tname,--target_module_name TEXT Set the module name and file name of target DUT, default is the same as source. For example, -T top, will generate UTtop.cpp and UTtop.hpp with UTtop class --tdir,--target_dir TEXT Target directory to store all the results. If it ends with '/' or is empty, the directory name will be the same as the target module name --internal TEXT Exported internal signal config file, default is empty, means no internal pin -F,--frequency TEXT [100MHz] Set the frequency of the **only VCS** DUT, default is 100MHz, use Hz, KHz, MHz, GHz as unit -w,--wave_file_name TEXT Wave file name, emtpy mean don't dump wave -c,--coverage Enable coverage, default is not selected as OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1] Copy xspcomm lib to generated DUT dir, default is true -V,--vflag TEXT User defined simulator compile args, passthrough. Eg: '-v -x-assign=fast -Wall --trace' || '-C vcs -cc -f filelist.f' -C,--cflag TEXT User defined gcc/clang compile command, passthrough. Eg:'-O3 -std=c++17 -I./include' --verbose Verbose mode -e,--example Build example project, default is OFF --autobuild BOOLEAN [1] Auto build the generated project, default is true pack子命令用于将UVM中的 sequence_item 转换为其他语言，然后通过TLM进行通信（目前支持Python，其他语言在开发中）\npicker pack –help\nPack uvm transaction as a uvm agent and python class Usage: picker pack [OPTIONS] file... Positionals: file TEXT ... REQUIRED Sv source file, contain the transaction define Options: -h,--help Print this help message and exit -e,--example Generate example project based on transaction, default is OFF -c,--force Force delete folder when the code has already generated by picker -r,--rename TEXT ... Rename transaction name in picker generate code 参数解释 export: file TEXT ... REQUIRED：必须。位置参数，DUT.v/.sv 源文件，包含顶层模块 -h,--help: 可选。打印此帮助信息并退出 --fs,--filelist TEXT ...: 可选。DUT .v/.sv 源文件，包含顶层模块，逗号分隔。或使用 ‘*.txt’ 文件，每行指定一个 RTL 文件路径来指定文件列表 --sim TEXT [verilator]: 可选。使用 vcs 或 verilator 作为模拟器，默认是 verilator --lang,--language TEXT:{python,cpp,java,scala,golang} [python]: 可选。构建示例项目，默认是 python，可选择 cpp、java 或 python --sdir,--source_dir TEXT: 可选。模板文件目录，默认是 ${picker_install_path}/../picker/template --sname,--source_module_name TEXT ...: 可选。在 DUT .v 文件中选择模块，默认是 -f 标记的文件中的最后一个模块 --tname,--target_module_name TEXT: 可选。设置目标 DUT 的模块名和文件名，默认与源相同。例如，-T top 将生成 UTtop.cpp 和 UTtop.hpp，并包含 UTtop 类 --tdir,--target_dir TEXT: 可选。代码生成渲染文件的目标目录，默认为DUT的模块名。如果该参数以’/‘结尾，则在该参数指定的目录中创建以DUT模块名的子目录。 --internal TEXT: 可选。导出的内部信号配置文件，默认为空，表示没有内部引脚 -F,--frequency TEXT [100MHz]: 可选。设置 仅 VCS DUT 的频率，默认是 100MHz，可以使用 Hz、KHz、MHz、GHz 作为单位 -w,--wave_file_name TEXT: 可选。波形文件名，空表示不导出波形 -c,--coverage: 可选。启用覆盖率，默认不选择为 OFF --cp_lib,--copy_xspcomm_lib BOOLEAN [1]: 可选。将 xspcomm 库复制到生成的 DUT 目录，默认是 true -V,--vflag TEXT: 可选。用户定义的模拟器编译参数，透传。例如：’-v -x-assign=fast -Wall –trace’ 或 ‘-C vcs -cc -f filelist.f’ -C,--cflag TEXT: 可选。用户定义的 gcc/clang 编译命令，透传。例如：’-O3 -std=c++17 -I./include’ --verbose: 可选。详细模式 -e,--example: 可选。构建示例项目，默认是 OFF --autobuild BOOLEAN [1]: 可选。自动构建生成的项目，默认是 true 静态多模块支持：\npicker在生成dut_top.sv/v的封装时，可以通过--sname参数指定多个模块名称和对应的数量。例如在a.v和b.v设计文件中分别有模块A和B，需要DUT中有2个A，3个B，生成的模块名称为C（若不指定，默认名称为A_B），则可执行如下命令：\npicker path/a.v,path/b.v --sname A,2,B,3 --tname C 环境变量：\nDUMPVARS_OPTION: 设置$dumpvars的option参数。例如DUMPVARS_OPTION=\"+mda\" picker .... 开启vcs中数组波形的支持。 SIMULATOR_FLAGS: 传递给后端仿真器的参数。具体可参考所使用的后端仿真器文档。 CFLAGS: 设置后端仿真器的-cflags参数。 pack: file: 必需。待解析的UVM transaction文件 --example, -e: 可选。根据UVM的transaction生成示例项目。 --force， -c: 可选。若已存在picker根据当前transaction解析出的文件，通过该命令可强制删除该文件，并重新生成 --rename, -r: 可选。配置生成文件以及生成的agent的名称，默认为transaction名。 测试Examples 编译完成后，在picker目录执行以下命令，进行测试：\nbash example/Adder/release-verilator.sh --lang cpp bash example/Adder/release-verilator.sh --lang python # 默认仅开启 cpp 和 Python 支持 # 支持其他语言编译命令为：make BUILD_XSPCOMM_SWIG=python,java,scala,golang bash example/Adder/release-verilator.sh --lang java bash example/Adder/release-verilator.sh --lang scala bash example/Adder/release-verilator.sh --lang golang bash example/RandomGenerator/release-verilator.sh --lang cpp bash example/RandomGenerator/release-verilator.sh --lang python bash example/RandomGenerator/release-verilator.sh --lang java 参考材料 如何基于picker进行芯片验证，可参考：https://open-verify.cc/mlvp/docs/\n","categories":["教程"],"description":"安装相关依赖，**下载、构建并安装**对应的工具。","excerpt":"安装相关依赖，**下载、构建并安装**对应的工具。","ref":"/mlvp/docs/quick-start/installer/","tags":["docs"],"title":"搭建验证环境"},{"body":" 为满足开放验证的环境要求，我们开发了 Picker 工具，用于将 RTL 设计转换为多语言接口，并在此基础上进行验证，我们将会使用 Picker 工具生成的环境作为基础的验证环境。接下来我们将介绍 Picker 工具，及其基础的使用方法。\nPicker 简介 picker 是一个芯片验证辅助工具，具有两个主要功能：\n打包RTL设计验证模块： picker 可以将 RTL 设计验证模块（.v/.scala/.sv）打包成动态库，并提供多种高级语言（目前支持 C++、Python、Java、Scala、Golang）的编程接口来驱动电路。 UVM-TLM代码自动生成： picker 能够基于用户提供的 UVM sequence_item 进行自动化的 TLM 代码封装，提供 UVM 与其他高级语言（如 Python）的通信接口。 该工具允许用户基于现有的软件测试框架，例如 pytest、junit、TestNG、go test 等，进行芯片单元测试。 基于 Picker 进行验证的优点:\n不泄露 RTL 设计：经过 Picker 转换后，原始的设计文件（.v）被转化成了二进制文件（.so），脱离原始设计文件后，依旧可进行验证，且验证者无法获取 RTL 源代码。 减少编译时间：当 DUT（设计待测）稳定时，只需要编译一次（打包成 .so 文件）。 用户范围广：提供的编程接口多，覆盖不同语言的开发者。 使用丰富的软件生态：支持 Python3、Java、Golang 等生态系统。 自动化的 UVM 事务封装：通过自动化封装 UVM 事务，实现 UVM 和 Python 的通信。 Picker 目前支持的 RTL 仿真器：\nVerilator Synopsys VCS Picker的工作原理\nPicker的主要功能就是将Verilog代码转换为C++或者Python代码，以Chisel开发的处理器为例:先通过Chisel自带的工具将其转换为Verilog代码，再通Picker提供高级编程语言接口。\nPython 模块生成 生成模块的过程 Picker 导出 Python Module 的方式是基于 C++ 的。\nPicker 是 代码生成(codegen)工具，它会先生成项目文件，再利用 make 编译出二进制文件。 Picker 首先会利用仿真器将 RTL 代码编译为 C++ Class，并编译为动态库。（见C++步骤详情） 再基于 Swig 工具，利用上一步生成的 C++ 的头文件定义，将动态库导出为 Python Module。 最终将生成的模块导出到目录，并按照需求清理或保留其他中间文件。 Swig 是一个用于将 C/C++ 导出为其他高级语言的工具。该工具会解析 C++ 头文件，并生成对应的中间代码。 如果希望详细了解生成过程，请参阅 Swig 官方文档。 如果希望知道 Picker 如何生成 C++ Class，请参阅 C++。\n该这个模块和标准的 Python 模块一样，可以被其他 Python 程序导入并调用，文件结构也与普通 Python 模块无异。 Python 模块使用 参数 --language python 或 --lang python 用于指定生成Python基础库。 参数 --example, -e 用于生成包含示例项目的可执行文件。 参数 --verbose, -v 用于保留生成项目时的中间文件。 使用工具生成Python的DUT类 以案例一中的简单加法器为例：\nPicker会自动生成Python的一个基础类，我们称之为DUT类，以前加法器为例，用户需要编写测试用例，即导入上一章节生成的 Python Module，并调用其中的方法，以实现对硬件模块的操作。 目录结构为： picker_out_adder ├── Adder # Picker 工具生成的项目 │ ├── _UT_Adder.so │ ├── __init__.py │ ├── libUTAdder.so │ ├── libUT_Adder.py │ └── signals.json └── example.py # 用户需要编写的代码 在DUT对应的DUTAdder类中共有8个方法(位于Adder/init.py文件)，具体如下： class DUTAdder: def InitClock(name: str) # 初始化时钟，参数时钟引脚对应的名称，例如clk def Step(i:int = 1) # 推进电路i个周期 def StepRis(callback: Callable, args=None, args=(), kwargs={}) # 设置上升沿回调函数 def StepFal(callback: Callable, args=None, args=(), kwargs={}) # 设置下降沿回调函数 def SetWaveform(filename) # 设置波形文件 def SetCoverage(filename) # 设置代码覆盖率文件 def RefreshComb() # 推进组合电路 def Finish() # 销毁电路 DUT对应的引脚，例如reset，clock等在DUTAdder类中以成员变量的形式呈现。如下所示，可以通过value进行引脚的读取和写入。 from Adder import * dut = DUTAdder() dut.a.value = 1 # 通过给引脚的.value属性赋值完成对引脚的赋值 dut.a[12] = 1 # 对引脚输入a的第12bit进行赋值 x = dut.a.value # 读取引脚a的值 y = dut.a[12] # 读取引脚a的第12bit的值 驱动DUT的一般流程 创建DUT，设置引脚模式。默认情况下，引脚是在一下个周期的上升沿进行赋值，如果是组合逻辑，需要设置赋值模式为立即赋值。 初始化时钟。其目的是将时钟引脚与DUT内置的xclock进行绑定。组合逻辑没有时钟可以忽略。 reset电路。大部分时序电路都需要reset。 给DUT输入引脚写入数据。通过“pin.Set(x)”接口，或者pin.vaulue=x进行赋值。 驱动电路。时序电路用Step，组合电路用RefreshComb。 获取DUT各个引脚的输出进行检测。例如和参考模型进行的结果进行assert对比。 完成验证，销毁DUT。调用Finish()时，会把波形，覆盖率等写入到文件。 对应伪代码如下：\n# Python DUT 的名字可通过 --tdir 指定 from DUT import * # 1 创建 dut = DUT() # 2 初始化 dut.SetWaveform(\"test.fst\") dut.InitClock(\"clock\") # 3 reset dut.reset = 1 dut.Step(1) dut.reset = 0 dut.Step(1) # 4 输入数据 dut.input_pin1.value = 0x123123 dut.input_pin3.value = \"0b1011\" # 5 驱动电路 dut.Step(1) # 6 得到结果 x = dut.output_pin.value print(\"result:\", x) # 7 销毁 dut.Finish() 其他数据类型 一般情况下，通过上述DUT类自带的接口就能完成绝大部分DUT的验证，但一些特殊情况需要其他对应的接口，例如自定义时钟、异步操作、推进组合电路并写入波形、修改引脚属性等。\n在picker生成的DUT类中，除了XData类型的引脚成员变量外，还有XClock类型的xclock和XPort类型的xport。\nclass DUTAdder(object): xport: XPort # 成员变量 xport，用于管理DUT中的所有引脚 xclock: XClock # 成员变量 xclock，用于管理时钟 # DUT 引脚 a: XData b: XData cin: XData cout: XData XData 类 DUT引脚中的数据通常位宽不确定，且有四种状态：0、1、Z和X。为此picker提供了XData进行电路引脚数据表示。 主要方法\nclass XData: #拆分XData，例如把一个32位XData中的第7-10位创建成为一个独立XData # name：名称，start：开始位，width：位宽，例如auto sub = a.SubDataRef(\"sub_pin\", 0, 4) def SubDataRef(name, start, width): XData def GetWriteMode():WriteMode #获取XData的写模式，写模式有三种：Imme立即写，Rise上升沿写，Fall下降沿写 def SetWriteMode(mode:WriteMode) #设置XData的写模式 eg: a.SetWriteMode(WriteMode::Imme) def DataValid():bool #检测数据是否有效（Value中含有X或者Z态返回false否者true） def W():int #获取XData的位宽（如果为0，表示XData为verilog中的logic类型，否则为Vec类型的位宽） def U():int #获取XData的值（无符号，同 x = a.value） def S():int #获取XData的值（有符号类型） def String():str #将XData转位16进制的字符串类型，eg: \"0x123ff\"，如果出现?，表现对应的4bit中有x或z态 def Equal(xdata):bool #判断2个XData是否相等 def Set(value) #对XData进行赋值，value类型可以为：XData, string, int, bytes等 def GetBytes(): bytes #以bytes格式获取XData中的数 def Connect(xdata):bool #连接2个XData，只有In和Out类型的可以连接，当Out数据发生变化时，In类型的XData会自动写入 def IsInIO():bool #判断XData是否为In类型，改类型可读可写 def IsOutIO():bool #判断XData是否为Out类型，改类型只可读 def IsBiIO():bool #判断XData是否为Bi类型，改类型可读可写 def IsImmWrite(): bool #判断XData是否为Imm写入模式 def IsRiseWrite(): bool #判断XData是否为Rise写入模式 def IsFallWrite(): bool #判断XData是否为Fall写入模式 def AsImmWrite() #更改XData的写模式为Imm def AsRiseWrite() #更改XData的写模式为Rise def AsFallWrite() #更改XData的写模式为Fall def AsBiIO() #更改XData为Bi类型 def AsInIO() #更改XData为In类型 def AsOutIO() #更改XData为Out类型 def FlipIOType() #将XData的IO类型进行取反，例如In变为Out或者Out变为In def Invert() #将XData中的数据进行取反 def At(index): PinBind #获取第index, eg: x = a.At(12).Get() or a.At(12).Set(1) def AsBinaryString() #将XData的数据变为二进制字符串，eg: \"1001011\" 为了简化赋值操作，XData 对 Set(value) 和 U() 方法进行了属性赋值重载，可以通过pin.value=x 和 x=pin.value进行赋值和取值。\n# 使用.value可以进行访问 # a 为XData类型 a.value = 12345 # 十进制赋值 a.value = 0b11011 # 二进制赋值 a.value = 0o12345 # 八进制赋值 a.value = 0x12345 # 十六进制赋值 a.value = -1 # 所有bit赋值1, a.value = x 与 a.Set(x) 等价 a[31] = 0 # 对第31位进行赋值 a.value = \"x\" # 赋值高阻态 a.value = \"z\" # 赋值不定态 x = a.value # 获取值，与 x = a.U() 等价 XPort 类 在处理少数几个XData引脚时，直接操作XData是比较清晰和直观的。但是，当涉及到多个XData时，进行批量管理就不太方便了。XPort是对XData的一种封装，它允许我们对多个XData进行集中操作。我们还提供了一些方法来方便地进行批量管理。 初始化与添加引脚\nport = XPort(\"p\") #创建前缀为p的XPort实例 主要方法\nclass XPort: def XPort(prefix = \"\") #创建前缀为prefix的port， eg：p = XPort(\"tile_link_\") def PortCount(): int #获取端口中的Pin数量（即绑定的XData个数） def Add(pin_name, XData) #添加Pin, eg：p.Add(\"reset\", dut.reset) def Del(pin_name) #删除Pin def Connect(xport2) #链接2个Port def NewSubPort(std::string subprefix): XPort # 创建子Port，以subprefix开头的所有Pin构成子Port def Get(key, raw_key = False): XData # 获取XData def SetZero() #设置Port中的所有XData为0 XClock 类 XClock是电路时钟的封装，用于驱动电路。在传统仿真工具（例如Verilator）中，需要手动为clk赋值，并通过step_eval函数更新状态。但在我们的工具中，我们提供了相应的方法，可以将时钟直接绑定到XClock上。只需使用我们的Step()方法，就可以同时更新clk和电路状态。 初始化与添加引脚\n# 初始化 clk = XClock(stepfunc) #参数stepfunc为DUT后端提供的电路推进方法，例如verilaor的step_eval等 主要方法\nclass XClock: def Add(xdata) #将Clock和时钟进行绑定， eg：clock.Add(dut.clk) def Add(xport) #将Clock和XData进行绑定 def RefreshComb() #推进电路状态，不推进时间，不dump波形 def RefreshCombT() #推进电路状态（推进时间，dump波形） def Step(int s = 1) #推进电路s个时钟周期， DUT.Step = DUT.xclock.Step def StepRis(func, args=(), kwargs={}) #设置上升沿回调函数，DUT.StepRis = DUT.xclock.StepRis def StepFal(func, args=(), kwargs={}) #设置下降沿回调函数，DUT.StepFal = DUT.xclock.StepFal # 异步方法 async AStep(cycle: int) #异步推进cycle个时钟， eg：await dut.AStep(5) async ACondition(condition) #异步等待conditon()为true async ANext() #异步推进一个时钟周期，等同AStep(1) async RunStep(cycle: int) #持续推进时钟cycle个时钟，用于最外层 ","categories":["教程"],"description":"验证工具的基本使用。","excerpt":"验证工具的基本使用。","ref":"/mlvp/docs/env_usage/picker_usage/","tags":["docs"],"title":"工具介绍"},{"body":"以Adder为例，各语言的验证代码和注释如下：\nCpp Java Scala Python Go #include \"UT_Adder.hpp\" int64_t random_int64() { static std::random_device rd; static std::mt19937_64 generator(rd()); static std::uniform_int_distribution distribution(INT64_MIN, INT64_MAX); return distribution(generator); } int main() { UTAdder *dut = new UTAdder(); dut-\u003eStep(1); printf(\"Initialized UTAdder\\n\"); struct input_t { uint64_t a; uint64_t b; uint64_t cin; }; struct output_t { uint64_t sum; uint64_t cout; }; for (int c = 0; c \u003c 114514; c++) { input_t i; output_t o_dut, o_ref; i.a = random_int64(); i.b = random_int64(); i.cin = random_int64() \u0026 1; auto dut_cal = [\u0026]() { dut-\u003ea = i.a; dut-\u003eb = i.b; dut-\u003ecin = i.cin; dut-\u003eStep(1); o_dut.sum = (uint64_t)dut-\u003esum; o_dut.cout = (uint64_t)dut-\u003ecout; }; auto ref_cal = [\u0026]() { uint64_t sum = i.a + i.b; bool carry = sum \u003c i.a; sum += i.cin; carry = carry || sum \u003c i.cin; o_ref.sum = sum; o_ref.cout = carry ; }; dut_cal(); ref_cal(); printf(\"[cycle %lu] a=0x%lx, b=0x%lx, cin=0x%lx\\n\", dut-\u003exclock.clk, i.a, i.b, i.cin); printf(\"DUT: sum=0x%lx, cout=0x%lx\\n\", o_dut.sum, o_dut.cout); printf(\"REF: sum=0x%lx, cout=0x%lx\\n\", o_ref.sum, o_ref.cout); Assert(o_dut.sum == o_ref.sum, \"sum mismatch\"); } dut-\u003eFinish(); printf(\"Test Passed, destory UTAdder\\n\"); return 0; } package com.ut; import java.math.BigInteger; // import the generated UT class import com.ut.UT_Adder; public class example { static public void main(String[] args){ UT_Adder adder = new UT_Adder(); for(int i=0; i\u003c10000; i++){ int N = 1000000; long a = (long) (Math.random() * N); long b = (long) (Math.random() * N); long c = (long) (Math.random() * N) \u003e 50 ? 1 : 0; // set inputs adder.a.Set(a); adder.b.Set(b); adder.cin.Set(c); // step adder.Step(); // reference model long sum = a + b; boolean carry = sum \u003c a ? true : false; sum += c; carry = carry || sum \u003c c; // assert assert adder.sum.U().longValue() == sum : \"sum mismatch: \" + adder.sum.U() + \" != \" + sum; assert adder.cout.U().intValue() == (carry ? 1 : 0) : \"carry mismatch: \" + adder.cout.U() + \" != \" + carry; } System.out.println(\"Java tests passed\"); adder.Finish(); } } package com.ut import java.lang.Math import com.ut.UT_Adder object example { def main(args: Array[String]): Unit = { val adder = new UT_Adder() for (i \u003c- 0 until 10000) { val N = 1000000 val a = (Math.random() * N).toLong val b = (Math.random() * N).toLong val c = if ((Math.random() * N).toLong \u003e 50) 1 else 0 // set inputs adder.a.Set(a) adder.b.Set(b) adder.cin.Set(c) // step adder.Step() // reference model var sum = a + b var carry = if (sum \u003c a) true else false sum += c carry = carry || (sum \u003c c) // assert assert(adder.sum.U().longValue() == sum, s\"sum mismatch: ${adder.sum.U()} != $sum\") assert(adder.cout.U().intValue() == (if (carry) 1 else 0), s\"carry mismatch: ${adder.cout.U()} != $carry\") println(s\"[cycle ${adder.xclock.getClk().intValue()}] a=${adder.a.U64()}, b=${adder.b.U64()}, cin=${adder.cin.U64()}\") } println(\"Scala tests passed\") adder.Finish() } } from UT_Adder import * import random class input_t: def __init__(self, a, b, cin): self.a = a self.b = b self.cin = cin class output_t: def __init__(self): self.sum = 0 self.cout = 0 def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 128) - 1) def as_uint(x, nbits): return x \u0026 ((1 \u003c\u003c nbits) - 1) def main(): dut = DUTAdder() # Assuming USE_VERILATOR print(\"Initialized UTAdder\") for c in range(11451): i = input_t(random_int(), random_int(), random_int() \u0026 1) o_dut, o_ref = output_t(), output_t() def dut_cal(): dut.a.value, dut.b.value, dut.cin.value = i.a, i.b, i.cin dut.Step(1) o_dut.sum = dut.sum.value o_dut.cout = dut.cout.value def ref_cal(): sum = as_uint( i.a + i.b + i.cin, 128+1) o_ref.sum = as_uint(sum, 128) o_ref.cout = as_uint(sum \u003e\u003e 128, 1) dut_cal() ref_cal() print(f\"[cycle {dut.xclock.clk}] a=0x{i.a:x}, b=0x{i.b:x}, cin=0x{i.cin:x}\") print(f\"DUT: sum=0x{o_dut.sum:x}, cout=0x{o_dut.cout:x}\") print(f\"REF: sum=0x{o_ref.sum:x}, cout=0x{o_ref.cout:x}\") assert o_dut.sum == o_ref.sum, \"sum mismatch\" dut.Finish() print(\"Test Passed, destroy UTAdder\") if __name__ == \"__main__\": main() package main import ( \"fmt\" \"time\" \"math/rand\" ut \"UT_Adder\" ) func assert(cond bool, msg string) { if !cond { panic(msg) } } func main() { adder := ut.NewUT_Adder() rand.Seed(time.Now().UnixNano()) for i := 0; i \u003c 10000; i++ { N := 1000000 a := rand.Int63n(int64(N)) b := rand.Int63n(int64(N)) var c int64 if rand.Int63n(int64(N)) \u003e 50 { c = 1 } else { c = 0 } adder.A.Set(a) adder.B.Set(b) adder.Cin.Set(c) adder.Step() // reflerence model sum := a + b carry := sum \u003c a sum += c carry = carry || sum \u003c c // assert assert(adder.Sum.U64() == uint64(sum), fmt.Sprintf(\"sum mismatch: %d != %d\\n\", adder.Sum.U64(), uint64(sum))) var carry_bool uint64 if carry { carry_bool = 1 } else { carry_bool = 0 } assert(adder.Cout.U64() == carry_bool, fmt.Sprintf(\"carry mismatch: %d != %t\\n\", adder.Cout.U().Int64(), carry)) } adder.Finish(); fmt.Println(\"Golang tests passed\") } ","categories":["教程"],"description":"使用C++、Java、Python和Golang验证加法器的案例","excerpt":"使用C++、Java、Python和Golang验证加法器的案例","ref":"/mlvp/docs/multi-lang/examples/adder/","tags":["docs"],"title":"加法器"},{"body":"安装 toffee Toffee 是一款基于 Python 的硬件验证框架，旨在帮助用户更加便捷、规范地使用 Python 构建硬件验证环境。它依托于多语言转换工具 picker，该工具能够将硬件设计的 Verilog 代码转换为 Python Package，使得用户可以使用 Python 来驱动并验证硬件设计。\nToffee 需要的依赖有：\nPython 3.6.8+ Picker 0.9.0+ 当安装好上述依赖后,可通过pip安装toffee：\npip install pytoffee 或通过以下命令安装最新版本的toffee：\npip install pytoffee@git+https://github.com/XS-MLVP/toffee@master 或通过以下方式进行本地安装：\ngit clone https://github.com/XS-MLVP/toffee.git cd toffee pip install . toffee-test Toffee-test 是一个用于为 Toffee 框架提供测试支持的 Pytest 插件，他为 toffee 框架提供了将测试用例函数标识为 toffee 的测试用例对象，使其可以被 toffee 框架识别并执行;测试用例资源的管理功能;测试报告生成功能，以便于用户编写测试用例。\n通过 pip 安装 toffee-test\npip install toffee-test 或安装开发版本\npip install toffee-test@git+https://github.com/XS-MLVP/toffee-test@master 或通过源码安装\ngit clone https://github.com/XS-MLVP/toffee-test.git cd toffee-test pip install . 搭建简单的验证环境 我们使用一个简单的加法器示例来演示 toffee 的使用方法，该示例位于 example/adder 目录下。\n加法器的设计如下：\nmodule Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] io_a, input [WIDTH-1:0] io_b, input io_cin, output [WIDTH-1:0] io_sum, output io_cout ); assign {io_cout, io_sum} = io_a + io_b + io_cin; endmodule 首先使用 picker 将其转换为 Python Package，再使用 toffee 来为其建立验证环境。安装好依赖后，可以直接在 example/adder 目录下运行以下命令来完成转换：\nmake dut 为了验证加法器的功能，我们使用 toffee 提供的方法来建立验证环境。\n首先需要为其创建加法器接口的驱动方法，这里用到了 Bundle 来描述需要驱动的某类接口，Agent 用于编写对该接口的驱动方法。如下所示：\nclass AdderBundle(Bundle): a, b, cin, sum, cout = Signals(5) class AdderAgent(Agent): @driver_method() async def exec_add(self, a, b, cin): self.bundle.a.value = a self.bundle.b.value = b self.bundle.cin.value = cin await self.bundle.step() return self.bundle.sum.value, self.bundle.cout.value 我们使用了 driver_method 装饰器来标记 Agent 中用于驱动的方法 exec_add，该方法完成了对加法器的一次驱动操作，每当该方法被调用，其会将输入信号 a、b、cin 的值分别赋给加法器的输入端口，并在下一个时钟周期后读取加法器的输出信号 sum 和 cout 的值并返回。\nBundle 是该 Agent 需要驱动的接口的描述。在 Bundle 中提供了一系列的连接方法来连接到 DUT 的输入输出端口。这样一来，我们可以通过此 Agent 完成所有拥有相同接口的 DUT 的驱动操作。\n为了验证加法器的功能，我们还需要为其创建一个参考模型，用于验证加法器的输出是否正确。在 toffee 中，我们使用 Model 来定义参考模型。如下所示：\nclass AdderModel(Model): @driver_hook(agent_name=\"add_agent\") def exec_add(self, a, b, cin): result = a + b + cin sum = result \u0026 ((1 \u003c\u003c 64) - 1) cout = result \u003e\u003e 64 return sum, cout 在参考模型中，我们同样定义了一个 exec_add 方法，该方法与 Agent 中的 exec_add 方法含有相同的输入参数，我们用程序代码计算出了加法器的标准返回值。我们使用了 driver_hook 装饰器来标记该方法，以便该方法可以与 Agent 中的 exec_add 方法进行关联。\n接下来，我们需要创建一个顶层的测试环境，将上述的驱动方法与参考模型相关联，如下所示：\nclass AdderEnv(Env): def __init__(self, adder_bundle): super().__init__() self.add_agent = AdderAgent(adder_bundle) self.attach(AdderModel()) 此时，验证环境已经搭建完成，toffee 会自动驱动参考模型并收集结果，并将结果与加法器的输出进行比对。\n之后，需要编写测试用例来验证加法器的功能，通过 toffee-test，可以使用如下方式编写测试用例。\n@toffee_test.testcase async def test_random(adder_env): for _ in range(1000): a = random.randint(0, 2**64 - 1) b = random.randint(0, 2**64 - 1) cin = random.randint(0, 1) await adder_env.add_agent.exec_add(a, b, cin) @toffee_test.testcase async def test_boundary(adder_env): for cin in [0, 1]: for a in [0, 2**64 - 1]: for b in [0, 2**64 - 1]: await adder_env.add_agent.exec_add(a, b, cin) 可以直接在 example/adder 目录下运行以下命令来运行该示例：\nmake run 运行结束后报告将自动在reports目录下生成。\n","categories":"","description":"","excerpt":"安装 toffee Toffee 是一款基于 Python 的硬件验证框架，旨在帮助用户更加便捷、规范地使用 Python 构建硬件验证环 …","ref":"/mlvp/docs/mlvp/quick-start/","tags":"","title":"快速开始"},{"body":" 在开始前本页会 简单的介绍什么是验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n芯片验证 芯片验证是确保芯片设计正确性和可靠性的重要环节，主要包括功能验证、形式验证和物理验证等形式，本学习材料仅仅包含对功能验证的介绍，且侧重于基于仿真器的芯片功能验证。芯片功能验证的流程和方法与软件测试有比较大的共同点，例如都有单元测试、系统测试、黑盒测试、白盒测试等。在验证指标上也有共同特点，例如功能覆盖率、代码覆盖率等等。从某种形式上说，除了使用的工具和编程语言不一样外，他们的目标和流程几乎相同。因此，在不考虑工具和编程语言的情况下，会软件测试的工程师应当就会芯片验证。 但在实际工作中，软件测试和芯片验证属于两个完全不相交的行业，其主要原因是验证工具和验证语言的不同，导致软件测试工程师很难实现跨界。在芯片验证领域，通常使用硬件描述语言进行验证（例如 Verilog 或者 System Verilog），使用专业商业工具进行电路仿真。硬件描述语言不同于C++/Python等高级软件编程语言，具有独特的“时钟”特性，对于软件领域的工程师不友好，学习成本高。\n为了打通芯片验证与传统软件测试之间的壁垒，让更多的人参与到芯片验证，本项目提供如下内容：\n多语言验证工具（Picker），让用户可以使用自己擅长的编程语言进行芯片验证 验证框架（toffee），如何在不关心时钟的情况下进行功能验证\n介绍基本电路、验证知识，方便软件背景爱好者更能容易的理解电路特征\n提供基本学习材料，学习基本验证知识\n提供真实高性能芯片验证案例，让爱好者可以远程参与验证工作\n基本术语 DUT： DUT（Design Under Test）指待测试设计，通常指设计好的RTL代码。\nRM： Reference Model （RM）指代待测试单元对应的参考模型，参考模型通常被认为是标准的，没有错误的。\nRTL： 指寄存器传输级（Register Transfer Level），通常指代芯片设计对应的 verilog 或者 vhdl 代码。\n覆盖率： 测试覆盖率是指测试范围与整个需求范围的百分比。在芯片验证领域，通常有代码行覆盖率、函数覆盖率、功能覆盖率等。\nDV： DV中的D通常指设计（Desgin），V指验证（Verification）。合在一起指设计与验证协同工作。\n差分测试（difftest）： 选取两个（或以上）功能相同的被测对象，选取符合被测对象要求的同一测试用例分别提交被测对象进行执行，以观测执行结果是否存在差异的过程。\n工具介绍 本学习材料用到的核心工具为 picker （https://github.com/XS-MLVP/picker），它的作用是将RTL编写的设计模块自动提供高级编程语言接口（Python/C++等）。基于该工具，软件开发（测试）背景的验证人员可以不用去学习 Verilog/VHDL 等硬件描述语言进行芯片验证。\n系统需求 建议操作系统：Ubuntu 22.04 LTS\n在系统结构开发、科研的过程中，Linux 是最为常用的平台，这主要是因为 Linux 拥有丰富的软件、工具资源：由于 Linux 的开源性，各大重要工具软件（如 Verilator）可以很容易地面向 Linux 进行开发。 在本课程的实验中，多语言验证工具Picker、Swig等工具都可以在 Linux 上稳定运行。 ","categories":["示例项目","教程"],"description":"如何使用开放验证平台的环境参与到硬件验证中来。","excerpt":"如何使用开放验证平台的环境参与到硬件验证中来。","ref":"/mlvp/docs/quick-start/","tags":["examples","docs"],"title":"快速开始"},{"body":"如何同时调用多个驱动函数 当验证环境搭建完成后，可以通过验证环境提供的接口来编写测试用例。然而，通过普通的串行代码，往往无法完成两个驱动函数的同时调用。在多个接口需要同时驱动的情况下，这种情况变得尤为重要，toffee 为这种场景提供了简便的调用方式。\n同时调用多个不同类别的驱动函数 例如目前的 Env 结构如下：\nDualPortStackEnv - port1_agent - @driver_method push - @driver_method pop - port2_agent - @driver_method push - @driver_method pop 我们期望在测试用例中同时调用 port1_agent 和 port2_agent 的 push 函数，以便同时驱动两个接口。\n在 toffee 中，可以通过 Executor 来完成。\nfrom toffee import Executor def test_push(env): async with Executor() as exec: exec(env.port1_agent.push(1)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) 我们使用 async with 来创建一个 Executor 对象，并建立一个执行块，通过直接调用 exec 可以添加需要执行的驱动函数。当 Executor 对象退出作用域时，会将所有添加的驱动函数同时执行。Executor 会自动等待所有驱动函数执行完毕。\n如果需要获取驱动函数的返回值，可以通过 get_results 方法来获取，get_results 会以字典的形式返回所有驱动函数的返回值，其中键为驱动函数的名称，值为一个列表，列表中存放了对应驱动函数的返回值。\n同一驱动函数被多次调用 如果在在执行块中多次调用同一驱动函数，Executor 会自动将这些调用串行执行。\nfrom toffee import Executor def test_push(env): async with Executor() as exec: for i in range(5): exec(env.port1_agent.push(1)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) 例如上述代码中，port1_agent.push 会被调用 5 次，port2_agent.push 会被调用 1 次。由于 port1_agent.push 是同一驱动函数，Executor 会自动将这 10 次调用串行执行，其返回值会被依次存放在返回值列表中。通过，port2_agent.push 将会与 port1_agent.push 并行执行。\n上述过程中，我们创建了这样一个调度过程：\n------------------ current time -------------------- +---------------------+ +---------------------+ | group \"agent1.push\" | | group \"agent2.push\" | | +-----------------+ | | +-----------------+ | | | agent1.push | | | | agent2.push | | | +-----------------+ | | +-----------------+ | | +-----------------+ | +---------------------+ | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | | +-----------------+ | | | agent1.push | | | +-----------------+ | +---------------------+ ------------------- Executor exit ------------------- Executor 根据两个驱动函数的函数名自动创建了两个调度组，并按照调用顺序将驱动函数添加到对应的调度组中。在调度组内部，驱动函数会按照添加的顺序依次执行。在调度组之间，驱动函数会并行执行。\n调度组的默认名称为以 . 分隔的驱动函数路径名。\n通过 sche_group 参数，你可以在执行函数时手动指定驱动函数调用时所属的调度组，例如\nfrom toffee import Executor def test_push(env): async with Executor() as exec: for i in range(5): exec(env.port1_agent.push(1), sche_group=\"group1\") exec(env.port2_agent.push(2), sche_group=\"group1\") print(\"result\", exec.get_results()) 这样一来，port1_agent.push 和 port2_agent.push 将会被按顺序添加到同一个调度组 group1 中，表现出串行执行的特性。同时 get_results 返回的字典中，group1 会作为键，其值为一个列表，列表中存放了 group1 中所有驱动函数的返回值。\n将自定义函数加入 Executor 如果我们在一个自定义函数中调用了驱动函数或其他驱动函数，并希望自定义函数也可以通过 Executor 来调度，可以通过与添加驱动函数相同的方式来添加自定义函数。\nfrom toffee import Executor async def multi_push_port1(env, times): for i in range(times): await env.port1_agent.push(1) async def test_push(env): async with Executor() as exec: for i in range(2): exec(multi_push_port1(env, 5)) exec(env.port2_agent.push(2)) print(\"result\", exec.get_results()) 此时，multi_push_port1 会被添加到 Executor 中，并创建以 multi_push_port1 为名称的调度组，并向其中添加两次调用。其会与 port2_agent.push 调度组并行执行。\n我们也可以在自定义函数中使用 Executor，或调用其他自定义函数。这样一来，我们可以通过 Executor 完成任意复杂的调度。以下提供了若干个案例：\n案例一\n环境接口如下：\nEnv - agent1 - @driver_method send - agent2 - @driver_method send 两个 Agent 中的 send 函数各需要被并行调用 5 次，并且调用时需要发送上一次的返回结果，第一次发送时发送 0，两个函数调用相互独立。\nfrom toffee import Executor async def send(agent): result = 0 for i in range(5): result = await agent.send(result) async def test_send(env): async with Executor() as exec: exec(send(env.agent1), sche_group=\"agent1\") exec(send(env.agent2), sche_group=\"agent2\") print(\"result\", exec.get_results()) 案例二\n环境接口如下：\nenv - agent1 - @driver_method long_task - agent2 - @driver_method task1 - @driver_method task2 task1 和 task2 需要并行执行，并且一次调用结束后需要同步，task1 和 task2 都需要调用 5 次，long_task 需要与 task1 和 task2 并行执行。\nfrom toffee import Executor async def exec_once(env): async with Executor() as exec: exec(env.agent2.task1()) exec(env.agent2.task2()) async def test_case(env): async with Executor() as exec: for i in range(5): exec(exec_once(env)) exec(env.agent1.long_task()) print(\"result\", exec.get_results()) 设置 Executor 的退出条件 Executor 会等待所有添加的驱动函数执行完毕后退出，但有时我们并不需要等待所有驱动函数执行完毕，可以通过在创建 Executor 时使用 exit 参数来设置退出条件。\nexit 参数可以被设置为 all, any 或 none 三种值，分别表示所有调度组执行完毕后退出、任意一个调度组执行完毕后退出、不等待直接退出。\nfrom toffee import Executor async def send_forever(agent): result = 0 while True: result = await agent.send(result) async def test_send(env): async with Executor(exit=\"any\") as exec: exec(send_forever(env.agent1)) exec(env.agent2.send(1)) print(\"result\", exec.get_results()) 例如上述代码中 send_forever 函数是一个无限循环的函数，将 exit 设置为 any 后，Executor 会在 env.agent2.send 函数执行完毕后退出，而不会等待 send_forever 函数执行完毕。\n如果后续需要等待所有任务执行完毕，可以通过等待 exec.wait_all 来实现。\n如何控制参考模型调度 在 toffee 中，参考模型的调度是由 toffee 自动完成的，但在某些情况下需要手动控制参考模型的调度顺序，例如在参考模型中需要调用多个函数，且这些函数之间存在调用顺序的情况。或者是控制参考模型与驱动函数之间的调用顺序。\n参考模型的调度顺序 在使用 Executor 执行时，可以使用参数 sche_order 来控制参考模型是在驱动函数之前、之后或同时执行。当为 model_first 时，参考模型会在驱动函数之前执行；当为 dut_first 时，驱动函数会在参考模型之前执行；当为 parallel 时，参考模型会与驱动函数同时执行。默认情况下为并行执行。\ndef test_push(env): async with Executor() as exec: exec(env.port1_agent.push(1), sche_order=\"dut_first\") exec(env.port2_agent.push(2), sche_order=\"dut_first\") print(\"result\", exec.get_results()) 上述代码中，参考模型将会在对应的驱动函数结束之后才会被调用。\n参考模型函数之间的调用顺序 当使用函数调用模式编写参考模型时，参考模型中的函数之间可能存在调用顺序相关的一来，例如一个函数在调用之前必须需要另一个函数先被调用。\n这一过程若不使用 Executor 使函数并行执行，很容易得到控制，串行执行的代码中函数的调用顺序即为其执行顺序。\n但如果使用 Executor 并行执行函数，两个参考模型之间的调用顺序就无法保证。toffee 为此场景提供了 priority 参数，用于指定参考模型函数的调用顺序，数值越小其优先级较高。\nfrom toffee import Executor def test_push(env): async with Executor() as exec: exec(env.port1_agent.push(1), priority=1) exec(env.port2_agent.push(2), priority=0) print(\"result\", exec.get_results()) 例如上述代码中，port2_agent.push 和 port1_agent.push 两个函数会并行执行，其参考模型的调用也将在同一时钟周期内完成。由于我们指定了port2_agent.push 的优先级为 0，port1_agent.push 的优先级为 1，因此在该周期的执行过程中，port2_agent.push 会优先被调用。\n注意，优先级只在同一时钟周期内有效，若两个函数调用跨越了时钟周期，那么时钟周期靠前的函数依然会被优先调用。\n","categories":"","description":"","excerpt":"如何同时调用多个驱动函数 当验证环境搭建完成后，可以通过验证环境提供的接口来编写测试用例。然而，通过普通的串行代码，往往无法完成两个驱动函数 …","ref":"/mlvp/docs/mlvp/cases/executor/","tags":"","title":"如何使用测试环境接口进行驱动"},{"body":"启动事件循环 在如前介绍的验证环境中，设计了一套规范的验证环境。但是如果尝试用朴素单线程程序编写，会发现会遇到较为复杂的实现问题。\n例如，我们创建了两个驱动方法分别驱动两个接口，在每一个驱动方法内部，都需要等待 DUT 经过若干个时钟周期，并且这两个驱动方法需要同时运行。这时候，如果使用朴素的单线程程序，会发现很难同时让两个驱动方法同时运行，即便我们使用多线程强势使他们同时运行，也会发现缺乏一种机制，使他们能够等待 DUT 经过若干时钟周期。这是因为在 Picker 提供的接口中，我们只能去推动 DUT 向前执行一个周期，而无法去等待 DUT 执行一个周期。\n更不用说我们还会遇到有众多环境组件需要同时运行的情况了，因此我们首先需要一个能够运行异步程序的环境。toffee 使用了 Python 的协程来完成对异步程序的管理，其在单线程之上建立了一个事件循环，用于管理多个同时运行的协程，协程之间可以相互等待并通过事件循环来进行切换。\n在启动事件循环之前，我们首先需要了解两个关键字 async 和 await 来了解 Python 对与协程的管理。\n当我们在函数前加上 async 关键字时，这个函数就变成了一个协程函数，例如\nasync def my_coro(): ... 当我们在协程函数内部使用 await 关键字时，我们就可以执行一个协程函数，并等待其执行完成并返回结果，例如\nasync def my_coro(): return \"my_coro\" async def my_coro2(): result = await my_coro() print(result) 如果不想等待一个协程函数完成，只想将这一函数加入到事件循环中放入后台运行，可以使用 toffee 提供的 create_task 方法，例如\nimport toffee async def my_coro(): return \"my_coro\" async def my_coro2(): toffee.create_task(my_coro()) 那么如何启动事件循环，并使事件循环开始运行 my_coro2 呢？在 toffee 中，我们使用 toffee.run 来启动事件循环，并运行异步程序。\nimport toffee toffee.run(my_coro2()) toffee 中的环境组件都需要在事件循环中运行，因此当启动 toffee 验证环境时，必须通过 toffee.run 先启动事件循环，然后在事件循环中去创建 toffee 验证环境。\n因此，在验证环境创建时，应该以类似如下的方式：\nimport toffee async def start_test(): # 创建验证环境 env = MyEnv() ... toffee.run(start_test()) 如何管理 DUT 时钟 正如开头提出的问题，如果我们需要两个驱动方法同时运行，并且在每个驱动方法需要等待 DUT 经过若干个时钟周期。异步环境给予了我们等待某个事件的能力，但 Picker 只提供了推动 DUT 向前执行一个周期的能力，没有有提供一个事件让我们来等待。\ntoffee 中提供了对这类功能的支持，它通过创建一个后台时钟，来实现对 DUT 进行一个个周期的向前推动，每推动一个周期，后台时钟就会向其他协程发出时钟信号，使得其他协程能够继续执行。因此，DUT 的实际执行周期推动是由后台时钟来完成的，其他协程中只需要等待后台时钟发布的时钟信号即可。\n在 toffee 中，通过start_clock来创建后台时钟：\nimport toffee async def start_test(): dut = MyDUT() toffee.start_clock(dut) toffee.run(start_test()) 只需要在事件循环中调用 start_clock 即可创建后台时钟，它需要一个 DUT 对象作为参数，用于推动 DUT 的执行，以及将时钟信号绑定到 DUT 以及其各个引脚。\n在其他协程中，我们可以通过 ClockCycles 来等待时钟信号到来，ClockCycles 的参数可以是 DUT，也可以是 DUT 的每一个引脚。例如：\nimport toffee from toffee.triggers import * async my_coro(dut): await ClockCycles(dut, 10) print(\"10 cycles passed\") async def start_test(): dut = MyDUT() toffee.start_clock(dut) await my_coro(dut) toffee.run(start_test()) 在 my_coro 中，通过 ClockCycles 来等待 DUT 经过 10 个时钟周期，当 10 个时钟周期经过后，my_coro 就会继续执行，并打印 “10 cycles passed”。\ntoffee 中提供了多种等待时钟信号的方法，例如：\nClockCycles 等待 DUT 经过若干个时钟周期 Value 等待 DUT 的某个引脚的值等于某个值 AllValid 等待 DUT 的所有引脚的值同时有效 Condition 等待某个条件满足 Change 等待 DUT 的某个引脚的值发生变化 RisingEdge 等待 DUT 的某个引脚的上升沿 FallingEdge 等待 DUT 的某个引脚的下降沿 更多等待时钟信号的方法，参见 API 文档。\n","categories":"","description":"","excerpt":"启动事件循环 在如前介绍的验证环境中，设计了一套规范的验证环境。但是如果尝试用朴素单线程程序编写，会发现会遇到较为复杂的实现问题。\n例如，我 …","ref":"/mlvp/docs/mlvp/env/start_test/","tags":"","title":"如何使用异步环境"},{"body":"验证报告 https://github.com/XS-MLVP/Example-NutShellCache/blob/master/nutshell_cache_report_demo.pdf\n验证环境\u0026用例代码 https://github.com/XS-MLVP/Example-NutShellCache\n","categories":["示例项目","教程"],"description":"利用Python语言对果壳Cache进行验证，","excerpt":"利用Python语言对果壳Cache进行验证，","ref":"/mlvp/docs/advance_case/nutshellcache/","tags":["examples","docs"],"title":"完整果壳 Cache 验证"},{"body":" 本页简单介绍什么是芯片验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n芯片验证过程需要和企业、团队的实际情况契合，没有符合所有要求，必须参考的绝对标准。\n什么是芯片验证 芯片从设计到成品的过程主要包括芯片设计、芯片制造、芯片封测试三大阶段。在芯片设计中，又分前端设计和后端设计，前端设计也称之为逻辑设计，目标是让电路逻辑达到预期功能要求。后端设计也称为物理设计，主要工作是优化布局布线，减小芯片面积，降低功耗，提高频率等。芯片验证（Chip Verification）是芯片设计流程中的一个重要环节。它的目标是确保设计的芯片在功能、性能和功耗等方面都满足预定的规格。验证过程通常包括功能验证、时序验证和功耗验证等多个步骤，使用的方法和工具包括仿真、形式验证、硬件加速和原型制作等。针对本文，芯片验证仅包含对芯片前端设计的验证，验证设计的电路逻辑是否满足既定需求（“Does this proposed design do what is intended?\"），通常也称为功能验证（Functional verification），不包含功耗、频率等后端设计。\n对于芯片产品，一旦设计错误被制造出来修改成本将会非常高昂，因为可能需要召回产品，并重新制造芯片，无论是经济成本还是时间成本都十分昂贵。经典由于芯片验证不足导致失败的典型案例如下： Intel Pentium FDIV Bug：在1994年，Intel的Pentium处理器被发现存在一个严重的除法错误，这个错误被称为FDIV bug。这个错误是由于在芯片的浮点单元中，一个查找表中的几个条目错误导致的。这个错误在大多数应用中不会出现，但在一些特定的计算中会导致结果错误。由于这个错误，Intel不得不召回了大量的处理器，造成了巨大的经济损失。\nAriane 5 Rocket Failure：虽然这不是一个芯片的例子，但它展示了硬件验证的重要性。在1996年，欧洲空间局的Ariane 5火箭在发射后不久就爆炸了。原因是火箭的导航系统中的一个64位浮点数被转换为16位整数时溢出，导致系统崩溃。这个错误在设计阶段没有被发现，导致了火箭的失败。\nAMD Barcelona Bug：在2007年，AMD的Barcelona处理器被发现存在一个严重的转译查找缓冲（TLB）错误。这个错误会导致系统崩溃或者重启。AMD不得不通过降低处理器的频率和发布BIOS更新来解决这个问题，这对AMD的声誉和财务状况造成了重大影响。\n这些案例都强调了芯片验证的重要性。如果在设计阶段就能发现并修复这些错误，那么就可以避免这些昂贵的失败。验证不足的案例不仅发生在过去，也发生在现在，例如某新入局 ASIC 芯片市场的互联网企业打造一款 55 纳米芯片，极力追求面积缩减并跳过验证环节，最终导致算法失败，三次流片皆未通过测试，平均每次流片失败导致企业损失约 50 万美元。\n芯片验证流程 芯片设计和验证的耦合关系如上图所示，设计和验证有同样的输入，即规范文档（specification）。参考规范，设计与验证人员双方按照各自的理解，以及各自的需求进行独立编码实现。设计方需要满足的前提是编码的RTL代码“可综合”，需要考虑电路特性，而验证方一般只要考虑功能是否满足要求，编码限制少。双方完成模块开发后，需要进行健全性对比测试（Sanity Test），判定功能是否表现一致，若不一致需要进行协同排查，确定问题所在并进行修复，再进行对比测试，直到所有功能点都满足预期。由于芯片设计和芯片验证耦合度很高，因此有些企业在研发队伍上也进行了直接耦合，为每个子模块的设计团队都配置了对应的验证团队（DV）。上图中的设计与验证的耦合流程为粗粒度的关系，具体到具体芯片（例如Soc、DDR）、具体企业等都有其适合自身的合作模式。\n在上述对比测试中，设计方的产出的模块通常称为DUT（Design Under Test），验证方开发的模型通常称为RM（Reference Model）。针对图中的验证工作，按照流程可以有：编写验证计划、创建验证平台、整理功能点、构建测试用例、运行调试、收集Bug/覆盖率、回归测试、编写测试报告等多个阶段。\n验证计划： 验证计划描述了如何进行验证，以及如何保证验证质量，达到功能验证要求。在文档结构上通常包含验证目标，验证策略、验证环境、验证项、验证过程、风险防范、资源及时间表、结果和报告等部分。验证目标明确需要验证的功能或性能指标，这些目标应该直接从芯片的规范文档中提取。验证策略描述如何进行验证，包括可能使用的验证方法，例如仿真、形式化、FPGA加速等，以及如何组织验证任务。验证环境用于描述具体的测试环境，例如验证工具类型，版本号等。验证项包含了需要验证的具体项以及预期结果。验证计划可以有总计划，也可以针对具体验证的子任务进行编写。\n平台搭建： 验证平台是具体验证任务的执行环境，同一类验证任务可以使用相同的验证平台。验证平台的搭建是验证流程中的关键步骤、具体包含验证工具选择（例如是采用软件仿真，还是采用形式化验证，或者硬件加速）、环境配置（例如配置服务器环境，FPGA环境）、创建测试环境、基本测试案例等。创建好基本测试平台，跑通基本测试案例，也通常称为“冒烟测试”。后继具体的测试代码，都将基于该测试平台进行，因此测试平台需要具有可重用性。验证平台通常包含测试框架和被测试代码，以及对应的基本信号激励。\n功能点整理： 根据规范手册（spec）列出DUT的基本功能，并对其进行明确的描述，以及如何对该功能点进行测试。功能点整理过程中，需要根据重要性、风险、复杂性等因数对其进行优先级排序。功能点整理还需要对各个功能点进行追踪和状态，如果发现原始功能点有更新需要及时进行对应计划的同步。\n测试用例： 测试用例是指一组条件或变量，用于确定DUT是否满足特定需求并能正确运行。每个测试用例通常包含测试条件，输入数据，预期结果，实际结果和测试结果。通过运行测试用例并比较预期结果和实际结果，可以确定系统或应用是否正确实现了特定的功能或需求。在芯片验证中，测试用例是用来验证芯片设计是否满足规格要求的重要工具。\n编码实现： 编码实现即测试用例的具体执行过程，包括测试数据生成、测试框架选择、编程语言选择、参考模型编写等。编码实现是对功能点和测试用例充分理解后工作，如果理解不到位，可能导致DUT无法驱动，不能发现潜在bug等问题。\n收集bug/覆盖率： 验证的目标就是提前发现设计中存在的bug，因此需要对发现的bug进行收集和管理。每发现一个新缺陷，需要给定唯一标号，并同设计工程师进行bug定级，然后进行状态追踪。能发现bug最好，但在实际验证中不是每次测试都能发现bug，因此需要另外一个指标评价验证是否到位。该指标通常采用覆盖率，当覆盖率超过一点阈值（例如代码覆盖率大于90%）后方可任务进行了充分验证。\n回归测试： 验证和设计是一个相互迭代的过程，因此当验证出bug后，需要设计进行修正，且需要保证修正后的DUT仍然能正常工作。这种测试的目的是捕获可能由于修改而引入的新错误，或者重新激活旧错误。回归测试可以是全面的，也就是说，它涵盖了所有的功能，或者可以是选择性的，只针对某些特定的功能或系统部分。\n测试报告： 测试报告是对整个验证过程的总结，它提供了关于测试活动的全面视图，包括测试的目标、执行的测试用例、发现的问题和缺陷、测试覆盖率和测试效率等。\n芯片验证层次 按照验证对象的大小，芯片验证通常包含UT、BT、IT、ST四个层次。\n单元测试（Unit Testing， UT）： 这是最低的验证层次，主要针对单个模块或组件进行。目标是验证每个模块或组件的功能是否正确。\n块测试（Block Testing，BT）： 很多时候，单个模块和其他模块存在紧耦合，如果进行单独UT测试，可能存在信号处理复杂，功能验证不准确等问题，这时候可以把多个有耦合关系的模块合并成一个DUT块进行测试。\n集成测试（Integration Testing）： 在单元测试的基础上，将多个模块或组件组合在一起，验证它们能否正确地协同工作，通常用于测试子系统功能是否正常。\n系统测试（System Testing）： ST通常也称为Top验证，在集成测试的基础上，将所有的模块或组件组合在一起，形成一个完整的系统，验证系统的功能是否正确，以及系统的性能是否满足要求。\n理论上，这些层次的验证通常按照从低到高的顺序进行，每个层次的验证都建立在前一个层次的验证的基础上。但实际验证活动中，需要根据企业验证人员的规模、熟练度，功能需求等进行选择，不一定所有层次的验证都需要涉及。在每个层次，都需要编写相应的测试用例，运行测试，收集和分析结果，以确保芯片设计的正确性和质量。\n芯片验证指标 芯片验证的指标，通常包含功能正确性、测试覆盖率、缺陷密度、验证效率、验证成本等多个方面。功能正确性是最基本的验证指标，即芯片是否能够正确地执行其设计的功能。这通常通过运行一系列的功能测试用例来验证，包括正常情况下的功能测试，以及异常情况下的鲁棒性测试。测试覆盖率是指测试用例覆盖了多少设计的功能点，以及覆盖的程度如何。高的测试覆盖率通常意味着更高的验证质量。测试覆盖率可以进一步细分为代码覆盖率、功能覆盖率、条件覆盖率等。缺陷密度是指在一定的设计规模或代码量中，发现的缺陷的数量。低的缺陷密度通常意味着更高的设计质量。验证效率是指在一定的时间和资源下，能够完成的验证工作量。高的验证效率通常意味着更高的验证生产力。验证成本是指进行验证所需要的总体资源，包括人力、设备、时间等。低的验证成本通常意味着更高的验证经济性。\n功能正确性是验证的绝对指标，但在实践中，很多时候无法确定测试方案是否完备，所有测试空间是否全部测试到位，因此需要一个可量化的指标来指导验证是否足够充分，是否可以结束验证。该指标通常采用“测试覆盖率”。测试覆盖率通常有代码覆盖率（行，函数，分支）、功能覆盖率。\n代码行覆盖率： 即在测试过程中，DUT的设计代码中有多少行被执行；\n函数覆盖率： 即在测试过程中，DUT的设计代码中有多少函数被执行；\n分支覆盖率： 即在测试过程中，DUT的设计代码中有多少分支被执行（if else）；\n功能覆盖率： 即在测试过程中，有多少预定义功能被触发。\n高的代码覆盖率可以提高验证的质量和可靠性，但并不能保证验证的完全正确性，因为它不能覆盖所有的输入和状态组合。因此，除了追求高的代码覆盖率，还需要结合其他测试方法和指标，如功能测试、性能测试、缺陷密度等。\n芯片验证管理 芯片验证管理是一个涵盖了芯片验证过程中所有活动的管理过程，包括之前提到的验证策略的制定、验证环境的搭建、测试用例的编写和执行、结果的收集和分析、以及问题和缺陷的跟踪和修复等。芯片验证管理的目标是确保芯片设计满足所有的功能和性能要求，以及规格和标准。\n在芯片验证管理中，首先需要制定一个详细的验证策略，包括验证的目标、范围、方法、时间表等。然后，需要搭建一个适合的验证环境，包括硬件设备、软件工具、测试数据等。接下来，需要编写一系列的测试用例，覆盖所有的功能和性能点，然后执行这些测试用例，收集和分析结果，找出问题和缺陷。最后，需要跟踪和修复这些问题和缺陷，直到所有的测试用例都能通过。\n芯片验证管理是一个复杂的过程，需要多种技能和知识，包括芯片设计、测试方法、项目管理等。它需要与芯片设计、生产、销售等其他活动紧密协作，以确保芯片的质量和性能。芯片验证管理的效果直接影响到芯片的成功和公司的竞争力。因此，芯片验证管理是芯片开发过程中的一个重要环节。\n芯片验证管理过程可以基于“项目管理平台”和“bug管理平台”进行，基于平台的管理效率通常情况下明显高于基于人工的管理模式。\n芯片验证现状 当前，芯片验证通常是在芯片设计公司内部完成的，这一过程不仅技术上复杂，而且具有巨大的成本。从验收与设计的紧密关系来看，芯片验证不可避免地涉及芯片设计的源代码。然而，芯片设计公司通常将芯片设计源代码视为商业机密，这使得必须由公司内部人员来执行芯片验证，难以将验证工作外包。\n芯片验证的重要性在于确保设计的芯片在各种条件下能够可靠运行。验证工作不仅仅是为了满足技术规格，还需要应对不断增长的复杂性和新兴技术的要求。随着半导体行业的发展，芯片验证的工作量不断增加，尤其是对于复杂的芯片而言，验证工作已经超过了设计工作，占比超过70%。这使得在工程师人员配比上，验证工程师人数通常是设计工程师人数的2倍或以上（例如zeku的三千人规模团队中，大约有一千人的设计工程师，两千人的验证工程师。其他大型芯片设计公司的验证人员占比类似或更高）。\n由于验证工作的特殊性，需要对芯片设计源代码进行访问，这在很大程度上限制了芯片验证的外包可能性。芯片设计源代码被视为公司的核心商业机密，涉及到技术细节和创新，因此在安全和法律层面上不太可能与外部方共享。这也导致了公司内部人员必须承担验证工作的重任，增加了公司内部的工作负担和成本。\n在当前情况下，芯片验证工程师的需求持续增加。他们需要具备深厚的技术背景，熟悉各种验证工具和方法，并且对新兴技术有敏锐的洞察力。由于验证工作的复杂性，验证团队通常需要庞大的规模，这与设计团队规模形成鲜明对比。\n为了应对这一挑战，行业可能需要不断探索创新的验证方法和工具，以提高验证效率，降低成本。\n小结：复杂芯片验证成本昂贵，表现在如下几个方面 验证工作量大： 对于复杂芯片，验证工作在整个芯片设计工作中，占比超过 70%。\n人力成本高： 验证工程师人数是设计工程师人数的2倍，对于复杂业务，工程师数量在千人以上。\n内部验证： 芯片设计公司为了保证商业秘密（芯片设计代码）不被泄露，只能选择招聘大量验证工程师，在公司内部进行验证工作。\n芯片验证众包 相比与硬件，软件领域为了减少软件测试成本，测试外包（分包）已经成为常态，该领域的分包业务非常成熟，市场规模已经是千亿人民币级别，并朝万亿级别规模进发。从工作内容上看，软件测试和硬件验证，有非常大的共同特征（系统的目的不同的对象），如果以软件的方式对硬件验证进行分包是否可行？\n把芯片验证工作进行外包（分包）面临诸多挑战，例如： 从业人员基数少： 相比软件领域，硬件开发者数量少了几个数量级。例如在github的统计上（https://madnight.github.io/githut/#/pull_requests/2023/2），传统软件编程语言占（Python、Java、C++，Go）比接近 50%， 而硬件描述语言，verilog占比仅 0.076%，这能从侧面反应出各自领域的开发者数量。\n验证工具商业化： 企业中使用的验证工具（仿真器、形式化、数据分析）几乎都是商业工具，这类工具对于普通人来说几乎不可见，自学难度高。\n开放学习资料少： 芯片验证涉及到访问芯片设计的源代码，而这些源代码通常被视为公司的商业机密和专有技术。芯片设计公司可能不愿意公开详细的验证过程和技术，限制了学习材料的可用性。\n可行性分析 虽然芯片验证领域一直以来相对封闭，但从技术角度而言，采用分包的方式进行验证是一种可行的选择。这主要得益于以下几个因素：\n首先，随着开源芯片项目的逐渐增多，验证过程中所涉及的源代码已经变得更加开放和透明。这些开源项目在设计和验证过程中没有商业机密的顾虑，为学习和研究提供了更多的可能性。即使某些项目涉及商业机密，也可以通过采用加密等方式来隐藏设计代码，从而在一定程度上解决了商业机密的问题，使验证更容易实现。\n其次，芯片验证领域已经涌现出大量的基础验证工具，如verilator和systemc等。这些工具为验证工程师提供了强大的支持，帮助他们更高效地进行验证工作。通过这些工具，验证过程的复杂性和难度得到了一定程度的缓解，为采用分包的验证方法提供了更为可行的技术基础。\n在开源软件领域，已经有一些成功的案例可供参考。例如，Linux内核的验证过程采用了分包的方式，不同的开发者和团队分别负责不同的模块验证，最终形成一个整体完备的系统。类似地，机器学习领域的ImageNet项目也采用了分包标注的策略，通过众包的方式完成大规模的图像标注任务。这些案例为芯片验证领域提供了成功的经验，证明了分包验证在提高效率、降低成本方面的潜力。\n因此，尽管芯片验证领域相对于其他技术领域而言仍显得封闭，但技术的进步和开源项目的增多为采用分包验证提供了新的可能性。通过借鉴其他领域的成功经验和利用现有的验证工具，我们有望在芯片验证中推动更加开放、高效的验证方法的应用，进一步促进行业的发展。这种技术的开放性和灵活性将为验证工程师提供更多的选择，推动芯片验证领域迎来更为创新和多样化的发展。\n技术路线 为了克服挑战，让更多的人参与到芯片验证，本项目从如下几个技术方向进行持续尝试\n提供多语言验证工具： 传统芯片验证是基于System Verilog编程语言进行，但是该语言用户基数少，为了让其他软件开发/测试的技术人员参与到芯片验证，本项目提供多语言验证转换工具Picker，它可以让验证者使用自己熟悉的编程语言（例如C++/Python/Java/Go）基于开源验证工具参与验证工作。\n提供验证学习材料： 芯片验证学习材料少，主要原因由于商业公司几乎不可能公开其内部资料，为此本项目会持续更新学习材料，让验证人员可在线，免费学习所需要的技能。\n提供真实芯片验证案例： 为了让学习材料更具使用性，本项目以“香山昆明湖（工业级高性能risc-v处理器）IP核”作为基础，从中摘取模块持续更新验证案例。\n组织芯片设计分包验证： 学以致用是每个人学习的期望目标，为此本项目定期组织芯片设计的验证分包，让所有人（无论你是大学生、验证专家、软件开发测试者、还是中学生）都可以参与到真实芯片的设计工作中去。\n本项目的目标是达到如下愿景，“打开传统验证模式的黑盒，让所有感兴趣的人可以随时随地的，用自己擅长的编程语言参与芯片验证”。\n","categories":"","description":"关于芯片验证的基本概念\n","excerpt":"关于芯片验证的基本概念\n","ref":"/mlvp/docs/basic/ic_verify/","tags":"","title":"芯片验证"},{"body":"生成库文件 picker可以通过参数--lang指定转换的对应语言（参数已支持cpp、python、java、scala、golang），由于不同编程语言对应的“库”不同，因此生成的库文件有所区别，例如java生成的是jar包，python生成的为文件夹。picker导出对应编程语言的库，需要xcomm的支持，可以通过picker --check查看支持情况：\n$picker --check [OK ] Version: 0.9.0---dirty [OK ] Exec path: /home/yaozhicheng/mambaforge/lib/python3.11/site-packages/picker/bin/picker [OK ] Template path: /home/yaozhicheng/mambaforge/lib/python3.11/site-packages/picker/share/picker/template [OK ] Support Cpp (find: '/home/yaozhicheng/mambaforge/lib/python3.11/site-packages/picker/share/picker/include' success) [Err] Support Java (find: 'java/xspcomm-java.jar' fail) [Err] Support Scala (find: 'scala/xspcomm-scala.jar' fail) [OK ] Support Python (find: '/home/yaozhicheng/mambaforge/lib/python3.11/site-packages/picker/share/picker/python' success) [Err] Support Golang (find: 'golang' fail) 输出显示success表示支持，fail表示不支持。\nC++ 对于C++语言，picker生成的为so动态链接库，和对应的头文件。例如：\nUT_Adder/ ├── UT_Adder.cpp # DUT 文件 ├── UT_Adder.hpp # DUT 头文件 ├── UT_Adder_dpi.hpp # DPI 头文件 ├── dut_base.hpp # DUT base 头文件 ├── libDPIAdder.a # DPI 静态库 └── libUTAdder.so # DUT 动态库 在使用时，设置好LD路径，然后再测试代码中#include UT_Adder.hpp\nPython Python语言生成的为目录（Python module以目录的方式表示）\nUT_Adder/ ├── _UT_Adder.so ├── __init__.py ├── libUTAdder.so └── libUT_Adder.py 设置PYTHONPATH后，可以在test中import UT_Adder\nJava/scala 对于Java和scala基于JVM的编程语言，picker生成的为对应的jar包。\nUT_Adder/ ├── UT_Adder-scala.jar └── UT_Adder-java.jar go go语言生成的为目录（类似python）。\nUT_Adder/ └── golang └── src └── UT_Adder ├── UT_Adder.go ├── UT_Adder.so ├── UT_Adder_Wrapper.go ├── go.mod └── libUTAdder.so 设置GOPATH后，可直接进行import\n验证接口 DUT验证接口可以参考连接：https://github.com/XS-MLVP/picker/blob/master/doc/API.zh.md\nxspcomm库接口请参考连接：https://github.com/XS-MLVP/xcomm/blob/master/docs/APIs.cn.md\n","categories":["教程"],"description":"DUT文件与编程语言都支持的验证接口","excerpt":"DUT文件与编程语言都支持的验证接口","ref":"/mlvp/docs/multi-lang/interface/","tags":["docs"],"title":"验证接口"},{"body":" This page introduces the basics of digital circuits. Digital circuits use digital signals and are the foundation of most modern computers.\nWhat Are Digital Circuits Digital circuits are electronic circuits that use two discrete voltage levels to represent information. Typically, digital circuits use two power supply voltages to indicate high (H) and low (L) levels, representing the digits 1 and 0 respectively. This representation uses binary signals to transmit and process information.\nMost digital circuits are built using field-effect transistors, with MOSFETs (Metal-Oxide-Semiconductor Field-Effect Transistors) being the most common. MOSFETs are semiconductor devices that control current flow using an electric field, enabling digital signal processing.\nIn digital circuits, MOSFETs are combined to form various logic gates like AND, OR, and NOT gates. These logic gates are combined in different ways to create the various functions and operations in digital circuits. Here are some key features of digital circuits:\n(1) Voltage Representation： Digital circuits use two voltage levels, high and low, to represent digital information. Typically, a high level represents the digit 1, and a low level represents the digit 0.\n(2) MOSFET Implementation： MOSFETs are one of the most commonly used components in digital circuits. By controlling the on and off states of MOSFETs, digital signal processing and logic operations can be achieved.\n(3) Logic Gate Combinations： Logic gates, composed of MOSFETs, are the basic building blocks of digital circuits. By combining different logic gates, complex digital circuits can be built to perform various logical functions.\n(4) Binary Representation： Information in digital circuits is typically represented using the binary system. Each digit can be made up of a series of binary bits, which can be processed and operated on within digital circuits.\n(5) Signal Processing： Digital circuits convert and process signals through changes in voltage and logic operations. This discrete processing method makes digital circuits well-suited for computing and information processing tasks.\nWhy Learn Digital Circuits Learning digital circuits is fundamental and necessary for the chip verification process, primarily for the following reasons:\n(1) Understanding Design Principles： Digital circuits are the foundation of chip design. Knowing the basic principles and design methods of digital circuits is crucial for understanding the structure and function of chips. The goal of chip verification is to ensure that the designed digital circuits work according to specifications in actual hardware, and understanding digital circuits is key to comprehending the design.\n(2) Design Standards： Chip verification typically involves checking whether the design meets specific standards and functional requirements. Learning digital circuits helps in understanding these standards, thus building better test cases and verification processes to ensure thorough and accurate verification.\n(3) Timing and Clocks： Timing issues are common challenges in digital circuit design and verification. Learning digital circuits helps in understanding concepts of timing and clocks, ensuring that timing issues are correctly handled during verification, avoiding timing delays and conflicts in the circuit.\n(4) Logical Analysis： Chip verification often involves logical analysis to ensure circuit correctness. Learning digital circuits fosters a deep understanding of logic, aiding in logical analysis and troubleshooting.\n(5) Writing Test Cases： In chip verification, various test cases need to be written to ensure design correctness. Understanding digital circuits helps in designing comprehensive and targeted test cases, covering all aspects of the circuit.\n(6) Signal Integrity： Learning digital circuits helps in understanding signal propagation and integrity issues within circuits. Ensuring proper signal transmission under different conditions is crucial, especially in high-speed designs.\nOverall, learning digital circuits provides foundational knowledge and tools for chip verification, enabling verification engineers to better understand designs, write effective test cases, analyze verification results, and troubleshoot issues. Theoretical and practical experience with digital circuits is indispensable for chip verification engineers.\nDigital Circuits Basics You can learn digital circuits through the following online resources：\nTsinghua University’s Digital Circuits Basics USTC Digital Circuit Lab Digital Design and Computer Architecture MIT Analysis and Design of Digital Integrated Circuits Hardware Description Language Chisel Traditional Description Languages Hardware Description Languages (HDL) are languages used to describe digital circuits, systems, and hardware. They allow engineers to describe hardware structure, function, and behavior through text files, enabling abstraction and modeling of hardware designs.\nHDL is commonly used for designing and simulating digital circuits such as processors, memory, controllers, etc. It provides a formal method to describe the behavior and structure of hardware circuits, making it easier for design engineers to perform hardware design, verification, and simulation.\nCommon hardware description languages include:\nVerilog：One of the most used HDLs, Verilog is an event-driven language widely used for digital circuit design, verification, and simulation. VHDL：Another common HDL, VHDL is an object-oriented language offering richer abstraction and modular design methods. SystemVerilog：An extension of Verilog, SystemVerilog introduces advanced features like object-oriented programming and randomized testing, making Verilog more suitable for complex system design and verification. Chisel Chisel is a modern, advanced hardware description language that differs from traditional Verilog and VHDL. It’s a hardware construction language based on Scala. Chisel offers a more modern and flexible way to describe hardware, leveraging Scala’s features to easily implement parameterization, abstraction, and reuse while maintaining hardware-level efficiency and performance.\nChisel’s features include:\nModern Syntax: Chisel’s syntax is more similar to software programming languages like Scala, making hardware description more intuitive and concise. Parameterization and Abstraction: Chisel supports parameterization and abstraction, allowing for the creation of configurable and reusable hardware modules. Type Safety: Based on Scala, Chisel has type safety features, enabling many errors to be detected at compile-time. Generating Performance-Optimized Hardware: Chisel code can be converted to Verilog and then synthesized, placed, routed, and simulated by standard EDA toolchains to generate performance-optimized hardware. Strong Simulation Support: Chisel provides simulation support integrated with ScalaTest and Firrtl, making hardware simulation and verification more convenient and flexible. Chisel Example of a Full Adder The circuit design is shown below:\nComplete Chisel code:\npackage examples import chisel3._ class FullAdder extends Module { // Define IO ports val io = IO(new Bundle { val a = Input(UInt(1.W)) // Input port 'a' of width 1 bit val b = Input(UInt(1.W)) // Input port 'b' of width 1 bit val cin = Input(UInt(1.W)) // Input port 'cin' (carry-in) of width 1 bit val sum = Output(UInt(1.W)) // Output port 'sum' of width 1 bit val cout = Output(UInt(1.W))// Output port 'cout' (carry-out) of width 1 bit }) // Calculate sum bit (sum of a, b, and cin) val s1 = io.a ^ io.b // XOR operation between 'a' and 'b' io.sum := s1 ^ io.cin // XOR operation between 's1' and 'cin', result assigned to 'sum' // Calculate carry-out bit val s3 = io.a \u0026 io.b // AND operation between 'a' and 'b', result assigned to 's3' val s2 = s1 \u0026 io.cin // AND operation between 's1' and 'cin', result assigned to 's2' io.cout := s2 | s3 // OR operation between 's2' and 's3', result assigned to 'cout' } You can refer to Chisel learning materials from the official documentation: https://www.chisel-lang.org/docs\n","categories":"","description":"Basic concepts of digital circuits\n","excerpt":"Basic concepts of digital circuits\n","ref":"/mlvp/en/docs/basic/ic_base/","tags":"","title":"Digital Circuits"},{"body":" This page will briefly introduce what verification is and concepts used in the examples, such as DUT (Design Under Test) and RM (Reference Model).\n","categories":["Sample Projects","Tutorials"],"description":"Detailed usage instructions for the Open Verification Platform environment.","excerpt":"Detailed usage instructions for the Open Verification Platform …","ref":"/mlvp/en/docs/env_usage/","tags":["examples","docs"],"title":"Environment Usage"},{"body":"Bundle serves as an intermediary layer in the mlvp verification environment, facilitating interaction between the Agent and the DUT while ensuring their decoupling. Additionally, Bundle helps define the hierarchy of DUT interface layers, making access to the DUT interface clearer and more convenient.\nA Simple Definition of a Bundle To define a Bundle, you need to create a new class that inherits from the Bundle class in mlvp. Here’s a simple example of defining a Bundle:\nfrom mlvp import Bundle, Signals class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) This Bundle defines a simple adder interface. In the AdderBundle class, we define five signals: a, b, sum, cin, and cout, which represent the input ports a and b, the output port sum, and the carry input and output ports cin and cout, respectively.After the definition, we can access these signals through an instance of the AdderBundle class, for example:\nadder_bundle = AdderBundle() adder_bundle.a.value = 1 adder_bundle.b.value = 2 adder_bundle.cin.value = 0 print(adder_bundle.sum.value) print(adder_bundle.cout.value) Binding the DUT to the Bundle In the code above, we created an instance of a bundle and drove it, but we did not bind this bundle to any DUT, which means operations on this bundle cannot actually affect the DUT. Using the bind method, we can bind a DUT to a bundle. For example, if we have a simple adder DUT whose interface names match those defined in the Bundle:\nadder = DUTAdder() adder_bundle = AdderBundle() adder_bundle.bind(adder) The bind function will automatically retrieve all interfaces from the DUT and bind those with the same names. Once bound, operations on the bundle will directly affect the DUT.However, if the interface names of the DUT differ from those defined in the Bundle, using bind directly will not bind them correctly. In the Bundle, we provide various binding methods to accommodate different binding needs.\nBinding via a Dictionary In the bind function, you can specify a mapping between the DUT’s interface names and the Bundle’s interface names by passing in a dictionary. Suppose the interface names in the Bundle correspond to those in the DUT as follows:\na -\u003e a_in b -\u003e b_in sum -\u003e sum_out cin -\u003e cin_in cout -\u003e cout_out When instantiating the bundle, we can create it using the from_dict method and provide a dictionary to inform the Bundle to bind in this way.\nadder = DUTAdder() adder_bundle = AdderBundle.from_dict({ 'a': 'a_in', 'b': 'b_in', 'sum': 'sum_out', 'cin': 'cin_in', 'cout': 'cout_out' }) adder_bundle.bind(adder) Now, adder_bundle is correctly bound to adder.\nBinding via a Prefix If the DUT’s interface names correspond to those in the Bundle as follows:\na -\u003e io_a b -\u003e io_b sum -\u003e io_sum cin -\u003e io_cin cout -\u003e io_cout You can see that the DUT’s interface names have an io_ prefix compared to those in the Bundle. In this case, you can create the Bundle using the from_prefix method, providing the prefix name to instruct the Bundle to bind using the prefix.\nadder = DUTAdder() adder_bundle = AdderBundle.from_prefix('io_') adder_bundle.bind(adder) Binding via Regular Expressions In some cases, the correspondence between the DUT’s interface names and the Bundle’s interface names may not be a simple prefix or dictionary relationship but instead follow more complex rules. For example, the mapping may be:\na -\u003e io_a_in b -\u003e io_b_in sum -\u003e io_sum_out cin -\u003e io_cin_in cout -\u003e io_cout_out In such cases, you can pass a regular expression to inform the Bundle to bind using that regular expression.\nadder = DUTAdder() adder_bundle = AdderBundle.from_regex(r'io_(.*)_.*') adder_bundle.bind(adder) When using a regular expression, the Bundle attempts to match the DUT’s interface names with the regular expression. For successful matches, the Bundle reads all capture groups from the regular expression, concatenating them into a string. This string is then used to match against the Bundle’s interface names. For example, in the code above, io_a_in matches the regular expression successfully, capturing a as the unique capture group. The name a matches the Bundle’s interface name a, so io_a_in is correctly bound to a.\nCreating Sub-Bundles Often, we may need a Bundle to contain one or more other Bundles. In this case, we can include already defined Bundles as sub-Bundles of the current Bundle.\nfrom mlvp import Bundle, Signal, Signals class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) class MultiplierBundle(Bundle): a, b, product = Signals(3) class ArithmeticBundle(Bundle): selector = Signal() adder = AdderBundle.from_prefix('add_') multiplier = MultiplierBundle.from_prefix('mul_') In the code above, we define an ArithmeticBundle that contains its own signal selector. In addition, it includes an AdderBundle and a MultiplierBundle, which are named adder and multiplier, respectively.When accessing the sub-Bundles within the ArithmeticBundle, you can use the . operator:\narithmetic_bundle = ArithmeticBundle() arithmetic_bundle.selector.value = 1 arithmetic_bundle.adder.a.value = 1 arithmetic_bundle.adder.b.value = 2 arithmetic_bundle.multiplier.a.value = 3 arithmetic_bundle.multiplier.b.value = 4 Furthermore, when defining in this manner, binding the top-level Bundle will also bind the sub-Bundles to the DUT. The previously mentioned various binding methods can still be used when defining sub-Bundles. It is important to note that the method for creating sub-Bundles matches signal names that have been processed by the previous Bundle’s creation method. For example, in the code above, if the top-level Bundle’s matching method is set to from_prefix('io_'), then the signal names matched within the AdderBundle will be those stripped of the io_ prefix. Similarly, the dictionary matching method will pass the names transformed into the mapped names for matching with the sub-Bundle, while the regular expression matching method will pass the names captured by the regular expression for matching with the sub-Bundle.\nPractical Operations in a Bundle Signal Access and Assignment Accessing Signal Values In a Bundle, signals can be accessed not only through the . operator but also through the [] operator.\nadder_bundle = AdderBundle() adder_bundle['a'].value = 1 Accessing Unconnected Signals\ndef bind(self, dut, unconnected_signal_access=True) When binding, you can pass the unconnected_signal_access parameter to control whether accessing unconnected signals is allowed. By default, it is True, meaning unconnected signals can be accessed. In this case, writing to the signal will not change it, and reading the signal will return None. When unconnected_signal_access is set to False, accessing unconnected signals will raise an exception.Assigning All Signals Simultaneously You can use the set_all method to change all input signals at once.\nadder_bundle.set_all(0) Changing Signal Assignment Mode The signal assignment mode is a concept in picker that controls how signals are assigned. Please refer to the picker documentation for more details.In a Bundle, you can change the assignment mode for the entire Bundle using the set_write_mode method.Additionally, there are shortcut methods: set_write_mode_as_imme, set_write_mode_as_rise, and set_write_mode_as_fall, which set the Bundle’s assignment mode to immediate, rising edge, and falling edge assignments, respectively.\nMessage Support Default Message Type Assignment mlvp supports assigning a default message type to a Bundle’s signals using the assign method with a dictionary.\nadder_bundle.assign({ 'a': 1, 'b': 2, 'cin': 0 }) The Bundle will automatically assign the values from the dictionary to the corresponding signals. If you want to assign unspecified signals to a default value, use * to specify a default value:\nadder_bundle.assign({ '*': 0, 'a': 1, }) Default Message Assignment for Sub-Bundles If you want to assign signals in sub-Bundles using default message types, this can be achieved in two ways. When the multilevel parameter in assign is set to True, the Bundle supports multi-level dictionary assignments.\narithmetic_bundle.assign({ 'selector': 1, 'adder': { '*': 0, 'cin': 0 }, 'multiplier': { 'a': 3, 'b': 4 } }, multilevel=True) When multilevel is False, the Bundle supports specifying sub-Bundle signals using the . operator.\narithmetic_bundle.assign({ '*': 0, 'selector': 1, 'adder.cin': 0, 'multiplier.a': 3, 'multiplier.b': 4 }, multilevel=False) Reading Default Message Types You can convert the current signal values in a Bundle into a dictionary using the as_dict method. It supports two formats: when multilevel is True, a multi-level dictionary is returned; when multilevel is False, a flattened dictionary is returned.\n\u003e arithmetic_bundle.as_dict(multilevel=True) { 'selector': 1, 'adder': { 'a': 0, 'b': 0, 'sum': 0, 'cin': 0, 'cout': 0 }, 'multiplier': { 'a': 0, 'b': 0, 'product': 0 } } \u003e arithmetic_bundle.as_dict(multilevel=False) { 'selector': 1, 'adder.a': 0, 'adder.b': 0, 'adder.sum': 0, 'adder.cin': 0, 'adder.cout': 0, 'multiplier.a': 0, 'multiplier.b': 0, 'multiplier.product': 0 } Custom Message Types In custom message structures, rules can be defined to assign signals to a Bundle. One approach is to implement the as_dict function in the custom message structure to convert it into a dictionary, which can then be assigned to the Bundle using the assign method.Another approach is to implement the __bundle_assign__ function in the custom message structure, which accepts a Bundle instance and assigns values to its signals. Once this is implemented, the assign method can be used to assign the message to the Bundle, and the Bundle will automatically call the __bundle_assign__ function for assignment.\nclass MyMessage: def __init__(self): self.a = 0 self.b = 0 self.cin = 0 def __bundle_assign__(self, bundle): bundle.a.value = self.a bundle.b.value = self.b bundle.cin.value = self.cin my_message = MyMessage() adder_bundle.assign(my_message) When you need to convert the signal values in a Bundle into a custom message structure, implement a from_bundle class method in the custom message structure. This method accepts a Bundle instance and returns the custom message structure.\nclass MyMessage: def __init__(self): self.a = 0 self.b = 0 self.cin = 0 @classmethod def from_bundle(cls, bundle): message = cls() message.a = bundle.a.value message.b = bundle.b.value message.cin = bundle.cin.value return message my_message = MyMessage.from_bundle(adder_bundle) Timing Encapsulation In addition to encapsulating DUT pins, the Bundle class also provides timing encapsulation based on arrays, which can be applied to simple timing scenarios. The Bundle class offers a process_requests(data_list) function that accepts an array as input. On the i-th clock cycle, data_list[i] will assign the corresponding data to the pins. The data_list can contain data in the form of a dict or a callable object (callable(cycle, bundle_ins)). For the dict type, special keys include:\n__funcs__: func(cycle, self) # Callable object, can be an array of functions [f1, f2, ..] __condition_func__: func(cycle, self, cargs) # Conditional function, assignment occurs when this returns true, otherwise, the clock advances __condition_args__: cargs # Arguments for the conditional function __return_bundles__: bundle # Specifies which bundle data should be returned when this dict is processed. Can be list[bundle] If the input dict contains __return_bundles__, the function will return the corresponding bundle values, such as {\"data\": x, \"cycle\": y}. For example, consider the Adder bundle where the result is expected after the third addition:\n# The Adder is combinational logic but used here as sequential logic class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) # Define the pins def __init__(self, dut): super().__init__() # init clock # dut.InitClock(\"clock\") self.bind(dut) # Bind to the DUT def add_list(data_list=[(1, 2), (3, 4), (5, 6), (7, 8)]): # Create the input dict data = [] for i, (a, b) in enumerate(data_list): x = {\"a\": a, \"b\": b, \"*\": 0} # Build the dict for bundle assignment if i \u003e= 2: x[\"__return_bundles__\"] = self # Set the bundle to be returned return self.process_requests(data) # Drive the clock, assign values, return results After calling add_list(), the returned result is:\n[ {\"data\": {\"a\":5, \"b\":6, \"cin\": 0, \"sum\":11, \"cout\": 0}, \"cycle\":3}, {\"data\": {\"a\":7, \"b\":8, \"cin\": 0, \"sum\":15, \"cout\": 0}, \"cycle\":4} ] Asynchronous Support In the Bundle, a step function is provided to conveniently synchronize with the clock signal of the DUT. When the Bundle is connected to any signal of the DUT, the step function automatically synchronizes with the DUT’s clock signal.The step function can be used to wait for clock cycles.\nasync def adder_process(adder_bundle): adder_bundle.a.value = 1 adder_bundle.b.value = 2 adder_bundle.cin.value = 0 await adder_bundle.step() print(adder_bundle.sum.value) print(adder_bundle.cout.value) Signal Connectivity Signal Connectivity Rules Once the Bundle instance is defined, you can call the all_signals_rule method to get the connection rules for all signals, helping the user check if the connection rules are as expected.\nadder_bundle.all_signals_rule() Signal Connectivity Check The detect_connectivity method checks if a specific signal name can connect to any signal in the Bundle.\nadder_bundle.detect_connectivity('io_a') The detect_specific_connectivity method checks if a specific signal name can connect to a particular signal in the Bundle.\nadder_bundle.detect_specific_connectivity('io_a', 'a') To check connectivity for signals in sub-Bundles, use the . operator to specify the sub-Bundle.\nDUT Signal Connectivity Check Unconnected Signal Check The detect_unconnected_signals method checks for any signals in the DUT that are not connected to any Bundle.\nBundle.detect_unconnected_signals(adder) Duplicate Connection Check The detect_multiple_connections method checks for signals in the DUT that are connected to multiple Bundles.\nBundle.detect_multiple_connections(adder) Other Practical Operations Set Bundle Name You can set the name of a Bundle using the set_name method.\nadder_bundle.set_name('adder') Once the name is set, more intuitive prompt information is provided. Get All Signals in the Bundle The all_signals method returns a generator containing all signals, including those in sub-Bundles.\nfor signal in adder_bundle.all_signals(): print(signal) Automatic Bundle Generation Script In many cases, the interface of a DUT can be complex, making it tedious to manually write the Bundle definitions. However, since Bundle serves as an intermediate layer, providing an exact definition of signal names is essential. To address this, mlvp provides an automatic generation script that generates Bundle definitions from the DUT’s interface.The script bundle_code_gen.py can be found in the scripts folder of the mlvp repository. This script can automatically generate Bundle definitions by parsing a DUT instance and the specified binding rules. It provides three functions:\ndef gen_bundle_code_from_dict(bundle_name: str, dut, dict: dict, max_width: int = 120) def gen_bundle_code_from_prefix(bundle_name: str, dut, prefix: str = \"\", max_width: int = 120): def gen_bundle_code_from_regex(bundle_name: str, dut, regex: str, max_width: int = 120): These functions generate Bundle definitions based on a dictionary, prefix, or regular expression, respectively.To use, specify the Bundle name, DUT instance, and the corresponding generation rules to generate the Bundle definition. You can also use the max_width parameter to set the maximum width of the generated code.\nfrom bundle_code_gen import * gen_bundle_code_from_dict('AdderBundle', dut, { 'a': 'io_a', 'b': 'io_b', 'sum': 'io_sum', 'cin': 'io_cin', 'cout': 'io_cout' }) gen_bundle_code_from_prefix('AdderBundle', dut, 'io_') gen_bundle_code_from_regex('AdderBundle', dut, r'io_(.*)') The generated code can be copied directly into your project or used with minor modifications. It can also serve as a sub-Bundle definition for use in other Bundles.\n","categories":"","description":"","excerpt":"Bundle serves as an intermediary layer in the mlvp verification …","ref":"/mlvp/en/docs/mlvp/env/bundle/","tags":"","title":"How to Use Bundle"},{"body":"Writing Test Cases In mlvp, test cases are managed using pytest. pytest is a powerful Python testing framework. If you are not familiar with pytest, you can refer to the official pytest documentation .\nWriting Your First Test Case First, we need to create a test case file, for example, test_adder.py. The file should start with test_ or end with _test.py so that pytest can recognize it. Then we can write our first test case in it.\n# test_adder.py async def my_test(): env = AdderEnv() env.add_agent.exec_add(1, 2, 0) def test_adder(): mlvp.run(my_test()) pytest cannot directly run coroutine test cases, so we need to call mlvp.run in the test case to execute the asynchronous test case.Once the test case is written, we can run pytest in the terminal.\npytest pytest will look for all files in the current directory that start with test_ or end with _test.py and will run the functions that start with test_, treating each function as a test case.\nRunning Coroutine Test Cases To enable pytest to run coroutine test cases directly, mlvp provides the mlvp_async marker to mark asynchronous test cases.\n# test_adder.py @pytest.mark.mlvp_async async def test_adder(): env = AdderEnv(DUTAdder()) await env.add_agent.exec_add(1, 2, 0) As shown, we simply need to add the @pytest.mark.mlvp_async marker to the test case function, and pytest will be able to run coroutine test cases directly.\nGenerating Test Reports When running pytest, mlvp will automatically collect the execution results of test cases, tally coverage information, and generate a validation report. To generate this report, you need to add the --mlvp-report parameter when calling pytest.\npytest --mlvp-report By default, mlvp will generate a default report name for each run and place the report in the reports directory. You can specify the report storage directory using the --report-dir parameter and the report name using the --report-name parameter.However, at this point, since mlvp cannot determine the coverage file name, the report cannot display coverage information. If you want coverage information to be shown in the report, you need to pass the functional coverage group and line coverage file name in each test case.\n@pytest.mark.mlvp_async async def test_adder(request): adder = DUTAdder( waveform_filename=\"adder.fst\", coverage_filename=\"adder.dat\" ) g = CovGroup(\"Adder\") env = AdderEnv(adder) await env.add_agent.exec_add(1, 2, 0) adder.Finish() set_func_coverage(request, cov_groups) set_line_coverage(request, \"adder.dat\") In the code above, when creating the DUT, we pass the names of the waveform file and coverage file, allowing the DUT to generate a coverage file with the specified name during execution. Then we define a coverage group to collect the functional coverage information of the DUT, which will be explained in detail in the next document. Next, we call the DUT’s Finish method to stop recording the waveform file. Finally, we use the set_func_coverage and set_line_coverage functions to set the functional coverage group and line coverage file information.When we run pytest again, mlvp will automatically collect the coverage information and display it in the report.Managing Resources with mlvp However, the above process is quite cumbersome, and to ensure that file names do not conflict between each test case, we need to pass different file names in each test case. Additionally, if a test case encounters an exception, it will not complete, resulting in the coverage file not being generated. Therefore, mlvp provides the mlvp_pre_request Fixture to manage resources and simplify the writing of test cases.\n# test_adder.py @pytest.mark.mlvp_async async def test_adder(my_request): dut = my_request env = AdderEnv(dut) await env.add_agent.exec_add(1, 2, 0) @pytest.fixture() def my_request(mlvp_pre_request: PreRequest): mlvp_pre_request.add_cov_groups(CovGroup(\"Adder\")) return mlvp_pre_request.create_dut(DUTAdder) Fixtures are a concept in pytest. In the code above, we define a fixture named my_request. If any other test case has an output parameter containing the my_request parameter, pytest will automatically call the my_request fixture and pass its return value to the test case.In the code above, we defined a custom fixture my_request and used it in the test case, which means that the resource management will be handled within the fixture, allowing the test case to focus solely on the test logic. The my_request fixture must use mlvp’s provided mlvp_pre_request fixture as a parameter for resource management. The mlvp_pre_request fixture provides a series of methods for managing resources.By using add_cov_groups, the coverage group will be automatically included in the report. Using create_dut, a DUT instance is created, and mlvp will automatically manage the generation of the DUT’s waveform and coverage files, ensuring that file names do not conflict.In my_request, you can customize the return values passed to the test case. If you want any test case to access the fixture, you can define the fixture in the conftest.py file. Thus, we have achieved the separation of test case resource management and logic writing, eliminating the need to manually manage resource creation and release in each test case.\n","categories":"","description":"","excerpt":"Writing Test Cases In mlvp, test cases are managed using pytest. …","ref":"/mlvp/en/docs/mlvp/cases/pytest/","tags":"","title":"How to Use Pytest to Manage Test Cases"},{"body":"","categories":["Example Projects","Tutorials"],"description":"Using TileLink Protocol for L2 Cache Driven by C++","excerpt":"Using TileLink Protocol for L2 Cache Driven by C++","ref":"/mlvp/en/docs/advance_case/tilelink/","tags":["examples","docs"],"title":"TileLink Protocol"},{"body":"","categories":["示例项目","教程"],"description":"基于C++驱动使用 TillLink 协议的 L2 Cache","excerpt":"基于C++驱动使用 TillLink 协议的 L2 Cache","ref":"/mlvp/docs/advance_case/tilelink/","tags":["examples","docs"],"title":"TileLink 协议"},{"body":" Introduction to chip verification using the Guoke Cache as an example, covering the basic verification process and report writing.\n","categories":["Sample Projects","Tutorials"],"description":"Introduction to the basic knowledge required for working with the open verification platform.","excerpt":"Introduction to the basic knowledge required for working with the open …","ref":"/mlvp/en/docs/basic/","tags":["examples","docs"],"title":"Verification Basics"},{"body":"Usage When using the Picker tool to encapsulate the DUT, use the -w [wave_file] option to specify the waveform file to be saved. Different waveform file types are supported for different backend simulators, as follows:\nVerilator .vcd format waveform file. .fst format waveform file, a more efficient compressed file. VCS .fsdb format waveform file, a more efficient compressed file. Note that if you choose to generate the libDPI_____.so file yourself, the waveform file format is not restricted by the above constraints. The waveform file format is determined when the simulator constructs libDPI.so, so if you generate it yourself, you need to specify the waveform file format using the corresponding simulator’s configuration.\nPython Example Normally, the DUT needs to be explicitly declared complete to notify the simulator to perform post-processing tasks (writing waveform, coverage files, etc.). In Python, after completing all tests, call the .finalize() method of the DUT to notify the simulator that the task is complete, and then flush the files to disk.\nUsing the Adder Example, the test program is as follows:\nfrom UT_Adder import * if __name__ == \"__main__\": dut = DUTAdder() for i in range(10): dut.a.value = i * 2 dut.b.value = int(i / 4) dut.Step(1) print(dut.sum.value, dut.cout.value) dut.finalize() # flush the wave file to disk After the run is completed, the waveform file with the specified name will be generated.\nViewing Results GTKWave Use GTKWave to open fst or vcd waveform files to view the waveform.\nVerdi Use Verdi to open fsdb or vcd waveform files to view the waveform.\n","categories":["Sample Projects","Tutorials"],"description":"Generate circuit waveforms.","excerpt":"Generate circuit waveforms.","ref":"/mlvp/en/docs/env_usage/wave/","tags":["examples","docs"],"title":"Waveform Generation"},{"body":"Overview The main task of writing verification code can be broadly divided into two parts: building the verification environment and writing test cases .Building the verification environment aims to encapsulate the Design Under Test (DUT) so that the verification engineer does not have to deal with complex interface signals when driving the DUT, but can instead directly use the high-level interfaces provided by the verification environment. If a reference model needs to be written, it should also be part of the verification environment.Writing test cases involves using the interfaces provided by the verification environment to write individual test cases for functional verification of the DUT. Building the verification environment can be quite challenging, especially when the DUT is highly complex with numerous interface signals. In such cases, without a unified standard, constructing the verification environment can become chaotic, making it difficult for one person’s verification environment to be maintained by others. Additionally, when new verification tasks overlap with existing ones, it can be difficult to reuse the previous verification environment due to the lack of standardization.\nThis section will introduce the characteristics that a standardized verification environment should have, which will help in understanding the process of building the verification environment in mlvp.\nNon-Reusable Verification Code Take a simple adder as an example, which has two input ports, io_a and io_b, and one output port, io_sum. If we do not consider the possibility of reusing the verification code for other tasks, we might write the following driving code:\ndef exec_add(dut, a, b): dut.io_a.value = a dut.io_b.value = b dut.Step(1) return dut.io_sum.value In the above code, we wrote an exec_add function, which essentially encapsulates the addition operation of the adder at a high level. With the exec_add function, we no longer need to worry about how to assign values to the interface signals of the adder or how to drive the adder and retrieve its output. We simply need to call the exec_add function to drive the adder and complete an addition operation. However, this driving function has a major drawback—it directly uses the DUT’s interface signals to interact with the DUT, meaning that this driving function can only be used for this specific adder. Unlike software testing, in hardware verification, we frequently encounter scenarios where the interface structures are identical. Suppose we have another adder with the same functionality, but its interface signals are named io_a_0, io_b_0, and io_sum_0. In this case, the original driving function would fail and could not be reused. To drive this new adder, we would have to rewrite a new driving function. If writing a driving function for an adder is already this problematic, imagine the difficulty when dealing with a DUT with complex interfaces. After putting in a lot of effort to write the driving code for such a DUT, we might later realize that the code needs to be migrated to a similar structure with some changes in the interface, leading to a significant amount of rework. Issues such as interface name changes, missing or additional signals, or unused references in the driving code would emerge.\nThe root cause of these issues lies in directly operating the DUT’s interface signals in the verification code. As illustrated in the diagram below, this approach is problematic:\n+-----------+ +-----------+ | |--\u003e| | | Test Code | | DUT | | |\u003c--| | +-----------+ +-----------+ Decoupling Verification Code from the DUT To solve the above problems, we need to decouple the verification code from the DUT, so that the verification code no longer directly manipulates the DUT’s interface signals. Instead, it interacts with the DUT through an intermediate layer. This intermediate layer is a user-defined interface structure, referred to as a Bundle in mlvp, and we will use Bundle to represent this intermediate layer throughout the document.Using the adder as an example, we can define a Bundle structure that includes the signals a, b, and sum, and let the test code interact directly with this Bundle:\ndef exec_add(bundle, a, b): bundle.a.value = a bundle.b.value = b bundle.Step(1) return bundle.sum.value In this case, the exec_add function does not directly manipulate the DUT’s interface signals, and it does not even need to know the names of the DUT’s interface signals. It interacts directly with the signals defined in the Bundle.How do we connect the signals in the Bundle to the DUT’s pins? This can be done by simply specifying how each signal in the Bundle is connected to the DUT’s pins. For example:\nbundle.a \u003c-\u003e dut.io_a bundle.b \u003c-\u003e dut.io_b bundle.sum \u003c-\u003e dut.io_sum If the DUT’s interface signal names change, we only need to modify this connection process:\nbundle.a \u003c-\u003e dut.io_a_0 bundle.b \u003c-\u003e dut.io_b_0 bundle.sum \u003c-\u003e dut.io_sum_0 In this way, regardless of how the DUT’s interface changes, as long as the structure remains the same, we can use the original driving code to operate the DUT, with only the connection process needing adjustment. The relationship between the verification code and the DUT now looks like this:\n+-----------+ +--------+ +-----------+ | |-\u003e| | | | | Test Code | | Bundle |-- connect --| DUT | | |\u003c-| | | | +-----------+ +--------+ +-----------+ In mlvp, we provide a simple way to define Bundles and a variety of connection methods to make defining and connecting the intermediate layer easy. Additionally, Bundles offer many practical features to help verification engineers interact with interface signals more effectively.\nCategorizing DUT Interfaces for Driving We now know that a Bundle must be defined to decouple the test code from the DUT. However, if the DUT’s interface signals are too complex, we might face a new issue—only this particular DUT can be connected to the Bundle. This is because we would be defining a Bundle structure that includes all the DUT’s pins, meaning only a DUT with an identical interface could be connected to this Bundle, which is too restrictive.In such cases, the intermediate layer loses its purpose. However, we often observe that a DUT’s interface structure is logically organized and usually composed of several independent sub-interfaces. For example, the dual-port stack mentioned here has two sub-interfaces with identical structures. Instead of covering the entire dual-port stack interface in a single Bundle, we can split it into two Bundles, each corresponding to one sub-interface.Moreover, for the dual-port stack, the two sub-interfaces have identical structures, so we can use the same Bundle to describe both sub-interfaces without redefining it. Since both share the same Bundle, the driving code written for this Bundle is fully reusable! This is the essence of reusability in verification environments.In summary, for every DUT, we should divide its interface signals into several independent sub-interfaces, each with its own function, and then define a Bundle for each sub-interface. The driving code for each Bundle should then be written. At this point, the relationship between the verification code and the DUT looks like this:\n+-----------+ +--------+ +-----------+ | |-\u003e| | | | | Test Code | | Bundle |-- connect --| | | |\u003c-| | | | +-----------+ +--------+ | | | | ... ... | DUT | | | +-----------+ +--------+ | | | |-\u003e| | | | | Test Code | | Bundle |-- connect --| | | |\u003c-| | | | +-----------+ +--------+ +-----------+ Now, our approach to building the verification environment becomes clear: we write high-level abstractions for each independent sub-interface.\nStructure of Independent Interface Drivers We write high-level abstractions for each Bundle, and these pieces of code are independent and highly reusable. If we separate the interaction logic between the high-level operations and place it in the test cases, then a combination of multiple Test Code + Bundle units will form the entire driving environment for the DUT.We can assign a name to each Test Code + Bundle combination. In mlvp, this structure is called an Agent. An Agent is independent of the DUT and handles all interactions with a specific interface. The relationship between the verification code and the DUT now looks like this:\n+---------+ +-----------+ | | | | | Agent |----| | | | | | +---------+ | | | | ... | DUT | | | +---------+ | | | | | | | Agent |----| | | | | | +---------+ +-----------+ Thus, the process of building the driving environment is essentially the process of writing one Agent after another. However, we have not yet discussed how to write a standardized Agent. If everyone writes Agents differently, the verification environment will still become difficult to manage.\nWriting a Standardized “Agent” To understand how to write a standardized Agent, we first need to grasp the main functions an Agent is supposed to accomplish. As mentioned earlier, an Agent implements all the interactions with a specific class of interfaces and provides high-level abstraction. Let’s explore the interactions between the verification code and the interface. Assuming that the verification code has the ability to read input ports, we can categorize the interactions based on whether the verification code actively initiates communication or passively receives data, as follows:\nVerification Code Actively Initiates Actively reads the value of input/output ports\nActively assigns values to input ports\nVerification Code Passively Receives Passively receives the values of output/input ports These two types of operations cover all interactions between the verification code and the interface, so an Agent must support both. Interactions Actively Initiated by the Verification Code Let’s first consider the two types of interactions actively initiated by the verification code. To encapsulate these interactions at a high level, the Agent must have two capabilities:\nThe driver should be able to convert high-level semantic information into assignments to interface signals.\nIt should convert interface signals into high-level semantic information and return this to the initiator. There are various ways to implement these interactions. However, since mlvp is a verification framework based on a software testing language, and we want to keep the verification code as simple as possible, mlvp standardizes the use of functions to carry out these interactions. Because functions are the most basic abstraction unit in programming, their input parameters can directly represent high-level semantic information and be passed to the function body. Within the function body, assignments and reading operations can handle the translation between semantic information and interface signals. Finally, the return value can be used to pass the converted interface signal back to the initiator as high-level semantic information. In mlvp, such functions that facilitate interactions actively initiated by the verification code are called driver methods . In mlvp, we use the driver_method decorator to mark these functions.\nInteractions Passively Received by the Verification Code Next, let’s look at interactions passively received by the verification code. These interactions occur when the interface sends output signals to the verification code upon meeting specific conditions, without the verification code actively initiating the process.\nFor example, the verification code might want to passively obtain output signals from the DUT after the DUT completes an operation and convert them into high-level semantic information. Alternatively, the verification code might want to passively retrieve output signals at every cycle and convert them. Similar to the driver_method, mlvp also standardizes the use of functions to carry out this type of interaction. However, these functions have no input parameters and are not actively controlled by the verification code. When specific conditions are met, the function is triggered to read interface signals and convert them into high-level semantic information. This information is then stored for later use by the verification code.These functions in mlvp, which facilitate passively received interactions, are referred to as monitor methods . We use the monitor_method decorator in mlvp to mark such functions.\nA Standardized “Agent” Structure In summary, we use functions as carriers to facilitate all interactions between the verification code and the interface. These functions are categorized into two types: driver methods and monitor methods . These methods handle the interactions actively initiated and passively received by the verification code, respectively.Thus, writing an Agent essentially involves creating a series of driver methods and monitor methods. Once an Agent is created, simply providing the list of its internal driver and monitor methods will describe the entire functionality of the Agent.An Agent structure can be described using the following diagram:\n+--------------------+ | Agent | | | | @driver_method | | @driver_method | | ... | | | | @monitor_method | | @monitor_method | | ... | | | +--------------------+ Verifying the DUT’s Functional Correctness At this point, we have completed the encapsulation of high-level operations on the DUT and established interaction between the verification code and the DUT through functions. Now, to verify the functional correctness of the DUT, we would write test cases that use the driver methods to drive the DUT through specific operations. Simultaneously, the monitor methods are automatically triggered to collect relevant information from the DUT.But how do we verify that the DUT’s functionality is correct? After driving the DUT in the test case, the output information we obtain from the DUT comes in two forms: one is actively retrieved through the driver methods, and the other is collected through the monitor methods. Therefore, verifying the DUT’s functionality essentially involves checking whether this information matches the expected results. How do we determine whether this information is as expected? In one case, we already know what the DUT’s output should be or what conditions it should meet. In this situation, after obtaining the information in the test case, we can directly check it against our expectations. In another case, we do not know the expected output of the DUT. In this scenario, we can create a Reference Model (RM) with the same functionality as the DUT. Whenever we send input to the DUT, we simultaneously send the same input to the reference model. To verify the two types of output information, we can compare the DUT’s output with the reference model’s output, obtained at the same time, to ensure consistency. These are the two methods of verifying the DUT’s correctness: direct comparison and reference model comparison .\nHow to Add a Reference Model For direct comparison, the comparison logic can be written directly into the test case. However, if we use the reference model method, the test case might involve additional steps: sending information to both the DUT and the model simultaneously, collecting both DUT and model outputs, and writing logic for comparing passive signals from the DUT with the reference model. This can clutter the test case code and mix the reference model interaction logic with the test logic, making maintenance difficult.\nWe can observe that every call to a driver function represents an operation on the DUT, which also needs to be forwarded to the reference model. The reference model doesn’t need to know how the DUT’s interface is driven; it only needs to process high-level semantic information and update its internal state. Therefore, the reference model only needs to receive the high-level semantic information (i.e., the input parameters of the driver function).\nThus, the reference model only needs to define how to react when driver functions are called. The task of passing call information to the reference model can be handled by the framework. Similarly, comparing return values or monitor signals can also be automatically managed by the framework.\nWith this, test cases only need to focus on driving the DUT, while synchronization and comparison with the reference model will be automatically managed by the framework. To achieve reference model synchronization, mlvp defines a set of reference model matching specifications. By following these specifications, you can automatically forward and compare data to the reference model. Additionally, mlvp provides the Env concept to package the entire verification environment. Once the reference model is implemented, it can be linked to the Env for automatic synchronization.\nConclusion Thus, our verification environment becomes the following structure:\n+--------------------------------+ | Env | | +-----------+ | +-----------+ | +---------+ | | | | | | | Agent |----| | |-\u003e| Reference | | +---------+ | DUT | | | Model | | +---------+ | | |\u003c-| | | | Agent |----| | | | | | +---------+ | | | +-----------+ | ... +-----------+ | +--------------------------------+ At this stage, building the verification environment becomes clear and standardized. For reuse, you simply select the appropriate Agent, connect it to the DUT, and package everything into an Env. To implement a reference model, you just follow the Env interface specification and implement the reference model logic.The test cases are separated from the verification environment. Once the environment is set up, the interfaces provided by each Agent can be used to write the driving logic for the test cases. The synchronization and comparison with the reference model will be automatically handled by the framework. This is the idea behind constructing the verification environment in mlvp, which offers many features to help you build a standardized verification environment. It also provides test case management methods to make writing and managing test cases easier.\n","categories":"","description":"","excerpt":"Overview The main task of writing verification code can be broadly …","ref":"/mlvp/en/docs/mlvp/canonical_env/","tags":"","title":"Writing a Standardized Verification Environment"},{"body":"概述 一个验证任务编写代码的主体工作可以大致分为两部分，验证环境的搭建 和 测试用例的编写。\n验证环境的搭建 旨在完成对待测设计（DUT）的封装，使得验证人员在驱动 DUT 时，不必面临繁杂的接口信号，而是可以直接使用验证环境中提供的高级接口。如果需要编写参考模型，则参考模型也应是验证环境的一部分。\n测试用例的编写 则是测试人员使用验证环境提供的接口，编写一个个测试用例，对 DUT 进行功能验证。\n搭建验证环境是一件相当有挑战的事情，当 DUT 极度复杂，特别是在接口信号繁多的情况下，搭建验证环境的难度会更大。此时，若没有一个统一的规范，验证环境的搭建将会变得混乱不堪，一个人编写的验证环境很难被其他人维护。并且当出现新的验证任务与原有验证任务有交集时，因为原有的验证环境缺乏规范，很难将原有的验证环境复用。\n本节将会介绍一个规范的验证环境所应该具备的特性，这将有助于理解 mlvp 验证环境的搭建流程。\n无法复用的验证代码 以一个简单的加法器为例，该加法器拥有两个输入端口 io_a 和 io_b，一个输出端口 io_sum。 在没有意识到验证代码可能会被用于其他验证任务的情况下，我们可能会编写出这样的驱动代码：\ndef exec_add(dut, a, b): dut.io_a.value = a dut.io_b.value = b dut.Step(1) return dut.io_sum.value 上述代码中，我们编写了一个 exec_add 方法，该方法本质上是对加法器加法操作的一次高层封装。拥有 exec_add 方法以后，我们无需再关心如何对加法器的接口信号进行赋值，也无需关心怎样驱动加法器并获取其输出，只需要调用 exec_add 方法即可驱动加法器完成一次加法操作。\n然而，这个驱动函数却有一个很大的弊端，它直接使用了 DUT 的接口信号来与 DUT 进行交互，这也就意味着，这个驱动函数只能用于这个加法器。\n与软件测试不同，在硬件验证中我们每时每刻都能碰到接口结构相同的情况。假设我们拥有另一个具有相同功能的加法器，但其接口信号名称分别是 io_a_0、io_b_0 和 io_sum_0，那么这个驱动函数对这个加法器则直接失效，无法复用。要想驱动，只能重新编写一个新的驱动函数。\n一个加法器尚且如此，倘若我们拿到了一个拥有繁杂接口的 DUT，费尽心思为其编写了驱动代码。当后续发现驱动代码需要迁移至另一个相似结构的接口上时，我们将会面临巨大的工作量。例如出现接口名称改变、部分接口缺少但驱动代码中却有引用，部分接口新增等等一系列的问题。\n出现这种问题的根本原因在于，我们在验证代码中直接对 DUT 的接口信号进行操作，如下图所示，这种做法是不可取的。\n+-----------+ +-----------+ | |--\u003e| | | Test Code | | DUT | | |\u003c--| | +-----------+ +-----------+ 将验证代码与 DUT 进行解耦 为了解决上述问题，我们需要将验证代码与 DUT 进行解耦，使得验证代码不再直接操作 DUT 的接口信号，而是通过一个中间层来与 DUT 进行交互。这个中间层是人为定义的一个接口结构，在 toffee 中，我们将这个中间层定义为 Bundle，下文也将会使用 Bundle 来代指这个中间层。\n以上述加法器为例，我们可以定义一个 Bundle 结构，其中包含 a, b 和 sum 三个信号，并让测试代码与这个 Bundle 进行直接交互。\ndef exec_add(bundle, a, b): bundle.a.value = a bundle.b.value = b bundle.Step(1) return bundle.sum.value 此时，在 exec_add 中并没有直接操作 DUT 的接口信号，甚至不知道 DUT 中的接口信号名称是什么，其直接与我们在 Bundle 中定义的接口信号进行交互。\n那如何让 Bundle 中的信号与 DUT 的引脚进行关联呢？只需要添加一个连接操作即可，最简单的连接方法是，我们直接指定 Bundle 中的每一个信号具体与 DUT 的哪一个引脚相连，例如：\nbundle.a \u003c-\u003e dut.io_a bundle.b \u003c-\u003e dut.io_b bundle.sum \u003c-\u003e dut.io_sum 如果 DUT 的接口信号名称发生了变化，我们只需要修改这个连接过程，例如：\nbundle.a \u003c-\u003e dut.io_a_0 bundle.b \u003c-\u003e dut.io_b_0 bundle.sum \u003c-\u003e dut.io_sum_0 这样一来，无论 DUT 的接口如何变化，只要其拥有相同的结构，都可以通过原有的驱动代码来驱动，需要修改的仅仅是连接过程。此时的验证代码与 DUT 的关系如下图所示：\n+-----------+ +--------+ +-----------+ | |-\u003e| | | | | Test Code | | Bundle |-- connect --| DUT | | |\u003c-| | | | +-----------+ +--------+ +-----------+ 在 toffee 中，我们为 Bundle 提供了简洁的定义过程以及大量的连接方法，可极大方便中间层的定义与连接，除此之外，Bundle 还提供了大量的实用功能来帮助验证人员更好的与接口信号进行交互。\n将 DUT 接口进行分类驱动 我们已经知道，需要定义一个 Bundle 来完成测试代码与 DUT 之间的解耦，但是如果 DUT 的接口信号过于复杂，我们将会面临一个新的问题————可能只有这一个 DUT 能与这个 Bundle 进行连接。因为我们会定义一个含有众多信号的中间层，将整个 DUT 的引脚全部涵盖在内，这样一来，只有与整个 DUT 结构相同的 DUT 才能与这个 Bundle 进行连接，这个条件是极为苛刻的。\n这样一来，中间层的设置也就失去意义了。但我们观察到，每个 DUT 的接口结构往往是有规律的，他们通常由若干个具有独立功能的接口组成。例如 这里 提到的双端口栈，它的接口则由两个结构完全相同的子接口构成。因此，相比于将整个双端口栈的接口信号全部涵盖在一个 Bundle 中，我们可以将其拆分为两个 Bundle，每个 Bundle 分别对应一个子接口。\n并且，对于双端口栈来说，两个子接口的结构是完全相同的，因此我们可以使用同一个 Bundle 来描述这两个子接口，无需重复定义。既然他们拥有同样的 Bundle，那么我们针对这个 Bundle 编写的驱动代码也是完全可以复用的！这就是验证环境可复用性的魅力。\n总结一下，对于所有的 DUT 来说，我们应该将其接口信号划分成若干个独立的子接口，每个子接口拥有独立的功能，然后为每个子接口定义一个 Bundle，并编写与这个 Bundle 相关的驱动代码。\n此时，我们的验证代码与 DUT 的关系如下图所示：\n+-----------+ +--------+ +-----------+ | |-\u003e| | | | | Test Code | | Bundle |-- connect --| | | |\u003c-| | | | +-----------+ +--------+ | | | | ... ... | DUT | | | +-----------+ +--------+ | | | |-\u003e| | | | | Test Code | | Bundle |-- connect --| | | |\u003c-| | | | +-----------+ +--------+ +-----------+ 同时，我们搭建验证环境的思路也变得清晰起来，只需要为分别每个独立的子接口编写高层封装的操作即可。\n驱动独立接口的结构 我们为每个 Bundle 都编写了高层封装的操作，这些代码之间相互独立，拥有极高的可复用性。如果我们把不同 Bundle 高层封装之间的交互逻辑都划分出去，放到测试用例中来完成，那么多个 Test Code + Bundle 的组合将会完成对整个 DUT 的驱动环境的搭建。\n我们不妨对单个 Test Code + Bundle 的组合起一个名字，在 toffee 中，该结构被称之为 Agent。Agent 独立于 DUT，负责完成对一类接口的所有交互操作。\n此时，我们的验证代码与 DUT 的关系如下图所示：\n+---------+ +-----------+ | | | | | Agent |----| | | | | | +---------+ | | | | ... | DUT | | | +---------+ | | | | | | | Agent |----| | | | | | +---------+ +-----------+ 因此编写驱动环境的过程，就是编写一个个 Agent 的过程。但至此，我们还没有讨论过编写 Agent 的具体规范，如果每个人编写的 Agent 都各不相同，那么验证环境依然会变得较为混乱。\n编写规范的 “Agent” 为了探寻如何编写一个规范的 Agent，我们首先要明白 Agent 主要完成怎样的功能。如上文所述，Agent 中实现了对一类接口的所有交互操作，并提供高层封装。\n我们首先来探讨，验证代码究竟会与接口产生怎样的交互，假设验证代码具备读取输入端口的能力，我们可以按照验证代码是否主动发起交互，与接口的方向性来划分为以下几类交互。\n验证代码主动发起 验证代码主动读取输入/输出端口的值 验证代码主动给输入端口赋值 验证代码被动接收 验证代码被动接收输出/输出端口的值 这两类操作涵盖了验证代码侧与接口之间的的所有操作，因此 Agent 也必须具备这两类操作的能力。\n验证代码主动发起的交互 我们首先考虑验证代码主动发起的两类交互，对这两类交互完成高层封装，就要求 Agent 必须具备两种能力：\n驱动发起者能够将上层语义信息转换为对接口信号的赋值操作 能够将接口信号转换为上层语义信息并返回给发起者 能够完成这两类交互的形式有很多。但由于 toffee 是一个基于软件测试语言的验证框架，且我们希望验证代码的编写形式尽量简洁，toffee 中规范了使用 函数 为载体来完成这两类交互。\n因为函数是编程语言中最基本的抽象单元，输入参数可直接作为上层语义信息并传递给函数体，函数体中通过赋值和读取操作完成上层语义信息与接口信号的转换，最后通过返回值将接口信号转换为上层语义信息并返回给发起者。\ntoffee 此类用于验证代码主动发起交互的函数称之为驱动方法，在 toffee 中，我们使用 driver_method 装饰器来标记此类函数。\n验证代码被动接收的交互 接下来我们考虑验证代码被动接收的交互，并对此类交互完成高层封装。这类交互的呈现形式为，验证代码并不去主动发起对接口的输入输出，而是当满足特定的条件时，接口会将输出信号传递给验证代码。\n例如，验证代码想要在 DUT 完成一次操作后，被动获取 DUT 的输出信号并转换为上层语义信息。再如，验证代码想在每一周期都被动获取 DUT 的输出信号并转换为上层语义信息。\n与 driver_method 类似，toffee 中同样规范了使用 函数 为载体来完成这类交互，只不过这个函数没有输入参数，并且不受验证代码的主动控制。当特定条件满足时，该函数会被调用，完成对接口信号的读取操作，并转换为上层语义信息。该信息会被保存，等待验证代码的使用。\n类似的，toffee 将此类用于验证代码被动接收交互的函数称之为监测方法，在 toffee 中，我们使用 monitor_method 装饰器来标记此类函数。\n一个规范的 “Agent” 结构 综上所述，我们使用 函数 作为载体来完成验证代码与接口的所有交互，并将其分为两类：驱动方法 和 监测方法。这两类方法分别完成验证代码主动发起的交互和被动接收的交互。\n因此，编写 Agent，其实就是编写一系列的驱动方法和检测方法。一个 Agent 编写好之后，也只需要提供其内部驱动方法和检测方法的列表，便能描述整个 Agent 的功能。\n一个 Agent 的结构可以使用下图来描述：\n+--------------------+ | Agent | | | | @driver_method | | @driver_method | | ... | | | | @monitor_method | | @monitor_method | | ... | | | +--------------------+ 验证 DUT 的功能正确性 目前为止，我们完成了对 DUT 高层操作的封装，并使用函数完成了验证代码与 DUT 之间的交互。此时，为了验证 DUT 的功能是否正确，我们会编写测试用例，通过我们封装好的驱动方法来驱动起 DUT 完成特定的执行过程。与此同时，监测方法 在自动地被调用并监测收集 DUT 的相关信息。\n然而如何去验证 DUT 的功能是否正确呢？\n在测试用例中对 DUT 进行驱动后，能够得到 DUT 输出的信息包含两种，一种是通过驱动方法主动获取的信息，另一种是通过检测方法收集到的信息。因此，验证 DUT 的功能是否正确，实际上就是验证这两种信息是否符合预期。\n那如何判断这两种信息是否符合预期呢？\n一种情况是，我们本来就知道 DUT 的输出应该是什么，或是满足什么样的条件。这时我们在测试用例中拿到这两种信息后，直接对这两种信息（或是其中一种，取决于验证用例的需求）进行检查即可。\n另一种情况是，我们并不知道 DUT 的输出应该是什么。此时我们只能编写一个与 DUT 功能相同的 参考模型(RM, Reference Model)，当主动发送给 DUT 任何信息时，同时将这些信息主动发送给参考模型。\n为了对验证两类信息是否符合预期。当主动获取 DUT 的输出信息是时，同时主动去获取参考模型中的输出信息，并将两者进行比对；当监测方法监测到 DUT 的输出信息时，同时参考模型也应主动提供输出信息，并将两者进行比对。\n这便是验证 DUT 功能正确性的两类方法：直接比对 和 参考模型比对。\n如何添加参考模型 对于直接比对的验证方法，我们直接在测试用例中编写比对逻辑即可。而如果我们使用参考模型的比对方式，测试用例可能会面临一些比较繁琐的步骤：驱动接口的同时，需要将信息同时发送给模型；收集接口信息的同时，需要同时收集模型的信息；对于被动监测的接口信息，还要额外编写逻辑来完成与参考模型的比对。这样一来，测试用例的代码会混乱，与参考模型的交互逻辑会混杂在测试用例中，不利于代码的维护。\n我们注意到，对于驱动函数的每一次调用，代表着对 DUT 的每一次操作，这些操作都需要转发给参考模型。而参考模型的编写无需考虑 DUT 接口是怎样驱动的，它只需要分析高层语义信息，并且完成自身状态更新即可，因此参考模型中只需要获取上层发来的高层语义信息（即驱动函数的输入参数）。\n所以在参考模型中只需要实现当驱动函数被调用时，如何做出反应即可。对于将调用信息传递给参考模型的操作完全可以交由框架来完成。与此同时，每一次操作的返回值、监测方法的检测值比较，也可以通过框架来自动完成。\n这样一来，测试用例中只需要编写驱动 DUT 的逻辑，参考模型的同步与比对工作将会被框架自动完成。\n为了实现参考模型的同步，toffee 中定义了一套参考模型的匹配规范，只需要按照此规范编写参考模型调用接口，便可实现参考模型的自动转发与比较。同时为了方便参考模型与整个验证环境相关联，toffee 中提供了 Env 的概念来打包整个验证环境，写好参考模型后，只需要将其与 Env 相关联，便可实现参考模型的自动同步。\n总结 这样一来，我们的验证环境变成了如下的结构：\n+--------------------------------+ | Env | | +-----------+ | +-----------+ | +---------+ | | | | | | | Agent |----| | |-\u003e| Reference | | +---------+ | DUT | | | Model | | +---------+ | | |\u003c-| | | | Agent |----| | | | | | +---------+ | | | +-----------+ | ... +-----------+ | +--------------------------------+ 此时，整个验证环境的搭建变得清晰而规范，需要复用时，只需挑选合适的 Agent，连接至 DUT，并打包到 Env 中。需要编写参考模型时，只需要根据 Env 中调用接口规范，实现参考模型的逻辑即可。\n测试用例的编写与验证环境是分开的，当测试环境搭建完毕后，测试环境的接口就是每个 Agent 中的调用接口。测试用例可以清晰地使用这些接口来完成驱动逻辑的编写。参考模型的同步及对比工作也将由框架自动完成。\n这是 toffee 中验证环境的搭建思想，toffee 中提供了大量的功能，来帮助你建立起这样一个规范的验证环境。同时，它提供了测试用例的管理方法，使得测试用例更易于编写和管理。\n","categories":"","description":"","excerpt":"概述 一个验证任务编写代码的主体工作可以大致分为两部分，验证环境的搭建 和 测试用例的编写。\n验证环境的搭建 旨在完成对待测设计（DUT）的 …","ref":"/mlvp/docs/mlvp/canonical_env/","tags":"","title":"编写规范的验证环境"},{"body":"使用方法 在使用 Picker 工具封装 DUT 时，使用选项-w [wave_file]指定需要保存的波形文件。 针对不同的后端仿真器，支持不同的波形文件类型，具体如下：\nVerilator .vcd格式的波形文件。 .fst格式的波形文件，更高效的压缩文件。 VCS .fsdb格式的波形文件，更高效的压缩文件。 需要注意的是，如果你选择自行生成 libDPI_____.so 文件，那么波形文件格式不受上述约束的限制。因为波形文件是在仿真器构建 libDPI.so 时决定的，如果你自行生成，那么波形文件格式也需要自行用对应仿真器的配置指定。\nPython 示例 正常情况下，dut需要被显式地声明完成任务，以通知进行模拟器的后处理工作（写入波形、覆盖率等文件）。 在Python中，需要在完成所有测试后，调用dut的.finalize()方法以通知模拟器任务已完成，进而将文件flush到磁盘。\n以加法器为例，以下为测试程序：\nfrom UT_Adder import * if __name__ == \"__main__\": dut = DUTAdder() for i in range(10): dut.a.value = i * 2 dut.b.value = int(i / 4) dut.Step(1) print(dut.sum.value, dut.cout.value) dut.finalize() # flush the wave file to disk 运行结束后即可生成指定文件名的波形文件。\n查看结果 GTKWave 使用 GTKWave 打开 fst 或 vcd 波形文件，即可查看波形图。\nVerdi 使用 Verdi 打开 fsdb 或 vcd 波形文件，即可查看波形图。\n","categories":["示例项目","教程"],"description":"生成电路波形","excerpt":"生成电路波形","ref":"/mlvp/docs/env_usage/wave/","tags":["examples","docs"],"title":"波形生成"},{"body":" 在开始前本页会 简单的介绍什么是验证，以及示例里面用到的概念，如 DUT (Design Under Test) 和 RM (Reference Model) 。\n","categories":["示例项目","教程"],"description":"开放验证平台的基础工具的详细使用方法。","excerpt":"开放验证平台的基础工具的详细使用方法。","ref":"/mlvp/docs/env_usage/","tags":["examples","docs"],"title":"基础工具"},{"body":"Bundle 在 toffee 验证环境中，用于构建 Agent 与 DUT 之间交互的中间层，以保证 Agent 与 DUT 之间的解耦。同时 Bundle 也起到了对 DUT 接口层次结构划分的作用，使得对 DUT 接口的访问变得更加清晰、方便。\n一个简单的 Bundle 的定义 为了定义一个 Bundle，需要自定义一个新类，并继承 toffee 中的 Bundle 类。下面是一个简单的 Bundle 的定义示例：\nfrom toffee import Bundle, Signals class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) 该 Bundle 定义了一个简单的加法器接口，在 AdderBundle 类中，我们定义了五个信号 a, b, sum, cin, cout，这五个信号分别代表了加法器的输入端口 a, b，输出端口 sum，以及进位输入端口 cin 和进位输出端口 cout。\n定义完成后，我们可以通过 AdderBundle 类的实例来访问这些信号，例如：\nadder_bundle = AdderBundle() adder_bundle.a.value = 1 adder_bundle.b.value = 2 adder_bundle.cin.value = 0 print(adder_bundle.sum.value) print(adder_bundle.cout.value) 将 DUT 绑定到 Bundle 在上述代码中，我们虽然创建了一个 bundle 实例，并对他进行了驱动，但是我们并没有将这个 bundle 与任何 DUT 绑定，也就意味着对这个 bundle 的操作，无法真正影响到 DUT。\n使用 bind 方法，可以将一个 DUT 绑定到 bundle 上。例如我们有一个简单的加法器 DUT，其接口名称与 Bundle 中定义的名称相同。\nadder = DUTAdder() adder_bundle = AdderBundle() adder_bundle.bind(adder) bind 函数会自动检索 DUT 中所有的接口，并将名称相同的接口进行绑定。绑定完成后，对 bundle 的操作，就会直接影响到 DUT。\n但是，如果 DUT 的接口名称与 Bundle 中定义的名称不同，直接使用 bind 则无法正确绑定。在 Bundle 中，我们提供多种绑定方法，以适应不同的绑定需求。\n通过字典进行绑定 在 bind 函数中，我们可以通过传入一个字典，来指定 DUT 中的接口名称与 Bundle 中的接口名称之间的映射关系。\n假设 Bundle 中的接口名称与 DUT 中的接口名称拥有如下对应关系：\na -\u003e a_in b -\u003e b_in sum -\u003e sum_out cin -\u003e cin_in cout -\u003e cout_out 在实例化 bundle 时，我们可以通过 from_dict 方法创建，并传入一个字典，告知 Bundle 以字典的方式进行绑定。\nadder = DUTAdder() adder_bundle = AdderBundle.from_dict({ 'a': 'a_in', 'b': 'b_in', 'sum': 'sum_out', 'cin': 'cin_in', 'cout': 'cout_out' }) adder_bundle.bind(adder) 此时，adder_bundle 可正确绑定至 adder。\n通过前缀进行绑定 假设 DUT 中的接口名称与 Bundle 中的接口名称拥有如下对应关系：\na -\u003e io_a b -\u003e io_b sum -\u003e io_sum cin -\u003e io_cin cout -\u003e io_cout 可以发现，实际 DUT 的接口名称相比于 Bundle 中的接口名称，都多了一个 io_ 的前缀。在这种情况下，我们可以通过 from_prefix 方法创建 Bundle，并传入前缀名称，告知 Bundle 以前缀的方式进行绑定。\nadder = DUTAdder() adder_bundle = AdderBundle.from_prefix('io_') adder_bundle.bind(adder) 通过正则表达式进行绑定 在某些情况下，DUT 中的接口名称与 Bundle 中的接口名称之间的对应关系并不是简单的前缀或者字典关系，而是更为复杂的规则。例如，DUT 中的接口名称与 Bundle 中的接口名称之间的对应关系为：\na -\u003e io_a_in b -\u003e io_b_in sum -\u003e io_sum_out cin -\u003e io_cin_in cout -\u003e io_cout_out 在这种情况下，我们可以通过传入正则表达式，来告知 Bundle 以正则表达式的方式进行绑定。\nadder = DUTAdder() adder_bundle = AdderBundle.from_regex(r'io_(.*)_.*') adder_bundle.bind(adder) 使用正则表达式时，Bundle 会尝试将 DUT 中的接口名称与正则表达式进行匹配，匹配成功的接口，将会读取正则表达式中的所有捕获组，将其连接为一个字符串。再使用这个字符串与 Bundle 中的接口名称进行匹配。\n例如对于上面代码中的正则表达式，io_a_in 会与正则表达式成功匹配，唯一的捕获组捕获到的内容为 a。a 这个名称与 Bundle 中的接口名称 a 匹配，因此 io_a_in 会被正确绑定至 a。\n创建子 Bundle 很多时候，我们会需要一个 Bundle 包含一个或多个其他 Bundle 的情况，这时我们可以将其他已经定义好的 Bundle 作为当前 Bundle 的子 Bundle。\nfrom toffee import Bundle, Signal, Signals class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) class MultiplierBundle(Bundle): a, b, product = Signals(3) class ArithmeticBundle(Bundle): selector = Signal() adder = AdderBundle.from_prefix('add_') multiplier = MultiplierBundle.from_prefix('mul_') 在上面的代码中，我们定义了一个 ArithmeticBundle，它包含了自己的信号 selector。除此之外它还包含了一个 AdderBundle 和一个 MultiplierBundle，这两个子 Bundle 分别被命名为 adder 和 multiplier。\n当我们需要访问 ArithmeticBundle 中的子 Bundle 时，可以通过 . 运算符来访问：\narithmetic_bundle = ArithmeticBundle() arithmetic_bundle.selector.value = 1 arithmetic_bundle.adder.a.value = 1 arithmetic_bundle.adder.b.value = 2 arithmetic_bundle.multiplier.a.value = 3 arithmetic_bundle.multiplier.b.value = 4 同时，当我们以这种定义方式进行定义后，在最顶层的 Bundle 进行绑定时，会同时将子 Bundle 也绑定到 DUT 上，在定义子 Bundle 时，依然可以使用前文提到的多种绑定方式。\n需要注意的是，子 Bundle 的创建方法去匹配的信号名称，是经过上一次 Bundle 的创建方法进行处理过后的名称。例如在上面的代码中，我们将顶层 Bundle 的匹配方式设置为 from_prefix('io_')，那么在 AdderBundle 中去匹配的信号，是去除了 io_ 前缀后的名称。\n同时，字典匹配方法会将信号名称转换为字典映射后的名称传递给子 Bundle 进行匹配，正则表达式匹配方法会将正则表达式捕获到的名称传递给子 Bundle 进行匹配。\nBundle 中的实用操作 信号访问与赋值 访问信号值\n在 Bundle 中，我们不仅可以通过 . 运算符来访问 Bundle 中的信号，也可以通过 [] 运算符来访问 Bundle 中的信号。\nadder_bundle = AdderBundle() adder_bundle['a'].value = 1 访问未连接信号\ndef bind(self, dut, unconnected_signal_access=True) 在 bind 时，我们可以通过传入 unconnected_signal_access 参数来控制是否允许访问未连接的信号。默认为 True，即允许访问未连接的信号，此时当写入该信号时，该信号不会发生变化，当读取该信号时，会返回 None。 当 unconnected_signal_access 为 False 时，访问未连接的信号会抛出异常。\n同时赋值所有信号\n可以通过 set_all 方法同时将所有输入信号更改为某个值。\nadder_bundle.set_all(0) 随机赋值所有信号\n可以通过 randomize_all 方法随机赋值所有信号。“value_range” 参数用于指定随机值的范围，“exclude_signals” 参数用于指定不需要随机赋值的信号，“random_func” 参数用于指定随机函数。\nadder_bundle.randomize_all() 信号赋值模式更改\n信号赋值模式是 picker 中的概念，用于控制信号的赋值方式，请查阅 picker 文档以了解更多信息。\nBundle 中支持通过 set_write_mode 来改变整个 Bundle 的赋值模式。\n同时，Bundle 提供了设置的快捷方法：set_write_mode_as_imme, set_write_mode_as_rise 与 set_write_mode_as_fall，分别用于设置 Bundle 的赋值模式为立即赋值、上升沿赋值与下降沿赋值。\n消息支持 默认消息类型赋值\ntoffee 支持一个默认的消息类型，可以通过 assign 方法将一个字典赋值给 Bundle 中的信号。\nadder_bundle.assign({ 'a': 1, 'b': 2, 'cin': 0 }) Bundle 将会自动将字典中的值赋值给对应的信号，当需要将未指定的信号赋值成某个默认值时，可以通过 * 来指定默认值：\nadder_bundle.assign({ '*': 0, 'a': 1, }) 子 Bundle 的默认消息赋值支持\n如果希望通过默认消息类型同时赋值子 Bundle 中的信号，可以通过两种方式实现。当 assign 中的 multilevel 参数为 True 时，Bundle 支持多级字典赋值。\narithmetic_bundle.assign({ 'selector': 1, 'adder': { '*': 0, 'cin': 0 }, 'multiplier': { 'a': 3, 'b': 4 } }, multilevel=True) 当 multilevel 为 False 时，Bundle 支持通过 . 来指定子 Bundle 的赋值。\narithmetic_bundle.assign({ '*': 0, 'selector': 1, 'adder.cin': 0, 'multiplier.a': 3, 'multiplier.b': 4 }, multilevel=False) 默认消息类型读取\n在 Bundle 中可以使用，as_dict 方法将 Bundle 当前的信号值转换为字典。其同样支持两种格式，当 multilevel 为 True 时，返回多级字典；当 multilevel 为 False 时，返回扁平化的字典。\n\u003e arithmetic_bundle.as_dict(multilevel=True) { 'selector': 1, 'adder': { 'a': 0, 'b': 0, 'sum': 0, 'cin': 0, 'cout': 0 }, 'multiplier': { 'a': 0, 'b': 0, 'product': 0 } } \u003e arithmetic_bundle.as_dict(multilevel=False) { 'selector': 1, 'adder.a': 0, 'adder.b': 0, 'adder.sum': 0, 'adder.cin': 0, 'adder.cout': 0, 'multiplier.a': 0, 'multiplier.b': 0, 'multiplier.product': 0 } 自定义消息类型\n在我们自定义的消息结构中，可以执行规则将其赋值给 Bundle 中的信号。\n一种方法是，在自定义消息结构中，实现 as_dict 函数，将自定义消息结构转换为字典，然后通过 assign 方法赋值给 Bundle。\n另一种方法是，在自定义消息结构中，实现 __bundle_assign__ 函数，其接收一个 Bundle 实例，将自定义消息结构赋值给 Bundle。实现后，可以通过 assign 方法赋值给 Bundle，Bundle 将会自动调用 __bundle_assign__ 函数进行赋值。\nclass MyMessage: def __init__(self): self.a = 0 self.b = 0 self.cin = 0 def __bundle_assign__(self, bundle): bundle.a.value = self.a bundle.b.value = self.b bundle.cin.value = self.cin my_message = MyMessage() adder_bundle.assign(my_message) 当需要将 Bundle 中的信号值转换为自定义消息结构时，简易在自定义消息结构中实现 from_bundle 的类方法，接收一个 Bundle 实例，返回一个自定义消息结构。在创建自定义消息结构时，可以通过 from_bundle 方法将 Bundle 中的信号值转换为自定义消息结构。\nclass MyMessage: def __init__(self): self.a = 0 self.b = 0 self.cin = 0 @classmethod def from_bundle(cls, bundle): message = cls() message.a = bundle.a.value message.b = bundle.b.value message.cin = bundle.cin.value return message my_message = MyMessage.from_bundle(adder_bundle) 时序封装 Bundle 类除了对 DUT 的引脚进行封装外，还提供了基于数组的时序封装，可以适用于部分简单时序场景。Bundle 类提供了process_requests(data_list)函数，他接受一个数组输入，第i个时钟周期，会将data_list[i]对应的数据赋值给引脚。data_list中的数据可以是dict类型，或者callable(cycle, bundle_ins)类型的可调用对象。对于dict类型，特殊key有：\n__funcs__: func(cycle, self) # 可调用对象，可以为函数数组[f1,f2,..] __condition_func__: func(cycle, slef, cargs) # 条件函数，当改函数返回为true时，进行赋值，否则继续推进时钟 __condition_args__: cargs # 条件函数需要的参数 __return_bundles__: bundle # 需要本次dict赋值时返回的bundle数据，可以是list[bundle] 如果输入的dict中有__return_bundles__，则函数会返回该输入对应的 bundle 值，例如{\"data\": x, \"cycle\": y}。以 Adder 为例，期望第三次加后返回结果：\n# Adder虽然为存组合逻辑，但此处当时序逻辑使用 class AdderBundle(Bundle): a, b, sum, cin, cout = Signals(5) # 指定引脚 def __init__(self, dut): super().__init__() # init clock # dut.InitClock(\"clock\") self.bind(dut) # 绑定到dut def add_list(data_list =[(1,2),(3,4),(5,6),(7,8)]): # make input dit data = [] for i, (a, b) in enumerate(data_list): x = {\"a\":a, \"b\":b, \"*\":0} # 构建budle赋值的dict if i \u003e= 2: x[\"__return_bundles__\"] = self # 设置需要返回的bundle data.append(X) return self.process_requests(data) # 推动时钟，赋值，返回结果 当调用add_list()后，返回的结果为:\n[ {\"data\": {\"a\":5, \"b\":6, \"cin\": 0, \"sum\":11, \"cout\": 0}, \"cycle\":3}, {\"data\": {\"a\":7, \"b\":8, \"cin\": 0, \"sum\":15, \"cout\": 0}, \"cycle\":4} ] 异步支持 在 Bundle 中，为了方便的接收时钟信息，提供了 step 函数。当 Bundle 连接至 DUT 的任意一个信号时，step 函数会自动同步至 DUT 的时钟信号。\n可以通过 step 函数来完成时钟周期的等待。\nasync def adder_process(adder_bundle): adder_bundle.a.value = 1 adder_bundle.b.value = 2 adder_bundle.cin.value = 0 await adder_bundle.step() print(adder_bundle.sum.value) print(adder_bundle.cout.value) 信号连接 信号连接规则\n当定义好 Bundle 实例后，可以调用 all_signals_rule 方法，获取所有信号的连接规则，以帮助用户检查信号的连接规则是否符合预期。\nadder_bundle.all_signals_rule() 信号可连接性检查\ndetect_connectivity 方法可以检查一个特定的信号名称是否可以连接到该 Bundle 中的某个信号。\nadder_bundle.detect_connectivity('io_a') detect_specific_connectivity 方法可以检查一个特定的信号名称是否可以连接到该 Bundle 中的某个特定的信号。\nadder_bundle.detect_specific_connectivity('io_a', 'a') 如果需要检测子 Bundle 的信号连接性，可以通过 . 运算符来指定。\nDUT 信号连接检查 未连接信号检查\ndetect_unconnected_signals 方法可以检查 DUT 中未连接到任何 Bundle 的信号。\nBundle.detect_unconnected_signals(adder) 重复连接检查\ndetect_multiple_connections 方法可以检查 DUT 中同时连接到多个 Bundle 的信号。\nBundle.detect_multiple_connections(adder) 其他实用操作 设置 Bundle 名称\n可以通过 set_name 方法设置 Bundle 的名称。\nadder_bundle.set_name('adder') 设置名称之后，将会得到更加直观的提示信息。\n获取 Bundle 中所有信号\nall_signals 信号会返回一个 generator，其中包含了包括子 Bundle 信号在内的所有信号。\nfor signal in adder_bundle.all_signals(): print(signal) Bundle 的自动生成脚本 在很多情况下，DUT 的接口可能过于复杂，手动去编写 Bundle 的定义会变得非常繁琐。然而，Bundle 作为中间层，提供一个确切的信号名称定义是必要的。为了解决这个问题，toffee 提供了一个自动生成 Bundle 的脚本来从 DUT 的接口定义中生成 Bundle 的定义。\n可以在 toffee 仓库目录下的 scripts 文件夹中找到 bundle_code_gen.py 脚本。该脚本可以通过解析 DUT 实例，以及指定的绑定规则自动生成 Bundle 的定义。\n其中提供了三个函数\ndef gen_bundle_code_from_dict(bundle_name: str, dut, dict: dict, max_width: int = 120) def gen_bundle_code_from_prefix(bundle_name: str, dut, prefix: str = \"\", max_width: int = 120): def gen_bundle_code_from_regex(bundle_name: str, dut, regex: str, max_width: int = 120): 分别用于通过字典、前缀、正则表达式的方式生成 Bundle 的定义。\n使用时，指定 Bundle 的名称，DUT 实例，以及对应的生成规则便可生成 Bundle 的定义，还可以通过 max_width 参数来指定生成的代码的最大宽度。\nfrom bundle_code_gen import * gen_bundle_code_from_dict('AdderBundle', dut, { 'a': 'io_a', 'b': 'io_b', 'sum': 'io_sum', 'cin': 'io_cin', 'cout': 'io_cout' }) gen_bundle_code_from_prefix('AdderBundle', dut, 'io_') gen_bundle_code_from_regex('AdderBundle', dut, r'io_(.*)') 生成好的代码可以直接或经过简单的修改后，复制到代码中使用。也可以作为子 Bundle 的定义，应用到其他 Bundle 中。\n","categories":"","description":"","excerpt":"Bundle 在 toffee 验证环境中，用于构建 Agent 与 DUT 之间交互的中间层，以保证 Agent 与 DUT 之间的解耦。 …","ref":"/mlvp/docs/mlvp/env/bundle/","tags":"","title":"如何使用 Bundle"},{"body":"编写测试用例 在 toffee 中，测试用例是通过 pytest 来管理的。pytest 是一个功能强大的 Python 测试框架，如果你不熟悉 pytest，可以查看 pytest 官方文档。\n编写第一个测试用例 首先，我们需要创建一个测试用例文件，例如 test_adder.py，该文件需要以 test_ 开头，或以 _test.py 结尾，以便 pytest 能够识别。接着可以在其中编写我们的第一个测试用例。\n# test_adder.py async def my_test(): env = AdderEnv() env.add_agent.exec_add(1, 2, 0) def test_adder(): toffee.run(my_test()) pytest 并不能直接运行协程测试用例，因此我们需要在测试用例中调用 toffee.run 来运行异步测试用例。\n用例编写完成后，我们可以在终端中运行 pytest。\npytest pytest 会查找当前目录下所有以 test_ 开头或以 _test.py 结尾的文件，并运行其中以 test_ 开头的函数，每一个函数被视作一个测试用例。\n运行协程测试用例 为了使 pytest 能够直接运行协程测试用例，toffee 提供了 toffee_async 标记来标记异步测试用例。\n# test_adder.py @pytest.mark.toffee_async async def test_adder(): env = AdderEnv(DUTAdder()) await env.add_agent.exec_add(1, 2, 0) 如图所示，我们只需要在测试用例函数上添加 @pytest.mark.toffee_async 标记，pytest 就能够直接运行协程测试用例。\n生成测试报告 在运行 pytest 时，toffee 会自动收集测试用例的执行结果，自动统计覆盖率信息，并生成一个验证报告，想要生成该报告，需要在调用 pytest 时添加 --toffee-report 参数。\npytest --toffee-report 默认情况下，toffee 将会为每次运行生成一个默认报告名称，并将报告放至 reports 目录下。可以通过 --report-dir 参数来指定报告的存放目录，通过 --report-name 参数来指定报告的名称。\n但此时，由于 toffee 无法得知覆盖率文件名称，因此在报告中无法显示覆盖率信息，如果想要在报告中显示覆盖率信息，需要在每个测试用例中传入功能覆盖组及行覆盖率文件的名称。\n@pytest.mark.toffee_async async def test_adder(request): adder = DUTAdder( waveform_filename=\"adder.fst\", coverage_filename=\"adder.dat\" ) g = CovGroup(\"Adder\") env = AdderEnv(adder) await env.add_agent.exec_add(1, 2, 0) adder.Finish() set_func_coverage(request, cov_groups) set_line_coverage(request, \"adder.dat\") 上述代码中，在创建 DUT 时，我们传入了波形文件和覆盖率文件的名称，使得 DUT 在运行时可以生成指定名称的覆盖率文件。接着我们定义了一个覆盖组，来收集 DUT 的功能覆盖率信息，具体如何使用将在下个文档中介绍。\n接着，调用了 DUT 的 Finish 方法，用于结束波形文件的记录。最终我们通过 set_func_coverage 和 set_line_coverage 函数来设置功能覆盖组及行覆盖率文件信息。\n此时再次运行 pytest 时，toffee 将会自动收集覆盖率信息，并在报告中显示。\n使用 toffee-test 管理资源 然而，上述过程过于繁琐，并且为了保证每个测试用例之间文件名称不产生冲突，我们需要在每个测试用例中传入不一样的文件名称。并且在测试用例出现异常时，测试用例并不会运行完毕，导致覆盖率文件无法生成。\n因此，toffee-test 提供了 toffee_request Fixture 来管理资源，简化了测试用例的编写。\n# test_adder.py @pytest.mark.toffee_async async def test_adder(my_request): dut = my_request env = AdderEnv(dut) await env.add_agent.exec_add(1, 2, 0) @pytest.fixture() def my_request(toffee_request: ToffeeRequest): toffee_request.add_cov_groups(CovGroup(\"Adder\")) return toffee_request.create_dut(DUTAdder) Fixture 是 pytest 中的概念，例如上述代码中定义了一个名为 my_request 的 Fixture。如果在其他测试用例的输出参数中含有 my_request 参数，pytest 将会自动调用 my_request Fixture，并将其返回值传入测试用例。\n上述代码中自定义了一个 Fixture my_request，并在测试用例中进行使用，这也就意味着资源的管理工作都将会在 Fixture 中完成，测试用例只需要关注测试逻辑即可。my_request 必须使用 toffee-test 提供的 toffee_request Fixture 作为参数，以便进行资源管理，toffee_request 提供了一系列的方法来管理资源。\n通过 add_cov_groups 添加覆盖组，toffee-test 会自动将其生成至报告中。 通过 create_dut 创建 DUT 实例，toffee-test 会自动管理 DUT 的波形文件和覆盖率文件的生成，并确保文件名称不产生冲突。\n在 my_request 中，可以自定义返回值传入测试用例中。如果想要任意测试用例都可以访问到该 Fixture，可以将 Fixture 定义在 conftest.py 文件中。\n至此，我们实现了测试用例资源管理和逻辑编写的分离，无需在每个测试用例中手动管理资源的创建与释放。\n","categories":"","description":"","excerpt":"编写测试用例 在 toffee 中，测试用例是通过 pytest 来管理的。pytest 是一个功能强大的 Python 测试框架，如果你不 …","ref":"/mlvp/docs/mlvp/cases/pytest/","tags":"","title":"如何使用 Pytest 管理测试用例"},{"body":" 本页将介绍数字电路的基础知识。数字电路是利用数字信号的电子电路。近年来，绝大多数的计算机都是基于数字电路实现的。\n什么是数字电路 数字电路是一种利用两种不连续的电位来表示信息的电子电路。在数字电路中，通常使用两个电源电压，分别表示高电平（H）和低电平（L），分别代表数字1和0。这样的表示方式通过离散的电信号，以二进制形式传递和处理信息。\n大多数数字电路的实现基于场效应管，其中最常用的是 MOSFET（Metal-Oxide-Semiconductor Field-Effect Transistor，金属氧化物半导体场效应管）。MOSFET 是一种半导体器件，可以在电场的控制下调控电流流动，从而实现数字信号的处理。\n在数字电路中，MOSFET 被组合成各种逻辑电路，如与门、或门、非门等。这些逻辑门通过不同的组合方式，构建了数字电路中的各种功能和操作。以下是一些数字电路的基本特征：\n(1) 电位表示信息： 数字电路使用两种电位，即高电平和低电平，来表示数字信息。通常，高电平代表数字1，低电平代表数字0。\n(2) MOSFET 实现： MOSFET 是数字电路中最常用的元件之一。通过控制 MOSFET 的导通和截止状态，可以实现数字信号的处理和逻辑运算。\n(3) 逻辑门的组合： 逻辑门是数字电路的基本构建块，由 MOSFET 组成。通过组合不同的逻辑门，可以构建复杂的数字电路，实现各种逻辑功能。\n(4) 二进制表达： 数字电路中的信息通常使用二进制系统进行表示。每个数字都可以由一串二进制位组成，这些位可以在数字电路中被处理和操作。\n(5) 电平转换和信号处理： 数字电路通过电平的变化和逻辑操作，实现信号的转换和处理。这种离散的处理方式使得数字电路非常适用于计算和信息处理任务。\n为什么要学习数字电路 学习数字电路是芯片验证过程中的基础和必要前提，主要体现在以下多个方面：\n(1) 理解设计原理： 数字电路是芯片设计的基础，了解数字电路的基本原理和设计方法是理解芯片结构和功能的关键。芯片验证的目的是确保设计的数字电路在实际硬件中按照规格正常工作，而理解数字电路原理是理解设计的关键。\n(2) 设计规范： 芯片验证通常涉及验证设计是否符合特定的规范和功能要求。学习数字电路可以帮助理解这些规范，从而更好地构建测试用例和验证流程，确保验证的全面性和准确性。\n(3) 时序和时钟： 时序问题是数字电路设计和验证中的常见挑战。学习数字电路可以帮助理解时序和时钟的概念，以确保验证过程中能够正确处理时序问题，避免电路中的时序迟滞和冲突。\n(4) 逻辑分析： 芯片验证通常涉及对逻辑的分析，确保电路的逻辑正确性。学习数字电路可以培养对逻辑的深刻理解，从而更好地进行逻辑分析和故障排查。\n(5) 测试用例编写： 在芯片验证中，需要编写各种测试用例来确保设计的正确性。对数字电路的理解可以帮助设计更全面、有针对性的测试用例，涵盖电路的各个方面。\n(6) 信号完整性： 学习数字电路有助于理解信号在电路中的传播和完整性问题。在芯片验证中，确保信号在不同条件下的正常传递是至关重要的，特别是在高速设计中。\n整体而言，学习数字电路为芯片验证提供了基础知识和工具，使验证工程师能够更好地理解设计，编写有效的测试用例，分析验证结果，并解决可能出现的问题。数字电路的理论和实践经验对于芯片验证工程师来说都是不可或缺的。\n数字电路基础知识 可以通过以下在线资源进行数字电路学习：\n清华大学数字电路基础 中科大数字电路实验 数字设计和计算机体系结构 MIT 数字集成电路分析与设计 硬件描述语言Chisel 传统描述语言 硬件描述语言（Hardware Description Language，简称 HDL）是一种用于描述数字电路、系统和硬件的语言。它允许工程师通过编写文本文件来描述硬件的结构、功能和行为，从而实现对硬件设计的抽象和建模。\nHDL 通常被用于设计和仿真数字电路，如处理器、存储器、控制器等。它提供了一种形式化的方法来描述硬件电路的行为和结构，使得设计工程师可以更方便地进行硬件设计、验证和仿真。\n常见的硬件描述语言包括：\nVerilog：Verilog 是最常用的 HDL 之一，它是一种基于事件驱动的硬件描述语言，广泛应用于数字电路设计、验证和仿真。 VHDL：VHDL 是另一种常用的 HDL，它是一种面向对象的硬件描述语言，提供了更丰富的抽象和模块化的设计方法。 SystemVerilog：SystemVerilog 是 Verilog 的扩展，它引入了一些高级特性，如对象导向编程、随机化测试等，使得 Verilog 更适用于复杂系统的设计和验证。 Chisel Chisel 是一种现代化高级的硬件描述语言，与传统的 Verilog 和 VHDL 不同，它是基于 Scala 编程语言的硬件构建语言。Chisel 提供了一种更加现代化和灵活的方法来描述硬件，通过利用 Scala 的特性，可以轻松地实现参数化、抽象化和复用，同时保持硬件级别的效率和性能。\nChisel 的特点包括：\n现代化的语法：Chisel 的语法更加接近软件编程语言，如 Scala，使得硬件描述更加直观和简洁。 参数化和抽象化：Chisel 支持参数化和抽象化，可以轻松地创建可配置和可重用的硬件模块。 类型安全：Chisel 是基于 Scala 的，因此具有类型安全的特性，可以在编译时检测到许多错误。 生成性能优化的硬件：Chisel 代码可以被转换成 Verilog，然后由标准的 EDA 工具链进行综合、布局布线和仿真，生成性能优化的硬件。 强大的仿真支持：Chisel 提供了与 ScalaTest 和 Firrtl 集成的仿真支持，使得对硬件进行仿真和验证更加方便和灵活。 Chisel版的全加法器实例 电路设计如下图所示：\n完整的Chisel代码如下：\npackage examples import chisel3._ class FullAdder extends Module { // Define IO ports val io = IO(new Bundle { val a = Input(UInt(1.W)) // Input port 'a' of width 1 bit val b = Input(UInt(1.W)) // Input port 'b' of width 1 bit val cin = Input(UInt(1.W)) // Input port 'cin' (carry-in) of width 1 bit val sum = Output(UInt(1.W)) // Output port 'sum' of width 1 bit val cout = Output(UInt(1.W))// Output port 'cout' (carry-out) of width 1 bit }) // Calculate sum bit (sum of a, b, and cin) val s1 = io.a ^ io.b // XOR operation between 'a' and 'b' io.sum := s1 ^ io.cin // XOR operation between 's1' and 'cin', result assigned to 'sum' // Calculate carry-out bit val s3 = io.a \u0026 io.b // AND operation between 'a' and 'b', result assigned to 's3' val s2 = s1 \u0026 io.cin // AND operation between 's1' and 'cin', result assigned to 's2' io.cout := s2 | s3 // OR operation between 's2' and 's3', result assigned to 'cout' } Chisel 学习材料可以参考官方文档：https://www.chisel-lang.org/docs\n","categories":"","description":"关于数字电路的基本概念\n","excerpt":"关于数字电路的基本概念\n","ref":"/mlvp/docs/basic/ic_base/","tags":"","title":"数字电路"},{"body":" 本页将展示使用多种语言验证的各种案例。\n","categories":["教程"],"description":"多语言案例介绍","excerpt":"多语言案例介绍","ref":"/mlvp/docs/multi-lang/examples/","tags":["docs"],"title":"验证案例"},{"body":" 介绍芯片验证，以果壳 Cache 为例，介绍基本的验证流程、报告撰写。\n","categories":["示例项目","教程"],"description":"介绍开放验证平台工作所需要的基础知识。","excerpt":"介绍开放验证平台工作所需要的基础知识。","ref":"/mlvp/docs/basic/","tags":["examples","docs"],"title":"验证基础"},{"body":"RTL Source Code In this case, we drive a 64-bit adder (combinational circuit) with the following source code:\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-1:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule This adder contains a 64-bit adder with inputs of two 64-bit numbers and a carry-in signal, outputting a 64-bit sum and a carry-out signal.\nTesting Process During the testing process, we will create a folder named Adder, containing a file called Adder.v. This file contains the above RTL source code.\nExporting RTL to Python Module Generating Intermediate Files Navigate to the Adder folder and execute the following command:\npicker export --autobuild=false Adder.v -w Adder.fst --sname Adder --tdir picker_out_adder --lang python -e --sim verilator This command performs the following actions:\nUses Adder.v as the top file, with Adder as the top module, and generates a dynamic library using the Verilator simulator with Python as the target language.\nEnables waveform output, with the target waveform file as Adder.fst.\nIncludes files for driving the example project (-e), and does not automatically compile after code generation (-autobuild=false).\nThe final file output path is picker_out_adder.\nSome command-line parameters were not used in this command, and they will be introduced in later sections. The output directory structure is as follows. Note that these are all intermediate files and cannot be used directly:\npicker_out_adder |-- Adder.v # Original RTL source code |-- Adder_top.sv # Generated Adder_top top-level wrapper, using DPI to drive Adder module inputs and outputs |-- Adder_top.v # Generated Adder_top top-level wrapper in Verilog, needed because Verdi does not support importing SV source code |-- CMakeLists.txt # For invoking the simulator to compile the basic C++ class and package it into a bare DPI function binary dynamic library (libDPIAdder.so) |-- Makefile # Generated Makefile for invoking CMakeLists.txt, allowing users to compile libAdder.so through the make command, with manual adjustment of Makefile configuration parameters, or to compile the example project |-- cmake # Generated cmake folder for invoking different simulators to compile RTL code | |-- vcs.cmake | `-- verilator.cmake |-- cpp # CPP example directory containing sample code | |-- CMakeLists.txt # For wrapping libDPIAdder.so using basic data types into a directly operable class (libUTAdder.so), not just bare DPI functions | |-- Makefile | |-- cmake | | |-- vcs.cmake | | `-- verilator.cmake | |-- dut.cpp # Generated CPP UT wrapper, including calls to libDPIAdder.so, and UTAdder class declaration and implementation | |-- dut.hpp # Header file | `-- example.cpp # Sample code calling UTAdder class |-- dut_base.cpp # Base class for invoking and driving simulation results from different simulators, encapsulated into a unified class to hide all simulator-related code details |-- dut_base.hpp |-- filelist.f # Additional file list for multi-file projects, check the -f parameter introduction. Empty in this case |-- mk | |-- cpp.mk # Controls Makefile when targeting C++ language, including logic for compiling example projects (-e, example) | `-- python.mk # Same as above, but with Python as the target language `-- python |-- CMakeLists.txt |-- Makefile |-- cmake | |-- vcs.cmake | `-- verilator.cmake |-- dut.i # SWIG configuration file for exporting libDPIAdder.so’s base class and function declarations to Python, enabling Python calls `-- dut.py # Generated Python UT wrapper, including calls to libDPIAdder.so, and UTAdder class declaration and implementation, equivalent to libUTAdder.so Building Intermediate Files Navigate to the picker_out_adder directory and execute the make command to generate the final files.\nUse the simulator invocation script defined by cmake/*.cmake to compile Adder_top.sv and related files into the libDPIAdder.so dynamic library.Use the compilation script defined by CMakeLists.txt to wrap libDPIAdder.so into the libUTAdder.so dynamic library through dut_base.cpp. Both outputs from steps 1 and 2 are copied to the UT_Adder directory.Generate the wrapper layer using the SWIG tool with dut_base.hpp and dut.hpp header files, and finally build a Python module in the UT_Adder directory.If the -e parameter is included, the pre-defined example.py is placed in the parent directory of the UT_Adder directory as a sample code for calling this Python module. The final directory structure is:\n. |-- Adder.fst # Waveform file for testing |-- UT_Adder | |-- Adder.fst.hier | |-- _UT_Adder.so # Wrapper dynamic library generated by SWIG | |-- __init__.py # Python module initialization file, also the library definition file | |-- libDPIAdder.a # Library file generated by the simulator | |-- libUTAdder.so # DPI dynamic library wrapper generated based on dut_base | `-- libUT_Adder.py # Python module generated by SWIG | `-- xspcomm # Base library folder, no need to pay attention to this `-- example.py # Sample code Setting Up Test Code Replace the content in example.py with the following Python test code.\nfrom UT_Adder import * import random # Generate unsigned random numbers def random_int(): return random.randint(-(2**63), 2**63 - 1) \u0026 ((1 \u003c\u003c 63) - 1) # Reference model for the adder implemented in Python def reference_adder(a, b, cin): sum = (a + b) \u0026 ((1 \u003c\u003c 64) - 1) carry = sum \u003c a sum += cin carry = carry or sum \u003c cin return sum, 1 if carry else 0 def random_test(): # Create DUT dut = DUTAdder() # By default, pin assignments do not write immediately but write on the next clock rising edge, which is suitable for sequential circuits. However, since the Adder is a combinational circuit, we need to write immediately # Therefore, the AsImmWrite() method is called to change pin assignment behavior dut.a.AsImmWrite() dut.b.AsImmWrite() dut.cin.AsImmWrite() # Loop test for i in range 114514): a, b, cin = random_int(), random_int(), random_int() \u0026 1 # DUT: Assign values to Adder circuit pins, then drive the combinational circuit (for sequential circuits or waveform viewing, use dut.Step() to drive) dut.a.value, dut.b.value, dut.cin.value = a, b, cin dut.RefreshComb() # Reference model: Calculate results ref_sum, ref_cout = reference_adder(a, b, cin) # Check results assert dut.sum.value == ref_sum, \"sum mismatch: 0x{dut.sum.value:x} != 0x{ref_sum:x}\" assert dut.cout.value == ref_cout, \"cout mismatch: 0x{dut.cout.value:x} != 0x{ref_cout:x}\" print(f\"[test {i}] a=0x{a:x}, b=0x{b:x}, cin=0x{cin:x} =\u003e sum: 0x{ref_sum}, cout: 0x{ref_cout}\") # Test complete dut.Finish() print(\"Test Passed\") if __name__ == \"__main__\": random_test() Running the Test In the picker_out_adder directory, execute the python3 example.py command to run the test. After the test is complete, we can see the output of the example project.\n[...] [test 114507] a=0x7adc43f36682cffe, b=0x30a718d8cf3cc3b1, cin=0x0 =\u003e sum: 0x12358823834579604399, cout: 0x0 [test 114508] a=0x3eb778d6097e3a72, b=0x1ce6af17b4e9128, cin=0x0 =\u003e sum: 0x4649372636395916186, cout: 0x0 [test 114509] a=0x42d6f3290b18d4e9, b=0x23e4926ef419b4aa, cin=0x1 =\u003e sum: 0x7402657300381600148, cout: 0x0 [test 114510] a=0x505046adecabcc, b=0x6d1d4998ed457b06, cin=0x0 =\u003e sum: 0x7885127708256118482, cout: 0x0 [test 114511] a=0x16bb10f22bd0af50, b=0x5813373e1759387, cin=0x1 =\u003e sum: 0x2034576336764682968, cout: 0x0 [test 114512] a=0xc46c9f4aa798106, b=0x4d8f52637f0417c4, cin=0x0 =\u003e sum: 0x6473392679370463434, cout: 0x0 [test 114513] a=0x3b5387ba95a7ac39, b=0x1a378f2d11b38412, cin=0x0 =\u003e sum: 0x6164045699187683403, cout: 0x0 Test Passed ","categories":["Example Projects","Tutorials"],"description":"Demonstrates the principles and usage of the tool based on a simple adder verification. This adder is implemented using simple combinational logic.","excerpt":"Demonstrates the principles and usage of the tool based on a simple …","ref":"/mlvp/en/docs/quick-start/eg-adder/","tags":["examples","docs"],"title":"Case 1: Adder"},{"body":"RTL源码 在本案例中，我们驱动一个 64 位的加法器（组合电路），其源码如下：\n// A verilog 64-bit full adder with carry in and carry out module Adder #( parameter WIDTH = 64 ) ( input [WIDTH-1:0] a, input [WIDTH-1:0] b, input cin, output [WIDTH-1:0] sum, output cout ); assign {cout, sum} = a + b + cin; endmodule 该加法器包含一个 64 位的加法器，其输入为两个 64 位的数和一个进位信号，输出为一个 64 位的和和一个进位信号。\n测试过程 在测试过程中，我们将创建一个名为 Adder 的文件夹，其中包含一个 Adder.v 文件。该文件内容即为上述的 RTL 源码。\n将RTL导出为 Python Module 生成中间文件 进入 Adder 文件夹，执行如下命令：\npicker export --autobuild=false Adder.v -w Adder.fst --sname Adder --tdir picker_out_adder/ --lang python -e --sim verilator *注：–tdir 指定的是目标构建目录，如果该参数值为空或者以“/”结尾，picker则会自动以DUT的目标模块名创建构建目录。例如 --tdir picker_out_adder指定了当前目录下的picker_out_adder为构建目录，而参数--tdir picker_out_adder/则指定picker在当前目录picker_out_adder中创建Adder目录作为目标构建目录。\n该命令的含义是：\n将 Adder.v 作为 Top 文件，并将 Adder 作为 Top Module，基于 verilator 仿真器生成动态库，生成目标语言为 Python。 启用波形输出，目标波形文件为Adder.fst。 包含用于驱动示例项目的文件(-e)，同时codegen完成后不自动编译(-autobuild=false)。 最终的文件输出路径是 picker_out_adder 在使用该命令时，还有部分命令行参数没有使用，这些命令将在后续的章节中介绍。\n输出的目录结构如下，请注意这部分均为中间文件，不能直接使用：\npicker_out_adder/ └── Adder |-- Adder.v # 原始的RTL源码 |-- Adder_top.sv # 生成的Adder_top顶层封装，使用DPI驱动Adder模块的inputs和outputs |-- Adder_top.v # 生成的Adder_top顶层封装，因为Verdi不支持导入SV源码使用，因此需要生成一个Verilog版本 |-- CMakeLists.txt # 用于调用仿真器编译基本的cpp class并将其打包成有裸DPI函数二进制动态库(libDPIAdder.so) |-- Makefile # 生成的Makefile，用于调用CMakeLists.txt，并让用户可以通过make命令编译出libAdder.so，并手动调整Makefile的配置参数。或者编译示例项目 |-- cmake # 生成的cmake文件夹，用于调用不同仿真器编译RTL代码 | |-- vcs.cmake | `-- verilator.cmake |-- cpp # CPP example目录，包含示例代码 | |-- CMakeLists.txt # 用于将libDPIAdder.so使用基础数据类型封装为一个可直接操作的类（libUTAdder.so），而非裸DPI函数。 | |-- Makefile | |-- cmake | | |-- vcs.cmake | | `-- verilator.cmake | |-- dut.cpp # 生成的cpp UT封装，包含了对libDPIAdder.so的调用，及UTAdder类的声明及实现 | |-- dut.hpp # 头文件 | `-- example.cpp # 调用UTAdder类的示例代码 |-- dut_base.cpp # 用于调用与驱动不同仿真器编译结果的基类，通过继承封装为统一的类，用于隐藏所有仿真器相关的代码细节。 |-- dut_base.hpp |-- filelist.f # 多文件项目使用的其他文件列表，请查看 -f 参数的介绍。本案例中为空 |-- mk | |-- cpp.mk # 用于控制以cpp为目标语言时的Makefile，包含控制编译示例项目（-e，example）的逻辑 | `-- python.mk # 同上，目标语言是python `-- python |-- CMakeLists.txt |-- Makefile |-- cmake | |-- vcs.cmake | `-- verilator.cmake |-- dut.i # SWIG配置文件，用于将libDPIAdder.so的基类与函数声明，依据规则用swig导出到python，提供python调用的能力 `-- dut.py # 生成的python UT封装，包含了对libDPIAdder.so的调用，及UTAdder类的声明及实现，等价于 libUTAdder.so 构建中间文件 进入 picker_out_adder/Adder 目录并执行 make 命令，即可生成最终的文件。\n由 Makefile 定义的自动编译过程流如下：\n通过 cmake/*.cmake 定义的仿真器调用脚本，编译 Adder_top.sv 及相关文件为 libDPIAdder.so 动态库。 通过 CMakelists.txt 定义的编译脚本，将 libDPIAdder.so 通过 dut_base.cpp 封装为 libUTAdder.so 动态库。并将1、2步产物拷贝到 UT_Adder 目录下。 通过 dut_base.hpp 及 dut.hpp 等头文件，利用 SWIG 工具生成封装层，并最终在 UT_Adder 这一目录中构建一个 Python Module。 如果有 -e 参数，则将预先定义好的 example.py 置于 UT_Adder 目录的上级目录，作为如何调用该 Python Module 的示例代码。 最终目录结果为：\npicker_out_adder/ └── Adder |-- _UT_Adder.so # Swig生成的wrapper动态库 |-- __init__.py # Python Module的初始化文件，也是库的定义文件 |-- libDPIAdder.a # 仿真器生成的库文件 |-- libUTAdder.so # 基于dut_base生成的libDPI动态库封装 |-- libUT_Adder.py # Swig生成的Python Module `-- xspcomm # xspcomm基础库，固定文件夹，不需要关注 配置测试代码 在picker_out_adder中添加 example.py：\nfrom Adder import * import random # 生成无符号随机数 def random_int(): return random.randint(-(2**63), 2**63 - 1) \u0026 ((1 \u003c\u003c 63) - 1) # 通过python实现的加法器参考模型 def referce_adder(a, b, cin): sum = (a + b) \u0026 ((1 \u003c\u003c 64) - 1) carry = sum \u003c a sum += cin carry = carry or sum \u003c cin return sum, 1 if carry else 0 def random_test(): # 创建DUT dut = DUTAdder() # 默认情况下，引脚赋值不会立马写入，而是在下一次时钟上升沿写入，这对于时序电路适用，但是Adder为组合电路，所以需要立即写入 # 因此需要调用AsImmWrite()方法更改引脚赋值行为 dut.a.AsImmWrite() dut.b.AsImmWrite() dut.cin.AsImmWrite() # 循环测试 for i in range(114514): a, b, cin = random_int(), random_int(), random_int() \u0026 1 # DUT：对Adder电路引脚赋值，然后驱动组合电路 （对于时序电路，或者需要查看波形，可通过dut.Step()进行驱动） dut.a.value, dut.b.value, dut.cin.value = a, b, cin dut.RefreshComb() # 参考模型：计算结果 ref_sum, ref_cout = referce_adder(a, b, cin) # 检查结果 assert dut.sum.value == ref_sum, \"sum mismatch: 0x{dut.sum.value:x} != 0x{ref_sum:x}\" assert dut.cout.value == ref_cout, \"cout mismatch: 0x{dut.cout.value:x} != 0x{ref_cout:x}\" print(f\"[test {i}] a=0x{a:x}, b=0x{b:x}, cin=0x{cin:x} =\u003e sum: 0x{ref_sum}, cout: 0x{ref_cout}\") # 完成测试 dut.Finish() print(\"Test Passed\") if __name__ == \"__main__\": random_test() 运行测试 在 picker_out_adder 目录下执行 python3 example.py 命令，即可运行测试。在测试完成后我们即可看到 example 示例项目的输出。\n[...] [test 114507] a=0x7adc43f36682cffe, b=0x30a718d8cf3cc3b1, cin=0x0 =\u003e sum: 0x12358823834579604399, cout: 0x0 [test 114508] a=0x3eb778d6097e3a72, b=0x1ce6af17b4e9128, cin=0x0 =\u003e sum: 0x4649372636395916186, cout: 0x0 [test 114509] a=0x42d6f3290b18d4e9, b=0x23e4926ef419b4aa, cin=0x1 =\u003e sum: 0x7402657300381600148, cout: 0x0 [test 114510] a=0x505046adecabcc, b=0x6d1d4998ed457b06, cin=0x0 =\u003e sum: 0x7885127708256118482, cout: 0x0 [test 114511] a=0x16bb10f22bd0af50, b=0x5813373e1759387, cin=0x1 =\u003e sum: 0x2034576336764682968, cout: 0x0 [test 114512] a=0xc46c9f4aa798106, b=0x4d8f52637f0417c4, cin=0x0 =\u003e sum: 0x6473392679370463434, cout: 0x0 [test 114513] a=0x3b5387ba95a7ac39, b=0x1a378f2d11b38412, cin=0x0 =\u003e sum: 0x6164045699187683403, cout: 0x0 Test Passed ","categories":["示例项目","教程"],"description":"基于一个简单的加法器验证展示工具的原理和使用方法，这个加法器内部是简单的组合逻辑。","excerpt":"基于一个简单的加法器验证展示工具的原理和使用方法，这个加法器内部是简单的组合逻辑。","ref":"/mlvp/docs/quick-start/eg-adder/","tags":["examples","docs"],"title":"案例一：简单加法器"},{"body":"CoupledL2是一个非阻塞的L2 Cache。\n下面的代码会对CoupledL2进行简单的验证，并使用数组作为参考模型，验证过程如下：\n生成随机的地址addr、执行AcquireBlock，请求读取addr的数据。 执行GrantData，接收DUT响应的数据。 把接收到的数据和参考模型的内容进行比较，验证行为是否一致。 执行GrantAck，响应DUT。 执行ReleaseData，向DUT请求在addr写入随机数据data。 同步参考模型，把addr的数据更新为data。 执行ReleaseAck，接收DUT的写入响应。 上述步骤会重复4000次。\n验证代码：\nCpp Java Python #include \"UT_CoupledL2.hpp\" using TLDataArray = std::array; enum class OpcodeA : uint32_t { PutFullData = 0x0, PutPartialData = 0x1, ArithmeticData = 0x2, LogicalData = 0x3, Get = 0x4, Hint = 0x5, AcquireBlock = 0x6, AcquirePerm = 0x7, }; enum class OpcodeB : uint32_t { ProbeBlock = 0x6, ProbePerm = 0x7 }; enum class OpcodeC : uint32_t { ProbeAck = 0x4, ProbeAckData = 0x5, Release = 0x6, ReleaseData = 0x7 }; enum class OpcodeD : uint32_t { AccessAck, AccessAckData, HintAck, Grant = 0x4, GrantData = 0x5, ReleaseAck = 0x6 }; enum class OpcodeE : uint32_t { GrantAck = 0x4 }; constexpr std::initializer_list ARGS = {\"+verilator+rand+reset+0\"}; auto dut = UTCoupledL2(ARGS); auto \u0026clk = dut.xclock; void sendA(OpcodeA opcode, uint32_t size, uint32_t address) { const auto \u0026valid = dut.master_port_0_0_a_valid; const auto \u0026ready = dut.master_port_0_0_a_ready; while (ready.value == 0x0) clk.Step(); valid.value = 1; dut.master_port_0_0_a_bits_opcode.value = opcode; dut.master_port_0_0_a_bits_size.value = size; dut.master_port_0_0_a_bits_address.value = address; clk.Step(); valid.value = 0; } void getB() { assert(false); const auto \u0026valid = dut.master_port_0_0_b_valid; const auto \u0026ready = dut.master_port_0_0_b_ready; ready.value = 1; while (valid.value == 0) clk.Step(); dut.master_port_0_0_b_bits_opcode = 0x0; dut.master_port_0_0_b_bits_param = 0x0; dut.master_port_0_0_b_bits_size = 0x0; dut.master_port_0_0_b_bits_source = 0x0; dut.master_port_0_0_b_bits_address = 0x0; dut.master_port_0_0_b_bits_mask = 0x0; dut.master_port_0_0_b_bits_data = 0x0; dut.master_port_0_0_b_bits_corrupt = 0x0; clk.Step(); ready.value = 0; } void sendC(OpcodeC opcode, uint32_t size, uint32_t address, uint64_t data) { const auto \u0026valid = dut.master_port_0_0_c_valid; const auto \u0026ready = dut.master_port_0_0_c_ready; while (ready.value == 0) clk.Step(); valid.value = 1; dut.master_port_0_0_c_bits_opcode.value = opcode; dut.master_port_0_0_c_bits_size.value = size; dut.master_port_0_0_c_bits_address.value = address; dut.master_port_0_0_c_bits_data.value = data; clk.Step(); valid.value = 0; } void getD() { const auto \u0026valid = dut.master_port_0_0_d_valid; const auto \u0026ready = dut.master_port_0_0_d_ready; ready.value = 1; clk.Step(); while (valid.value == 0) clk.Step(); ready.value = 0; } void sendE(uint32_t sink) { const auto \u0026valid = dut.master_port_0_0_e_valid; const auto \u0026ready = dut.master_port_0_0_e_ready; while (ready.value == 0) clk.Step(); valid.value = 1; dut.master_port_0_0_e_bits_sink.value = sink; clk.Step(); valid.value = 0; } void AcquireBlock(uint32_t address) { sendA(OpcodeA::AcquireBlock, 0x6, address); } void GrantData(TLDataArray \u0026r_data) { const auto \u0026opcode = dut.master_port_0_0_d_bits_opcode; const auto \u0026data = dut.master_port_0_0_d_bits_data; for (int i = 0; i \u003c 2; i++) { do { getD(); } while (opcode.value != OpcodeD::GrantData); r_data[i] = data.value; } } void GrantAck(uint32_t sink) { sendE(sink); } void ReleaseData(uint32_t address, const TLDataArray \u0026data) { for (int i = 0; i \u003c 2; i++) sendC(OpcodeC::ReleaseData, 0x6, address, data[i]); } void ReleaseAck() { const auto \u0026opcode = dut.master_port_0_0_d_bits_opcode; do { getD(); } while (opcode.value != OpcodeD::ReleaseAck); } int main() { TLDataArray ref_data[16] = {}; /* Random generator */ std::random_device rd; std::mt19937_64 gen_rand(rd()); std::uniform_int_distribution distrib(0, 0xf - 1); /* DUT init */ dut.InitClock(\"clock\"); dut.reset.SetWriteMode(xspcomm::WriteMode::Imme); dut.reset.value = 1; clk.Step(); dut.reset.value = 0; for (int i = 0; i \u003c 100; i++) clk.Step(); /* Test loop */ for (int test_loop = 0; test_loop \u003c 4000; test_loop++) { uint32_t d_sink; TLDataArray data{}, r_data{}; /* Generate random */ const auto address = distrib(gen_rand) \u003c\u003c 6; for (auto \u0026i : data) i = gen_rand(); printf(\"[CoupledL2 Test\\t%d]: At address(0x%03x), \", test_loop + 1, address); /* Read */ AcquireBlock(address); GrantData(r_data); // Print read result printf(\"Read: \"); for (const auto \u0026x : r_data) printf(\"%08lx\", x); d_sink = dut.master_port_0_0_d_bits_sink.value; assert ((r_data == ref_data[address \u003e\u003e 6]) \u0026\u0026 \"Read Failed\"); GrantAck(d_sink); /* Write */ ReleaseData(address, data); ref_data[address \u003e\u003e 6] = data; ReleaseAck(); // Print write data printf(\", Write: \"); for (const auto \u0026x : data) printf(\"%08lx\", x); printf(\".\\n\"); } return 0; } import com.ut.UT_CoupledL2; import com.xspcomm.WriteMode; import java.io.BufferedWriter; import java.io.IOException; import java.io.OutputStreamWriter; import java.io.PrintWriter; import java.math.BigInteger; import java.util.Arrays; import java.util.Random; import java.util.random.RandomGenerator; class Opcode { public enum A { PutFullData(0x0), PutPartialData(0x1), ArithmeticData(0x2), LogicalData(0x3), Get(0x4), Hint(0x5), AcquireBlock(0x6), AcquirePerm(0x7); private final int value; A(int value) { this.value = value; } public int getValue() { return value; } } public enum B { ProbeBlock(0x6), ProbePerm(0x7); private final int value; B(int value) { this.value = value; } public int getValue() { return value; } } public enum C { ProbeAck(0x4), ProbeAckData(0x5), Release(0x6), ReleaseData(0x7); private final int value; C(int value) { this.value = value; } public int getValue() { return value; } } public enum D { AccessAck(0x0), AccessAckData(0x1), HintAck(0x2), Grant(0x4), GrantData(0x5), ReleaseAck(0x6); private final int value; D(int value) { this.value = value; } public int getValue() { return value; } } public enum E { GrantAck(0x4); private final int value; E(int value) { this.value = value; } public int getValue() { return value; } } } public class TestCoupledL2 { static PrintWriter pwOut = new PrintWriter(new BufferedWriter(new OutputStreamWriter(System.out))); static UT_CoupledL2 dut; static void sendA(int opcode, int size, int address) { var valid = dut.master_port_0_0_a_valid; var ready = dut.master_port_0_0_a_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_a_bits_opcode.Set(opcode); dut.master_port_0_0_a_bits_size.Set(size); dut.master_port_0_0_a_bits_address.Set(address); dut.xclock.Step(); valid.Set(0); } static void getB() { var valid = dut.master_port_0_0_b_valid; var ready = dut.master_port_0_0_b_ready; ready.Set(1); while (!valid.B()) dut.xclock.Step(); ready.Set(0); } static void sendC(int opcode, int size, int address, long data) { var valid = dut.master_port_0_0_c_valid; var ready = dut.master_port_0_0_c_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_c_bits_opcode.Set(opcode); dut.master_port_0_0_c_bits_size.Set(size); dut.master_port_0_0_c_bits_address.Set(address); dut.master_port_0_0_c_bits_data.Set(data); dut.xclock.Step(); valid.Set(0); } static void getD() { var valid = dut.master_port_0_0_d_valid; var ready = dut.master_port_0_0_d_ready; ready.Set(1); dut.xclock.Step(); while (!valid.B()) dut.xclock.Step(); ready.Set(0); } static void sendE(int sink) { var valid = dut.master_port_0_0_e_valid; var ready = dut.master_port_0_0_e_ready; while (!ready.B()) dut.xclock.Step(); valid.Set(1); dut.master_port_0_0_e_bits_sink.Set(sink); dut.xclock.Step(); valid.Set(0); } static void AcquireBlock(int address) { sendA(Opcode.A.AcquireBlock.getValue(), 0x6, address); } static BigInteger GrantData() { var opcode = dut.master_port_0_0_d_bits_opcode; var data = dut.master_port_0_0_d_bits_data; do { getD(); } while (opcode.Get().intValue() != Opcode.D.GrantData.getValue()); var r_data = data.U64().shiftLeft(64); do { getD(); } while (opcode.Get().intValue() != Opcode.D.GrantData.getValue()); return r_data.or(data.U64()); } static void GrantAck(int sink) { sendE(sink); } static void ReleaseData(int address, BigInteger data) { sendC(Opcode.C.ReleaseData.getValue(), 0x6, address, data.longValue()); sendC(Opcode.C.ReleaseData.getValue(), 0x6, address, data.shiftRight(64).longValue()); } static void ReleaseAck() { var opcode = dut.master_port_0_0_d_bits_opcode; do { getD(); } while (opcode.Get().intValue() != Opcode.D.ReleaseAck.getValue()); } public static void main(String[] args) throws IOException { /* Random Generator */ var gen_rand = RandomGenerator.getDefault(); /* DUT init */ final String[] ARGS = {\"+verilator+rand+reset+0\"}; dut = new UT_CoupledL2(ARGS); dut.InitClock(\"clock\"); dut.reset.SetWriteMode(WriteMode.Imme); dut.reset.Set(1); dut.xclock.Step(); dut.reset.Set(0); for (int i = 0; i \u003c 100; i++) dut.xclock.Step(); dut.xclock.Step(); /* Ref */ BigInteger[] ref_data = new BigInteger[16]; Arrays.fill(ref_data, BigInteger.ZERO); /* Test loop */ for (int test_loop = 0; test_loop \u003c 4000; test_loop++) { var address = gen_rand.nextInt(0xf) \u003c\u003c 6; var data = new BigInteger(128, Random.from(gen_rand)); pwOut.print(\"[CoupledL2 Test%d]: At address(%#03x), \".formatted(test_loop + 1, address)); /* Read */ AcquireBlock(address); var r_data = GrantData(); assert (r_data.equals(ref_data[address \u003e\u003e 6])); var sink = dut.master_port_0_0_d_bits_sink.Get().intValue(); GrantAck(sink); /* Write */ ReleaseData(address, data); ref_data[address \u003e\u003e 6] = data; ReleaseAck(); pwOut.println(\"Read: %s, Write: %s\".formatted(r_data.toString(), data.toString())); pwOut.flush(); } } } ################ bundle.py ################ from toffee import Bundle, Signals, Signal class DecoupledBundle(Bundle): ready, valid = Signals(2) class TileLinkBundleA(DecoupledBundle): opcode, param, size, source, address, user_alias, mask, data, corrupt = Signals(9) class TileLinkBundleB(DecoupledBundle): opcode, param, size, source, address, mask, data, corrupt = Signals(8) class TileLinkBundleC(DecoupledBundle): opcode, param, size, source, address, user_alias, data, corrupt = Signals(8) class TileLinkBundleD(DecoupledBundle): opcode, param, size, source, sink, denied, data, corrupt = Signals(8) class TileLinkBundleE(DecoupledBundle): sink = Signal() class TileLinkBundle(Bundle): a = TileLinkBundleA.from_regex(r\"a_(?:(valid|ready)|bits_(.*))\") b = TileLinkBundleB.from_regex(r\"b_(?:(valid|ready)|bits_(.*))\") c = TileLinkBundleC.from_regex(r\"c_(?:(valid|ready)|bits_(.*))\") d = TileLinkBundleD.from_regex(r\"d_(?:(valid|ready)|bits_(.*))\") e = TileLinkBundleE.from_regex(r\"e_(?:(valid|ready)|bits_(.*))\") ################ agent.py ################ from toffee import Agent, driver_method from toffee.triggers import Value from bundle import TileLinkBundle class TilelinkOPCodes: class A: PutFullData = 0x0 PutPartialData = 0x1 ArithmeticData = 0x2 LogicalData = 0x3 Get = 0x4 Hint = 0x5 AcquireBlock = 0x6 AcquirePerm = 0x7 class B: Probe = 0x8 class C: ProbeAck = 0x4 ProbeAckData = 0x5 Release = 0x6 ReleaseData = 0x7 class D: AccessAck = 0x0 AccessAckData = 0x1 HintAck = 0x2 Grant = 0x4 GrantData = 0x5 ReleaseAck = 0x6 class E: GrantAck = 0x4 class TileLinkAgent(Agent): def __init__(self, tlbundle: TileLinkBundle): super().__init__(tlbundle.step) self.tlbundle = tlbundle @driver_method() async def put_a(self, dict): dict[\"valid\"] = 1 self.tlbundle.a.assign(dict) await Value(self.tlbundle.a.ready, 1) self.tlbundle.a.valid.value = 0 @driver_method() async def get_d(self): self.tlbundle.d.ready.value = 1 await Value(self.tlbundle.d.valid, 1) result = self.tlbundle.d.as_dict() self.tlbundle.d.ready.value = 0 return result @driver_method() async def get_b(self): self.tlbundle.b.ready.value = 1 await Value(self.tlbundle.b.valid, 1) result = self.tlbundle.b.as_dict() self.tlbundle.b.ready.value = 0 return result @driver_method() async def put_c(self, dict): dict[\"valid\"] = 1 self.tlbundle.c.assign(dict) await Value(self.tlbundle.c.ready, 1) self.tlbundle.c.valid.value = 0 @driver_method() async def put_e(self, dict): dict[\"valid\"] = 1 self.tlbundle.e.assign(dict) await Value(self.tlbundle.e.ready, 1) self.tlbundle.e.valid.value = 0 ################################ async def aquire_block(self, address): await self.put_a( { \"*\": 0, \"size\": 0x6, \"opcode\": TilelinkOPCodes.A.AcquireBlock, \"address\": address, } ) data = 0x0 for i in range(2): ret = await self.get_d() while ret[\"opcode\"] != TilelinkOPCodes.D.GrantData: ret = await self.get_d() data = (ret[\"data\"] \u003c\u003c (256 * i)) | data await self.put_e({\"sink\": ret[\"sink\"]}) return data async def release_data(self, address, data): for _ in range(2): await self.put_c( { \"*\": 0, \"size\": 0x6, \"opcode\": TilelinkOPCodes.C.ReleaseData, \"address\": address, \"data\": data % (2**256), } ) data = data \u003e\u003e 256 x = await self.get_d() while x[\"opcode\"] != TilelinkOPCodes.D.ReleaseAck: x = await self.get_d() ################ test.py ################ import toffee import random from toffee.triggers import ClockCycles from UT_CoupledL2 import DUTCoupledL2 from bundle import TileLinkBundle from agent import TileLinkAgent async def test_top(dut: DUTCoupledL2): toffee.start_clock(dut) dut.reset.value = 1 await ClockCycles(dut, 100) dut.reset.value = 0 tlbundle = TileLinkBundle.from_prefix(\"master_port_0_0_\").bind(dut) tlbundle.set_all(0) tlagent = TileLinkAgent(tlbundle) await ClockCycles(dut, 20) ref_data = [0] * 0x10 for _ in range(4000): # Read address = random.randint(0, 0xF) \u003c\u003c 6 r_data = await tlagent.aquire_block(address) print(f\"Read {address} = {hex(r_data)}\") assert r_data == ref_data[address \u003e\u003e 6] # Write send_data = random.randint(0, 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF) await tlagent.release_data(address, send_data) ref_data[address \u003e\u003e 6] = send_data print(f\"Write {address} = {hex(send_data)}\") if __name__ == \"__main__\": toffee.setup_logging(toffee.INFO) dut = DUTCoupledL2([\"+verilator+rand+reset+0\"]) dut.InitClock(\"clock\") dut.reset.AsImmWrite() toffee.run(test_top(dut)) dut.Finish() ","categories":["教程"],"description":"用C++、Java和Python简单验证香山L2 Cache的案例","excerpt":"用C++、Java和Python简单验证香山L2 Cache的案例","ref":"/mlvp/docs/multi-lang/examples/coupledl2/","tags":["docs"],"title":"CoupledL2"},{"body":" Using Guoke Cache as an example, this document introduces how to create a DUT based on Chisel.\nIn this document, a DUT (Design Under Test) refers to the circuit or system being verified during the chip verification process. The DUT is the primary subject of verification. When creating a DUT based on the picker tool, it is essential to consider the functionality, performance requirements, and verification goals of the subject under test. These goals may include the need for faster execution speed or more detailed test information. Generally, the DUT, written in RTL, is combined with its surrounding environment to form the verification environment (test_env), where test cases are written. In this project, the DUT is the Python module that needs to be tested and converted through RTL. Traditional RTL languages include Verilog, System Verilog, VHDL, etc. However, as an emerging RTL design language, （https://www.chisel-lang.org/） is playing an increasingly important role in RTL design due to its object-oriented features and ease of use. This chapter introduces how to create a DUT using the conversion of the cache source code from the Guoke Processor-NutShell to a Python module as an example.\nChisel and Guoke Chisel is a high-level hardware construction language (HCL) based on the Scala language. Traditional HDLs describe circuits, while HCLs generate circuits, making them more abstract and advanced. The Stage package provided in Chisel can convert HCL designs into traditional HDL languages such as Verilog and System Verilog. With tools like Mill and Sbt, automation in development can be achieved.\nGuoke is a sequential single-issue processor implementation based on the RISC-V RV64 open instruction set, modularly designed using the Chisel language. For a more detailed introduction to Guoke, please refer to the link: https://oscpu.github.io/NutShell-doc/.\nGuoke cache The Guoke Cache (Nutshell Cache) is the cache module used in the Guoke processor. It features a three-stage pipeline design. When the third stage pipeline detects that the current request is MMIO or a refill occurs, it will block the pipeline. The Guoke Cache also uses a customizable modular design that can generate different-sized L1 Caches or L2 Caches by changing parameters. Additionally, the Guoke Cache has a coherence interface to handle coherence-related requests.\nChisel to Verilog The stage library in Chisel helps generate traditional HDL code such as Verilog and System Verilog from Chisel code. Below is a brief introduction on how to convert a cache implementation based on Chisel into the corresponding Verilog circuit description.\nInitializing the Guoke Environment First, download the entire Guoke source code from the source repository and initialize it:\nmkdir cache-ut cd cache-ut git clone https://github.com/OSCPU/NutShell.git cd NutShell \u0026\u0026 git checkout 97a025d make init Creating Scala Compilation Configuration Then, create build.sc in the cache-ut directory with the following content:\nimport $file.NutShell.build import mill._, scalalib._ import coursier.maven.MavenRepository import mill.scalalib.TestModule._ // Specify Nutshell dependencies object difftest extends NutShell.build.CommonNS { override def millSourcePath = os.pwd / \"NutShell\" / \"difftest\" } // Nutshell configuration object NtShell extends NutShell.build.CommonNS with NutShell.build.HasChiselTests { override def millSourcePath = os.pwd / \"NutShell\" override def moduleDeps = super.moduleDeps ++ Seq( difftest, ) } // UT environment configuration object ut extends NutShell.build.CommonNS with ScalaTest{ override def millSourcePath = os.pwd override def moduleDeps = super.moduleDeps ++ Seq( NtShell ) } Instantiating cache After creating the configuration information, create the src/main/scala source code directory according to the Scala specification. Then, in the source code directory, create nut_cache.scala and use the following code to instantiate the Cache and convert it into Verilog code:\npackage ut_nutshell import chisel3._ import chisel3.util._ import nutcore._ import top._ import chisel3.stage._ object CacheMain extends App { (new ChiselStage).execute(args, Seq( ChiselGeneratorAnnotation(() =\u003e new Cache()(CacheConfig(ro = false, name = \"tcache\", userBits = 16))) )) } Generating RTL After creating all the files (build.sc, src/main/scala/nut_cache.scala), execute the following command in the cache-ut directory:\nmkdir build mill --no-server -d ut.runMain ut_nutshell.CacheMain --target-dir build --output-file Cache Note: For the Mill environment configuration, please refer to https://mill-build.com/mill/Intro_to_Mill.html.\nAfter successfully executing the above command, a Verilog file Cache.v will be generated in the build directory. Then, the picker tool can be used to convert Cache.v into a Python module. Besides Chisel, almost all other HCL languages can generate corresponding RTL codes, so the basic process above also applies to other HCLs.\nDUT Compilation Generally, if you need the DUT to generate waveforms, coverage, etc., it will slow down the DUT’s execution speed. Therefore, when generating a Python module through the picker tool, it will be generated according to various configurations: (1) Turn off all debug information; (2) Enable waveforms; (3) Enable code line coverage. The first configuration aims to quickly build the environment for regression testing, etc.; the second is used to analyze specific errors, timing, etc.; the third is used to improve coverage.\n","categories":["Sample Projects","Learning Materials"],"description":"Using Guoke Cache as an example, this document introduces how to create a DUT based on Chisel.","excerpt":"Using Guoke Cache as an example, this document introduces how to …","ref":"/mlvp/en/docs/basic/create_dut/","tags":["examples","docs"],"title":"Creating DUT"},{"body":"An Agent in the mlvp verification environment provides a high-level encapsulation of signals within a class of Bundles, allowing the upper-level driver code to drive and monitor the signals in the Bundle without worrying about specific signal assignments.An Agent consists of driver methods and monitor methods , where the driver methods actively drive the signals in the Bundle, and the monitor methods passively observe the signals in the Bundle.\nInitializing the Agent To define an Agent, you need to create a new class that inherits from the Agent class in mlvp. Here’s a simple example of defining an Agent:\nfrom mlvp.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle In the initialization of the AdderAgent class, you need to pass the Bundle that this Agent will drive and provide a clock synchronization function to the parent Agent class. This function will be used by the Agent to determine when to call the monitor methods. Generally, it can be set to bundle.step, which is the clock synchronization function in the Bundle, synchronized with the DUT’s clock.\nCreating Driver Methods In the Agent, the driver method is an asynchronous function used to actively drive the signals in the Bundle. The driver function needs to parse its input parameters and assign values to the signals in the Bundle based on the parsed results, which can span multiple clock cycles. If you need to obtain signal values from the Bundle, you should write the corresponding logic in the function and return the needed data through the function’s return value.Each driver method should be an asynchronous function and decorated with the @driver_method decorator so that the Agent can recognize it as a driver method. Here’s a simple example of defining a driver method:\nfrom mlvp.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle @driver_method() async def exec_add(self, a, b, cin): self.bundle.a.value = a self.bundle.b.value = b self.bundle.cin.value = cin await self.bundle.step() return self.bundle.sum.value, self.bundle.cout.value In the exec_add function, we assign the incoming parameters a, b, and cin to the corresponding signals in the Bundle. We then wait for one clock cycle. After the clock cycle ends, we return the values of sum and cout signals from the Bundle.During the development of the driver function, you can use all the synchronization methods for waiting for clock signals introduced in How to Use the Asynchronous Environment , such as ClockCycles, Value, etc. Once created, you can call this driver method in your driving code like a regular function:\nadder_bundle = AdderBundle() adder_agent = AdderAgent(adder_bundle) sum, cout = await adder_agent.exec_add(1, 2, 0) print(sum, cout) Functions marked with @driver_method have various features when called; this will be elaborated on when writing test cases. Additionally, these functions will handle matching against the reference model and automatically call back to return values for comparison; this will be discussed in the reference model section.\nCreating Monitor Methods The monitor method also needs to be an asynchronous function and should be decorated with the @monitor_method decorator so that the Agent can recognize it as a monitor method. Here’s a simple example of defining a monitor method:\nfrom mlvp.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle @monitor_method() async def monitor_sum(self): if self.bundle.sum.value \u003e 0: return self.bundle.as_dict() In the monitor_sum function, we use the sum signal in the Bundle as the object to monitor. When the value of the sum signal is greater than 0, we collect the default message type generated by the Bundle. The collected return value will be stored in the internal message queue.Once the monitor_method decorator is added, the monitor_sum function will be automatically called by the Agent, which will use the clock synchronization function provided during the Agent’s initialization to decide when to call the monitor method. By default, the Agent will call the monitor method once in each clock cycle. If the monitor method has a return value, it will be stored in the internal message queue. If the execution of a single call to the monitor method spans multiple clock cycles, the Agent will wait until the previous call to the monitor method has finished before calling it again. If you write a monitor method like this:\n@monitor_method() async def monitor_sum(self): return self.bundle.as_dict() This monitor method will add a message to the message queue in every cycle. Retrieving Monitor Messages Since this monitor method is marked with @monitor_method, it will be automatically called by the Agent. If you try to directly call this function in your test case as follows, it will not execute as expected:\nadder_bundle = AdderBundle() adder_agent = AdderAgent(adder_bundle) result = await adder_agent.monitor_sum() Instead, when called in the above manner, the monitor method will pop the earliest collected message from the message queue and return it. If the message queue is empty, this call will wait until there are messages in the queue before returning.\nIf you want to get the number of messages in the message queue, you can do so as follows:\nmessage_count = adder_agent.monitor_size(\"monitor_sum\") By creating monitor methods, you can easily add a background monitoring task that observes the signal values in the Bundle and collects messages when certain conditions are met. Once a function is marked as a monitor method, the framework will also provide it with matching against the reference model and automatic collection for comparison; this will be detailed in the reference model section. By writing multiple driver methods and monitor methods within the Agent, you complete the entire Agent implementation.\n","categories":"","description":"","excerpt":"An Agent in the mlvp verification environment provides a high-level …","ref":"/mlvp/en/docs/mlvp/env/agent/","tags":"","title":"How to Write an Agent"},{"body":"Test Points in Verification In mlvp, a test point (Cover Point) refers to the smallest unit of verification for a specific function of the design, while a test group (Cover Group) is a collection of related test points. To define a test point, you need to specify the name of the test point and its trigger condition. For example, you can define a test point such as, “When the result of the adder operation is non-zero, the result is correct.” In this case, the trigger condition for the test point could be “the sum signal of the adder is non-zero.”\nWhen the trigger condition of the test point is met, the test point is triggered. At this moment, the verification report will record the triggering of the test point and increase the functional coverage of the verification. When all test points are triggered, the functional coverage of the verification reaches 100%.\nHow to Write Test Points Before writing test points, you first need to create a test group and specify the name of the test group:\nfrom mlvp.reporter import CovGroup g = CovGroup(\"Adder addition function\") Next, you need to add test points to this test group:\n# import mlvp.funcov as fc # g.add_watch_point(adder.io_cout, {\"io_cout is 0\": fc.Eq(0)}, name=\"Cout is 0\") TBD (To Be Determined)\n","categories":"","description":"","excerpt":"Test Points in Verification In mlvp, a test point (Cover Point) refers …","ref":"/mlvp/en/docs/mlvp/cases/cov/","tags":"","title":"How to Write Test Points"},{"body":"Multi-File Input and Output In many cases, a module in one file may instantiate modules in other files. In such cases, you can use the picker tool’s -f option to process multiple Verilog source files. For example, suppose you have three source files: Cache.sv, CacheStage.sv, and CacheMeta.sv:\nFile List Cache.sv // In module Cache( ... ); CacheStage s1( ... ); CacheStage s2( ... ); CacheStage s3( ... ); CacheMeta cachemeta( ... ); endmodule CacheStage.sv // In CacheStage.sv module CacheStage( ... ); ... endmodule CacheMeta.sv // In CacheMeta.sv module CacheMeta( ... ); ... endmodule Usage In this case, the module under test is Cache, which is in Cache.sv. You can generate the DUT using the following command:\nCommand Line Specification picker export Cache.sv --fs CacheStage.sv,CacheMeta.sv --sname Cache Specification through a File List File You can also use a .txt file to specify multiple input files:\npicker export Cache.sv --fs src.txt --sname Cache Where the contents of src.txt are:\nCacheStage.sv CacheMeta.sv Notes It is important to note that even when using multiple file inputs, you still need to specify the file containing the top-level module under test, as shown in the example above with Cache.sv. When using multiple file inputs, Picker will pass all files to the simulator, which will compile them simultaneously. Therefore, it is necessary to ensure that the module names in all files are unique. ","categories":["Sample Projects","Tutorials"],"description":"Handling multiple Verilog source files","excerpt":"Handling multiple Verilog source files","ref":"/mlvp/en/docs/env_usage/multifile/","tags":["examples","docs"],"title":"Multi-File Input"},{"body":"mlvp provides the methods and tools needed for the complete process of setting up a verification environment. This chapter will explain in detail how to use mlvp to build a complete verification environment.Before proceeding, please ensure you have read How to Write a Canonical Verification Environment and are familiar with the basic structure of mlvp’s canonical verification environment. For a completely new verification task, following the environment setup steps, the process of building a verification environment can be divided into the following steps:\nPartition the DUT interface based on logical functions and define Bundles.\nWrite an Agent for each Bundle, completing the high-level encapsulation of the Bundle.\nEncapsulate multiple Agents into an Env, completing the high-level encapsulation of the entire DUT.\nWrite the reference model according to the interface specifications of the Env and bind it to the Env.\nThis chapter will introduce how to use mlvp tools to meet the requirements for setting up the environment in each step.\n","categories":"","description":"","excerpt":"mlvp provides the methods and tools needed for the complete process of …","ref":"/mlvp/en/docs/mlvp/env/","tags":"","title":"Setting Up a Verification Environment"},{"body":"Currently, Picker supports C++/Python. Other languages such as Java, Golang, Javascript, Scala, etc., will be supported after the Python interface is stabilized.\n","categories":["Tutorials"],"description":"Encapsulate the DUT hardware runtime environment with Java and package it as a jar file.","excerpt":"Encapsulate the DUT hardware runtime environment with Java and package …","ref":"/mlvp/en/docs/multi-lang/java/","tags":["docs"],"title":"Using Java ..."},{"body":"mlvp is a hardware verification framework written in Python. It relies on a multi-language conversion tool called picker , which converts Verilog hardware design code into a Python package, allowing users to drive and verify hardware designs using Python. It incorporates some concepts from the UVM verification methodology to ensure the standardization and reusability of the verification environment. The entire setup of the verification environment has been redesigned to better align with software development practices, making it easier for software developers to get started with hardware verification.\n","categories":"","description":"mlvp is a Python-based hardware verification framework that helps users establish hardware","excerpt":"mlvp is a Python-based hardware verification framework that helps …","ref":"/mlvp/en/docs/mlvp/","tags":"","title":"Verification Framework"},{"body":" 以果壳cache为例，介绍如何创建基于Chisel的DUT\n在本文档中，DUT（Design Under Test）是指在芯片验证过程中，被验证的对象电路或系统。DUT是验证的主体，在基于picker工具创建DUT时，需要考虑被测对象的功能、性能要求和验证目标，例如是需要更快的执行速度，还是需要更详细的测试信息。通常情况下DUT，即RTL编写的源码，与外围环境一起构成验证环境（test_env），然后基于该验证环境编写测试用例。在本项目中，DUT是需要测试的Python模块，需要通过RTL进行转换。传统的RTL语言包括Verilog、System Verilog、VHDL等，然而作为新兴的RTL设计语言，Chisel（https://www.chisel-lang.org/）也以其面向对象的特征和便捷性，逐渐在RTL设计中扮演越来越重要的角色。本章以果壳处理器-NutShell中的cache源代码到Python模块的转换为例进行介绍如何创建DUT。\nChisel与果壳 准确来说，Chisel是基于Scala语言的高级硬件构造（HCL）语言。传统HDL是描述电路，而HCL则是生成电路，更加抽象和高级。同时Chisel中提供的Stage包则可以将HCL设计转化成Verilog、System Verilog等传统的HDL语言设计。配合上Mill、Sbt等Scala工具则可以实现自动化的开发。\n果壳是使用 Chisel 语言模块化设计的、基于 RISC-V RV64 开放指令集的顺序单发射处理器实现。果壳更详细的介绍请参考链接：https://oscpu.github.io/NutShell-doc/\n果壳 cache 果壳cache（Nutshell Cache）是果壳处理器中使用的缓存模块。其采用三级流水设计，当第三级流水检出当前请求为MMIO或者发生重填（refill）时，会阻塞流水线。同时，果壳cache采用可定制的模块化设计，通过改变参数可以生成存储空间大小不同的一级cache（L1 Cache）或者二级cache（L2 Cache）。此外，果壳cache留有一致性（coherence）接口，可以处理一致性相关的请求。\nChisel 转 Verilog Chisel中的stage库可以帮助由Chisel代码生成Verilog、System Verilog等传统的HDL代码。以下将简单介绍如何由基于Chisel的cache实现转换成对应的Verilog电路描述。\n初始化果壳环境 首先从源仓库下载整个果壳源代码，并进行初始化：\nmkdir cache-ut cd cache-ut git clone https://github.com/OSCPU/NutShell.git cd NutShell \u0026\u0026 git checkout 97a025d make init 创建scala编译配置 在cache-ut目录下创建build.sc，其中内容如下：\nimport $file.NutShell.build import mill._, scalalib._ import coursier.maven.MavenRepository import mill.scalalib.TestModule._ // 指定Nutshell的依赖 object difftest extends NutShell.build.CommonNS { override def millSourcePath = os.pwd / \"NutShell\" / \"difftest\" } // Nutshell 配置 object NtShell extends NutShell.build.CommonNS with NutShell.build.HasChiselTests { override def millSourcePath = os.pwd / \"NutShell\" override def moduleDeps = super.moduleDeps ++ Seq( difftest, ) } // UT环境配置 object ut extends NutShell.build.CommonNS with ScalaTest{ override def millSourcePath = os.pwd override def moduleDeps = super.moduleDeps ++ Seq( NtShell ) } 实例化 cache 创建好配置信息后，按照scala规范，创建src/main/scala源代码目录。之后，就可以在源码目录中创建nut_cache.scala，利用如下代码实例化Cache并转换成Verilog代码：\npackage ut_nutshell import chisel3._ import chisel3.util._ import nutcore._ import top._ import chisel3.stage._ object CacheMain extends App { (new ChiselStage).execute(args, Seq( ChiselGeneratorAnnotation(() =\u003e new Cache()(CacheConfig(ro = false, name = \"tcache\", userBits = 16))) )) } 生成RTL 完成上述所有文件的创建后（build.sc，src/main/scala/nut_cache.scala），在cache-ut目录下执行如下命令：\nmkdir build mill --no-server -d ut.runMain ut_nutshell.CacheMain --target-dir build --output-file Cache 注：mill环境的配置请参考 https://mill-build.com/mill/Intro_to_Mill.html\n上述命令成功执行完成后，会在build目录下生成verilog文件：Cache.v。之后就可以通过picker工具进行Cache.v到 Python模块的转换。除去chisel外，其他HCL语言几乎都能生成对应的 RTL代码，因此上述基本流程也适用于其他HCL。\nDUT编译 一般情况下，如果需要DUT生成波形、覆盖率等会导致DUT的执行速度变慢，因此在通过picker工具生成python模块时会根据多种配置进行生成：（1）关闭所有debug信息；（2）开启波形；（3）开启代码行覆盖率。其中第一种配置的目标是快速构建环境，进行回归测试等；第二种配置用于分析具体错误，时序等；第三种用于提升覆盖率。\n","categories":["示例项目","学习材料"],"description":"以果壳cache为例，介绍如何创建基于chisel的DUT","excerpt":"以果壳cache为例，介绍如何创建基于chisel的DUT","ref":"/mlvp/docs/basic/create_dut/","tags":["examples","docs"],"title":"创建DUT"},{"body":"toffee和toffee-test 提供了搭建验证环境全流程所需要的方法和工具，本章中将详细介绍如何使用 toffee和toffee-test 搭建一个完整的验证环境。\n在阅读前请确保您已经阅读了 如何编写规范的验证环境，并了解了 toffee 规范验证环境的基本结构。\n对于一次全新的验证工作来说，按照环境搭建步骤的开始顺序，搭建验证环境可以分为以下几个步骤：\n按照逻辑功能划分 DUT 接口，并定义 Bundle 为每个 Bundle 编写 Agent，完成对 Bundle 的高层封装 将多个 Agent 封装成 Env，完成对整个 DUT 的高层封装 按照 Env 的接口规范编写参考模型，并将其与 Env 进行绑定 本章将会分别介绍每个步骤中如何使用 toffee和toffee-test 中的工具来完成环境搭建需求。\n","categories":"","description":"","excerpt":"toffee和toffee-test 提供了搭建验证环境全流程所需要的方法和工具，本章中将详细介绍如何使用 toffee …","ref":"/mlvp/docs/mlvp/env/","tags":"","title":"搭建验证环境"},{"body":"多文件输入输出 在许多情况中，某文件下的某个模块会例化其他文件下的模块，在这种情况下您可以使用Picker工具的-f选项处理多个verilog源文件。例如，假设您有Cache.sv, CacheStage.sv以及CacheMeta.sv三个源文件：\n文件列表 Cache.sv // In module Cache( ... ); CacheStage s1( ... ); CacheStage s2( ... ); CacheStage s3( ... ); CacheMeta cachemeta( ... ); endmodule CacheStage.sv // In CacheStage.sv module CacheStage( ... ); ... endmodule CacheMeta.sv // In CacheMeta.sv module CacheMeta( ... ); ... endmodule 应用方式 其中，待测模块为Cache，位于Cache.sv中，则您可以通过以下命令生成DUT：\n命令行指定 picker export Cache.sv --fs CacheStage.sv,CacheMeta.sv --sname Cache 通过文件列表文件指定 您也可以通过传入.txt文件的方式来实现多文件输入：\npicker export Cache.sv --fs src.txt --sname Cache 其中src.txt的内容为:\nCacheStage.sv CacheMeta.sv 注意事项 需要注意的是，使用多文件输入时仍需要指定待测顶层模块所在的文件，例如上文中所示的Cache.sv。 在使用多文件输入时，Picker会将所有文件都交给仿真器，仿真器同时进行编译，因此需要保证所有文件中的模块名不重复。 ","categories":["示例项目","教程"],"description":"处理多个Verilog源文件","excerpt":"处理多个Verilog源文件","ref":"/mlvp/docs/env_usage/multifile/","tags":["examples","docs"],"title":"多文件输入"},{"body":"什么是功能检查点 在 toffee 中，功能检查点(Cover Point) 是指对设计的某个功能进行验证的最小单元，判断该功能是否满足设计目标。测试组(Cover Croup) 是一类检查点的集合。\n定义一个检查点，需要指定检查点的名称及检查点的触发条件（触发条件可以有多个，最终的检查结果为所有条件取“逻辑与”，触发条件称为Cover Bin）。例如，可以定义了一个检查点，“当加法器运算结果不为 0 时，结果运算正确”，此时，检查点的触发条件可以为 “加法器的 sum 信号不为零”。\n当检查点的所有触发条件都满足时，检查点被触发，此时，验证报告将会记录下该检查点的触发。并会提升验证的功能覆盖率。当所有检查点都被触发时，验证的功能覆盖率达到 100%。\n如何编写检查点 编写检查点前，首先需要创建一个测试组，并指定测试组的名称\nimport toffee.funcov as fc g = fc.CovGroup(\"Group-A\") 接着，需要往这个测试组中添加检查点。一般情况下，一个功能点对应一个或多个检查点，用来检查是否满足该功能。例如我们需要检查Adder的cout是否有0出现，我们可以通过如下方式添加：\ng.add_watch_point(adder.io_cout, {\"io_cout is 0\": fc.Eq(0)}, name=\"cover_point_1\") 在上述检查点中，需要观察的数据为io_cout引脚，检查条件(Cover Bin)的名称为io_cout is 0，检查点名称为cover_point_1。函数add_watch_point的参数说明如下：\ndef add_watch_point(target, bins: dict, name: str = \"\", once=None): \"\"\" @param target: 检查目标，可以是一个引脚，也可以是一个DUT对象 @param bins: 检查条件，dict格式，key为条件名称，value为具体检查方法或者检查方法的数组。 @param name: 检查点名称 @param once，如果once=True，表明只检查一次，一旦该检查点满足要求后就不再进行重复条件判断。 通常情况下，target为DUT引脚，bins中的检查函数来检查target的value是否满足预定义条件。funcov模块内存了部分检查函数，例如Eq(x), Gt(x), Lt(x), Ge(x), Le(x), Ne(x), In(list), NotIn(list), isInRange([low,high])等。当内置检查函数不满足要求时，也可以自定义，例如需要跨时钟周期进行检查等。自定义检查函数的输入参数为target，返回值为bool。例如：\ng.add_watch_point(adder.io_cout, { \"io_cout is 0\": lambda x: x.value == 0, \"io_cout is 1\": lambda x: x.value == 1, \"io_cout is x\": [fc.Eq(0), fc.In([0,1]), lambda x:x.value \u003c 4], }, name=\"cover_point_1\") 当添加完所有的检查点后，需要在DUT的Step回调函数中调用CovGroup的sample()方法进行判断。在检查过程中，或者测试运行完后，可以通过CovGroup的as_dict()方法查看检查情况。\ndut.StepRis(lambda x: g.sample()) ... print(g.as_dict()) 如何在测报告中展示 在测试case每次运行结束时，可以通过set_func_coverage(request, cov_groups)告诉框架对所有的功能覆盖情况进行合并收集。相同名字的CoverGroup会被自动合并。下面是一个简单的例子：\nimport pytest import toffee.funcov as fc from toffee_test.reporter import set_func_coverage g = fc.CovGroup(\"Group X\") def init_function_coverage(g): # add your points here pass @pytest.fixture() def dut_input(request): # before test init_function_coverage(g) dut = DUT() dut.InitClock(\"clock\") dut.StepRis(lambda x: g.sample()) yield dut # after test dut.Finish() set_func_coverage(request, g) g.clear() def test_case1(dut_input): assert True def test_case2(dut_input): assert True # ... 在上述例子中，每个case都会通过dut_input函数来创建输入参数。该函数用yield返回dut，在运行case前初始化dut，并且设置在dut的step回调中执行g.sample()。运行完case后，调用set_func_coverage收集覆盖率，然后清空收集的信息。所有测试运行完成后，可在生成的测试报告中查看具体的覆盖情况。\n","categories":"","description":"","excerpt":"什么是功能检查点 在 toffee 中，功能检查点(Cover Point) 是指对设计的某个功能进行验证的最小单元，判断该功能是否满足设计 …","ref":"/mlvp/docs/mlvp/cases/cov/","tags":"","title":"功能检查点（功能覆盖率）"},{"body":"Agent 在 toffee 验证环境中实现了对一类 Bundle 中信号的高层封装，使得上层驱动代码可以在不关心具体信号赋值的情况下，完成对 Bundle 中信号的驱动及监测。\n一个 Agent 由 驱动方法(driver_method) 和 监测方法(monitor_method) 组成，其中驱动方法用于主动驱动 Bundle 中的信号，而监测方法用于被动监测 Bundle 中的信号。\n初始化 Agent 为了定义一个 Agent，需要自定义一个新类，并继承 toffee 中的 Agent 类。下面是一个简单的 Agent 的定义示例：\nfrom toffee.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle 在 AdderAgent 类初始化时，需要外界传入该 Agent 需要驱动的 Bundle，并且需要向父类 Agent 中传入一个时钟同步函数，以便 Agent 使用这一函数来决定何时调用监测方法。一般来说，可以将其设置为 bundle.step，即 Bundle 中的时钟同步函数，Bundle 中的 step 与 DUT 中的时钟同步。\n创建驱动方法 在 Agent 中，驱动方法是一个异步函数，用于主动驱动 Bundle 中的信号。驱动函数需要将函数的传入参数进行解析，并根据解析结果对 Bundle 中的信号进行赋值，赋值的过程可以跨越多个时钟周期。如果需要获取 Bundle 的信号值，那么在函数中编写相应的逻辑，并将其转换为需要的数据，通过函数返回值返回。\n每一个驱动方法都应是一个异步函数，并且使用 @driver_method 装饰器进行修饰，以便 Agent 能够识别该函数为驱动方法。\n下面是一个简单的驱动方法的定义示例：\nfrom toffee.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle @driver_method() async def exec_add(self, a, b, cin): self.bundle.a.value = a self.bundle.b.value = b self.bundle.cin.value = cin await self.bundle.step() return self.bundle.sum.value, self.bundle.cout.value 在 drive 函数中，我们将传入的 a, b, cin 三个参数分别赋值给 Bundle 中的 a, b, cin 信号，并等待一个时钟周期。在时钟周期结束后，我们返回 Bundle 中的 sum, cout 信号值。\n在驱动函数的编写过程中，你可以使用 如何使用异步环境 中介绍的所有等待时钟信号的同步方法，例如 ClockCycles, Value 等。\n创建完毕后，你可以像调用普通函数一样在驱动代码中调用该驱动方法，例如：\nadder_bundle = AdderBundle() adder_agent = AdderAgent(adder_bundle) sum, cout = await adder_agent.exec_add(1, 2, 0) print(sum, cout) 被标识为 @driver_method 的函数在调用时拥有诸多特性，这一部分将在编写测试用例中详细介绍。同时，该类函数还会完成参考模型的匹配与自动调用以返回值对比，这一部分将在编写参考模型中详细介绍。\n创建监测方法 监测方法同样需要是一个异步函数，并且使用 @monitor_method 装饰器进行修饰，以便 Agent 能够识别该函数为监测方法。\n一个简单的监测方法的定义示例如下：\nfrom toffee.agent import * class AdderAgent(Agent): def __init__(self, bundle): super().__init__(bundle.step) self.bundle = bundle @monitor_method() async def monitor_sum(self): if self.bundle.sum.value \u003e 0: return self.bundle.as_dict() 在 monitor_sum 函数中，我们以 Bundle 中的 sum 信号作为监测对象，当 sum 信号的值大于 0 时，收集 Bundle 生成的默认消息类型，收集到的返回值将会被存储到内部的消息队列中。\n添加 monitor_method 装饰器后，monitor_sum 函数将会被 Agent 自动调用，它会使用 Agent 初始化时提供的时钟同步函数来决定何时调用监测方法。默认情况下，Agent 会在每个时钟周期都调用一次监测方法，如果监测方法有返回值，那么返回值将会被存储到内部的消息队列中。若监测方法的一次调用会经过多个时钟周期，Agent 会等待上一次监测方法调用结束后再次调用监测方法。\n如果编写了类似下面的监测方法：\n@monitor_method() async def monitor_sum(self): return self.bundle.as_dict() 该监测方法将会在每个周期都往消息队列中添加一个消息。\n获取监测消息\n由于该监测方法被标记为了 @monitor_method，因此该方法将会被 Agent 自动调用，在测试用例中如果按照以下方式直接调用该函数，并不能执行该函数的预期行为。\nadder_bundle = AdderBundle() adder_agent = AdderAgent(adder_bundle) result = await adder_agent.monitor_sum() 相反的，按照上述方式调用监测方法，它将会弹出消息队列中收集到的最早的消息，并返回该消息。如果消息队列为空，该次调用将会等待消息队列中有消息后再返回。\n如果想获取消息队列中的消息数量，可以使用如下方式获取：\nmessage_count = adder_agent.monitor_size(\"monitor_sum\") 通过创建监测方法，你可以方便地添加一个后台监测任务，监测 Bundle 中的信号值，并在满足条件时收集消息。将函数标记为监测方法后，框架还会为这一方法提供与参考模型的匹配与自动收集对比，这一部分将在编写参考模型中详细介绍。\n通过在 Agent 中编写多个驱动方法和监测方法，便完成了整个 Agent 的编写。\n","categories":"","description":"","excerpt":"Agent 在 toffee 验证环境中实现了对一类 Bundle 中信号的高层封装，使得上层驱动代码可以在不关心具体信号赋值的情况下，完成 …","ref":"/mlvp/docs/mlvp/env/agent/","tags":"","title":"如何编写 Agent"},{"body":"Toffee 是使用 Python 语言编写的一套硬件验证框架，它依赖于多语言转换工具 picker，该工具能够将硬件设计的 Verilog 代码转换为 Python Package，使得用户可以使用 Python 来驱动并验证硬件设计。\n其吸收了部分 UVM 验证方法学，以保证验证环境的规范性和可复用性，并重新设计了整套验证环境的搭建方式，使其更符合软件领域开发者的使用习惯，从而使软件开发者可以轻易地上手硬件验证工作。\n","categories":"","description":"Toffee 是一套基于 Python 的硬件验证框架，帮助用户更加方便、规范地使用 Python 建立起硬件验证环境","excerpt":"Toffee 是一套基于 Python 的硬件验证框架，帮助用户更加方便、规范地使用 Python 建立起硬件验证环境","ref":"/mlvp/docs/mlvp/","tags":"","title":"验证框架"},{"body":"","categories":["Example Projects","Tutorials"],"description":"Complex case studies completed using the open verification platform.","excerpt":"Complex case studies completed using the open verification platform.","ref":"/mlvp/en/docs/advance_case/","tags":["examples","docs"],"title":"Advanced Case Studies"},{"body":"RTL Source Code In this example, we drive a random number generator, with the source code as follows:\nmodule RandomGenerator ( input wire clk, input wire reset, input [15:0] seed, output [15:0] random_number ); reg [15:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[14:0], lfsr[15] ^ lfsr[14]}; end end assign random_number = lfsr; endmodule This random number generator contains a 16-bit LFSR, with a 16-bit seed as input and a 16-bit random number as output. The LFSR is updated according to the following rules:\nXOR the highest bit and the second-highest bit of the current LFSR to generate a new_bit.\nShift the original LFSR left by one bit, and place new_bit in the lowest bit.\nDiscard the highest bit.\nTesting Process During testing, we will create a folder named RandomGenerator, which contains a RandomGenerator.v file. The content of this file is the RTL source code mentioned above.\nBuilding the RTL into a Python Module Generating Intermediate Files Navigate to the RandomGenerator folder and execute the following command:\npicker export --autobuild=false RandomGenerator.v -w RandomGenerator.fst --sname RandomGenerator --tdir picker_out_rmg --lang python -e --sim verilator This command does the following:\nUses RandomGenerator.v as the top file and RandomGenerator as the top module, generating a dynamic library with the Verilator simulator, targeting Python as the output language.\nEnables waveform output, with the target waveform file being RandomGenerator.fst.\nIncludes files for driving the example project (-e), and does not automatically compile after code generation (-autobuild=false).\nOutputs the final files to the picker_out_rmg directory. The output directory structure is similar to Adder Verification - Generating Intermediate Files , so it will not be elaborated here.\nBuilding Intermediate Files Navigate to the picker_out_rmg directory and execute the make command to generate the final files.\nNote: The compilation process is similar to Adder Verification - Compilation Process , so it will not be elaborated here. The final directory structure will be:\npicker_out_rmg |-- RandomGenerator.fst # Waveform file from the test |-- UT_RandomGenerator | |-- RandomGenerator.fst.hier | |-- _UT_RandomGenerator.so # Swig-generated wrapper dynamic library | |-- __init__.py # Initialization file for the Python module, also the library definition file | |-- libDPIRandomGenerator.a # Library file generated by the simulator | |-- libUTRandomGenerator.so # libDPI dynamic library wrapper generated based on dut_base | `-- libUT_RandomGenerator.py # Python module generated by Swig | `-- xspcomm # xspcomm base library, fixed folder, no need to pay attention to it `-- example.py # Example code Configuring the Test Code Replace the content of example.py with the following code.\nfrom UT_RandomGenerator import * import random # Define the reference model class LFSR_16: def __init__(self, seed): self.state = seed \u0026 ((1 \u003c\u003c 16) - 1) def Step(self): new_bit = (self.state \u003e\u003e 15) ^ (self.state \u003e\u003e 14) \u0026 1 self.state = ((self.state \u003c\u003c 1) | new_bit ) \u0026 ((1 \u003c\u003c 16) - 1) if __name__ == \"__main__\": dut = DUTRandomGenerator() # Create the DUT dut.InitClock(\"clk\") # Specify the clock pin and initialize the clock seed = random.randint(0, 2**16 - 1) # Generate a random seed dut.seed.value = seed # Set the DUT seed ref = LFSR_16(seed) # Create a reference model for comparison # Reset the DUT dut.reset.value = 1 # Set reset signal to 1 dut.Step() # Advance one clock cycle (DUTRandomGenerator is a sequential circuit, it requires advancing via Step) dut.reset.value = 0 # Set reset signal to 0 dut.Step() # Advance one clock cycle for i in range(65536): # Loop 65536 times dut.Step() # Advance one clock cycle for the DUT, generating a random number ref.Step() # Advance one clock cycle for the reference model, generating a random number assert dut.random_number.value == ref.state, \"Mismatch\" # Compare the random numbers generated by the DUT and the reference model print(f\"Cycle {i}, DUT: {dut.random_number.value:x}, REF: {ref.state:x}\") # Print the results # Complete the test print(\"Test Passed\") dut.Finish() # Finish function will complete the writing of waveform, coverage, and other files Running the Test Program Execute python example.py in the picker_out_rmg directory to run the test program. After the execution, if Test Passed is output, the test is considered passed. After the run is complete, a waveform file RandomGenerator.fst will be generated, which can be viewed in the terminal using the following command:\ngtkwave RandomGenerator.fst Example output:\n··· Cycle 65529, DUT: d9ea, REF: d9ea Cycle 65530, DUT: b3d4, REF: b3d4 Cycle 65531, DUT: 67a9, REF: 67a9 Cycle 65532, DUT: cf53, REF: cf53 Cycle 65533, DUT: 9ea6, REF: 9ea6 Cycle 65534, DUT: 3d4d, REF: 3d4d Cycle 65535, DUT: 7a9a, REF: 7a9a Test Passed, destroy UT_RandomGenerator ","categories":["Example Projects","Tutorials"],"description":"Demonstrating the tool usage with a 16-bit LFSR random number generator, which includes a clock signal, sequential logic, and registers.","excerpt":"Demonstrating the tool usage with a 16-bit LFSR random number …","ref":"/mlvp/en/docs/quick-start/eg-rmg/","tags":["examples","docs"],"title":"Case 2: Random Number Generator"},{"body":" The Picker tool supports generating code line coverage reports, and the MLVP（https://github.com/XS-MLVP/mlvp）project supports generating functional coverage reports.\nCode Line Coverage Currently, the Picker tool supports generating code line coverage reports based on the Verilator simulator.\nVerilator The Verilator simulator provides coverage support.\nThe implementation is as follows:\nUse the verilator_coverage tool to process or merge coverage databases, ultimately generating a coverage.info file for multiple DUTs. Use the genhtml command of the lcov tool based on coverage.info and RTL code source files to generate a complete code coverage report. The process is as follows:\nEnable the COVERAGE feature when generating the DUT with Picker (add the -c option). After the simulator runs, a coverage database file V{DUT_NAME}.dat will be generated after dut.finalize() is called. Use the write-info function of verilator_coverage to convert it to a .info file. Use the genhtml function of lcov to generate an HTML report using the .info file and the RTL source files specified in the file. Note: The RTL source files specified in the file refer to the source file paths used when generating the DUT, and these paths need to be valid in the current environment. In simple terms, all .sv/.v files used for compilation need to exist in the current environment, and the directory remains unchanged.\nverilator_coverage The verilator_coverage tool is used to process coverage data generated by the DUT after running .dat files. The tool can process and merge multiple .dat files and has two main functions:\nGenerate a .info file based on the .dat file for subsequent generation of a web page report.\n-annotate \u003coutput_dir\u003e：Present the coverage situation in the source file in annotated form, and save the result to output_dir. The format is as follows:\n100000 input logic a; // Begins with whitespace, because // number of hits (100000) is above the limit. %000000 input logic b; // Begins with %, because // number of hits (0) is below the limit. -annotate-min \u003ccount\u003e：Specify the limit as count for the above.\nCombine the .dat file with the source code file, and write the coverage data in annotated form into the specified directory.\n-write \u003cmerged-datafile\u003e -read \u003cdatafiles\u003e：Merge several .dat (datafiles) files into one .dat file. -write-info \u003cmerged-info\u003e -read \u003cdatafiles\u003e：Merge several .dat (datafiles) files into one .info file. genhtml The genhtml provided by the lcov package can export a more readable HTML report from the .info file. The command format is: genhtml [OPTIONS] \u003cinfofiles\u003e. It is recommended to use the -o \u003coutputdir\u003e option to output the results to a specified directory.\nFor example, in theAddr project. Usage Example If you enable the -c option when using Picker, after the simulation ends, a V{DUT_NAME}.dat file will be generated. And there will be a Makefile in the top-level directory, which contains the command to generate the coverage report.\nThe command is as follows:\ncoverage: ... verilator_coverage -write-info coverage.info ./${TARGET}/V${PROJECT}_coverage.dat genhtml coverage.info --output-directory coverage ... Enter make coverage in the shell, which will generate coverage.info based on the generated .dat file and then use genhtml to generate an html report in the coverage directory.\nVCS Documentation for VCS is currently being finalized.\n","categories":["Sample Projects","Tutorials"],"description":"Coverage tools","excerpt":"Coverage tools","ref":"/mlvp/en/docs/env_usage/coverage/","tags":["examples","docs"],"title":"Coverage Statistics"},{"body":"Env is used in the mlvp verification environment to package the entire verification setup. It directly instantiates all the agents needed in the verification environment and is responsible for passing the required bundles to these agents. Once the Env is created, the specification for writing reference models is also determined. Reference models written according to this specification can be directly attached to the Env, allowing it to handle automatic synchronization of the reference models.\nCreating an Env To define an Env, you need to create a new class that inherits from the Env class in mlvp. Here’s a simple example of defining an Env:\nfrom mlvp.env import * class DualPortStackEnv(Env): def __init__(self, port1_bundle, port2_bundle): super().__init__() self.port1_agent = StackAgent(port1_bundle) self.port2_agent = StackAgent(port2_bundle) In this example, we define a DualPortStackEnv class that instantiates two identical StackAgent objects, each responsible for driving different Bundles. You can choose to connect the Bundles outside the Env or within the Env itself, as long as you ensure that the correct Bundles are passed to the Agents.\nAt this point, if you do not need to write additional reference models, the entire verification environment setup is complete, and you can directly write test cases using the interfaces provided by the Env. For example:\nport1_bundle = StackPortBundle() port2_bundle = StackPortBundle() env = DualPortStackEnv(port1_bundle, port2_bundle) await env.port1_agent.push(1) await env.port2_agent.push(1) print(await env.port1_agent.pop()) print(await env.port2_agent.pop()) Attaching Reference Models Once the Env is defined, the interfaces for the entire verification environment are also established, for example:\nDualPortStackEnv - port1_agent - @driver_method push - @driver_method pop - @monitor_method some_monitor - port2_agent - @driver_method push - @driver_method pop - @monitor_method some_monitor Reference models written according to this specification can be directly attached to the Env, allowing it to automatically synchronize the reference models. This can be done as follows:\nenv = DualPortStackEnv(port1_bundle, port2_bundle) env.attach(StackRefModel()) An Env can attach multiple reference models, all of which will be automatically synchronized by the Env.\nThe specific method for writing reference models will be detailed in the section on writing reference models.\n","categories":"","description":"","excerpt":"Env is used in the mlvp verification environment to package the entire …","ref":"/mlvp/en/docs/mlvp/env/build_env/","tags":"","title":"How to Build an Env"},{"body":"Writing test cases requires utilizing the interfaces defined in the verification environment. However, it is often necessary to drive multiple interfaces simultaneously in the test case, and there are often different synchronization needs with reference simulations. This section will provide a detailed explanation of how to better use the interfaces in the verification environment for writing test cases. Once the verification environment is set up, test cases are written to verify whether the design functions as expected. Two important aspects of hardware verification are functional coverage and line coverage . Functional coverage means whether the test cases cover all the functions of the design, while line coverage means whether the test cases trigger all lines of the design’s code. In mlvp, not only is support provided for both types of coverage, but after each run, the tool automatically calculates the results for both and generates a verification report. mlvp uses pytest to manage test cases, which provides powerful test case management capabilities. In this section, we will cover how to write test cases to take advantage of the powerful features provided by mlvp in the following areas:\nHow to use test environment interfaces for driving\nHow to manage test cases with pytest\nHow to add functional test points\n","categories":"","description":"","excerpt":"Writing test cases requires utilizing the interfaces defined in the …","ref":"/mlvp/en/docs/mlvp/cases/","tags":"","title":"Writing Test Cases"},{"body":"RTL源码 在本案例中，我们驱动一个随机数生成器，其源码如下：\nmodule RandomGenerator ( input wire clk, input wire reset, input [15:0] seed, output [15:0] random_number ); reg [15:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[14:0], lfsr[15] ^ lfsr[14]}; end end assign random_number = lfsr; endmodule 该随机数生成器包含一个 16 位的 LFSR，其输入为一个 16 位的种子数，输出为一个 16 位的随机数。LFSR 的更新规则为：\n将当前的 LFSR 的最高位与次高位异或，称为new_bit。 将原来的 LFSR 向左平移一位，将 new_bit 放在最低位。 丢弃最高位。 测试过程 在测试过程中，我们将创建一个名为 RandomGenerator 的文件夹，其中包含一个 RandomGenerator.v 文件。该文件内容即为上述的 RTL 源码。\n将RTL构建为 Python Module 生成中间文件 进入 RandomGenerator 文件夹，执行如下命令：\npicker export --autobuild=false RandomGenerator.v -w RandomGenerator.fst --sname RandomGenerator --tdir picker_out_rmg/ --lang python -e --sim verilator 该命令的含义是：\n将RandomGenerator.v作为 Top 文件，并将RandomGenerator作为 Top Module，基于 verilator 仿真器生成动态库，生成目标语言为 Python。 启用波形输出，目标波形文件为RandomGenerator.fst 包含用于驱动示例项目的文件(-e)，同时codegen完成后不自动编译(-autobuild=false)。 最终的文件输出路径是 picker_out_rmg 输出的目录类似加法器验证-生成中间文件，这里不再赘述。\n构建中间文件 进入 picker_out_rmg 目录并执行 make 命令，即可生成最终的文件。\n备注：其编译过程类似于 加法器验证-编译流程，这里不再赘述。\n最终目录结果为：\npicker_out_rmg └── RandomGenerator |-- RandomGenerator.fst # 测试的波形文件 |-- UT_RandomGenerator | |-- RandomGenerator.fst.hier | |-- _UT_RandomGenerator.so # Swig生成的wrapper动态库 | |-- __init__.py # Python Module的初始化文件，也是库的定义文件 | |-- libDPIRandomGenerator.a # 仿真器生成的库文件 | |-- libUTRandomGenerator.so # 基于dut_base生成的libDPI动态库封装 | `-- libUT_RandomGenerator.py # Swig生成的Python Module | `-- xspcomm # xspcomm基础库，固定文件夹，不需要关注 `-- example.py # 示例代码 配置测试代码 在picker_out_rmg中创建 example.py：\nfrom RandomGenerator import * import random # 定义参考模型 class LFSR_16: def __init__(self, seed): self.state = seed \u0026 ((1 \u003c\u003c 16) - 1) def Step(self): new_bit = (self.state \u003e\u003e 15) ^ (self.state \u003e\u003e 14) \u0026 1 self.state = ((self.state \u003c\u003c 1) | new_bit ) \u0026 ((1 \u003c\u003c 16) - 1) if __name__ == \"__main__\": dut = DUTRandomGenerator() # 创建DUT dut.InitClock(\"clk\") # 指定时钟引脚，初始化时钟 seed = random.randint(0, 2**16 - 1) # 生成随机种子 dut.seed.value = seed # 设置DUT种子 ref = LFSR_16(seed) # 创建参考模型用于对比 # reset DUT dut.reset.value = 1 # reset 信号置1 dut.Step() # 推进一个时钟周期（DUTRandomGenerator是时序电路，需要通过Step推进） dut.reset.value = 0 # reset 信号置0 dut.Step() # 推进一个时钟周期 for i in range(65536): # 循环65536次 dut.Step() # dut 推进一个时钟周期，生成随机数 ref.Step() # ref 推进一个时钟周期，生成随机数 assert dut.random_number.value == ref.state, \"Mismatch\" # 对比DUT和参考模型生成的随机数 print(f\"Cycle {i}, DUT: {dut.random_number.value:x}, REF: {ref.state:x}\") # 打印结果 # 完成测试 print(\"Test Passed\") dut.Finish() # Finish函数会完成波形、覆盖率等文件的写入 运行测试程序 在 picker_out_rmg 目录下执行 python example.py 即可运行测试程序。在运行完成后，若输出 Test Passed，则表示测试通过。完成运行后，会生成波形文件：RandomGenerator.fst，可在bash中通过以下命令进行查看。\ngtkwave RandomGenerator.fst\n输出示例：\n··· Cycle 65529, DUT: d9ea, REF: d9ea Cycle 65530, DUT: b3d4, REF: b3d4 Cycle 65531, DUT: 67a9, REF: 67a9 Cycle 65532, DUT: cf53, REF: cf53 Cycle 65533, DUT: 9ea6, REF: 9ea6 Cycle 65534, DUT: 3d4d, REF: 3d4d Cycle 65535, DUT: 7a9a, REF: 7a9a Test Passed, destroy UT_RandomGenerator ","categories":["示例项目","教程"],"description":"基于一个16bit的LFSR随机数生成器展示工具的用法，该随机数生成器内部存在时钟信号、时序逻辑与寄存器。","excerpt":"基于一个16bit的LFSR随机数生成器展示工具的用法，该随机数生成器内部存在时钟信号、时序逻辑与寄存器。","ref":"/mlvp/docs/quick-start/eg-rmg/","tags":["examples","docs"],"title":"案例二：随机数生成器"},{"body":"编写测试用例需要使用验证环境中定义好的接口来实现，但在用例中，往往会遇到同时驱动多个接口的情况，并且对于参考模拟的同步往往也有不同的需求，这一部分将详细介绍如何更好地使用验证环境中的接口来编写测试用例。\n当验证环境搭建完成后，编写测试用例用于验证设计的功能是否符合预期。对于硬件验证中的验证，两个重要的导向是：功能覆盖率和行覆盖率，功能覆盖率意味着测试用例是否覆盖了设计的所有功能，行覆盖率意味着测试用例是否触发了设计的所有代码行。在 toffee-test 中，不仅提供了对这两种覆盖率的支持，还会再每次运行过后，自动计算出这两种覆盖率的结果，并生成一个验证报告。toffee-test 使用 pytest 来管理测试用例，使其拥有强大的测试用例管理能力。\n在本节中，会在以下几个方面来讲述如何编写测试用例，以使用 toffee 和 toffee-test 提供的强大功能：\n如何使用测试环境接口进行驱动 如何使用 pytest 管理测试用例 如何添加功能测试点 ","categories":"","description":"","excerpt":"编写测试用例需要使用验证环境中定义好的接口来实现，但在用例中，往往会遇到同时驱动多个接口的情况，并且对于参考模拟的同步往往也有不同的需求，这 …","ref":"/mlvp/docs/mlvp/cases/","tags":"","title":"编写测试用例"},{"body":" Picker 工具支持生成代码行覆盖率报告，toffee-test（https://github.com/XS-MLVP/toffee-test） 项目支持生成功能覆盖率报告。\n代码行覆盖率 目前 Picker 工具支持基于 Verilator 仿真器生成的代码行覆盖率报告。\nVerilator Verilator 仿真器提供了覆盖率支持功能。\n该功能的实现方式是：\n利用 verilator_coverage 工具处理或合并覆盖率数据库，最终针对多个 DUT 生成一个 coverage.info 文件。 利用 lcov 工具的 genhtml 命令基于coverage.info和 RTL 代码源文件，生成完整的代码覆盖率报告。 使用时的流程如下：\n使用 Picker 生成 dut 时，使能 COVERAGE 功能 （添加-c选项）。 仿真器运行完成后，dut.finalize() 之后会生成覆盖率数据库文件 V{DUT_NAME}.dat。 基于 verilator_coverage 的 write-info 功能将其转换成 .info文件。 基于 lcov 的 genhtml 功能，使用.info文件和文件中指定的rtl 源文件，生成 html 报告。 注意： 文件中指定的rtl 源文件是指在生成的DUT时使用的源文件路径，需要保证这些路径在当前环境下是有效的。简单来说，需要编译时用到的所有.sv/.v文件都需要在当前环境下存在，并且目录不变。\nverilator_coverage verilator_coverage 工具用于处理 DUT 运行后生成的 .dat 的覆盖率数据。该工具可以处理并合并多个 .dat 文件，同时具有两类功能：\n基于 .dat 文件生成 .info 文件，用于后续生成网页报告。\n-annotate \u003coutput_dir\u003e：以标注的形式在源文件中呈现覆盖率情况，结果保存到output_dir中。形式如下：\n100000 input logic a; // Begins with whitespace, because // number of hits (100000) is above the limit. %000000 input logic b; // Begins with %, because // number of hits (0) is below the limit. -annotate-min \u003ccount\u003e：指定上述的 limit 为 count\n可以将 .dat 文件，结合源代码文件，将覆盖率数据以标注的形式与源代码结合在一起，并写入到指定目录。\n-write \u003cmerged-datafile\u003e -read \u003cdatafiles\u003e：将若干个.dat(datafiles)文件合并为一个.dat 文件 -write-info \u003cmerged-info\u003e -read \u003cdatafiles\u003e：将若干个.dat(datafiles)文件合并为一个.info 文件 genhtml 由 locv 包提供的 genhtml 可以由上述的.info 文件导出可读性更好的 html 报告。命令格式为：genhtml [OPTIONS] \u003cinfofiles\u003e。 建议使用-o \u003coutputdir\u003e选项将结果输出到指定目录。\n以加法器为例。\n使用示例 如果您使用 Picker 时打开了-c选项，那么在仿真结束后，会生成一个V{DUT_NAME}.dat文件。并且顶层目录会有一个 Makefile 文件，其中包含了生成覆盖率报告的命令。\n命令内容如下：\ncoverage: ... verilator_coverage -write-info coverage.info ./${TARGET}/V${PROJECT}_coverage.dat genhtml coverage.info --output-directory coverage ... 在 shell 中输入make coverage,其会根据生成的.dat 文件生成 coverage.info，再使用genhtml再 coverage 目录下生成 html 报告。\nVCS VCS 对应的文档正在完善当中。\n","categories":["示例项目","教程"],"description":"覆盖率工具","excerpt":"覆盖率工具","ref":"/mlvp/docs/env_usage/coverage/","tags":["examples","docs"],"title":"覆盖率统计"},{"body":"","categories":["示例项目","教程"],"description":"基于开放验证平台完成验证的复杂案例。","excerpt":"基于开放验证平台完成验证的复杂案例。","ref":"/mlvp/docs/advance_case/","tags":["examples","docs"],"title":"高级案例"},{"body":"Env 在 toffee 验证环境中用于打包整个验证环境，Env 中直接实例化了验证环境中需要用的所有 agent，并负责将这些 Agent 需要的 bundle 传递给它们。\n创建好 Env 后，参考模型的编写规范也随之确定，按照此规范编写的参考模型可直接附加到 Env 上，由 Env 来完成参考模型的自动同步。\n创建 Env 为了定义一个 Env，需要自定义一个新类，并继承 toffee 中的 Env 类。下面是一个简单的 Env 的定义示例：\nfrom toffee.env import * class DualPortStackEnv(Env): def __init__(self, port1_bundle, port2_bundle): super().__init__() self.port1_agent = StackAgent(port1_bundle) self.port2_agent = StackAgent(port2_bundle) 在这个例子中，我们定义了一个 DualPortStackEnv 类，该类中实例化了两个相同的 StackAgent，分别用于驱动两个不同的 Bundle。\n可以选择在 Env 之外连接 Bundle，也可以在 Env 内部连接 Bundle，只要能保证向 Agent 中传入正确的 Bundle 即可。\n此时，如果不需要编写额外的参考模型，那么整个验证环境的搭建就完成了，可以直接编写测试用例并且在测试用例中使用 Env 提供的接口，例如：\nport1_bundle = StackPortBundle() port2_bundle = StackPortBundle() env = DualPortStackEnv(port1_bundle, port2_bundle) await env.port1_agent.push(1) await env.port2_agent.push(1) print(await env.port1_agent.pop()) print(await env.port2_agent.pop()) 附加参考模型 定义好 Env 后，整个验证环境的接口也就随之确定，例如：\nDualPortStackEnv - port1_agent - @driver_method push - @driver_method pop - @monitor_method some_monitor - port2_agent - @driver_method push - @driver_method pop - @monitor_method some_monitor 按照此规范编写的参考模型都可以直接附加到 Env 上，由 Env 来完成参考模型的自动同步，方式如下：\nenv = DualPortStackEnv(port1_bundle, port2_bundle) env.attach(StackRefModel()) 一个 Env 可以附加多个参考模型，这些参考模型都将会被 Env 自动同步。\n参考模型的具体编写方式将在编写参考模型一节中详细介绍。\n","categories":"","description":"","excerpt":"Env 在 toffee 验证环境中用于打包整个验证环境，Env 中直接实例化了验证环境中需要用的所有 agent，并负责将这些 Agent …","ref":"/mlvp/docs/mlvp/env/build_env/","tags":"","title":"如何搭建 Env"},{"body":"Introduction to the Dual-Port Stack A dual-port stack is a data structure that supports simultaneous operations on two ports. Compared to a traditional single-port stack, a dual-port stack allows simultaneous read and write operations. In scenarios such as multithreaded concurrent read and write operations, the dual-port stack can provide better performance. In this example, we provide a simple dual-port stack implementation, with the source code as follows:\nmodule dual_port_stack ( input clk, input rst, // Interface 0 input in0_valid, output in0_ready, input [7:0] in0_data, input [1:0] in0_cmd, output out0_valid, input out0_ready, output [7:0] out0_data, output [1:0] out0_cmd, // Interface 1 input in1_valid, output in1_ready, input [7:0] in1_data, input [1:0] in1_cmd, output out1_valid, input out1_ready, output [7:0] out1_data, output [1:0] out1_cmd ); // Command definitions localparam CMD_PUSH = 2'b00; localparam CMD_POP = 2'b01; localparam CMD_PUSH_OKAY = 2'b10; localparam CMD_POP_OKAY = 2'b11; // Stack memory and pointer reg [7:0] stack_mem[0:255]; reg [7:0] sp; reg busy; reg [7:0] out0_data_reg, out1_data_reg; reg [1:0] out0_cmd_reg, out1_cmd_reg; reg out0_valid_reg, out1_valid_reg; assign out0_data = out0_data_reg; assign out0_cmd = out0_cmd_reg; assign out0_valid = out0_valid_reg; assign out1_data = out1_data_reg; assign out1_cmd = out1_cmd_reg; assign out1_valid = out1_valid_reg; always @(posedge clk or posedge rst) begin if (rst) begin sp \u003c= 0; busy \u003c= 0; end else begin // Interface 0 Request Handling if (!busy \u0026\u0026 in0_valid \u0026\u0026 in0_ready) begin case (in0_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out0_valid_reg \u003c= 1; stack_mem[sp] \u003c= in0_data; out0_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out0_valid_reg \u003c= 1; out0_data_reg \u003c= stack_mem[sp - 1]; out0_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out0_valid_reg \u003c= 0; end endcase end // Interface 1 Request Handling if (!busy \u0026\u0026 in1_valid \u0026\u0026 in1_ready) begin case (in1_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out1_valid_reg \u003c= 1; stack_mem[sp] \u003c= in1_data; out1_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out1_valid_reg \u003c= 1; out1_data_reg \u003c= stack_mem[sp - 1]; out1_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out1_valid_reg \u003c= 0; end endcase end // Interface 0 Response Handling if (busy \u0026\u0026 out0_ready) begin out0_valid_reg \u003c= 0; busy \u003c= 0; end // Interface 1 Response Handling if (busy \u0026\u0026 out1_ready) begin out1_valid_reg \u003c= 0; busy \u003c= 0; end end end assign in0_ready = (in0_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255 || in0_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy; assign in1_ready = (in1_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255 || in1_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy \u0026\u0026 !(in0_ready \u0026\u0026 in0_valid); endmodule In this implementation, aside from the clock signal (clk) and reset signal (rst), there are also input and output signals for the two ports, which have the same interface definition. The meaning of each signal for the ports is as follows:\nRequest Port (in)\nin_valid: Input data valid signal\nin_ready: Input data ready signal\nin_data: Input data\nin_cmd: Input command (0: PUSH, 1: POP)\nResponse Port (out)\nout_valid: Output data valid signal\nout_ready: Output data ready signal\nout_data: Output data\nout_cmd: Output command (2: PUSH_OKAY, 3: POP_OKAY)\nWhen we want to perform an operation on the stack through a port, we first need to write the required data and command to the input port, and then wait for the output port to return the result. Specifically, if we want to perform a PUSH operation on the stack, we should first write the data to be pushed into in_data, then set in_cmd to 0, indicating a PUSH operation, and set in_valid to 1, indicating that the input data is valid. Next, we need to wait for in_ready to be 1, ensuring that the data has been correctly received, at which point the PUSH request has been correctly sent.After the command is successfully sent, we need to wait for the stack’s response information on the response port. When out_valid is 1, it indicates that the stack has completed the corresponding operation. At this point, we can read the stack’s returned data from out_data (the returned data of the POP operation will be placed here) and read the stack’s returned command from out_cmd. After reading the data, we need to set out_ready to 1 to notify the stack that the returned information has been correctly received. If requests from both ports are valid simultaneously, the stack will prioritize processing requests from port 0.\nSetting Up the Driver Environment Similar to Case Study 1 and Case Study 2, before testing the dual-port stack, we first need to use the Picker tool to build the RTL code into a Python Module. After the build is complete, we will use a Python script to drive the RTL code for testing. First, create a file named dual_port_stack.v and copy the above RTL code into this file. Then, execute the following command in the same folder:\npicker export --autobuild=true dual_port_stack.v -w dual_port_stack.fst --sname dual_port_stack --tdir picker_out_dual_port_stack --lang python -e --sim verilator The generated driver environment is located in the picker_out_dual_port_stack folder. Inside, UT_dual_port_stack is the generated Python Module, and example.py is the test script. You can run the test script with the following commands:\ncd picker_out_dual_port_stack python3 example.py If no errors occur during the run, it means the environment has been set up correctly.\nDriving the DUT with Coroutines Driving the DUT with Callback Functions In this case, we need to drive a dual-port stack to test its functionality. However, you may quickly realize that the methods used in Cases 1 and 2 are insufficient for driving a dual-port stack. In the previous tests, the DUT had a single execution logic where you input data into the DUT and wait for the output.\nHowever, a dual-port stack is different because its two ports operate with independent execution logic. During the drive process, these two ports might be in entirely different states. For example, while port 0 is waiting for data from the DUT, port 1 might be sending a new request. In such situations, simple sequential execution logic will struggle to drive the DUT effectively.\nTherefore, in this case, we will use the dual-port stack as an example to introduce a callback function-based driving method to handle such DUTs.\nIntroduction to Callback Functions A callback function is a common programming technique that allows us to pass a function as an argument, which is then called when a certain condition is met. In the generated Python Module, we provide an interface StepRis for registering callback functions with the internal execution environment. Here’s how it works:\nfrom UT_dual_port_stack import DUTdual_port_stack def callback(cycles): print(f\"The current clock cycle is {cycles}\") dut = DUTdual_port_stack() dut.StepRis(callback) dut.Step(10) You can run this code directly to see the effect of the callback function. In the above code, we define a callback function callback that takes a cycles parameter and prints the current clock cycle each time it is called. We then register this callback function to the DUT via StepRis.Once the callback function is registered, each time the Step function is run, which corresponds to each clock cycle, the callback function is invoked on the rising edge of the clock signal, with the current clock cycle passed as an argument. Using this approach, we can write different execution logics as callback functions and register multiple callback functions to the DUT, thereby achieving parallel driving of the DUT.\nDual-Port Stack Driven by Callback Functions To complete a full execution logic using callback functions, we typically write it in the form of a state machine. Each callback function invocation triggers a state change within the state machine, and multiple invocations complete a full execution logic.\nBelow is an example code for driving a dual-port stack using callback functions:\nimport random from UT_dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class Status(Enum): IDLE = 0 WAIT_REQ_READY = 1 WAIT_RESP_VALID = 2 class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict self.status = self.Status.IDLE self.operation_num = 0 self.remaining_delay = 0 def push(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value self.port_dict[\"in_data\"].value = random.randint(0, 2**32-1) def pop(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.POP.value def step_callback(self, cycle): if self.status == self.Status.WAIT_REQ_READY: if self.port_dict[\"in_ready\"].value == 1: self.port_dict[\"in_valid\"].value = 0 self.port_dict[\"out_ready\"].value = 1 self.status = self.Status.WAIT_RESP_VALID if self.port_dict[\"in_cmd\"].value == self.BusCMD.PUSH.value: self.model.commit_push(self.port_dict[\"in_data\"].value) elif self.status == self.Status.WAIT_RESP_VALID: if self.port_dict[\"out_valid\"].value == 1: self.port_dict[\"out_ready\"].value = 0 self.status = self.Status.IDLE self.remaining_delay = random.randint(0, 5) if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) if self.status == self.Status.IDLE: if self.remaining_delay == 0: if self.operation_num \u003c 10: self.push() elif self.operation_num \u003c 20: self.pop() else: return self.operation_num += 1 self.status = self.Status.WAIT_REQ_READY else: self.remaining_delay -= 1 def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) dut.StepRis(port0.step_callback) dut.StepRis(port1.step_callback) dut.Step(200) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") test_stack(dut) dut.Finish() In the code above, the driving process is implemented such that each port independently drives the DUT, with a random delay added after each request is completed. Each port performs 10 PUSH operations and 10 POP operations.When a PUSH or POP request takes effect, the corresponding commit_push or commit_pop function in the StackModel is called to simulate stack behavior. After each POP operation, the data returned by the DUT is compared with the model’s data to ensure consistency.To implement the driving behavior for a single port, we created the SinglePortDriver class, which includes a method for sending and receiving data. The step_callback function handles the internal update logic.In the test_stack function, we create a SinglePortDriver instance for each port of the dual-port stack, pass the corresponding interfaces, and register the callback function to the DUT using the StepRis function. When dut.Step(200) is called, the callback function is automatically invoked each clock cycle to complete the entire driving logic.SinglePortDriver Driving Logic As mentioned earlier, callback functions typically require the execution logic to be implemented as a state machine. Therefore, in the SinglePortDriver class, the status of each port is recorded, including:\nIDLE: Idle state, waiting for the next operation.\nIn the idle state, check the remaining_delay status to determine whether the current delay has ended. If the delay has ended, proceed with the next operation; otherwise, continue waiting.\nWhen the next operation is ready, check the operation_num status (the number of operations already performed) to determine whether the next operation should be PUSH or POP. Then, call the corresponding function to assign values to the port and switch the status to WAIT_REQ_READY.\nWAIT_REQ_READY: Waiting for the request port to be ready.\nAfter the request is sent (in_valid is valid), wait for the in_ready signal to be valid to ensure the request has been correctly received.\nOnce the request is correctly received, set in_valid to 0 and out_ready to 1, indicating the request is complete and ready to receive a response.\nWAIT_RESP_VALID: Waiting for the response port to return data.\nAfter the request is correctly received, wait for the DUT’s response, i.e., wait for the out_valid signal to be valid. When the out_valid signal is valid, it indicates that the response has been generated and the request is complete. Set out_ready to 0 and switch the status to IDLE. Running the Test Copy the above code into example.py, and then run the following command:\ncd picker_out_dual_port_stack python3 example.py You can run the test code for this case directly, and you will see output similar to the following:\n... push 77 push 140 push 249 push 68 push 104 push 222 ... Pop 43 Pass: 43 == 43 Pop 211 Pass: 211 == 211 Pop 16 Pass: 16 == 16 Pop 255 Pass: 255 == 255 Pop 222 Pass: 222 == 222 Pop 104 ... In the output, you can see the data for each PUSH and POP operation, as well as the result of each POP operation. If there is no error message in the output, it indicates that the test has passed.\nPros and Cons of Callback-Driven Design By using callbacks, we can achieve parallel driving of the DUT, as demonstrated in this example. We utilized two callbacks to drive two ports with independent execution logic. In simple scenarios, callbacks offer a straightforward method for parallel driving.\nHowever, as shown in this example, even implementing a simple “request-response” flow requires maintaining a significant amount of internal state. Callbacks break down what should be a cohesive execution logic into multiple function calls, adding considerable complexity to both the code writing and debugging processes.\n","categories":["Example Projects","Tutorials"],"description":"A dual-port stack is a stack with two ports, each supporting push and pop operations. This case study uses a dual-port stack as an example to demonstrate how to use callback functions to drive the DUT.","excerpt":"A dual-port stack is a stack with two ports, each supporting push and …","ref":"/mlvp/en/docs/quick-start/eg-stack-callback/","tags":["examples","docs"],"title":"Case 3: Dual-Port Stack (Callback)"},{"body":" This section introduces the general process of verifying a DUT based on Picker.\nThe goal of the open verification platform is functional verification, which generally involves the following steps:\n1. Determine the verification object and goals Typically, the design documentation of the DUT is also delivered to the verification engineer. At this point, you need to read the documentation or source code to understand the basic functions, main structure, and expected functionalities of the verification object.\n2. Build the basic verification environment After fully understanding the design, you need to build the basic verification environment. For example, in addition to the DUT generated by Picker, you may also need to set up a reference model for comparison and a signal monitoring platform for evaluating subsequent functional points.\n3. Decompose functional points and test points Before officially starting the verification, you need to extract the functional points and further decompose them into test points. You can refer to: CSDN: Chip Verification Series - Decomposition of Testpoints\n4. Construct test cases With the test points, you need to construct test cases to cover the corresponding test points. A test case may cover multiple test points.\n5. Collect test results After running all the test cases, you need to summarize all the test results. Generally, this includes line coverage and functional coverage. The former can be obtained through the coverage function provided by the Picker tool, while the latter requires you to judge whether a function is covered by the test cases through monitoring the behavior of the DUT.\n6. Evaluate the test results Finally, you need to evaluate the obtained results, such as whether there are design errors, whether a function cannot be triggered, whether the design documentation description is consistent with the DUT behavior, and whether the design documentation is clearly described.\nNext, we will introduce the general verification process usingMMIO read and write of Nutshell Cache as an example:\n1 Determine the verification object and goals:：\nThe MMIO read and write function of the Nutshell Cache. MMIO is a special type of IO mapping that supports accessing IO device registers by accessing memory addresses. Since the register state of IO devices can change at any time, it is not suitable to cache it. When receiving an MMIO request, the Nutshell cache will directly access the MMIO memory area to read or write data instead of querying hit/miss in the ordinary cache line.\n2 Build the basic verification environment:：\nWe can roughly divide the verification environment into five parts: 1. Testcase Driver：Responsible for generating corresponding signals driven by test cases 2. Monitor：Monitors signals to determine whether functions are covered and correct 3. Ref Cache：A simple reference model 4. Memory/MMIO Ram：Simulates peripheral devices to simulate corresponding cache requests 5. Nutshell Cache Dut：DUT generated by Picker\nIn addition, you may need to further encapsulate the DUT interface to achieve more convenient read and write request operations. For details, refer to Nutshll cachewrapper.\n3 Decompose functional points and test points：\nNutshell cache can respond to MMIO requests, further decomposing into the following test points:\nTest Point 1：MMIO requests will be forwarded to the MMIO port Test Point 2：The cache will not issue burst transfer requests when responding to MMIO requests Test Point 3：The cache will block the pipeline when responding to MMIO requests\n4 Construct test cases： The construction of test cases is simple. Knowing that the MMIO address range of the Nutshell cache obtained through Creating DUTis 0x30000000~0x7fffffff, we only need to access this memory range to obtain the expected MMIO results. Note that to trigger the test point of blocking the pipeline, you may need to initiate requests continuously.\nHere is a simple test case:\n# import CacheWrapper here def mmio_test(cache: CacheWrapper): mmio_lb\t= 0x30000000 mmio_rb\t= 0x30001000 print(\"\\n[MMIO Test]: Start MMIO Serial Test\") for addr in range(mmio_lb, mmio_rb, 16): addr \u0026= ~(0xf) addr1 = addr addr2 = addr + 4 addr3 = addr + 8 cache.trigger_read_req(addr1) cache.trigger_read_req(addr2) cache.trigger_read_req(addr3) cache.recv() cache.recv() cache.recv() print(\"[MMIO Test]: Finish MMIO Serial Test\") 5 Collect test results：\n''' In tb_cache.py ''' # import packages here class TestCache(): def setup_class(self): color.print_blue(\"\\nCache Test Start\") self.dut = DUTCache(\"libDPICache.so\") self.dut.init_clock(\"clock\") # Init here # ... self.testlist = [\"mmio_serial\"] def teardown_class(self): self.dut.finalize() color.print_blue(\"\\nCache Test End\") def __reset(self): # Reset cache and devices # MMIO Test def test_mmio(self): if (\"mmio_serial\" in self.testlist): # Run test from ..test.test_mmio import mmio_test mmio_test(self.cache, self.ref_cache) else: print(\"\\nmmio test is not included\") def run(self): self.setup_class() # test self.test_mmio() self.teardown_class() pass if __name__ == \"__main__\": tb = TestCache() tb.run() Run：\npython3 tb_cache.py The above is only a rough execution process, for details refer to：Nutshell Cache Verify。\n6 Evaluate the running results\nAfter the run is complete, the following data can be obtained: Line coverage: Functional coverage: It can be seen that the preset MMIO functions are all covered and correctly triggered.\n","categories":["Example Projects","Learning Materials"],"description":"Overview of the general verification process","excerpt":"Overview of the general verification process","ref":"/mlvp/en/docs/basic/test_dut/","tags":["examples","docs"],"title":"DUT Verification"},{"body":" 本节介绍基于Picker验证DUT的一般流程\n开放验证平台的目标是功能性验证，其一般有以下步骤：\n1. 确定验证对象和目标 通常来说，同时交付给验证工程师的还有DUT的设计文档。此时您需要阅读文档或者源代码，了解验证对象的基本功能、主体结构以及预期功能。\n2. 构建基本验证环境 充分了解设计之后，您需要构建验证的基本环境。例如，除了由Picker生成的DUT外，您可能还需要搭建用于比对的参考模型，也可能需要为后续功能点的评测搭建信号的监听平台。\n3. 功能点与测试点分解 在正式开始验证之前，您还需要提取功能点，并将其进一步分解成测试点。提取和分解方法可以参考：CSDN:芯片验证系列——Testpoints分解\n4. 构造测试用例 有了测试点之后，您需要构造测试用例来覆盖相应的测试点。一个用例可能覆盖多个测试点。\n5. 收集测试结果 运行完所有的测试用例之后，您需要汇总所有的测试结果。一般来说包括代码行覆盖率以及功能覆盖率。前者可以通过Picker工具提供的覆盖率功能获得，后者则需要您通过监听DUT的行为判断某功能是否被用例覆盖到。\n6. 评估测试结果 最后您需要评估得到的结果，如是否存在错误的设计、某功能是否无法被触发、设计文档表述是否与DUT行为一致、设计文档是否表述清晰等。\n接下来我们以果壳Cache的MMIO读写为例，介绍一般验证流程：\n1 确定验证对象和目标：\n果壳Cache的MMIO读写功能。MMIO是一类特殊的IO映射，其支持通过访问内存地址的方式访问IO设备寄存器。由于IO设备的寄存器状态是随时可能改变的，因此不适合将其缓存在cache中。当收到MMIO请求时，果壳cache不会在普通的cache行中查询命中/缺失情况，而是会直接访问MMIO的内存区域来读取或者写入数据。\n2 构建基本验证环境：\n我们可以将验证环境大致分为五个部分：\n1. Testcase Driver：负责由用例产生相应的信号驱动\n2. Monitor：监听信号，判断功能是否被覆盖以及功能是否正确\n3. Ref Cache：一个简单的参考模型\n4. Memory/MMIO Ram：外围设备的模拟，用于模拟相应cache的请求\n5. Nutshell Cache Dut：由Picker生成的DUT\n此外，您可能还需要对DUT的接口做进一步封装以实现更方便的读写请求操作，具体可以参考Nutshll cachewrapper。\n3 功能点与测试点分解：\n果壳cache可以响应MMIO请求，进一步分解可以得到一下测试点：\n测试点1：MMIO请求会被转发到MMIO端口上\n测试点2：cache响应MMIO请求时，不会发出突发传输（Burst Transfer）的请求\n测试点3：cache响应MMIO请求时，会阻塞流水线\n4 构造测试用例： 测试用例的构造是简单的，已知通过创建DUT得到的Nutshell cache的MMIO地址范围是0x30000000~0x7fffffff，则我们只需访问这段内存区间，应当就能获得MMIO的预期结果。需要注意的是，为了触发阻塞流水线的测试点，您可能需要连续地发起请求。\n以下是一个简单的测试用例：\n# import CacheWrapper here def mmio_test(cache: CacheWrapper): mmio_lb\t= 0x30000000 mmio_rb\t= 0x30001000 print(\"\\n[MMIO Test]: Start MMIO Serial Test\") for addr in range(mmio_lb, mmio_rb, 16): addr \u0026= ~(0xf) addr1 = addr addr2 = addr + 4 addr3 = addr + 8 cache.trigger_read_req(addr1) cache.trigger_read_req(addr2) cache.trigger_read_req(addr3) cache.recv() cache.recv() cache.recv() print(\"[MMIO Test]: Finish MMIO Serial Test\") 5 收集测试结果：\n''' In tb_cache.py ''' # import packages here class TestCache(): def setup_class(self): color.print_blue(\"\\nCache Test Start\") self.dut = DUTCache(\"libDPICache.so\") self.dut.init_clock(\"clock\") # Init here # ... self.testlist = [\"mmio_serial\"] def teardown_class(self): self.dut.finalize() color.print_blue(\"\\nCache Test End\") def __reset(self): # Reset cache and devices # MMIO Test def test_mmio(self): if (\"mmio_serial\" in self.testlist): # Run test from ..test.test_mmio import mmio_test mmio_test(self.cache, self.ref_cache) else: print(\"\\nmmio test is not included\") def run(self): self.setup_class() # test self.test_mmio() self.teardown_class() pass if __name__ == \"__main__\": tb = TestCache() tb.run() 运行：\npython3 tb_cache.py 以上仅为大致的运行流程，具体可以参考：Nutshell Cache Verify。\n6 评估运行结果\n运行结束之后可以得到以下数据：\n行覆盖率：\n功能覆盖率：\n可以看到预设的MMIO功能均被覆盖且被正确触发。\n","categories":["示例项目","学习材料"],"description":"介绍验证的一般流程","excerpt":"介绍验证的一般流程","ref":"/mlvp/docs/basic/test_dut/","tags":["examples","docs"],"title":"DUT验证"},{"body":"A reference model is used to simulate the behavior of the design under verification, aiding in the validation process. In the mlvp verification environment, the reference model needs to follow the Env interface specifications so it can be attached to Env, allowing automatic synchronization by Env.\nTwo Ways to Implement a Reference Model mlvp provides two methods for implementing a reference model, both of which can be attached to Env for automatic synchronization. Depending on the scenario, you can choose the most suitable method for your reference model implementation.These two methods are function call mode and independent execution flow mode . Below, we will introduce both concepts in detail.\nFunction Call Mode Function call mode defines the reference model’s external interface as a series of functions, driving the reference model’s behavior by calling these functions. In this mode, data is passed to the reference model through input parameters, and the model’s output data is retrieved through return values. The internal state of the reference model is updated through the logic within the function body. Here is a simple example of a reference model implemented in function call mode:\nFor instance, this is a simple reference model of an adder:\nclass AdderRefModel(): def add(self, a, b): return a + b In this reference model, there is no need for any internal state. All functionalities are handled through a single external function interface.\nNote that reference models written in function call mode can only be executed through external function calls and cannot output internal data passively. As a result, they cannot be matched with the monitoring methods in an Agent. Writing monitoring methods in the Agent is meaningless when using a reference model written in function call mode.\nIndependent Execution Flow Mode Independent execution flow mode defines the reference model’s behavior as an independent execution flow. Instead of being controlled by external function calls, the reference model can actively fetch input data and output data. When external data is sent to the reference model, it does not respond immediately. Instead, it stores the data and waits for its logic to actively retrieve and process the data. Here is a code snippet that demonstrates this mode using concepts provided by mlvp, though understanding these concepts in detail is not required at the moment.\nclass AdderRefModel(Model): def __init__(self): super().__init__() self.add_port = DriverPort() self.sum_port = MonitorPort() async def main(): while True: operands = await self.add_port() sum = operands[\"a\"] + operands[\"b\"] await self.sum_port(sum) In this example, two types of interfaces are defined in the constructor of the reference model: a driver interface (DriverPort) , represented by add_port, which receives external input data, and a monitoring interface (MonitorPort) , represented by sum_port, which outputs data to the external environment.Once these interfaces are defined, the reference model does not trigger a specific function when data is sent to it. Instead, the data is sent to the add_port driver interface. At the same time, external code cannot proactively retrieve output data from the reference model. The model will actively output the result data via the sum_port monitoring interface.How does the reference model utilize these interfaces? The reference model has a main function, which is its execution entry point. When the reference model is created, the main function is automatically called and runs continuously in the background. In the code above, the main function continuously waits for data from the add_port, computes the result, and outputs the result to the sum_port.The reference model actively requests data from the add_port, and if there is no data, it waits for new data. Once data arrives, it processes the data and proactively outputs the result to the sum_port. This execution flow operates independently and is not controlled by external function calls. When the reference model becomes more complex, with multiple driver and monitoring interfaces, the independent execution flow is particularly useful for handling interactions, especially when the interfaces have a specific call order.\nHow to Write a Function Call Mode Reference Model Driver Function Matching Suppose the following interface is defined in the Env:\nStackEnv - port_agent - @driver_method push - @driver_method pop If you want to write a reference model that corresponds to this interface, you need to define the behavior of the reference model for each driver function. For each driver function, write a corresponding function in the reference model that will be automatically called when the driver function is invoked. To match a function in the reference model with a specific driver function, you should use the @driver_hook decorator to indicate that the function is a match for a driver function. Then, specify the corresponding Agent and driver function name in the decorator. Finally, ensure that the function parameters match those of the driver function, and the two will be linked.\nclass StackRefModel(Model): @driver_hook(agent_name=\"port_agent\", driver_name=\"push\") def push(self, data): pass @driver_hook(agent_name=\"port_agent\", driver_name=\"pop\") def pop(self): pass At this point, the driver function is linked with the reference model function. When a driver function in the Env is called, the corresponding reference model function will be automatically invoked, and their return values will be compared.\nmlvp also provides several matching methods to improve flexibility: Specify the Driver Function Path You can specify the driver function path using a “.”. For example:\nclass StackRefModel(Model): @driver_hook(\"port_agent.push\") def push(self, data): pass @driver_hook(\"port_agent.pop\") def pop(self): pass Match Driver Function Name with Function Name If the reference model function name is the same as the driver function name, you can omit the driver_name parameter:\nclass StackRefModel(Model): @driver_hook(agent_name=\"port_agent\") def push(self, data): pass @driver_hook(agent_name=\"port_agent\") def pop(self): pass Match Both Agent and Driver Function Names By using a double underscore __, you can match both the Agent and the driver function names:\nclass StackRefModel(Model): @driver_hook() def port_agent__push(self, data): pass @driver_hook() def port_agent__pop(self): pass Agent Matching Instead of writing a separate driver_hook for each driver function in the Agent, you can use the @agent_hook decorator to match all the driver functions in an Agent at once.\nclass StackRefModel(Model): @agent_hook(\"port_agent\") def port_agent(self, driver_name, args): pass In this example, the port_agent function will match all the driver functions in the port_agent Agent. When any driver function in the Agent is called, the port_agent function will be invoked automatically. Besides self, the port_agent function should take exactly two parameters: the first is the name of the driver function, and the second is the arguments passed to the driver function.When a driver function is called, the driver_name parameter will receive the name of the driver function, and the args parameter will receive the arguments passed during the call, represented as a dictionary. The port_agent function can then decide how to handle the driver function call based on driver_name and args and return the result. The framework will automatically compare the return value of this function with that of the driver function.Similar to driver functions, the @agent_hook decorator allows you to omit the agent_name parameter when the function name matches the Agent name.\nclass StackRefModel(Model): @agent_hook() def port_agent(self, driver_name, args): pass Using Both agent_hook and driver_hook Once an agent_hook is defined, in theory, there is no need to define any driver_hook to match driver functions in the Agent. However, if special handling is needed for a specific driver function, a driver_hook can still be defined to match that driver function.When both agent_hook and driver_hook are present, the framework will first call the agent_hook function, followed by the driver_hook function. The result of the driver_hook function will be used for comparison.Once all the driver functions in the Env have corresponding driver_hook or agent_hook matches, the reference model can be attached to the Env using the attach method.\nHow to Write an Independent Execution Flow Reference Model An independent execution flow reference model handles input and output through port interfaces, where it can actively request or send data. In mlvp, two types of interfaces are provided for this purpose: DriverPort and MonitorPort.Similarly, a series of DriverPort objects can be defined to match the driver functions in the Env, and a series of MonitorPort objects can be defined to match the monitor functions.When a driver function in the Env is called, the data from the call is sent to the DriverPort. The reference model will actively fetch this data, perform calculations, and output the result to the MonitorPort. When a monitor function in the Env is called, the comparator will automatically retrieve the data from the MonitorPort and compare it with the return value of the monitor function.\nDriver Method Interface Matching To receive all driver function calls from the Env, the reference model can define a corresponding DriverPort for each driver function. The DriverPort parameters agent_name and driver_name are used to match the driver functions in the Env.\nclass StackRefModel(Model): def __init__(self): super().__init__() self.push_port = DriverPort(agent_name=\"port_agent\", driver_name=\"push\") self.pop_port = DriverPort(agent_name=\"port_agent\", driver_name=\"pop\") Similar to driver_hook, you can also match the driver functions in the Env in the following ways:\n# Specify the driver function path using \".\" self.push_port = DriverPort(\"port_agent.push\") # If the variable name in the reference model matches the driver function name, you can omit the driver_name parameter self.push = DriverPort(agent_name=\"port_agent\") # Match both the Agent name and driver function name using `__` to separate them self.port_agent__push = DriverPort() Agent Interface Matching You can also define an AgentPort to match all driver functions in an Agent. Unlike agent_hook, once an AgentPort is defined, no DriverPort can be defined for any driver function in that Agent. All driver function calls will be sent to the AgentPort.\nclass StackRefModel(Model): def __init__(self): super().__init__() self.port_agent = AgentPort(agent_name=\"port_agent\") Similarly, when the variable name matches the Agent name, you can omit the agent_name parameter:\nself.port_agent = AgentPort() Monitor Method Interface Matching To match the monitor functions in the Env, the reference model needs to define a corresponding MonitorPort for each monitor function. The definition method is the same as for DriverPort.\nself.monitor_port = MonitorPort(agent_name=\"port_agent\", monitor_name=\"monitor\") # Specify the monitor function path using \".\" self.monitor_port = MonitorPort(\"port_agent.monitor\") # If the variable name in the reference model matches the monitor function name, you can omit the monitor_name parameter self.monitor = MonitorPort(agent_name=\"port_agent\") # Match both the Agent name and monitor function name using `__` to separate them self.port_agent__monitor = MonitorPort() The data sent to the MonitorPort will automatically be compared with the return value of the corresponding monitor function in the Env.Once all DriverPort, AgentPort, and MonitorPort definitions in the reference model successfully match the interfaces in the Env, the reference model can be attached to the Env using the attach method.\n","categories":"","description":"","excerpt":"A reference model is used to simulate the behavior of the design under …","ref":"/mlvp/en/docs/mlvp/env/ref_model/","tags":"","title":"How to Write a Reference Model"},{"body":" In traditional chip verification practices, frameworks like UVM are widely adopted. Although they provide a comprehensive set of verification methodologies, they are typically confined to specific hardware description languages and simulation environments. Our tool breaks these limitations by converting simulation code into C++ or Python, allowing us to leverage software verification tools for more comprehensive testing. Given Python’s robust ecosystem, this project primarily uses Python as an example, briefly introducing two classic software testing frameworks: Pytest and Hypothesis. Pytest handles various testing needs with its simple syntax and rich features. Meanwhile, Hypothesis enhances the thoroughness and depth of testing by generating test cases that uncover unexpected edge cases. Our project is designed from the outset to be compatible with various modern software testing frameworks. We encourage you to explore the potential of these tools and apply them to your testing processes. Through hands-on practice, you will gain a deeper understanding of how these tools can enhance code quality and reliability. Let’s work together to improve the quality of chip development.\n","categories":["Sample Projects","Tutorials"],"description":"Available Software Testing Frameworks","excerpt":"Available Software Testing Frameworks","ref":"/mlvp/en/docs/env_usage/frameworks/","tags":["examples","docs"],"title":"Integrated Testing Framework"},{"body":"With mlvp, you can now set up a complete verification environment and conveniently write test cases. However, in real-world scenarios, it can be challenging to understand how to get started and ultimately complete a verification task. After writing code, common issues may include difficulties in correctly partitioning the Bundle, misunderstanding the high-level semantic encapsulation of the Agent, and not knowing what to do after setting up the environment.\nIn this section, we will introduce how to complete a new verification task from scratch and how to use mlvp effectively to accomplish it.\n1. Understanding the Design Under Test (DUT) When you first encounter a new design, you may face dozens or even hundreds of input and output signals, which can be overwhelming. At this point, you must trust that these signals are defined by the design engineers, and by understanding the functionality of the design, you can infer the meaning of these signals.\nIf the design team provides documentation, you can read it to understand the functionality of the design and map the functions to the input and output signals. You should also gain a clear understanding of the signal timing and how to use these signals to drive the design. Generally, you will also need to review the design’s source code to uncover more detailed timing issues.\nOnce you have a basic understanding of the DUT’s functionality and how to drive its interface, you can start building the verification environment.\n2. Partitioning the Bundle The first step in setting up the environment is to logically partition the interface into several sets, with each set of interfaces considered as a Bundle. Each Bundle should be driven by an independent Agent.\nHowever, in practice, interfaces may appear like this:\n|---------------------- DUT Bundle -------------------------------| |------- Bundle 1 ------| |------ Bundle 2 ------| |-- Bundle 3 --| |-- B1.1 --| |-- B1.2 --| |-- B2.1 --| This raises the question: should B1.1 and B1.2 each have their own Agent, or should a single Agent be created for Bundle 1?\nThe answer depends on the logic of the interface. If a request requires simultaneous operations on both B1.1 and B1.2, then you should create one Agent for Bundle 1 rather than creating separate Agents for B1.1 and B1.2.\nThat said, creating individual Agents for B1.1 and B1.2 is also feasible. This increases the granularity of the Agent but sacrifices operational continuity, making the upper-level code and reference model more complex. Therefore, the appropriate granularity depends on the specific use case. In the end, all Agents combined should cover the entire DUT Bundle interface. In practice, for convenience in connecting the DUT, you can define a DUT Bundle that connects all interfaces to this Bundle at once, and then the Env can distribute the sub-Bundles to their respective Agents.\n3. Writing the Agent After partitioning the Bundle, you can start writing the Agents to drive them. You need to write an Agent for each Bundle.\nFirst, you can begin by writing the driver methods, which are high-level semantic encapsulations of the Bundle. These high-level semantic details should carry all the information necessary to drive the Bundle. If a signal in the Bundle requires a value but the method parameters don’t provide the corresponding information, then the encapsulation is incomplete. Avoid assuming any signal values within the driver methods; otherwise, the DUT’s output will be affected by these assumptions, potentially causing discrepancies between the reference model and the DUT.\nThis high-level encapsulation also defines the functionality of the reference model, which interacts directly with the high-level semantic information, not with the low-level signals.\nIf the reference model is written using function-call mode, the DUT’s outputs should be returned through function return values. If the reference model uses a separate execution flow, you should write monitoring methods that convert the DUT’s outputs into high-level semantic information and output them via the monitoring methods.\n4. Encapsulating into Env Once all the Agents are written or selected from existing Agents, you can encapsulate them into the Env.Env encapsulates the entire verification environment and defines the writing conventions for the reference model.\n5. Writing the Reference Model Writing the reference model doesn’t need to wait until the Env is complete—it can be done alongside the Agent development, with some driving code written in real-time to verify correctness. Of course, if the Agent is well-structured, writing the reference model after the complete Env is created is also feasible. The most important part of the reference model is choosing the appropriate mode—both function-call mode and separate execution flow mode are viable, but the selection depends on the specific use case.\n6. Identifying Functional and Test Points After writing the Env and reference model, you cannot immediately start writing test cases because there is no direction yet for writing them. Blindly writing test cases won’t ensure complete verification of the design. First, you need to list the functional and test points. Functional points refer to all the functionalities supported by the design. For example, for an arithmetic logic unit (ALU), functional points could be “supports addition” or “supports multiplication.” Each functional point should correspond to one or more test points, which break the function into different test scenarios to verify whether the functional point is correct. For example, for the “supports addition” functional point, test points could include “addition is correct when both inputs are positive.”\n7. Writing Test Cases Once the list of functional and test points is determined, you can start writing test cases. Each test case should cover one or more test points to verify whether the functional point is correct. All test cases combined should cover all test points (100% functional coverage) and all lines of code (100% line coverage), ensuring verification completeness. How can you ensure verification correctness? If the reference model comparison method is used, mlvp will automatically throw an exception when a mismatch occurs, causing the test case to fail. If a direct comparison method is used, you should use assert in the test case to write comparison code. When the comparison fails, the test case will also fail. When all test cases pass, the functionality is confirmed as correct.When writing, use the interfaces provided by Env to drive the DUT. If interaction between multiple driver methods is needed, you can use the Executor to encapsulate higher-level functions. In other words, interactions at the driver method level should be handled during test case development.\n8. Writing the Verification Report Once 100% line and functional coverage is achieved, the verification is complete. A verification report should be written to summarize the results. If issues are found in the DUT, the report should provide detailed descriptions of the causes. If 100% coverage is not achieved, the report should explain why. The format of the report should follow the company’s internal standards.\n","categories":"","description":"","excerpt":"With mlvp, you can now set up a complete verification environment and …","ref":"/mlvp/en/docs/mlvp/verification/","tags":"","title":"Starting a New Verification Task"},{"body":"双端口栈简介 双端口栈是一种数据结构，支持两个端口同时进行操作。与传统单端口栈相比，双端口栈允许同时进行数据的读写操作，在例如多线程并发读写等场景下，双端口栈能够提供更好的性能。本例中，我们提供了一个简易的双端口栈实现，其源码如下：\nmodule dual_port_stack ( input clk, input rst, // Interface 0 input in0_valid, output in0_ready, input [7:0] in0_data, input [1:0] in0_cmd, output out0_valid, input out0_ready, output [7:0] out0_data, output [1:0] out0_cmd, // Interface 1 input in1_valid, output in1_ready, input [7:0] in1_data, input [1:0] in1_cmd, output out1_valid, input out1_ready, output [7:0] out1_data, output [1:0] out1_cmd ); // Command definitions localparam CMD_PUSH = 2'b00; localparam CMD_POP = 2'b01; localparam CMD_PUSH_OKAY = 2'b10; localparam CMD_POP_OKAY = 2'b11; // Stack memory and pointer reg [7:0] stack_mem[0:255]; reg [7:0] sp; reg busy; reg [7:0] out0_data_reg, out1_data_reg; reg [1:0] out0_cmd_reg, out1_cmd_reg; reg out0_valid_reg, out1_valid_reg; assign out0_data = out0_data_reg; assign out0_cmd = out0_cmd_reg; assign out0_valid = out0_valid_reg; assign out1_data = out1_data_reg; assign out1_cmd = out1_cmd_reg; assign out1_valid = out1_valid_reg; always @(posedge clk or posedge rst) begin if (rst) begin sp \u003c= 0; busy \u003c= 0; end else begin // Interface 0 Request Handling if (!busy \u0026\u0026 in0_valid \u0026\u0026 in0_ready) begin case (in0_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out0_valid_reg \u003c= 1; stack_mem[sp] \u003c= in0_data; out0_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out0_valid_reg \u003c= 1; out0_data_reg \u003c= stack_mem[sp - 1]; out0_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out0_valid_reg \u003c= 0; end endcase end // Interface 1 Request Handling if (!busy \u0026\u0026 in1_valid \u0026\u0026 in1_ready) begin case (in1_cmd) CMD_PUSH: begin busy \u003c= 1; sp \u003c= sp + 1; out1_valid_reg \u003c= 1; stack_mem[sp] \u003c= in1_data; out1_cmd_reg \u003c= CMD_PUSH_OKAY; end CMD_POP: begin busy \u003c= 1; sp \u003c= sp - 1; out1_valid_reg \u003c= 1; out1_data_reg \u003c= stack_mem[sp - 1]; out1_cmd_reg \u003c= CMD_POP_OKAY; end default: begin out1_valid_reg \u003c= 0; end endcase end // Interface 0 Response Handling if (busy \u0026\u0026 out0_ready) begin out0_valid_reg \u003c= 0; busy \u003c= 0; end // Interface 1 Response Handling if (busy \u0026\u0026 out1_ready) begin out1_valid_reg \u003c= 0; busy \u003c= 0; end end end assign in0_ready = (in0_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255|| in0_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy; assign in1_ready = (in1_cmd == CMD_PUSH \u0026\u0026 sp \u003c 255|| in1_cmd == CMD_POP \u0026\u0026 sp \u003e 0) \u0026\u0026 !busy \u0026\u0026 !(in0_ready \u0026\u0026 in0_valid); endmodule 在该实现中，除了时钟信号(clk)和复位信号(rst)之外，还包含了两个端口的输入输出信号，它们拥有相同的接口定义。每个端口的信号含义如下：\n请求端口（in） in_valid 输入数据有效信号 in_ready 输入数据准备好信号 in_data 输入数据 in_cmd 输入命令 （0:PUSH, 1:POP） 响应端口（out） out_valid 输出数据有效信号 out_ready 输出数据准备好信号 out_data 输出数据 out_cmd 输出命令 （2:PUSH_OKAY, 3:POP_OKAY） 当我们想通过一个端口对栈进行一次操作时，首先需要将需要的数据和命令写入到输入端口，然后等待输出端口返回结果。\n具体地，如果我们想对栈进行一次 PUSH 操作。首先我们应该将需要 PUSH 的数据写入到 in_data 中，然后将 in_cmd 设置为 0，表示 PUSH 操作，并将 in_valid 置为 1，表示输入数据有效。接着，我们需要等待 in_ready 为 1，保证数据已经正确的被接收，此时 PUSH 请求已经被正确发送。\n命令发送成功后，我们需要在响应端口等待栈的响应信息。当 out_valid 为 1 时，表示栈已经完成了对应的操作，此时我们可以从 out_data 中读取栈的返回数据（POP 操作的返回数据将会放置于此），从 out_cmd 中读取栈的返回命令。当读取到数据后，需要将 out_ready 置为 1，以通知栈正确接收到了返回信息。\n如果两个端口的请求同时有效时，栈将会优先处理端口 0 的请求。\n构建驱动环境 与案例一和案例二类似，在对双端口栈进行测试之前，我们首先需要利用 Picker 工具将 RTL 代码构建为 Python Module。在构建完成后，我们将通过 Python 脚本驱动 RTL 代码进行测试。\n首先，创建名为 dual_port_stack.v 的文件，并将上述的 RTL 代码复制到该文件中，接着在相同文件夹下执行以下命令：\npicker export --autobuild=true dual_port_stack.v -w dual_port_stack.fst --sname dual_port_stack --tdir picker_out_dual_port_stack/ --lang python -e --sim verilator 生成好的驱动环境位于 picker_out_dual_port_stack 文件夹中, 其中 dual_port_stack 为生成的 Python Module。\n若自动编译运行过程中无错误发生，则代表环境被正确构建。\n利用回调函数驱动 DUT 在本案例中，为了测试双端口栈的功能，我们需要对其进行驱动。但你可能很快就会发现，仅仅使用案例一和案例二中的方法很难对双端口栈进行驱动。因为在此前的测试中，DUT只有一条执行逻辑，给DUT输入数据后等待DUT输出即可。\n但双端口栈却不同，它的两个端口是两个独立的执行逻辑，在驱动中，这两个端口可能处于完全不同的状态，例如端口0在等待DUT返回数据时，端口1有可能正在发送新的请求。这种情况下，使用简单的串行执行逻辑将很难对DUT进行驱动。\n因此我们在本案例中我们将以双端口栈为例，介绍一种基于回调函数的驱动方法，来完成此类DUT的驱动。\n回调函数简介 回调函数是一种常见的编程技术，它允许我们将一个函数传入，并等待某个条件满足后被调用。构建产生的 Python Module 中，我们提供了向内部执行环境注册回调函数的接口 StepRis，使用方法如下:\nfrom dual_port_stack import DUTdual_port_stack def callback(cycles): print(f\"The current clock cycle is {cycles}\") dut = DUTdual_port_stack() dut.StepRis(callback) dut.Step(10) 你可以直接运行该代码来查看回调函数的效果。\n在上述代码中，我们定义了一个回调函数 callback ，它接受一个参数 cycles ，并在每次调用时打印当前的时钟周期。接着通过 StepRis 将该回调函数注册到 DUT 中。\n注册回调函数后，每运行一次 Step 函数，即每个时钟周期，都会在时钟信号上升沿去调用该回调函数，并传入当前的时钟周期。\n通过这种方式，我们可以将不同的执行逻辑都写成回调函数的方式，并将多个回调函数注册到 DUT 中，从而实现对 DUT 的并行驱动。\n基于回调函数驱动的双端口栈 通过回调函数的形式来完成一条完整的执行逻辑，通常我们会使用状态机的模式进行编写。每调用一次回调函数，就会引起状态机内部的状态变化，多次调用回调函数，就会完成一次完整的执行逻辑。\n下面是一个基于回调函数驱动的双端口栈的示例代码：\nimport random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class Status(Enum): IDLE = 0 WAIT_REQ_READY = 1 WAIT_RESP_VALID = 2 class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict self.status = self.Status.IDLE self.operation_num = 0 self.remaining_delay = 0 def push(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value self.port_dict[\"in_data\"].value = random.randint(0, 2**32-1) def pop(self): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.POP.value def step_callback(self, cycle): if self.status == self.Status.WAIT_REQ_READY: if self.port_dict[\"in_ready\"].value == 1: self.port_dict[\"in_valid\"].value = 0 self.port_dict[\"out_ready\"].value = 1 self.status = self.Status.WAIT_RESP_VALID if self.port_dict[\"in_cmd\"].value == self.BusCMD.PUSH.value: self.model.commit_push(self.port_dict[\"in_data\"].value) elif self.status == self.Status.WAIT_RESP_VALID: if self.port_dict[\"out_valid\"].value == 1: self.port_dict[\"out_ready\"].value = 0 self.status = self.Status.IDLE self.remaining_delay = random.randint(0, 5) if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) if self.status == self.Status.IDLE: if self.remaining_delay == 0: if self.operation_num \u003c 10: self.push() elif self.operation_num \u003c 20: self.pop() else: return self.operation_num += 1 self.status = self.Status.WAIT_REQ_READY else: self.remaining_delay -= 1 def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) dut.StepRis(port0.step_callback) dut.StepRis(port1.step_callback) dut.Step(200) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") test_stack(dut) dut.Finish() 在上述代码中，实现了这样的驱动过程：每个端口独立对DUT进行驱动，并在一个请求完成后添加随机延迟，每个端口分别完成了 10 次 PUSH 操作与 10 次 POP 操作。\n在 PUSH 或 POP 请求生效时，会调用同一个 StackModel 中的 commit_push 或 commit_pop 函数，以模拟栈的行为，并在每次 POP 操作完成后对比 DUT 的返回数据与模型的数据是否一致。\n为了实现对单个端口的驱动行为，我们实现了 SinglePortDriver 类，其中实现了一个接口进行收发的完整过程，通过 step_callback 函数来实现内部的更新逻辑。\n在测试函数 test_stack 中，我们为双端口栈的每一个端口都创建了一个 SinglePortDriver 实例，传入了对应的接口，并通过 StepRis 函数将其对应的回到函数其注册到 DUT 中。之后调用 dut.Step(200) 时，每个时钟周期中都会自动调用一次回调函数，来完成整个驱动逻辑。\nSinglePortDriver 驱动逻辑\n上面提到，一般使用回调函数的形式需要将执行逻辑实现为状态机，因此在 SinglePortDriver 类中，需要记录包含端口所处的状态，它们分别是：\nIDLE：空闲状态，等待下一次操作 在空闲状态下，需要查看另一个状态 remaining_delay 来判断当前是否已经延时结束，如果延时结束可立即进行下一次操作，否则继续等待。 当需要执行下一次操作时，需要查看状态 operation_num （当前已经执行的操作数）来决定下一次操作时 PUSH 还是 POP。之后调用相关函数对端口进行一次赋值，并将状态切换至 WAIT_REQ_READY。 WAIT_REQ_READY：等待请求端口准备好 当请求发出后（in_valid 有效），此时需要等待 in_ready 信号有效，以确保请求已经被正确接受。 当请求被正确接受后，需要将 in_valid 置为 0，同时将 out_ready 置为 1，表示请求发送完毕，准备好接收回复。 WAIT_RESP_VALID：等待响应端口返回数据 当请求被正确接受后，需要等待 DUT 的回复，即等待 out_valid 信号有效。当 out_valid 信号有效时，表示回复已经产生，一次请求完成，于是将 out_ready 置为 0，同时将状态切换至 IDLE。 运行测试 在picker_out_dual_port_stack中创建exmaple.py文件，将上述代码复制到其中，然后执行以下命令：\ncd picker_out_dual_port_stack python3 example.py 可直接运行本案例的测试代码，你将会看到类似如下的输出：\n... push 77 push 140 push 249 push 68 push 104 push 222 ... Pop 43 Pass: 43 == 43 Pop 211 Pass: 211 == 211 Pop 16 Pass: 16 == 16 Pop 255 Pass: 255 == 255 Pop 222 Pass: 222 == 222 Pop 104 ... 在输出中，你可以看到每次 PUSH 和 POP 操作的数据，以及每次 POP 操作的结果。如果输出中没有错误信息，则表示测试通过。\n回调函数驱动的优劣 通过使用回调函数，我们能够完成对 DUT 的并行驱动，正如本例所示，我们通过两个回调函数实现了对拥有两个独立执行逻辑的端口的驱动。回调函数在简单的场景下，为我们提供了一种简单的并行驱动方法。\n但是通过本例也可以看出，仅仅实现一套简单的“请求-回复”流程，就需要维护大量的内部状态，回调函数将本应完整的执行逻辑拆分为了多次函数调用，为代码的编写和调试增加了诸多复杂性。\n","categories":["示例项目","教程"],"description":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用回调函数驱动DUT","excerpt":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用回调函数驱动DUT","ref":"/mlvp/docs/quick-start/eg-stack-callback/","tags":["examples","docs"],"title":"案例三：双端口栈（回调）"},{"body":"部分电路有多个时钟，XClock 类提供了分频功能，可以通过它实现对多时钟电路的驱动。\nXClock 中的 FreqDivWith 接口 XClock 函数提供如下分频接口\nvoid XClock::FreqDivWith(int div, // 分频数，，即绑定的XClock的频率为原时钟频率的div分之1 XClock \u0026clk, // 绑定的XClock int shift=0) // 对波形进行 shift 个半周期的移位 XClock 的一般驱动流程 创建 XClock，绑定 DUT 的驱动函数 # 假设已经创建了DUT，并将其命名为dut # 创建XClock xclock = XClock(dut.dut.simStep) 绑定关联 clk 引脚 # clk是dut的时钟引脚 xclock.Add(dut.clk) # Add方法具有别名：AddPin 通过 XPort 绑定与 clk 关联的引脚 因为在我们的工具中，对于端口的读写是通过 xclock 来驱动的，所以如果不将与 clk 关联的引脚绑定到 XClock 上，那么在驱动时，相关的引脚数值不会发生变化。\n比如，我们要进行复位操作，那么可以将 reset 绑定到 xclock 上。\n方法：\nclass XClock: def Add(xport) #将Clock和XData进行绑定 举例：\n# xclock.Add(dut.xport.Add(pin_name, XData)) xclock.Add(dut.xport.Add(\"reset\", dut.reset)) 在经过了前面的绑定之后，接下来可以使用了。\n我们根据需要来设置回调、设置分频。当然，时序电路肯定也要驱动时钟。\n这些方法都可以参照工具介绍。\n下面是举例：\n# func为回调函数，args为自定义参数 #设置上升沿回调函数 dut.StepRis(func, args=(), kwargs={}) #设置下降沿回调函数 dut.StepFal(func, args=(), kwargs={}) # 假设xclock是XClock的实例 xclock.FreqDivWith(2, half_clock) # 将xclock的频率分频为原来的一半 xclock.FreqDivWith(1, left_clock， -2) # 将xclock的频率不变，对波形进行一个周期的左移 dut.Step(10) #推进10个时钟周期 多时钟案例 例如多时钟电路有 6 个 clock，每个 clock 都有一个对应的计数器，设计代码如下：\nmodule multi_clock ( input wire clk1, input wire clk2, input wire clk3, input wire clk4, input wire clk5, input wire clk6, output reg [31:0] reg1, output reg [31:0] reg2, output reg [31:0] reg3, output reg [31:0] reg4, output reg [31:0] reg5, output reg [31:0] reg6 ); initial begin reg1 = 32'b0; reg2 = 32'b0; reg3 = 32'b0; reg4 = 32'b0; reg5 = 32'b0; reg6 = 32'b0; end always @(posedge clk1) begin reg1 \u003c= reg1 + 1; end always @(posedge clk2) begin reg2 \u003c= reg2 + 1; end always @(posedge clk3) begin reg3 \u003c= reg3 + 1; end always @(posedge clk4) begin reg4 \u003c= reg4 + 1; end always @(posedge clk5) begin reg5 \u003c= reg5 + 1; end always @(posedge clk6) begin reg6 \u003c= reg6 + 1; end endmodule 通过picker导出：\npicker export multi_clock.v -w mc.fst --tdir picker_out/MultiClock --lang python 可以通过如下 Python 进行多时钟驱动：\nfrom MultiClock import * from xspcomm import XClock def test_multi_clock(): # 创建DUT dut = DUTmulti_clock() # 创建主时钟 main_clock = XClock(dut.dut.simStep) # 创建子时钟 clk1, clk2, clk3 = XClock(lambda x: 0), XClock(lambda x: 0), XClock(lambda x: 0) clk4, clk5, clk6 = XClock(lambda x: 0), XClock(lambda x: 0), XClock(lambda x: 0) # 给子时钟添加相关的clock引脚及关联端口 clk1.Add(dut.xport.SelectPins([\"reg1\"])).AddPin(dut.clk1.xdata) clk2.Add(dut.xport.SelectPins([\"reg2\"])).AddPin(dut.clk2.xdata) clk3.Add(dut.xport.SelectPins([\"reg3\"])).AddPin(dut.clk3.xdata) clk4.Add(dut.xport.SelectPins([\"reg4\"])).AddPin(dut.clk4.xdata) clk5.Add(dut.xport.SelectPins([\"reg5\"])).AddPin(dut.clk5.xdata) clk6.Add(dut.xport.SelectPins([\"reg6\"])).AddPin(dut.clk6.xdata) # 将主时钟频率分频到子时钟 main_clock.FreqDivWith(1, clk1) main_clock.FreqDivWith(2, clk2) main_clock.FreqDivWith(3, clk3) main_clock.FreqDivWith(1, clk4, -1) main_clock.FreqDivWith(2, clk5, 1) main_clock.FreqDivWith(3, clk6, 2) # 驱动时钟 main_clock.Step(100) dut.Finish() if __name__ == \"__main__\": test_multi_clock() 上述代码输出的波形如下：\n可以看到：\nclk2 的周期是 clk1 的 2 倍 clk3 的周期是 clk1 的 3 倍， clk4 的周期和 clk1 相同，但是进行了半个周期的右移 clk5 的周期和 clk2 相同，但是进行了半个周期的左移 clk6 的周期和 clk3 相同，但是进行了一个周期的左移 ","categories":["示例项目","教程"],"description":"多时钟示例","excerpt":"多时钟示例","ref":"/mlvp/docs/env_usage/multiclock/","tags":["examples","docs"],"title":"多时钟"},{"body":"使用 toffee，你已经可以搭建出一个完整的验证环境，并且方便地去编写测试用例了。然而在实际的业务中，往往无法理解如何开始上手，并最终完成验证任务。实际编写代码后，会遇到无法正确划分 Bundle，无法正确理解 Agent 的高级语义封装，搭建完环境之后不知道做什么等问题。\n在这一节中，将会介绍如何从头开始完成一个新的验证任务，以及如何更好地使用 toffee 来完成验证任务。\n1. 了解待验证设计 拿到一个新的设计后，往往面对的是几十或数百个输入输出信号，如果直接看这些信号，很可能一头雾水，感觉无从下手。在这时，你必须坚信，输入输出信号都是设计人员来定义的，只要能够理解设计的功能，就能够理解这些信号的含义。\n如果设计人员提供了设计文档，那么你可以阅读设计文档，了解设计的功能，并一步步地将功能与输入输出信号对应起来，并且要清楚地理解输入输出信号的时序，以及如何使用这些信号来驱动设计。一般来说，你还需要阅读设计的源代码，来找寻更细节的接口时序问题。\n当大致了解了 DUT 的功能，并明白如何驱动起 DUT 接口之后，你就可以开始搭建验证环境了。\n2. 划分 Bundle 搭建环境的第一件事，就是根据接口的逻辑功能，将其划分为若干个接口集合，我们可以每一个接口集合视作一个 Bundle。划分为的每个 Bundle 都应是独立的，由一个独立的 Agent 来驱动。\n但是，往往实际中的接口是这样的：\n|---------------------- DUT Bundle -------------------------------| |------- Bundle 1 ------| |------ Bundle 2 ------| |-- Bundle 3 --| |-- B1.1 --| |-- B1.2 --| |-- B2.1 --| 那么问题就出现了，例如究竟是应该为 B1.1， B1.2 各自创建一个 Agent，还是应该直接为 Bundle 1 建立一个 Agent 呢？\n这还是取决于接口的逻辑功能，如果需要定义一个独立的请求，这个请求需要对 B1.1 和 B1.2 同时进行操作，那么就应该为 Bundle 1 创建一个 Agent，而不是为 B1.1 和 B1.2 分别创建 Agent。\n即便如此，为 B1.1 和 B1.2 定义 1.2 也是可行的，这增添了 Agent 的划分粒度，但牺牲了操作的连续性，上层代码和参考模型的编写都会变得复杂。因此选择合适的划分粒度是需要对具体业务的权衡。最终的划分，所有的 Agent 加起来应该能覆盖 DUT Bundle 的所有接口。\n实践中，为了方便 DUT 的连接，可以定义一个 DUT Bundle，一次性将所有的接口都连接到这个 Bundle 上，由 Env 将其中的子 Bundle 分发给各个 Agent。\n3. 编写 Agent 当 Bundle 划分完成后，就可以开始编写 Agent 来驱动这些 Bundle 了，你需要为每个 Bundle 编写一个 Agent。\n首先，可以从驱动方法开始写起，驱动方法实际上是对 Bundle 的一种高级语义封装，因此，高级语义信息应该携带了足以驱动 Bundle 的所有信息。如果 Bundle 中存在一个信号需要数字，但参数中并没有提供与这一信号相关的信息，那么这种高级语义封装就是不完整的。应尽量避免在驱动方法中对某个信号值进行假定，如果对这一信号在 Agent 中进行假定，DUT 的输出将会受到这一假定的影响，可能导致参考模型与 DUT 的行为不一致。\n同时，这一高层封装也决定了参考模型的功能层级，参考模型会直接与高层语义信息进行交互，并不会涉及到底层信号。\n如果参考模型需要用函数调用模式编写，那么应该将 DUT 的输出通过函数返回值来返回。如果参考模型需要用独立执行流模式编写，那么应该编写监测方法，将 DUT 的所有输出转换成高层语义信息，通过监测方法输出。\n4. 封装成 Env 当所有的 Agent 编写完成后，或者挑选之前已有的 Agent，就可以将这些 Agent 封装成 Env 了。\nEnv 封装了整个验证环境，并确定了参考模型的编写规范。\n5. 编写参考模型 参考模型的编写没有必要在 Env 编写完成之后再开始，可以与 Agent 的编写同时进行，并实时编写一些驱动代码，来检验编写的正确性。当然如果 Agent 的编写特别规范，编写完整 Env 后再编写参考模型也是可行的。\n参考模型最重要的是选择合适的编写模式，函数调用模式和独立执行流模式都是可行的，但在不同的场景下，选择不同的模式会更加方便。\n6. 确定功能点及测试点 编写好 Env 以及参考模型后，并不能直接开始编写测试用例，因为此时并没有测试用例的编写方向，盲目的编写测试用例，没有办法让待测设计验证完全。\n首先需要列出功能点及测试点列表。功能点是待测设计支持的所有功能，例如对于一个算术逻辑单元（ALU）来说，功能点的形式可能是“支持加法”，“支持乘法”等。每个功能点需要对应一个或多个测试点，测试点通过将功能划分为不同的测试场景，来验证功能点是否正确。例如对于“支持加法”这个功能点，可能有“当输入都为正数时，加法正确”等测试点。\n7. 编写测试用例 当功能点及测试点列表确定后，就可以开始编写测试用例了，一个测试用例需要能够覆盖一个或多个测试点，以验证功能点是否正确。所有的测试用例应该能够覆盖所有的测试点（功能覆盖率 100%），以及覆盖所有的设计行（行覆盖率 100%），这样一来就能保证验证的完备性。\n如何保证验证的正确性呢？如果采用参考模型比对的方式，当比对失败时，toffee 会自动抛出异常，使得测试用例失败。如果采用直接比对的方式，应该在测试用例中使用 assert 来编写比对代码，当比对失败时，测试用例也会失败。最终，当所有的测试用例都通过时，意味着功能已验证为正确。\n编写过程中，你需要使用 Env 中提供的接口来驱动 DUT，如果出现了需要多个驱动方法交互的情况，可以使用 Executor 来封装更高层的函数。也就是说驱动方法级的交互，是在测试用例的编写中完成的。\n8. 编写验证报告 当行覆盖率和功能覆盖率都达到了 100% 之后，意味着验证已经完成。最终需要编写一个验证报告，来总结验证任务的结果。如果验证出了待测设计的问题，也应在验证报告中详细描述问题的原因。如果行覆盖率或者功能覆盖率没有达到 100%，也应在验证报告中说明原因，报告的格式应该遵循公司内部统一的规范。\n","categories":"","description":"","excerpt":"使用 toffee，你已经可以搭建出一个完整的验证环境，并且方便地去编写测试用例了。然而在实际的业务中，往往无法理解如何开始上手，并最终完成 …","ref":"/mlvp/docs/mlvp/verification/","tags":"","title":"开始新的验证任务"},{"body":"参考模型 用于模拟待验证设计的行为，以便在验证过程中对设计进行验证。在 toffee 验证环境中，参考模型需要遵循 Env 的接口规范，以便能够附加到 Env 上，由 Env 来完成参考模型的自动同步。\n参考模型的两种实现方式 toffee 提供了两种参考模型的实现方式，这两种方式都可以被附加到 Env 上，并由 Env 来完成参考模型的自动同步。在不同的场景下，可以选择更适合的方式来实现参考模型。\n这两种方式分别是 函数调用模式 与 独立执行流模式，下面将分别介绍这两种方式的具体概念。\n函数调用模式 函数调用模式即是将参考模型的对外接口定义为一系列的函数，通过调用这些函数来驱动参考模型的行为。此时，我们通过输入参数向参考模型发送数据，并通过返回值获取参考模型的输出数据，参考模型通过函数体的逻辑来更新内部状态。\n下面是一个简单的函数调用模式的参考模型的定义示例：\n例如，这是一个简单的加法器参考模型：\nclass AdderRefModel(): def add(self, a, b): return a + b 在这个参考模型中，不需要任何内部状态，通过一个对外函数接口即可实现参考模型所有功能。\n需要注意的是，使用函数调用模式编写的参考模型，只能通过外部主动调用的方式来执行，无法被动输出内部数据。因此，其无法与 Agent 中的监测方法进行匹配。在 Agent 中编写监测方法，在函数调用模式编写参考模型时是没有意义的。\n独立执行流模式 独立执行流模式即是将参考模型的行为定义为一个独立的执行流，它不再受外部主动调用函数控制，而拥有了主动获取输入数据和主动输出数据的能力。当外部给参考模型发送数据时，参考模型不会立即响应，而是将这一数据保存起来，等待其执行逻辑主动获取该数据。\n我们用一段代码来说明这种模式，该示例中用到了 toffee 中提供的相关概念来实现，但目前无需关心这些概念的使用细节。\nclass AdderRefModel(Model): def __init__(self): super().__init__() self.add_port = DriverPort() self.sum_port = MonitorPort() async def main(): while True: operands = await self.add_port() sum = operands[\"a\"] + operands[\"b\"] await self.sum_port(sum) 在这里，我们在参考模型构造函数中定义了两类接口，一类为驱动接口(DriverPort)，即代码中的add_port，用于接收外部输入数据；另一类为监测接口(MonitorPort)，即代码中的sum_port，用于向外部输出数据。\n定义了这两个接口后，上层代码在给参考模型发送数据时，并不会触发参考模型中的某个函数，而是会将数据发送到 add_port 这个驱动接口中。同时，上层代码也无法主动获取到参考模型的输出数据了。参考模型的输出数据会通过 sum_port 这个监测接口，由参考模型主动输出。\n那么参考模型如何去使用这两个接口呢？在参考模型中，有一个 main 函数，这是参考模型执行的入口，当参考模型创建时, main 函数会被自动调用，并在后台持续运行。在上面代码中 main 函数里，参考模型通过不断重复这一过程：等待 add_port 中的数据、计算结果、将结果输出到 sum_port 中 来实现参考模型的行为。\n参考模型会主动向 add_port 请求数据，如果 add_port 中没有数据，参考模型会等待数据的到来。当数据到来后，参考模型将会进行计算，将计算结果主动的输出到 sum_port 中。它的执行过程是一个独立的执行流，不受外部的主动调用控制。当参考模型变得复杂时，其将会含有众多的驱动接口和监测接口，通过独立执行流的方式，可以更好的去处理结构之间的相互关系，尤其是接口之间存在调用顺序的情况。\n如何编写函数调用模式的参考模型 驱动函数匹配 假如 Env 中定义的接口如下：\nStackEnv - port_agent - @driver_method push - @driver_method pop 那么如果我们想要编写与之对应的参考模型，自然地，我们需要定义这四个驱动函数被调用时参考模型的行为。也就是说为每一个驱动函数编写一个对应的函数，这些函数将会在驱动函数被调用时被框架自动调用。\n如何让参考模型中定义的函数能够与某个驱动函数匹配呢？首先应该使用 @driver_hook 装饰器来表示这个函数是一个驱动函数的匹配函数。接着，为了建立对应关系，我们需要在装饰器中指定其对应的 Agent 和驱动函数的名称。最后，只需要保证函数的参数与驱动函数的参数一致，两个函数便能够建立对应关系。\nclass StackRefModel(Model): @driver_hook(agent_name=\"port_agent\", driver_name=\"push\") def push(self, data): pass @driver_hook(agent_name=\"port_agent\", driver_name=\"pop\") def pop(self): pass 此时，驱动函数与参考模型的对应关系已经建立，当 Env 中的某个驱动函数被调用时，参考模型中对应的函数将会被自动调用，并自动对比两者的返回值是否一致。\ntoffee 还提供了以下几种匹配方式，以便更好地匹配驱动函数：\n指定驱动函数路径\n可以通过 “.” 来指定驱动函数的路径，例如：\nclass StackRefModel(Model): @driver_hook(\"port_agent.push\") def push(self, data): pass @driver_hook(\"port_agent.pop\") def pop(self): pass 使用函数名称匹配驱动函数名称\n如果参考模型中的函数名称与驱动函数名称相同，可以省略 driver_name 参数，例如：\nclass StackRefModel(Model): @driver_hook(agent_name=\"port_agent\") def push(self, data): pass @driver_hook(agent_name=\"port_agent\") def pop(self): pass 使用函数名称同时匹配 Agent 名称与驱动函数名称\n可以在函数名中通过双下划线 “__” 来同时匹配 Agent 名称与驱动函数名称，例如：\nclass StackRefModel(Model): @driver_hook() def port_agent__push(self, data): pass @driver_hook() def port_agent__pop(self): pass Agent 匹配 除了对 Agent 中每一个驱动函数都编写一个 driver_hook 之外，还可以通过 @agent_hook 装饰器来一次性匹配 Agent 中的所有驱动函数。\nclass StackRefModel(Model): @agent_hook(\"port_agent\") def port_agent(self, driver_name, args): pass 在这个例子中，port_agent 函数将会匹配 port_agent Agent 中的所有驱动函数，当 Agent 中的任意一个驱动函数被调用时，port_agent 函数将会被自动调用。除了 self 之外，port_agent 函数还需接受且只接受两个参数，第一个参数为驱动函数的名称，第二个参数为驱动函数的参数。\n当某个驱动函数被调用时，driver_name 参数将会传入驱动函数的名称，args 参数将会传入该该驱动函数被调用时的参数，参数将会以字典的形式传入。port_agent 函数可以根据 driver_name 和 args 来决定如何处理这个驱动函数的调用，并将结果返回。此时框架将会使用此函数的返回值与驱动函数的返回值进行对比。\n与驱动函数类似，@agent_hook 装饰器也支持当函数名与 Agent 名称相同时省略 agent_name 参数。\nclass StackRefModel(Model): @agent_hook() def port_agent(self, driver_name, args): pass agent_hook 与 driver_hook 同时存在\n当 agent_hook 被定义后，理论上无需再定义任何 driver_hook 与 Agent 中的驱动函数进行匹配。但是，如果需要对某个驱动函数进行特殊处理，可以再定义一个 driver_hook 与该驱动函数进行匹配。\n当 agent_hook 与 driver_hook 同时存在时，框架会优先调用 agent_hook 函数，再调用 driver_hook 函数，并将 driver_hook 函数的返回值用于结果的对比。\n当 Env 中所有的驱动函数都能找到对应的 driver_hook 或 agent_hook 时，参考模型便能成功与 Env 建立匹配关系，此时可以直接通过 Env 中的 attach 方法将参考模型附加到 Env 上。\n如何编写独立执行流模式的参考模型 独立执行流模式的参考模型是通过 port 接口的形式来完成数据的输入输出，他可以主动向 port 请求数据，也可以主动向 port 输出数据。在 toffee 中，我们提供了两种接口来实现这一功能，分别是 DriverPort 和 MonitorPort。\n类似地，我们需要定义一系列的 DriverPort 使其与 Env 中的驱动函数匹配，同时定义一系列的 MonitorPort 使其与 Env 中的监测函数匹配。\n当 Env 中的驱动函数被调用时，调用数据将会被发送到 DriverPort 中，参考模型将会主动获取这些数据，并进行计算。计算结果将会被输出到 MonitorPort 中，当 Env 中的监测函数被调用时，比较器会自动从 MonitorPort 中获取数据，并与 Env 中的监测函数的返回值进行比较。\n驱动方法接口匹配 为了接收到 Env 中所有的驱动函数的调用，参考模型可以选择为每一个驱动函数编写对应的 DriverPort。可以通过 DriverPort 的参数 agent_name 与 driver_name 来匹配 Env 中的驱动函数。\nclass StackRefModel(Model): def __init__(self): super().__init__() self.push_port = DriverPort(agent_name=\"port_agent\", driver_name=\"push\") self.pop_port = DriverPort(agent_name=\"port_agent\", driver_name=\"pop\") 与 driver_hook 类似，也可以使用下面的方式来匹配 Env 中的驱动函数：\n# 使用 \".\" 来指定驱动函数的路径 self.push_port = DriverPort(\"port_agent.push\") # 如果参考模型中的变量名称与驱动函数名称相同，可以省略 driver_name 参数 self.push = DriverPort(agent_name=\"port_agent\") # 使用变量名称同时匹配 Agent 名称与驱动函数名称，并使用 `__` 分隔 self.port_agent__push = DriverPort() Agent 接口匹配 也可以选择定义 AgentPort 同时匹配一个 Agent 中的所有驱动函数。但与 agent_hook 不同的是，定义了 AgentPort 后，便不能为该 Agent 中的任何驱动函数再定义 DriverPort。所有的驱动函数调用将会被发送到 AgentPort 中。\nclass StackRefModel(Model): def __init__(self): super().__init__() self.port_agent = AgentPort(agent_name=\"port_agent\") 类似的，当变量名称与 Agent 名称相同时，可以省略 agent_name 参数：\nself.port_agent = AgentPort() 监测方法接口匹配 为了与 Env 中的监测函数匹配，参考模型需要为每一个监测函数编写对应的 MonitorPort，定义方法与 DriverPort 一致。\nself.monitor_port = MonitorPort(agent_name=\"port_agent\", monitor_name=\"monitor\") # 使用 \".\" 来指定监测函数的路径 self.monitor_port = MonitorPort(\"port_agent.monitor\") # 如果参考模型中的变量名称与监测函数名称相同，可以省略 monitor_name 参数 self.monitor = MonitorPort(agent_name=\"port_agent\") # 使用变量名称同时匹配 Agent 名称与监测函数名称，并使用 `__` 分隔 self.port_agent__monitor = MonitorPort() MonitorPort 中送入的数据，将会自动与 Env 中的监测函数的返回值进行比较，来完成参考模型的比对工作。\n当参考模型中定义的 DriverPort, AgentPort 和 MonitorPort 能够与 Env 中所有接口匹配时，参考模型便能成功与 Env 建立匹配关系，此时可以直接通过 Env 中的 attach 方法将参考模型附加到 Env 上。\n","categories":"","description":"","excerpt":"参考模型 用于模拟待验证设计的行为，以便在验证过程中对设计进行验证。在 toffee 验证环境中，参考模型需要遵循 Env 的接口规范，以便 …","ref":"/mlvp/docs/mlvp/env/ref_model/","tags":"","title":"如何编写参考模型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/docs/mlvp/api/","tags":"","title":"API Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/docs/mlvp/api/","tags":"","title":"API 文档"},{"body":"Introduction to the Dual-Port Stack and Environment Setup The dual-port stack used in this case is identical to the one implemented in Case 3. Please refer to the Introduction to the Dual-Port Stack and Driver Environment Setup in Case 3 for more details.\nDriving the DUT Using Coroutines In Case 3, we used callbacks to drive the DUT. While callbacks offer a way to perform parallel operations, they break the execution flow into multiple function calls and require maintaining a large amount of intermediate state, making the code more complex to write and debug.\nIn this case, we will introduce a method of driving the DUT using coroutines. This method not only allows for parallel operations but also avoids the issues associated with callbacks.\nIntroduction to Coroutines Coroutines are a form of “lightweight” threading that enables behavior similar to concurrent execution without the overhead of traditional threads. Coroutines operate on a single-threaded event loop, where multiple coroutines can be defined and added to the event loop, with the event loop managing their scheduling.\nTypically, a defined coroutine will continue to execute until it encounters an event that requires waiting. At this point, the event loop pauses the coroutine and schedules other coroutines to run. Once the event occurs, the event loop resumes the paused coroutine to continue execution.\nFor parallel execution in hardware verification, this behavior is precisely what we need. We can create multiple coroutines to handle various verification tasks. We can treat the clock execution as an event, and within each coroutine, wait for this event. When the clock signal arrives, the event loop wakes up all the waiting coroutines, allowing them to continue executing until they wait for the next clock signal. We use Python’s asyncio to implement coroutine support:\nimport asyncio from UT_dual_port_stack import * async def my_coro(dut, name): for i in range(10): print(f\"{name}: {i}\") await dut.AStep(1) async def test_dut(dut): asyncio.create_task(my_coro(dut, \"coroutine 1\")) asyncio.create_task(my_coro(dut, \"coroutine 2\")) await asyncio.create_task(dut.RunStep(10)) dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_dut(dut)) dut.Finish() You can run the above code directly to observe the execution of coroutines. In the code, we use create_task to create two coroutine tasks and add them to the event loop. Each coroutine task continuously prints a number and waits for the next clock signal.We use dut.RunStep(10) to create a background clock, which continuously generates clock synchronization signals, allowing other coroutines to continue execution when the clock signal arrives.\nDriving the Dual-Port Stack with Coroutines Using coroutines, we can write the logic for driving each port of the dual-port stack as an independent execution flow without needing to maintain a large amount of intermediate state.\nBelow is a simple verification code using coroutines:\nimport asyncio import random from UT_dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"Push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict async def send_req(self, is_push): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value if is_push else self.BusCMD.POP.value self.port_dict[\"in_data\"].value = random.randint(0, 2**8-1) await self.dut.AStep(1) await self.dut.Acondition(lambda: self.port_dict[\"in_ready\"].value == 1) self.port_dict[\"in_valid\"].value = 0 if is_push: self.model.commit_push(self.port_dict[\"in_data\"].value) async def receive_resp(self): self.port_dict[\"out_ready\"].value = 1 await self.dut.AStep(1) await self.dut.Acondition(lambda: self.port_dict[\"out_valid\"].value == 1) self.port_dict[\"out_ready\"].value = 0 if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) async def exec_once(self, is_push): await self.send_req(is_push) await self.receive_resp() for _ in range(random.randint(0, 5)): await self.dut.AStep(1) async def main(self): for _ in range(10): await self.exec_once(is_push=True) for _ in range(10): await self.exec_once(is_push=False) async def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) asyncio.create_task(port0.main()) asyncio.create_task(port1.main()) await asyncio.create_task(dut.RunStep(200)) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_stack(dut)) dut.Finish() Similar to Case 3, we define a SinglePortDriver class to handle the logic for driving a single port. In the main function, we create two instances of SinglePortDriver, each responsible for driving one of the two ports. We place the driving processes for both ports in the main function and add them to the event loop using asyncio.create_task. Finally, we use dut.RunStep(200) to create a background clock to drive the test. This code implements the same test logic as in Case 3, where each port performs 10 PUSH and 10 POP operations, followed by a random delay after each operation. As you can see, using coroutines eliminates the need to maintain any intermediate state. SinglePortDriver Logic In the SinglePortDriver class, we encapsulate a single operation into the exec_once function. In the main function, we first call exec_once(is_push=True) 10 times to complete the PUSH operations, and then call exec_once(is_push=False) 10 times to complete the POP operations.In the exec_once function, we first call send_req to send a request, then call receive_resp to receive the response, and finally wait for a random number of clock signals to simulate a delay.The send_req and receive_resp functions have similar logic; they set the corresponding input/output signals to the appropriate values and wait for the corresponding signals to become valid. The implementation can be written according to the execution sequence of the ports.Similarly, we use the StackModel class to simulate stack behavior. The commit_push and commit_pop functions simulate the PUSH and POP operations, respectively, with the POP operation comparing the data.\nRunning the Test Copy the above code into example.py and then execute the following commands:\ncd picker_out_dual_port_stack python3 example.py You can run the test code for this case directly, and you will see output similar to the following:\n... Push 141 Push 102 Push 63 Push 172 Push 208 Push 130 Push 151 ... Pop 102 Pass: 102 == 102 Pop 138 Pass: 138 == 138 Pop 56 Pass: 56 == 56 Pop 153 Pass: 153 == 153 Pop 129 Pass: 129 == 129 Pop 235 Pass: 235 == 235 Pop 151 ... In the output, you can see the data for each PUSH and POP operation, as well as the result of each POP operation. If there are no error messages in the output, it indicates that the test passed.\nPros and Cons of Coroutine-Driven Design Using coroutine functions, we can effectively achieve parallel operations while avoiding the issues that come with callback functions. Each independent execution flow can be fully retained as a coroutine, which greatly simplifies code writing.\nHowever, in more complex scenarios, you may find that having many coroutines can make synchronization and timing management between them more complicated. This is especially true when you need to synchronize between two coroutines that do not directly interact with the DUT. At this point, you’ll need a set of coroutine writing standards and design patterns for verification code to help you write coroutine-based verification code more effectively. Therefore, we provide the mlvp library, which offers a set of design patterns for coroutine-based verification code. You can learn more about mlvp and how it can help you write better verification code by visiting here .\n","categories":["Example Projects","Tutorials"],"description":"The dual-port stack is a stack with two ports, each supporting push and pop operations. This case study uses the dual-port stack as an example to demonstrate how to drive a DUT using coroutines.","excerpt":"The dual-port stack is a stack with two ports, each supporting push …","ref":"/mlvp/en/docs/quick-start/eg-stack-async/","tags":["examples","docs"],"title":"Case 4: Dual-Port Stack (Coroutines)"},{"body":" After we complete the DUT verification, writing a verification report is a crucial step. This section will provide an overview of the structure of the verification report and the content that needs to be covered.\nThe verification report is a review of the entire verification process and an important supporting document for determining the reasonableness of the verification. Generally, the verification report should include the following content:\nBasic document information (author, log, version, etc.) Verification object (verification target) Introduction to functional points Verification plan Breakdown of test points Test cases Test environment Result analysis Defect analysis Verification conclusion The following content provides further explanation of the list, with specific examples available innutshell_cache_report_demo.pdf\n1. Basic Information Including author, log, version, date, etc.\n2. Verification object (verification target) A necessary introduction to your verification object, which may include its structure, basic functions, interface information, etc.\n3. Introduction to functional points By reading the design documents or source code, you need to summarize the target functions of the DUT and break them down into various functional points.\n4. Verification plan Including your planned verification process and verification framework. Additionally, you should explain how each part of your framework works together.\n5. Breakdown of test points Proposed testing methods for the functional points. Specifically, it can include what signal output should be observed under certain signal inputs.\n6. Test cases The specific implementation of the test points. A test case can include multiple test points.\n7. Test environment Including hardware information, software version information, etc.\n8. Result analysis Result analysis generally refers to coverage analysis. Typically, two types of coverage should be considered: 1. Line Coverage： How many RTL lines of code are executed in the test cases. Generally, we require line coverage to be above 98%.\n2. Functional Coverage：Determine whether the extracted functional points are covered and correctly triggered based on the relevant signals. We generally require test cases to cover each functional point.\n9. Defect analysis Analyze the defects present in the DUT. This can include the specification and detail of the design documents, the correctness of the DUT functions (whether there are bugs), and whether the DUT functions can be triggered.\n10. Verification conclusion The final conclusion drawn after completing the chip verification process, summarizing the above content.\n","categories":["Example Projects","Learning Materials"],"description":"An overview of the structure and content of the verification report.","excerpt":"An overview of the structure and content of the verification report.","ref":"/mlvp/en/docs/basic/report/","tags":["examples","docs"],"title":"Verification Report"},{"body":"双端口栈简介与环境构建 本案例中使用的双端口栈与案例三中的实现完全相同，请查看案例三中的双端口栈简介及构建驱动环境。\n利用协程驱动 DUT 在案例三中，我们使用了回调函数的方式来驱动DUT，回调函数虽然给我们提供了一种能够完成并行操作的方式，然而其却把完成的执行流程割裂为多次函数调用，并需要维护大量中间状态，导致代码的编写及调试变得较为复杂。\n在本案例中，我们将会介绍一种通过协程驱动的方法，这种方法不仅能够做到并行操作，同时能够很好地避免回调函数所带来的问题。\n协程简介 协程是一种“轻量级”的线程，通过协程，你可以实现与线程相似的并发执行的行为，但其开销却远小于线程。其实现原理是，协程库实现了一个运行于单线程之上的事件循环（EventLoop），程序员可以定义若干协程并且加入到事件循环，由事件循环负责这些协程的调度。\n一般来说，我们定义的协程在执行过程中会持续执行，直到遇到一个需要等待的“事件”，此时事件循环就会暂停执行该协程，并调度其他协程运行。当事件发生后，事件循环会再次唤醒该协程，继续执行。\n对于硬件验证中的并行执行来说，这种特性正是我们所需要的，我们可以创建多个协程，来完成验证中的多个驱动任务。我们可以将时钟的执行当做事件，在每个协程中等待这个事件，当时钟信号到来时，事件循环会唤醒所有等待的协程，使其继续执行，直到他们等待下一个时钟信号。\n我们用 Python 中的 asyncio 来实现对协程的支持：\nimport asyncio from dual_port_stack import * async def my_coro(dut, name): for i in range(10): print(f\"{name}: {i}\") await dut.AStep(1) async def test_dut(dut): asyncio.create_task(my_coro(dut, \"coroutine 1\")) asyncio.create_task(my_coro(dut, \"coroutine 2\")) await asyncio.create_task(dut.RunStep(10)) dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_dut(dut)) dut.Finish() 你可以直接运行上述代码来观察协程的执行过程。在上述代码中我们用 create_task 创建了两个协程任务并加入到事件循环中，每个协程任务中，会不断打印一个数字并等待下一个时钟信号到来。\n我们使用 dut.RunStep(10) 来创建一个后台时钟，它会不断产生时钟同步信号，使得其他协程能够在时钟信号到来时继续执行。\n基于协程驱动的双端口栈 利用协程，我们就可以将驱动双端口栈中单个端口逻辑写成一个独立的执行流，不需要再去维护大量的中间状态。\n下面是我们提供的一个简单的使用协程驱动的验证代码：\nimport asyncio import random from dual_port_stack import * from enum import Enum class StackModel: def __init__(self): self.stack = [] def commit_push(self, data): self.stack.append(data) print(\"Push\", data) def commit_pop(self, dut_data): print(\"Pop\", dut_data) model_data = self.stack.pop() assert model_data == dut_data, f\"The model data {model_data} is not equal to the dut data {dut_data}\" print(f\"Pass: {model_data} == {dut_data}\") class SinglePortDriver: class BusCMD(Enum): PUSH = 0 POP = 1 PUSH_OKAY = 2 POP_OKAY = 3 def __init__(self, dut, model: StackModel, port_dict): self.dut = dut self.model = model self.port_dict = port_dict async def send_req(self, is_push): self.port_dict[\"in_valid\"].value = 1 self.port_dict[\"in_cmd\"].value = self.BusCMD.PUSH.value if is_push else self.BusCMD.POP.value self.port_dict[\"in_data\"].value = random.randint(0, 2**8-1) await self.dut.AStep(1) await self.dut.Acondition(lambda: self.port_dict[\"in_ready\"].value == 1) self.port_dict[\"in_valid\"].value = 0 if is_push: self.model.commit_push(self.port_dict[\"in_data\"].value) async def receive_resp(self): self.port_dict[\"out_ready\"].value = 1 await self.dut.AStep(1) await self.dut.Acondition(lambda: self.port_dict[\"out_valid\"].value == 1) self.port_dict[\"out_ready\"].value = 0 if self.port_dict[\"out_cmd\"].value == self.BusCMD.POP_OKAY.value: self.model.commit_pop(self.port_dict[\"out_data\"].value) async def exec_once(self, is_push): await self.send_req(is_push) await self.receive_resp() for _ in range(random.randint(0, 5)): await self.dut.AStep(1) async def main(self): for _ in range(10): await self.exec_once(is_push=True) for _ in range(10): await self.exec_once(is_push=False) async def test_stack(stack): model = StackModel() port0 = SinglePortDriver(stack, model, { \"in_valid\": stack.in0_valid, \"in_ready\": stack.in0_ready, \"in_data\": stack.in0_data, \"in_cmd\": stack.in0_cmd, \"out_valid\": stack.out0_valid, \"out_ready\": stack.out0_ready, \"out_data\": stack.out0_data, \"out_cmd\": stack.out0_cmd, }) port1 = SinglePortDriver(stack, model, { \"in_valid\": stack.in1_valid, \"in_ready\": stack.in1_ready, \"in_data\": stack.in1_data, \"in_cmd\": stack.in1_cmd, \"out_valid\": stack.out1_valid, \"out_ready\": stack.out1_ready, \"out_data\": stack.out1_data, \"out_cmd\": stack.out1_cmd, }) asyncio.create_task(port0.main()) asyncio.create_task(port1.main()) await asyncio.create_task(dut.RunStep(200)) if __name__ == \"__main__\": dut = DUTdual_port_stack() dut.InitClock(\"clk\") asyncio.run(test_stack(dut)) dut.Finish() 与案例三类似，我们定义了一个 SinglePortDriver 类，用于驱动单个端口的逻辑。在 main 函数中，我们创建了两个 SinglePortDriver 实例，分别用于驱动两个端口。我们将两个端口的驱动过程放在了入口函数 main 中，并通过 asyncio.create_task 将其加入到事件循环中，在最后我们通过 dut.RunStep(200) 来创建了后台时钟，以推动测试的进行。\n该代码实现了与案例三一致的测试逻辑，即在每个端口中对栈进行 10 次 PUSH 和 10 次 POP 操作，并在操作完成后添加随机延迟。但你可以清晰的看到，利用协程进行编写，不需要维护任何的中间状态。\nSinglePortDriver 逻辑\n在 SinglePortDriver 类中，我们将一次操作封装为 exec_once 这一个函数，在 main 函数中只需要首先调用 10 次 exec_once(is_push=True) 来完成 PUSH 操作，再调用 10 次 exec_once(is_push=False) 来完成 POP 操作即可。\n在 exec_once 函数中，我们首先调用 send_req 函数来发送请求，然后调用 receive_resp 函数来接收响应，最后通过等待随机次数的时钟信号来模拟延迟。\nsend_req 和 receive_resp 函数的实现逻辑类似，只需要将对应的输入输出信号设置为对应的值，然后等待对应的信号变为有效即可，可以完全根据端口的执行顺序来编写。\n类似地，我们使用 StackModel 类来模拟栈的行为，在 commit_push 和 commit_pop 函数中分别模拟了 PUSH 和 POP 操作，并在 POP 操作中进行了数据的比较。\n运行测试 在picker_out_dual_port_stack文件夹中创建example.py将上述代码复制到其中，然后执行以下命令：\ncd picker_out_dual_port_stack python3 example.py 可直接运行本案例的测试代码，你将会看到类似如下的输出：\n... Push 141 Push 102 Push 63 Push 172 Push 208 Push 130 Push 151 ... Pop 102 Pass: 102 == 102 Pop 138 Pass: 138 == 138 Pop 56 Pass: 56 == 56 Pop 153 Pass: 153 == 153 Pop 129 Pass: 129 == 129 Pop 235 Pass: 235 == 235 Pop 151 ... 在输出中，你可以看到每次 PUSH 和 POP 操作的数据，以及每次 POP 操作的结果。如果输出中没有错误信息，则表示测试通过。\n协程驱动的优劣 通过协程函数，我们可以很好地实现并行操作，同时避免了回调函数所带来的问题。每个独立的执行流都能被完整保留，实现为一个协程，大大方便了代码的编写。\n然而，在更为复杂的场景下你会发现，实现了众多协程，会使得协程之间的同步和时序管理变得复杂。尤其是你需要在两个不与DUT直接交互的协程之间进行同步时，这种现象会尤为明显。\n在这时候，你就需要一套协程编写的规范以及验证代码的设计模式，来帮助你更好地编写基于协程的验证代码。因此，我们提供了 toffee 库，它提供了一套基于协程的验证代码设计模式，你可以通过使用 toffee 来更好地编写验证代码，你可以在这里去进一步了解 toffee。\n","categories":["示例项目","教程"],"description":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用协程驱动DUT","excerpt":"双端口栈是一个拥有两个端口的栈，每个端口都支持push和pop操作。本案例以双端口栈为例，展示如何使用协程驱动DUT","ref":"/mlvp/docs/quick-start/eg-stack-async/","tags":["examples","docs"],"title":"案例四：双端口栈（协程）"},{"body":"在Verilog中，一个module只有一个实例，但很多测试场景下需要实现多个module，为此picker提供了动态多实例和静态多实例的支持。\n动态多实例 动态多实例相当于类的实例化，在创建dut的同时实例化对应的module，所以用户无感知。支持最大16个实例同时运行。\n例子：\n以Adder为例，我们可以在测试时根据需要在合适的位置创建多个dut，来动态创建多个Adder实例。 当需要销毁一个dut时，也不会影响后续创建新的dut。\n创建一个名为 picker_out_adder 的文件夹，其中包含一个 Adder.v 文件。该文件的源码参考案例一：简单加法器。\n运行下述命令将RTL导出为 Python Module：\npicker export Adder.v --autobuild true -w Adder.fst --sname Adder 在picker_out_adder中添加 example.py，动态创建多个Adder实例：\nfrom Adder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): dut=[] # 可以通过创建多个dut，实例化多个Adder，理论上支持最大16个实例同时运行 for i in range(7): # 这里通过循环创建了7个dut dut.append(DUTAdder(waveform_filename=f\"{i}.fst\")) for d in dut: d.a.value = random_int() d.b.value = random_int() d.cin.value = random_int() \u0026 1 d.Step(1) print(f\"DUT: sum={d.sum.value}, cout={d.cout.value}\") # 通过Finish()函数在合适的时机撤销某个dut，也即销毁某个实例 d.Finish() # 可以根据需要在合适的时机创建新的Adder实例 # 下面创建了一个新的dut，旨在说明可以在程序结束前的任何时机创建新的dut dut_new = DUTAdder(waveform_filename=f\"new.fst\") dut_new.a.value = random_int() dut_new.b.value = random_int() dut_new.cin.value = random_int() \u0026 1 dut_new.Step(1) print(f\"DUT: sum={dut_new.sum.value}, cout={dut_new.cout.value}\") dut_new.Finish() if __name__ == \"__main__\": main() 注：目前仅支持 verilator模拟器\n静态多实例 静态多实例的使用不如动态多实例灵活，相当于在进行dut封装时就创建了n个目标模块。 需要在使用 picker 生成 dut_top.sv/v 的封装时，通过–sname参数指定多个模块名称和对应的数量。\n单个模块需要多实例 同样以Adder为例，在使用picker对dut进行封装时执行如下命令：\npicker export Adder.v --autobuild true -w Adder.fst --sname Adder,3 通过–sname参数指定在dut中创建3个Adder，封装后dut的引脚定义为：\n# init.py # 这里仅放置了部分代码 class DUTAdder(object): ... # all Pins # 静态多实例 self.Adder_0_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_0_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_0_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_1_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_1_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_1_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_2_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_2_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_2_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_2_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_2_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) ... 可以看到在 picker 生成 dut 时，就在 DUTAdder 内创建了多个Adder实例。\n下面是简单的多实例代码举例：\nfrom Adder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): # 在dut内部实例化了多个Adder dut = DUTAdder(waveform_filename = \"1.fst\") dut.Adder_0_a.value = random_int() dut.Adder_0_b.value = random_int() dut.Adder_0_cin.value = random_int() \u0026 1 dut.Adder_1_a.value = random_int() dut.Adder_1_b.value = random_int() dut.Adder_1_cin.value = random_int() \u0026 1 dut.Adder_2_a.value = random_int() dut.Adder_2_b.value = random_int() dut.Adder_2_cin.value = random_int() \u0026 1 dut.Step(1) print(f\"Adder_0: sum={dut.Adder_0_sum.value}, cout={dut.Adder_0_cout.value}\") print(f\"Adder_1: sum={dut.Adder_1_sum.value}, cout={dut.Adder_1_cout.value}\") print(f\"Adder_2: sum={dut.Adder_2_sum.value}, cout={dut.Adder_2_cout.value}\") # 静态多实例不可以根据需要动态的创建新的Adder实例，三个Adder实例的周期与dut的生存周期相同 dut.Finish() if __name__ == \"__main__\": main() 多个模块需要多实例 例如在 Adder.v 和 RandomGenerator.v 设计文件中分别有模块 Adder 和 RandomGenerator，RandomGenerator.v文件的源码为：\nmodule RandomGenerator ( input wire clk, input wire reset, input [127:0] seed, output [127:0] random_number ); reg [127:0] lfsr; always @(posedge clk or posedge reset) begin if (reset) begin lfsr \u003c= seed; end else begin lfsr \u003c= {lfsr[126:0], lfsr[127] ^ lfsr[126]}; end end assign random_number = lfsr; endmodule 需要 DUT 中有 2 个 Adder，3 个 RandomGenerator，生成的模块名称为 RandomAdder（若不指定，默认名称为 Adder_Random），则可执行如下命令：\npicker export Adder.v,RandomGenerator.v --sname Adder,2,RandomGenerator,3 --tname RandomAdder -w randomadder.fst 得到封装后的dut为DUTRandomAdder，包含2个Adder实例和3个RandomGenerator实例。\n封装后dut的引脚定义为：\n# init.py # 这里仅放置了部分代码 class DUTRandomAdder(object): ... # all Pins # 静态多实例 self.Adder_0_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_0_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_0_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_0_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.Adder_1_a = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_b = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.Adder_1_cin = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.Adder_1_sum = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.Adder_1_cout = xsp.XPin(xsp.XData(0, xsp.XData.Out), self.event) self.RandomGenerator_0_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_0_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_0_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_0_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.RandomGenerator_1_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_1_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_1_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_1_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) self.RandomGenerator_2_clk = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_2_reset = xsp.XPin(xsp.XData(0, xsp.XData.In), self.event) self.RandomGenerator_2_seed = xsp.XPin(xsp.XData(128, xsp.XData.In), self.event) self.RandomGenerator_2_random_number = xsp.XPin(xsp.XData(128, xsp.XData.Out), self.event) ... 可以看到在 picker 生成 dut 时，就在 DUTAdder 内创建了多个Adder实例。\n对应的测试代码举例为：\nfrom RandomAdder import * import random def random_int(): return random.randint(-(2**127), 2**127 - 1) \u0026 ((1 \u003c\u003c 127) - 1) def main(): # 在dut内部实例化了多个Adder dut = DUTRandomAdder() dut.InitClock(\"RandomGenerator_0_clk\") dut.InitClock(\"RandomGenerator_1_clk\") dut.InitClock(\"RandomGenerator_2_clk\") dut.Adder_0_a.value = random_int() dut.Adder_0_b.value = random_int() dut.Adder_0_cin.value = random_int() \u0026 1 dut.Adder_1_a.value = random_int() dut.Adder_1_b.value = random_int() dut.Adder_1_cin.value = random_int() \u0026 1 # 在dut内部实例化了多个RandomGenerator seed = random.randint(0, 2**128 - 1) dut.RandomGenerator_0_seed.value = seed dut.RandomGenerator_0_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_0_random_number.value:x}\") dut.Step(1) dut.RandomGenerator_1_seed.value = seed dut.RandomGenerator_1_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_1_random_number.value:x}\") dut.Step(1) dut.RandomGenerator_2_seed.value = seed dut.RandomGenerator_2_reset.value = 1 dut.Step(1) for i in range(10): print(f\"Cycle {i}, DUT: {dut.RandomGenerator_2_random_number.value:x}\") dut.Step(1) print(f\"Adder_0: sum={dut.Adder_0_sum.value}, cout={dut.Adder_0_cout.value}\") print(f\"Adder_1: sum={dut.Adder_1_sum.value}, cout={dut.Adder_1_cout.value}\") # 静态多实例各个模块多个实例的生命周期与dut的生命周期相同 dut.Finish() if __name__ == \"__main__\": main() ","categories":["示例项目","教程"],"description":"多实例示例","excerpt":"多实例示例","ref":"/mlvp/docs/env_usage/multiinstance/","tags":["examples","docs"],"title":"多实例"},{"body":" 在我们完成DUT验证后，编写验证报告是至关重要的一环。本节将从整体角度概述验证报告的结构以及报告所需覆盖的内容。\n验证报告是对整个验证过程的回顾，是验证合理与否的重要支持文件。一般情况下，验证报告需要包含以下内容：\n文档基本信息（作者、日志、版本等） 验证对象（验证目标） 功能点介绍 验证方案 测试点分解 测试用例 测试环境 结果分析 缺陷分析 测试结论 以下内容对列表进行进一步解释，具体示例可以参考nutshell_cache_report_demo.pdf\n1. 基本信息 应当包括作者、日志、版本、日期等。\n2. 验证对象（验证目标） 需要对您的验证对象做必要的介绍，可以包括其结构、基本功能、接口信息等。\n3. 功能点介绍 通过阅读设计文档或者源码，您需要总结DUT的目标功能，并将其细化为各功能点。\n4. 验证方案 应当包括您计划的验证流程以及验证框架。同时，您也应当接受您的框架各部分是如何协同工作的。\n5. 测试点分解 针对功能点提出的测试方法。具体可以包括在怎样的信号输入下应当观测到怎样的信号输出。\n6. 测试用例 测试点的具体实现。一个测试用例可以包括多个测试点。\n7. 测试环境 包括硬件信息、软件版本信息等。\n8. 结果分析 结果分析一般指覆盖率分析。通常来说应当考虑两类覆盖率：\n1. 行覆盖率： 在测试用例中有多少RTL行代码被执行。一般来说我们要求行覆盖率在98%以上。\n2. 功能覆盖率：根据相应的信号判断您提取的功能点是否被覆盖且被正确触发。一般我们要求测试用例覆盖每个功能点。\n9. 缺陷分析 对DUT存在的缺陷进行分析。可以包括设计文档的规范性和详细性、DUT功能的正确性（是否存在bug）、DUT功能是否能被触发等方面。\n10. 验证结论 验证结论是在完成芯片验证过程后得出的最终结论，是对以上内容的总结。\n","categories":["示例项目","学习材料"],"description":"概述验证报告的结构与内容。","excerpt":"概述验证报告的结构与内容。","ref":"/mlvp/docs/basic/report/","tags":["examples","docs"],"title":"验证报告"},{"body":"内部信号指的是不在模块的IO端口中暴露，但会在模块内部发挥控制、数据传输、状态跟踪功能的信号。一般来说，在picker将rtl转换成dut的过程中，只有IO端口才会被暴露，这些信号不会被主动暴露。\n然而，当验证人员需要寻求对模块内部逻辑更精细的验证，或者需要根据已知的bug进一步定位问题时，就需要接触硬件模块内部的信号，此时除了使用verilator和VCS这些传统工具以外，也可以采用picker提供的内部信号提取机制作为辅助。\n动机 以一个自带上限的计数器为例：\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); wire upper; assign upper = (count == 4'b1111); always @(posedge clk) begin if (reset) begin count = 4'b0000; end else if (!upper) begin count = count + 1; end end endmodule 模块的IO信号指的是直接写在模块定义中的信号，也就是：\nmodule UpperCounter ( input wire clk, input wire reset, output reg [3:0] count ); 该部分中的clk，reset和count即IO信号，是可以暴露出来的。\n而紧接着的\"wire upper;“也就是内部信号，其值是由模块的输入和模块内部的行为共同决定的。\n本案例的计数器逻辑相对简单，然而，对于规模较大的硬件模块，则存在以下痛点：\n当模块最终的输出和预期不符，存在问题的代码范围较大，亟需快速缩小问题范围的手段，\n模块内部逻辑复杂，理解存在困难，此时也需要一些内部标记理清模块的关键逻辑。\n对于以上痛点，都可以考虑诉诸内部信号。传统的查看内部信号的方式包括使用verilator和VCS。为进一步降低验证人员的使用门槛，我们的picker也提供了以下两种导出内部信号的方法： DPI直接导出和VPI动态导出。\nDPI 直接导出 DPI即Direct Programming Interface，是verilog与其他语言交互的接口，在picker的默认实现中，支持了为待测硬件模块的IO端口提供DPI。在执行picker时，如果添加了--internal 选项，则可同样为待测模块的内部信号提供DPI。此时，picker将会基于预定义的内部信号文件，在将verilog转化为DUT时，同步抽取rtl中的内部信号和IO端口一并暴露出来。\n编写信号文件 信号文件是我们向picker指定需要提取的内部信号的媒介，它规定了需提取内部信号的模块和该模块需要提取的内部信号。\n我们创建一个internal.yaml，内容如下：\nUpperCounter: - \"wire upper\" 第一行是模块名称，如UpperCounter，第二行开始是需要提取的模块内部信号，以“类型 信号名”的格式写出。比如，upper的类型为wire，我们就写成“wire upper” （理论上只要信号名符合verilog代码中的变量名就可以匹配到对应的信号，类型随便写都没问题，但还是建议写verilog语法支持的类型，比如wire、log、logic等）\n内部信号提取的能力取决于模拟器，譬如，verilator就无法提取下划线_开头的信号。\n注：多位宽的内部信号需要显式写出位宽，所以实际的格式是“类型 [宽度] 信号名”\nUpperCounter: - \"wire upper\" - \"reg [3:0] another_multiples\" # 本案例中这个信号不存在，只是用于说明yaml的格式 选项支持 写好信号文件之后，需要在运行picker时显式指定内部文件，这通过internal选项完成：\n--internal=[internal_signal_file] 完整命令如下：\npicker export --autobuild=true upper_counter.sv -w upper_counter.fst --sname UpperCounter \\ --tdir picker_out_upper_counter/ --lang python -e --sim verilator --internal=internal.yaml 我们可以找到picker为DUT配套生成的signals.json文件：\n{ \"UpperCounter_upper\": { \"High\": -1, \"Low\": 0, \"Pin\": \"wire\", \"_\": true }, \"clk\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true }, \"count\": { \"High\": 3, \"Low\": 0, \"Pin\": \"output\", \"_\": true }, \"reset\": { \"High\": -1, \"Low\": 0, \"Pin\": \"input\", \"_\": true } } 这个文件展示了picker生成的信号接口，可以看到，第一个信号UpperCounter_upper就是我们需要提取的内部信号， 其中第一个下划线之前的部分是我们在internal.yaml中的第一行定义的模块名UpperCounter，后面的部分则是内部信号名。\n访问信号 picker完成提取之后，内部信号的访问和io信号的访问就没有什么区别了，本质上他们都是dut上的一个XData，使用“dut.信号名”的方式访问即可。\nprint(dut.UpperCounter_upper.value) 优点 DPI直接导出在编译dut的过程中完成内部信号的导出，没有引入额外的运行时损耗，运行速度快。\n局限 1、在编译DUT时，导出的内部信号就已经确定了，如果在测试中需要修改调用的内部信号，则需要重新修改内部信号文件并用picker完成转化。\n2、导出的内部信号只可读取，不可写入，如果需要写入，则需要考虑接下来要介绍的VPI动态获取方法。\nVPI 动态获取 TBD\n优点：动态获取，能读能写 缺点：速度慢，请谨慎使用\n","categories":["示例项目","教程"],"description":"内部信号示例","excerpt":"内部信号示例","ref":"/mlvp/docs/env_usage/internalsignal/","tags":["examples","docs"],"title":"内部信号"},{"body":"","categories":["Sample Projects","Tutorials"],"description":"The Open Verification Platform supports multiple languages.","excerpt":"The Open Verification Platform supports multiple languages.","ref":"/mlvp/en/docs/multi-lang/","tags":["examples","docs"],"title":"Multi-language Support"},{"body":"","categories":["示例项目","教程"],"description":"开放验证平台支持多种语言","excerpt":"开放验证平台支持多种语言","ref":"/mlvp/docs/multi-lang/","tags":["examples","docs"],"title":"多语言支持"},{"body":" This learning resource introduces the basic concepts and techniques related to verification, as well as how to use the open-source tools provided by this project for chip verification.\nBefore studying this material, it is assumed that you already have basic knowledge of Linux, Python, etc.\nRelevant learning materials:\n《Linux 101》Online Lecture Notes Python Official Python Tutorial Official Python Tutorial If you plan to participate in the “Open Source Verification Projects” published on this platform, it is recommended to complete the study of this material in advance.\n","categories":"","description":"","excerpt":" This learning resource introduces the basic concepts and techniques …","ref":"/mlvp/en/docs/","tags":"","title":"Learning Resources"},{"body":" 本学习资源介绍验证相关的基本概念、技术，以及如何使用本项目提供的开源工具进行芯片验证\n学习本材料前，假定您已经拥有linux、python等相关基础知识。\n相关学习材料：\n《Linux 101》在线讲义 Python官方教程 Javatpoint上的Git基础 若计划参与本平台上发布的“开源开放验证项目”，建议提前完本材料的学习。\n","categories":"","description":"","excerpt":" 本学习资源介绍验证相关的基本概念、技术，以及如何使用本项目提供的开源工具进行芯片验证\n学习本材料前，假定您已经拥有linux、python …","ref":"/mlvp/docs/","tags":"","title":"学习资源"},{"body":"Software Testing Before we start with pytest, let’s understand software testing. Software testing generally involves the following four aspects:\nUnit Testing: Also known as module testing, it involves checking the correctness of program modules, which are the smallest units in software design. Integration Testing: Also known as assembly testing, it usually builds on unit testing by sequentially and incrementally testing all program modules, focusing on the interface parts of different modules. System Testing: It treats the entire software system as a whole for testing, including testing the functionality, performance, and the software’s running environment. Acceptance Testing: Refers to testing the entire system according to the project task book, contract, and acceptance criteria agreed upon by both the supply and demand sides, to determine whether to accept or reject the system. pytest was initially designed as a unit testing framework, but it also provides many features that allow it to be used for a wider range of testing, including integration testing and system testing. It is a very mature full-featured Python testing framework. It simplifies test writing and execution by collecting test functions and modules and providing a rich assertion library. It is a very mature and powerful Python testing framework with the following key features:\nSimple and Flexible: Pytest is easy to get started with and is flexible. Supports Parameterization: You can easily provide different parameters for test cases. Full-featured: Pytest not only supports simple unit testing but can also handle complex functional testing. You can even use it for automation testing, such as Selenium or Appium testing, as well as interface automation testing (combining Pytest with the Requests library). Rich Plugin Ecosystem: Pytest has many third-party plugins, and you can also customize extensions. Some commonly used plugins include: pytest-selenium: Integrates Selenium. pytest-html: Generates HTML test reports. pytest-rerunfailures: Repeats test cases in case of failure. pytest-xdist: Supports multi-CPU distribution. Well Integrated with Jenkins. Supports Allure Report Framework. This article will briefly introduce the usage of pytest based on testing requirements. The complete manual is available here for students to study in depth.\nInstalling Pytest # Install pytest: pip install pytest # Upgrade pytest pip install -U pytest # Check pytest version pytest --version # Check installed package list pip list # Check pytest help documentation pytest -h # Install third-party plugins pip install pytest-sugar pip install pytest-rerunfailures pip install pytest-xdist pip install pytest-assume pip install pytest-html Using Pytest Naming Convention # When using pytest, our module names are usually prefixed with test or end with test. You can also modify the configuration file to customize the naming convention. # test_*.py or *_test.py test_demo1 demo2_test # The class name in the module must start with Test and cannot have an init method. class TestDemo1: class TestLogin: # The test methods defined in the class must start with test_ test_demo1(self) test_demo2(self) # Test Case class test_one: def test_demo1(self): print(\"Test Case 1\") def test_demo2(self): print(\"Test Case 2\") Pytest Parameters pytest supports many parameters, which can be viewed using the help command.\npytest -help Here are some commonly used ones:\n-m: Specify multiple tag names with an expression. pytest provides a decorator @pytest.mark.xxx for marking tests and grouping them (xxx is the group name you defined), so you can quickly select and run them, with different groups separated by and or or.\n-v: Outputs more detailed information during runtime. Without -v, the runtime does not display the specific test case names being run; with -v, it prints out the specific test cases in the console.\n-q: Similar to the verbosity in unittest, used to simplify the runtime output. When running tests with -q, only simple runtime information is displayed, for example:\n.s.. [100%] 3 passed, 1 skipped in 9.60s -k: You can run specified test cases using an expression. It is a fuzzy match, with and or or separating keywords, and the matching range includes file names, class names, and function names.\n-x: Exit the test if one test case fails. This is very useful for debugging. When a test fails, stop running the subsequent tests.\n-s: Display print content. When running test scripts, we often add some print content for debugging or printing some content. However, when running pytest, this content is not displayed. If you add -s, it will be displayed.\npytest test_se.py -s Selecting Test Cases to Execute with Pytest In Pytest, you can select and execute test cases based on different dimensions such as test folders, test files, test classes, and test methods.\nExecute by test folder # Execute all test cases in the current folder and subfolders pytest . # Execute all test cases in the tests folder and its subfolders, which are at the same level as the current folder pytest ../tests # Execute by test file # Run all test cases in test_se.py pytest test_se.py # Execute by test class, must be in the following format: pytest file_name.py::TestClass, where \"::\" is the separator used to separate the test module and test class. # Run all test cases under the class named TestSE in the test_se.py file pytest test_se.py::TestSE # Execute by test method, must be in the following format: pytest file_name.py::TestClass::TestMethod, where \"::\" is the separator used to separate the test module, test class, and test method. # Run the test case named test_get_new_message under the class named TestSE in the test_se.py file pytest test_se.py::TestSE::test_get_new_message # The above methods of selecting test cases are all on the **command line**. If you want to execute directly in the test program, you can directly call pytest.main(), the format is: pytest.main([module.py::class::method]) In addition, Pytest also supports multiple ways to control the execution of test cases, such as filtering execution, running in multiple processes, retrying execution, etc.\nWriting Validation with Pytest During testing, we use the previously validated adder. Go to the Adder folder, create a new test_adder.py file in the picker_out_adder directory, with the following content: # Import test modules and required libraries from UT_Adder import * import pytest import ctypes import random # Use pytest fixture to initialize and clean up resources @pytest.fixture def adder(): # Create an instance of DUTAdder, load the dynamic link library dut = DUTAdder() # Execute one clock step to prepare the DUT dut.Step(1) # The code after the yield statement will be executed after the test ends, used to clean up resources yield dut # Clean up DUT resources and generate test coverage reports and waveforms dut.Finish() class TestFullAdder: # Define full_adder as a static method, as it does not depend on class instances @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # Use the pytest.mark.usefixtures decorator to specify the fixture to use @pytest.mark.usefixtures(\"adder\") # Define the test method, where adder is injected by pytest through the fixture def test_adder(self, adder): # Perform multiple random tests for _ in range(114514): # Generate random 64-bit a, b, and 1-bit cin a = random.getrandbits(64) b = random.getrandbits(64) cin = random.getrandbits(1) # Set the input of the DUT adder.a.value = a adder.b.value = b adder.cin.value = cin # Execute one clock step adder.Step(1) # Calculate the expected result using a static method sum, cout = self.full_adder(a, b, cin) # Assert that the output of the DUT is the same as the expected result assert sum == adder.sum.value assert cout == adder.cout.value if __name__ == \"__main__\": pytest.main(['-v', 'test_adder.py::TestFullAdder']) After running the test, the output is as follows: collected 1 item test_adder.py ✓ 100% ██████████ Results (4.33s): The successful test indicates that after 114514 loops, our device has not found any bugs for now. However, using randomly generated test cases with multiple loops consumes a considerable amount of resources, and these randomly generated test cases may not effectively cover all boundary conditions. In the next section, we will introduce a more efficient method for generating test cases.\n","categories":["Sample Projects","Tutorials"],"description":"Used for managing tests and generating test reports.","excerpt":"Used for managing tests and generating test reports.","ref":"/mlvp/en/docs/env_usage/frameworks/pytest/","tags":["examples","docs"],"title":"PyTest"},{"body":"软件测试 在正式开始pytest 之间我们先了解一下软件的测试，软件测试一般分为如下四个方面\n单元测试：称模块测试，针对软件设计中的最小单位——程序模块，进行正确性检查的测试工作 集成测试：称组装测试，通常在单元测试的基础上，将所有程序模块进行有序的、递增测试，重点测试不同模块的接口部分 系统测试：将整个软件系统看成一个整体进行测试，包括对功能、性能以及软件所运行的软硬件环境进行测试 验收测试：指按照项目任务书或合同、供需双方约定的验收依据文档进行的对整个系统的测试与评审，决定是否接收或拒收系统 pytest最初是作为一个单元测试框架而设计的，但它也提供了许多功能，使其能够进行更广泛的测试，包括集成测试，系统测试，他是一个非常成熟的全功能的python 测试框架。 它通过收集测试函数和模块，并提供丰富的断言库来简化测试的编写和运行，是一个非常成熟且功能强大的 Python 测试框架，具有以下几个特点：\n简单灵活：Pytest 容易上手，且具有灵活性。 支持参数化：您可以轻松地为测试用例提供不同的参数。 全功能：Pytest 不仅支持简单的单元测试，还可以处理复杂的功能测试。您甚至可以使用它来进行自动化测试，如 Selenium 或 Appium 测试，以及接口自动化测试（结合 Pytest 和 Requests 库）。 丰富的插件生态：Pytest 有许多第三方插件，您还可以自定义扩展。一些常用的插件包括： pytest-selenium：集成 Selenium。 pytest-html：生成HTML测试报告。 pytest-rerunfailures：在失败的情况下重复执行测试用例。 pytest-xdist：支持多 CPU 分发。 与 Jenkins 集成良好。 支持 Allure 报告框架。 本文将基于测试需求简单介绍pytest的用法，其完整手册在这里，供同学们进行深入学习。\nPytest安装 # 安装pytest： pip install pytest # 升级pytest pip install -U pytest # 查看pytest版本 pytest --version # 查看已安装包列表 pip list # 查看pytest帮助文档 pytest -h # 安装第三方插件 pip install pytest-sugar pip install pytest-rerunfailures pip install pytest-xdist pip install pytest-assume pip install pytest-html Pytest使用 命名规则 # 首先在使用pytest 时我们的模块名通常是以test开头或者test结尾，也可以修改配置文件，自定义命名规则 # test_*.py 或 *_test.py test_demo1 demo2_test # 模块中的类名要以Test 开始且不能有init 方法 class TestDemo1: class TestLogin: # 类中定义的测试方法名要以test_开头 test_demo1(self) test_demo2(self) # 测试用例 class test_one: def test_demo1(self): print(\"测试用例1\") def test_demo2(self): print(\"测试用例2\") Pytest 参数 pytest支持很多参数，可以通过help命令查看\npytest -help 我们在这里列出来常用的几个：\n-m: 用表达式指定多个标记名。 pytest 提供了一个装饰器 @pytest.mark.xxx，用于标记测试并分组（xxx是你定义的分组名），以便你快速选中并运行，各个分组直接用 and、or 来分割。\n-v: 运行时输出更详细的用例执行信息 不使用-v参数，运行时不会显示运行的具体测试用例名称；使用-v参数，会在 console 里打印出具体哪条测试用例被运行。\n-q: 类似 unittest 里的 verbosity，用来简化运行输出信息。 使用 -q 运行测试用例，仅仅显示很简单的运行信息， 例如：\n.s.. [100%] 3 passed, 1 skipped in 9.60s -k: 可以通过表达式运行指定的测试用例。 它是一种模糊匹配，用 and 或 or 区分各个关键字，匹配范围有文件名、类名、函数名。\n-x: 出现一条测试用例失败就退出测试。 在调试时，这个功能非常有用。当出现测试失败时，停止运行后续的测试。\n-s: 显示print内容 在运行测试脚本时，为了调试或打印一些内容，我们会在代码中加一些print内容，但是在运行pytest时，这些内容不会显示出来。如果带上-s，就可以显示了。\npytest test_se.py -s Pytest 选择测试用例执行 在 Pytest 中，您可以按照测试文件夹、测试文件、测试类和测试方法的不同维度来选择执行测试用例。\n按照测试文件夹执行 # 执行所有当前文件夹及子文件夹下的所有测试用例 pytest . # 执行跟当前文件夹同级的tests文件夹及子文件夹下的所有测试用例 pytest ../tests # 按照测试文件执行 # 运行test_se.py下的所有的测试用例 pytest test_se.py # 按照测试类执行，必须以如下格式： pytest 文件名 .py:: 测试类，其中“::”是分隔符，用于分割测试module和测试类。 # 运行test_se.py文件下的，类名是TestSE下的所有测试用例 pytest test_se.py::TestSE # 测试方法执行，必须以如下格式： pytest 文件名 .py:: 测试类 :: 测试方法，其中 “::” 是分隔符，用于分割测试module、测试类，以及测试方法。 # 运行test_se.py文件下的，类名是TestSE下的，名字为test_get_new_message的测试用例 pytest test_se.py::TestSE::test_get_new_message # 以上选择测试用例的方法均是在**命令行**，如果您想直接在测试程序里执行可以直接在main函数中**调用pytest.main()**,其格式为： pytest.main([模块.py::类::方法]) 此外，Pytest 还支持控制测试用例执行的多种方式，例如过滤执行、多进程运行、重试运行等。\n使用Pytest编写验证 在测试过程中，我们使用之前验证过的加法器，进入Adder文件夹，在picker_out_adder目录下新建一个test_adder.py文件，内容如下： # 导入测试模块和所需的库 from UT_Adder import * import pytest import ctypes import random # 使用 pytest fixture 来初始化和清理资源 @pytest.fixture def adder(): # 创建 DUTAdder 实例，加载动态链接库 dut = DUTAdder() # 执行一次时钟步进，准备 DUT dut.Step(1) # yield 语句之后的代码会在测试结束后执行，用于清理资源 yield dut # 清理DUT资源，并生成测试覆盖率报告和波形 dut.Finish() class TestFullAdder: # 将 full_adder 定义为静态方法，因为它不依赖于类实例 @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # 使用 pytest.mark.usefixtures 装饰器指定使用的 fixture @pytest.mark.usefixtures(\"adder\") # 定义测试方法，adder 参数由 pytest 通过 fixture 注入 def test_adder(self, adder): # 进行多次随机测试 for _ in range(114514): # 随机生成 64 位的 a 和 b，以及 1 位的进位 cin a = random.getrandbits(64) b = random.getrandbits(64) cin = random.getrandbits(1) # 设置 DUT 的输入 adder.a.value = a adder.b.value = b adder.cin.value = cin # 执行一次时钟步进 adder.Step(1) # 使用静态方法计算预期结果 sum, cout = self.full_adder(a, b, cin) # 断言 DUT 的输出与预期结果相同 assert sum == adder.sum.value assert cout == adder.cout.value if __name__ == \"__main__\": pytest.main(['-v', 'test_adder.py::TestFullAdder']) 运行测试之后输出如下： collected 1 item test_adder.py ✓ 100% ██████████ Results (4.33s): 测试成功表明，在经过114514次循环之后，我们的设备暂时没有发现bug。然而，使用多次循环的随机数生成测试用例会消耗大量资源，并且这些随机生成的测试用例可能无法有效覆盖所有边界条件。在下一部分，我们将介绍一种更有效的测试用例生成方法。\n","categories":["示例项目","教程"],"description":"可用来管理测试，生成测试报告","excerpt":"可用来管理测试，生成测试报告","ref":"/mlvp/docs/env_usage/frameworks/pytest/","tags":["examples","docs"],"title":"PyTest"},{"body":"Hypothesis In the previous section, we manually wrote test cases and specified inputs and expected outputs for each case. This method has some issues, such as incomplete test case coverage and the tendency to overlook boundary conditions. Hypothesis is a Python library for property-based testing. Its main goal is to make testing simpler, faster, and more reliable. It uses a method called property-based testing, where you can write some hypotheses for your code, and Hypothesis will automatically generate test cases to verify these hypotheses. This makes it easier to write comprehensive and efficient tests. Hypothesis can automatically generate various types of input data, including basic types (e.g., integers, floats, strings), container types (e.g., lists, sets, dictionaries), and custom types. It tests based on the properties (assertions) you provide. If a test fails, it will try to narrow down the input data to find the smallest failing case. With Hypothesis, you can better cover the boundary conditions of your code and uncover errors you might not have considered. This helps improve the quality and reliability of your code.\nBasic Concepts Test Function: The function or method to be tested. Properties: Conditions that the test function should satisfy. Properties are applied to the test function as decorators. Strategy: A generator for test data. Hypothesis provides a range of built-in strategies, such as integers, strings, lists, etc. You can also define custom strategies. Test Generator: A function that generates test data based on strategies. Hypothesis automatically generates test data and passes it as parameters to the test function. This article will briefly introduce Hypothesis based on testing requirements. The complete manual is available for in-depth study.\nInstallation Install with pip and import in Python to use:\npip install hypothesis import hypothesis Basic Usage Properties and Strategies Hypothesis uses property decorators to define the properties of test functions. The most common decorator is @given, which specifies the properties the test function should satisfy. We can define a test function test_addition using the @given decorator and add properties to x. The test generator will automatically generate test data for the function and pass it as parameters, for example:\ndef addition(number: int) -\u003e int: return number + 1 @given(x=integers(), y=integers())　def test_addition(x, y):　assert x + 1 == addition（1） In this example, integers() is a built-in strategy for generating integer test data. Hypothesis offers a variety of built-in strategies for generating different types of test data. Besides integers(), there are strategies for strings, booleans, lists, dictionaries, etc. For instance, using the text() strategy to generate string test data and using lists(text()) to generate lists of strings:\n@given(s=text(), l=lists(text())) def test_string_concatenation(s, l):　result = s + \"\".join(l)　assert len(result) == len(s) + sum(len(x) for x in l) You can also define custom strategies to generate specific types of test data, for example, a strategy for non-negative integers:\ndef non_negative_integers(): return integers(min_value=0) @given(x=non_negative_integers()) def test_positive_addition(x): assert x + 1 \u003e x Expectations We can use expect to specify the expected result of a function:\n@given(x=integers()) def test_addition(x): expected = x + 1 actual = addition(x) Hypotheses and Assertions When using Hypothesis for testing, we can use standard Python assertions to verify the properties of the test function. Hypothesis will automatically generate test data and run the test function based on the properties defined in the decorator. If an assertion fails, Hypothesis will try to narrow down the test data to find the smallest failing case.\nSuppose we have a string reversal function. We can use an assert statement to check if reversing a string twice equals itself:\ndef test_reverse_string(s): expected = x + 1 actual = addition(x) assert actual == expected Writing Tests Tests in Hypothesis consist of two parts: a function that looks like a regular test in your chosen framework but with some extra parameters, and a @given decorator specifying how to provide those parameters. Here’s an example of how to use it to verify a full adder, which we tested previously:\nBased on the previous section’s code, we modify the method of generating test cases from random numbers to the integers() method. The modified code is as follows:\nfrom UT_Adder import * import pytest import ctypes import random from hypothesis import given, strategies as st # Initializing and Cleaning Up Resources Using pytest Fixture from UT_Adder import * import pytest import ctypes from hypothesis import given, strategies as st # Using pytest fixture to initialize and clean up resources @pytest.fixture(scope=\"class\") def adder(): # Create DUTAdder instance and load dynamic library dut = DUTAdder() # Perform a clock step to prepare the DUT dut.Step(1) # Code after yield executes after tests finish, for cleanup yield dut # Clean up DUT resources and generate coverage report and waveform dut.finalize() class TestFullAdder: # Define full_adder as a static method, as it doesn't depend on class instance @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = ctypes.c_uint64(a).value Sum += ctypes.c_uint64(b).value + cin Cout = (Sum \u003e\u003e 64) \u0026 0b1 Sum \u0026= 0xffffffffffffffff return Sum, Cout # Use Hypothesis to automatically generate test cases @given( a=st.integers(min_value=0, max_value=0xffffffffffffffff), b=st.integers(min_value=0, max_value=0xffffffffffffffff), cin=st.integers(min_value=0, max_value=1) ) # Define test method, adder parameter injected by pytest via fixture def test_full_adder_with_hypothesis(self, adder, a, b, cin): # Calculate expected sum and carry sum_expected, cout_expected = self.full_adder(a, b, cin) # Set DUT inputs adder.a.value = a adder.b.value = b adder.cin.value = cin # Perform a clock step adder.Step(1) # Assert DUT outputs match expected results assert sum_expected == adder.sum.value assert cout_expected == adder.cout.value if __name__ == \"__main__\": # Run specified tests in verbose mode pytest.main(['-v', 'test_adder.py::TestFullAdder']) In this example, the @given decorator and strategies are used to generate random data that meets specified conditions. st.integers() is a strategy for generating integers within a specified range, used to generate numbers between 0 and 0xffffffffffffffff for a and b, and between 0 and 1 for cin. Hypothesis will automatically rerun this test multiple times, each time using different random inputs, helping reveal potential boundary conditions or edge cases.\nRun the tests, and the output will be as follows: collected 1 item test_adder.py ✓ 100% ██████████ Results (0.42s): 1 passed As we can see, the tests were completed in a short amount of time.\n","categories":["Sample Projects","Tutorials"],"description":"Can Be Used to Generate Stimuli","excerpt":"Can Be Used to Generate Stimuli","ref":"/mlvp/en/docs/env_usage/frameworks/hypothesis/","tags":["examples","docs"],"title":"Hypothesis"},{"body":"Hypothesis 在上一节中，我们通过手动编写测试用例，并为每个用例指定输入和预期输出。这种方式存在一些问题，例如测试用例覆盖不全面、边界条件 容易被忽略等。它是一个用于属性基于断言的软件测试的 Python 库。Hypothesis 的主要目标是使测试更简单、更快速、更可靠。它使用了一种称为属性基于断言的测试方法，即你可以为你的代码编写一些假（hypotheses），然后 Hypothesis 将会自动生成测试用例并验证这些假设。这使得编写全面且高效的测试变得更加容易。Hypothesis 可以自动生成各种类型的输入数据，包括基本类型（例如整数、浮点数、字符串等）、容器类型（例如列表、集合、字典等）、自定义类型等。然后，它会根据你提供的属性（即断言）进行测试，如果发现测试失败，它将尝试缩小输入数据的范围以找出最小的失败案例。通过 Hypothesis，你可以更好地覆盖代码的边界条件，并发现那些你可能没有考虑到的错误情况。这有助于提高代码的质量和可靠性。\n基本概念 测试函数：即待测试的函数或方法，我们需要对其进行测试。 属性：定义了测试函数应该满足的条件。属性是以装饰器的形式应用于测试函数上的。 策略：用于生成测试数据的生成器。Hypothesis 提供了一系列内置的策略，如整数、字符串、列表等。我们也可以自定义策略。 测试生成器：基于策略生成测试数据的函数。Hypothesis 会自动为我们生成测试数据，并将其作为参数传递给测试函数。 本文将基于测试需求简单介绍Hypothesis的用法，其完整手册在这里，供同学们进行深入学习。\n安装 使用pip安装，在python中导入即可使用\npip install hypothesis import hypothesis 基本用法 属性和策略 Hypothesis 使用属性装饰器来定义测试函数的属性。最常用的装饰器是 @given，它指定了测试函数应该满足的属性。 我们可以通过@given 装饰器定义了一个测试函数 test_addition。并给x 添加对应的属性，测试生成器会自动为测试函数生成测试数据，并将其作为参数传递给函数，例如\ndef addition(number: int) -\u003e int: return number + 1 @given(x=integers(), y=integers())　def test_addition(x, y):　assert x + 1 == addition（1） 其中integers () 是一个内置的策略，用于生成整数类型的测试数据。Hypothesis 提供了丰富的内置策略，用于生成各种类型的测试数据。除了integers ()之外，还有字符串、布尔值、列表、字典等策略。例如使用 text () 策略生成字符串类型的测试数据，使用 lists (text ()) 策略生成字符串列表类型的测试数据\n@given(s=text(), l=lists(text())) def test_string_concatenation(s, l):　result = s + \"\".join(l)　assert len(result) == len(s) + sum(len(x) for x in l) 除了可以使用内置的策略以外，还可以使用自定义策略来生成特定类型的测试数据，例如我们可以生产一个非负整形的策略\ndef non_negative_integers(): return integers(min_value=0) @given(x=non_negative_integers()) def test_positive_addition(x): assert x + 1 \u003e x 期望 我们可以通过expect 来指明需要的函数期待得到的结果\n@given(x=integers()) def test_addition(x): expected = x + 1 actual = addition(x) 假设和断言 在使用 Hypothesis 进行测试时，我们可以使用标准的 Python 断言来验证测试函数的属性。Hypothesis 会自动为我们生成测试数据，并根据属性装饰器中定义的属性来运行测试函数。如果断言失败，Hypothesis 会尝试缩小测试数据的范围，以找出导致失败的最小样例。\n假如我们有一个字符串反转函数，我们可以通过assert 来判断翻转两次后他是不是等于自身\ndef test_reverse_string(s): expected = x + 1 actual = addition(x) assert actual == expected 编写测试 Hypothesis 中的测试由两部分组成：一个看起来像您选择的测试框架中的常规测试但带有一些附加参数的函数，以及一个@given指定如何提供这些参数的装饰器。以下是如何使用它来验证我们之前验证过的全加器的示例：\n在上一节的代码基础上，我们进行一些修改，将生成测试用例的方法从随机数修改为integers ()方法，修改后的代码如下：\nfrom Adder import * import pytest from hypothesis import given, strategies as st # 使用 pytest fixture 来初始化和清理资源 @pytest.fixture(scope=\"class\") def adder(): # 创建 DUTAdder 实例，加载动态链接库 dut = DUTAdder() # yield 语句之后的代码会在测试结束后执行，用于清理资源 yield dut # 清理DUT资源，并生成测试覆盖率报告和波形 dut.Finish() class TestFullAdder: # 将 full_adder 定义为静态方法，因为它不依赖于类实例 @staticmethod def full_adder(a, b, cin): cin = cin \u0026 0b1 Sum = a Sum += b + cin Cout = (Sum \u003e\u003e 128) \u0026 0b1 Sum \u0026= 0xffffffffffffffffffffffffffffffff return Sum, Cout # 使用 hypothesis 自动生成测试用例 @given( a=st.integers(min_value=0, max_value=0xffffffffffffffff), b=st.integers(min_value=0, max_value=0xffffffffffffffff), cin=st.integers(min_value=0, max_value=1) ) # 定义测试方法，adder 参数由 pytest 通过 fixture 注入 def test_full_adder_with_hypothesis(self, adder, a, b, cin): # 计算预期的和与进位 sum_expected, cout_expected = self.full_adder(a, b, cin) # 设置 DUT 的输入 adder.a.value = a adder.b.value = b adder.cin.value = cin # 执行一次时钟步进 adder.Step(1) # 断言 DUT 的输出与预期结果相同 assert sum_expected == adder.sum.value assert cout_expected == adder.cout.value if __name__ == \"__main__\": # 以详细模式运行指定的测试 pytest.main(['-v', 'test_adder.py::TestFullAdder']) 这个例子中，@given 装饰器和 strategies 用于生成符合条件的随机数据。st.integers() 是生成指定范围整数的策略，用于为 a 和 b 生成 0 到 0xffffffffffffffff 之间的数，以及为 cin 生成 0 或 1。Hypothesis会自动重复运行这个测试，每次都使用不同的随机输入，这有助于揭示潜在的边界条件或异常情况。\n运行测试，输出结果如下： collected 1 item test_adder.py ✓ 100% ██████████ Results (0.42s): 1 passed 可以看到在很短的时间里我们已经完成了测试\n","categories":["示例项目","教程"],"description":"可用来生成激励","excerpt":"可用来生成激励","ref":"/mlvp/docs/env_usage/frameworks/hypothesis/","tags":["examples","docs"],"title":"Hypothesis"},{"body":" 在芯片验证的传统实践中，UVM等框架被广泛采用。尽管它们提供了一整套验证方法，但通常只适用于特定的硬件描述语言和仿真环境。本工具突破了这些限制，能够将仿真代码转换成C++或Python，使得我们可以利用软件验证工具来进行更全面的测试。\n因为Python具有强大的生态系统，所以本项目主要以Python作为示例，简单介绍Pytest和Hypothesis两个经典软件测试框架。Pytest以其简洁的语法和丰富的功能，轻松应对各种测试需求。而Hypothesis则通过生成测试用例，揭示出意料之外的边缘情况，提高了测试的全面性和深度。\n我们的项目从一开始就设计为与多种现代软件测试框架兼容。我们鼓励您探索这些工具的潜力，并将其应用于您的测试流程中。通过亲身实践，您将更深刻地理解这些工具如何提升代码的质量和可靠性。让我们一起努力，提高芯片开发的质量。\n","categories":["示例项目","教程"],"description":"可用软件测试框架","excerpt":"可用软件测试框架","ref":"/mlvp/docs/env_usage/frameworks/","tags":["examples","docs"],"title":"集成测试框架"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/example-projects/","tags":"","title":"Example Projects"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/tutorials/","tags":"","title":"Tutorials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E6%95%99%E7%A8%8B/","tags":"","title":"教程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E7%A4%BA%E4%BE%8B%E9%A1%B9%E7%9B%AE/","tags":"","title":"示例项目"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/mlvp/en/","tags":"","title":"Goldydocs"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/mlvp/","tags":"","title":"Goldydocs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/learning-materials/","tags":"","title":"Learning Materials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/sample-projects/","tags":"","title":"Sample Projects"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/en/categories/tutorial/","tags":"","title":"Tutorial"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/search/","tags":"","title":"搜索结果"},{"body":"","categories":"","description":"","excerpt":"","ref":"/mlvp/categories/%E5%AD%A6%E4%B9%A0%E6%9D%90%E6%96%99/","tags":"","title":"学习材料"}]