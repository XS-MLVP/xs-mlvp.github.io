<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenVerify Courses – Branch Predictor and Kunming Lake Microarchitecture Implementation</title>
    <link>https://xs-mlvp.github.io/xs-bpu/en/docs/</link>
    <description>Recent content in Branch Predictor and Kunming Lake Microarchitecture Implementation on OpenVerify Courses</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	  <atom:link href="https://xs-mlvp.github.io/xs-bpu/en/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: BPU Top Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/00_bpufeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/00_bpufeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;upport uFTB sub-predictor&lt;/li&gt;
&lt;li&gt;Support TAGE-SC sub-predictor&lt;/li&gt;
&lt;li&gt;Support FTB sub-predictor&lt;/li&gt;
&lt;li&gt;Support ITTAGE sub-predictor&lt;/li&gt;
&lt;li&gt;Support RAS sub-predictor&lt;/li&gt;
&lt;li&gt;Support three-stage prediction result and other information output&lt;/li&gt;
&lt;li&gt;Support prediction result redirection signal generation&lt;/li&gt;
&lt;li&gt;Support pipeline control signal generation&lt;/li&gt;
&lt;li&gt;Support PC generation&lt;/li&gt;
&lt;li&gt;Support global branch history maintenance&lt;/li&gt;
&lt;li&gt;Support branch folding history maintenance&lt;/li&gt;
&lt;li&gt;Support redirection request response, state restoration&lt;/li&gt;
&lt;li&gt;Support update request response&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: BPU Top Module</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/00_bpu_top/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/00_bpu_top/</guid>
      <description>
        
        
        &lt;p&gt;The overall function and structure of the BPU top level have been roughly described in previous documents. For those verifying the BPU top level, a more detailed description might be needed. Due to the many functions of the BPU top level, this section divides the BPU into several major functional points for further description. However, since there are too many details at the BPU top level, further details need to be understood by referring to the code.&lt;/p&gt;
&lt;h2 id=&#34;generator-maintenance-method&#34;&gt;Generator Maintenance Method&lt;/h2&gt;
&lt;p&gt;From the basic design documents of Xiangshan, we know that the BPU top level maintains various variables in the s0 cycle through generators, such as PC, branch history, etc. The core concept is to decide which pipeline level&amp;rsquo;s result to adopt through the redirection signal of the prediction result.&lt;/p&gt;
&lt;p&gt;There are a total of 6 generators in the BPU top level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;npcGen&lt;/strong&gt; maintains the PC&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ghistPtrGen&lt;/strong&gt; maintains the global history pointer&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ghvBitWriteGens&lt;/strong&gt; maintains global history write data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;foledGhGen&lt;/strong&gt; maintains the folded history&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lastBrNumOHGen&lt;/strong&gt; maintains the position of the last effective branch instruction in the previous cycle&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;aheadFhObGen&lt;/strong&gt; maintains the oldest position of the branch history&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Except for &lt;code&gt;npcGen&lt;/code&gt;, the rest of the generators will be introduced in this document. In this section, we will focus on the method of generating the next prediction for the generators.&lt;/p&gt;
&lt;p&gt;In the code, you can see generators defined in a similar way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;val&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;PhyPriorityMuxGenerator&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;UInt&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, the code registers the data sources for the generators through multiple statements:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;register&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;B&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;reg&lt;/span&gt;&lt;span style=&#34;color:#a40000&#34;&gt;，&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;...)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;register&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;s1_valid&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s1_target&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;...)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;register&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;s2_redirect&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_target&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;...)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;register&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;s3_redirect&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_target&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;...)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;npcGen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;register&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;do_redirect&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;valid&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;do_redirect&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;bits&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;cfiUpdate&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;target&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;...)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each line is called a registration. In a registration, the first signal parameter is the data valid signal, and the second signal parameter contains the specific data. The priority of the generator is also determined in the order of registration; the later the registration, the higher the priority. Therefore, the priority at the same time, from low to high, is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;s0 blocked data&lt;/li&gt;
&lt;li&gt;Data updated based on s1 prediction results&lt;/li&gt;
&lt;li&gt;Data updated based on s2 prediction results&lt;/li&gt;
&lt;li&gt;Data updated based on s3 prediction results&lt;/li&gt;
&lt;li&gt;Data in external redirection of BPU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, when the redirection of the prediction result is valid, we can avoid using the earlier pipeline level&amp;rsquo;s prediction result and adopt the corrected prediction result. This allows us to handle external redirection requests with the highest priority.&lt;/p&gt;
&lt;p&gt;We can conclude the method by which all generators generate s0 signals: &lt;strong&gt;Among all data valid signals, if only one is valid, the corresponding data is selected; if multiple data valid signals are valid, the data with the highest priority is selected&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;global-branch-history&#34;&gt;Global Branch History&lt;/h2&gt;
&lt;p&gt;We know that the global branch history is maintained at the BPU top level, and the maintenance strategy is consistent with the PC maintenance strategy. That is, after the prediction result is generated at each stage of the pipeline, the global branch history is updated based on the corresponding signals.&lt;/p&gt;
&lt;p&gt;The top level defines two sets of signals to maintain the global branch history:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ghv&lt;/strong&gt; stores the global branch history (maximum length 256)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ghist_ptr&lt;/strong&gt; global branch history pointer, pointing to the current position of the global branch history&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to &lt;code&gt;s0_pc&lt;/code&gt;, &lt;code&gt;s1_pc&lt;/code&gt;, &lt;code&gt;s2_pc&lt;/code&gt;, the BPU top level also maintains signals for each stage of the global history pointer: &lt;code&gt;s0_ghist_ptr&lt;/code&gt;, &lt;code&gt;s1_ghist_ptr&lt;/code&gt;, &lt;code&gt;s2_ghist_ptr&lt;/code&gt;. However, the content in &lt;code&gt;ghv&lt;/code&gt; is fixed in position, and we only use &lt;code&gt;ghist_ptr&lt;/code&gt; to locate where the current global branch history starts.&lt;/p&gt;
&lt;h3 id=&#34;calculating-the-current-global-branch-history-with-ghist_ptr&#34;&gt;Calculating the Current Global Branch History with ghist_ptr&lt;/h3&gt;
&lt;p&gt;The use of &lt;code&gt;ghist_ptr&lt;/code&gt; is only visible at the BPU top level. What we pass to the sub-predictors is the global branch history after the data in the global history register is shifted based on &lt;code&gt;ghist_ptr&lt;/code&gt;. In the global branch history obtained by the sub-predictor, the least significant bit corresponds to the newest bit of the global branch history, and the most significant bit corresponds to the oldest bit.&lt;/p&gt;
&lt;p&gt;So how is the shifting done? First, let&amp;rsquo;s see how the global history is stored in &lt;code&gt;ghv&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;|===== ghist =====&amp;gt;| =======&amp;gt;|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n                  ^         0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   ghist_ptr
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As shown in the figure above, the sequence represents the entire &lt;code&gt;ghv&lt;/code&gt; register, and &lt;code&gt;ghist_ptr&lt;/code&gt; points to a position in &lt;code&gt;ghv&lt;/code&gt;. This position represents the newest bit of the global branch history. When a new global history record needs to be added, &lt;code&gt;ghist_ptr&lt;/code&gt; is first decremented by 1, and then this bit is written to the position it points to. When &lt;code&gt;ghist_ptr&lt;/code&gt; is decremented to 0, it will loop back to point to the highest position, thereby overwriting the previously written global branch history.&lt;/p&gt;
&lt;p&gt;No matter what, starting from the position pointed to by &lt;code&gt;ghist_ptr&lt;/code&gt;, the pointer increases and the history gets older. Therefore, when we need to calculate the current global branch history, we only need to circularly right-shift the &lt;code&gt;ghv&lt;/code&gt; register by &lt;code&gt;ghist_ptr positions&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;updating-global-branch-history&#34;&gt;Updating Global Branch History&lt;/h3&gt;
&lt;p&gt;The strategy for updating the global branch history is consistent with the strategy for updating the &lt;code&gt;pc&lt;/code&gt;. At each pipeline stage, a &lt;strong&gt;pointer for the current stage and an update description of &lt;code&gt;ghv&lt;/code&gt;&lt;/strong&gt; are generated based on the prediction result of the current stage, and all are sent to the relevant generator for processing.&lt;/p&gt;
&lt;p&gt;The update strategy for the global branch history is consistent with the &lt;code&gt;pc&lt;/code&gt; update strategy, requiring the generation of a &lt;strong&gt;current stage pointer and &lt;code&gt;ghv&lt;/code&gt; update instructions&lt;/strong&gt; based on the current stage&amp;rsquo;s prediction results at each pipeline stage. These instructions are ultimately sent to the relevant generators for processing.&lt;/p&gt;
&lt;p&gt;The update description of &lt;code&gt;ghv&lt;/code&gt; is some information used to guide the update of the &lt;code&gt;ghv&lt;/code&gt; register. Xiangshan BPU maintains two pieces of information to fulfill this duty:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ghv_wdata&lt;/code&gt; the data that needs to be written into ghv&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ghv_wens&lt;/code&gt; the write bit mask&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the final update, only the bits identified by &lt;code&gt;ghv_wens&lt;/code&gt; need to be written with the corresponding bits of &lt;code&gt;ghv_wdata&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Therefore, each pipeline stage needs to generate three sets of information: &lt;code&gt;ghist_ptr&lt;/code&gt;, &lt;code&gt;ghv_wdata&lt;/code&gt;, &lt;code&gt;ghv_wens&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, the prediction result can contain up to two branch instructions. We only need to set these pieces of information according to the actual situation. Here are some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only the first slot is valid, and the conditional branch instruction in it is predicted as not taken. Then the next position of &lt;code&gt;ghv_wens&lt;/code&gt; is set to 0, the corresponding position of &lt;code&gt;ghv_wens&lt;/code&gt; is set to 1, and &lt;code&gt;ghist_ptr&lt;/code&gt; is decremented by one.&lt;/li&gt;
&lt;li&gt;Both slots contain conditional branch instructions, the first is predicted as not taken, and the second is predicted as taken. At this time, &lt;code&gt;ghist_ptr&lt;/code&gt; should be decremented by two, and the other two pieces of information should indicate writing 01 to &lt;code&gt;ghv&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, only one piece of &lt;code&gt;ghv_wdata&lt;/code&gt; information is maintained in the generator (maintained by the &lt;code&gt;ghvBitWriteGens&lt;/code&gt; generator), and &lt;code&gt;ghv_wens&lt;/code&gt; is not maintained by the generator. This is because a small trick is used here, where the final output of the generator&amp;rsquo;s &lt;code&gt;ghv_wdata&lt;/code&gt; is the result of the selected stage, and &lt;code&gt;ghv_wens&lt;/code&gt; is used by performing a bitwise OR operation on &lt;code&gt;ghv_wens&lt;/code&gt; of all stages.&lt;/p&gt;
&lt;p&gt;This consideration is based on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the later pipeline stage is valid, the global history pointer is restored to an older position, even if the history of newer positions is modified by the earlier pipeline&amp;rsquo;s &lt;code&gt;ghv_wens&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the earlier pipeline stage is valid, the global history pointer continues to update to newer positions, and the later pipeline will not set &lt;code&gt;ghv_wens&lt;/code&gt; due to the ineffective redirect.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;branch-folded-history&#34;&gt;Branch Folded History&lt;/h2&gt;
&lt;p&gt;The branch folded history passed to the predictor is also maintained by the BPU top level. To shorten the update delay of the folded history, the BPU maintains many variables to support the fast update of the branch folded history. We will focus on this strategy and introduce the function of each variable.&lt;/p&gt;
&lt;p&gt;Before we start, let&amp;rsquo;s first look at how the branch folded history is defined and its structure.&lt;/p&gt;
&lt;h3 id=&#34;branch-folded-history-1&#34;&gt;Branch Folded History&lt;/h3&gt;
&lt;p&gt;If you have checked the BPU global interface documentation, you will know that the sub-predictor receives an array of bit vectors of different lengths, representing various lengths of folded history, and these folded histories are compressed from the global branch history.&lt;/p&gt;
&lt;p&gt;For the global branch history, we have a register that stores the global branch history with a length of 256. For example, suppose the length of the global branch history is 15 bits, and after shifting, we get a branch history like this: the least significant bit is the newest history record, and the most significant bit is the oldest history record.&lt;/p&gt;
&lt;p&gt;At this time, if we need to generate a 6-bit folded history from these 15 bits, we will use an XOR strategy for compression. The specific process is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    h[5]         h[4]       h[3]    h[2]   h[1]   h[0]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    h[11]        h[10]      h[9]    h[8]   h[7]   h[6]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;^                                   h[14]  h[13]  h[12]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---------------------------------------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    h[5]^h[11]   h[4]^h[10]         ...           h[0]^h[6]^h[12]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That is, after arranging it as shown above, perform an XOR operation on the values at each position, and the result is the folded history of length 6.&lt;/p&gt;
&lt;h3 id=&#34;method-for-updating-branch-folded-history&#34;&gt;Method for Updating Branch Folded History&lt;/h3&gt;
&lt;p&gt;Now we want to update this branch folded history. When we insert a new history into the global branch history, it is inserted from the least significant bit, meaning the original h[0] becomes h[1]. If we want to obtain the folded history at this time, we only need to perform the XOR operation again. But such efficiency is too low, because the XOR operation may become particularly long. We can explore the impact of one update on the branch folded history.&lt;/p&gt;
&lt;p&gt;In the example above, before inserting a new history, the 6-bit folded history is generated in the following arrangement:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[5]   h[4]   h[3]  h[2]  h[1]  h[0]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[11]  h[10]  h[9]  h[8]  h[7]  h[6]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    h[14] h[13] h[12]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After inserting a new history, it becomes like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[4]   h[3]   h[2]  h[1]  h[0]  h[new]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[10]  h[9]   h[8]  h[7]  h[6]  h[5]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           (h[14])  h[13] h[12] h[11]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can notice some patterns:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Before insertion:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[5]   {h[4]   h[3]  h[2]  h[1]  h[0] }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h[11]  {h[10]  h[9]  h[8]  h[7]  h[6] }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       {             h[14] h[13] h[12]}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;After insertion:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{h[4]   h[3]   h[2]  h[1]  h[0] } h[new]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{h[10]  h[9]   h[8]  h[7]  h[6] } h[5]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{           (h[14])  h[13] h[12]} h[11]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The content in the curly braces has undergone a complete left shift, with h[5] and h[11] moving from the most significant bit to the least significant bit. So, in the compressed history, isn&amp;rsquo;t this just a typical cyclic left shift that we often encounter!&lt;/p&gt;
&lt;p&gt;However, only two bits have changed: one is the newly inserted h[new], and the other is the discarded h[14]. h[new] must be in the first position, and the discarded position is fixed. &lt;strong&gt;Therefore, to complete an update, we only need to know the value of the newly inserted history and the oldest bit of the previous history&lt;/strong&gt;. After the cyclic shift, modifying these two positions according to the actual situation will give us the updated folded history.&lt;/p&gt;
&lt;h3 id=&#34;implementation-of-update-method&#34;&gt;Implementation of Update Method&lt;/h3&gt;
&lt;p&gt;To achieve this update in the top-level BPU, two additional variables are maintained:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ahead_fh_oldest_bits&lt;/strong&gt;: the oldest bit of the global branch history, with additional bits stored before it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_br_num_oh&lt;/strong&gt;: the slot number of the last effective branch instruction in the previous prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An optimization in the timing of operations occurs here because the global history pointer can only be updated based on the branch outcome when the pipeline-level prediction result is available. Updating the oldest bit after updating the global history pointer would increase the latency. Therefore, we maintain the branch outcome and update the oldest bit when it is needed in the next cycle.&lt;/p&gt;
&lt;p&gt;The oldest bit also needs to be maintained further back because after updating using the branch outcome, the relatively newer bits will become the oldest bits.&lt;/p&gt;
&lt;p&gt;Thus, there are three generators related to the folded history: &lt;code&gt;foldedGhGen&lt;/code&gt;, &lt;code&gt;lastBrNumOhGen&lt;/code&gt;, and &lt;code&gt;aheadFhObGen&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Information required for each update of the folded history&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Folded history information before the update&lt;/li&gt;
&lt;li&gt;Oldest bit of the global branch history (ahead_fh_oldest_bits)&lt;/li&gt;
&lt;li&gt;Last prediction&amp;rsquo;s branch outcome (last_br_num_oh)&lt;/li&gt;
&lt;li&gt;Whether there is a branch instruction in this update&lt;/li&gt;
&lt;li&gt;The branch outcome of this update: the slot number of the last effective branch instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each update of the folded history, the true oldest bit needs to be determined based on &lt;code&gt;last_br_num_oh&lt;/code&gt; and &lt;code&gt;ahead_fh_oldest_bits&lt;/code&gt;. Then, based on the oldest bit and the branch outcome of this update, several bits are modified, and finally, a cyclic left shift completes the update operation.&lt;/p&gt;
&lt;h2 id=&#34;pipeline-control-method&#34;&gt;Pipeline Control Method&lt;/h2&gt;
&lt;p&gt;Pipeline control is the core function of the BPU, with complex logic. All pipeline control signals in the top-level BPU are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s1_valid, s2_valid, s3_valid&lt;/strong&gt;: indicate the corresponding pipeline data is valid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s1_ready, s2_ready, s3_ready&lt;/strong&gt;: indicate the corresponding pipeline is ready to continue the prediction of the previous pipeline stage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s1_component_ready, s2_component_ready, s3_component_ready&lt;/strong&gt;: indicate the readiness of the corresponding pipeline sub-predictor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s0_fire, s1_fire, s2_fire, s3_fire&lt;/strong&gt;: successful handshake signals, indicating that the pipeline data is valid and has been successfully passed to the next pipeline&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s1_flush, s2_flush, s3_flush&lt;/strong&gt;: indicate whether the current pipeline needs flushing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s2_redirect, s3_redirect&lt;/strong&gt;: indicate whether the current pipeline needs to redirect due to a different prediction result&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;valid-ready-与-fire&#34;&gt;valid, ready 与 fire&lt;/h3&gt;
&lt;p&gt;We will introduce the purpose of each signal step by step. First, let&amp;rsquo;s look at the &lt;code&gt;fire&lt;/code&gt; signal, which indicates a successful handshake in the pipeline, meaning that the data has been successfully passed to the next pipeline. This signifies the end of the current cycle and the end of the prediction for this pipeline stage, with the prediction for the next pipeline stage about to begin in the next cycle.&lt;/p&gt;
&lt;p&gt;This requires two conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;valid&lt;/code&gt;: The data in the current pipeline stage is valid.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ready&lt;/code&gt;: Indicates whether the next pipeline stage in the BPU top level and the predictor are ready.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When these two signals are high simultaneously, the &lt;code&gt;fire&lt;/code&gt; signal is valid, indicating a successful handshake. If we isolate a single prediction, the timing should look like this (in reality, most of the time, each pipeline is valid continuously):&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Of the four sets of signals mentioned earlier, &lt;code&gt;component_ready&lt;/code&gt; is generated by the predictor, while the rest are maintained by the BPU top level, with only the fire set of signals exposed to the sub-predictor.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s take s2 as an example to see how each signal is maintained.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ready&lt;/strong&gt; &lt;strong&gt;Signal&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;s2_ready&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_fire&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;||&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;s2_valid&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This assignment statement is a combinational circuit assignment, meaning that &lt;code&gt;s2_ready&lt;/code&gt; is directly related to &lt;code&gt;s2_fire&lt;/code&gt; and &lt;code&gt;s2_valid&lt;/code&gt; for this cycle, with two possible situations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;s2_valid&lt;/code&gt; is invalid for this cycle, indicating that the s2 pipeline stage is empty and can accept new data, then &lt;code&gt;s2_ready&lt;/code&gt; is valid.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;s2_valid&lt;/code&gt; is valid for this cycle, indicating that the s2 pipeline stage has data that has not been passed to the next stage yet, but if &lt;code&gt;s2_fire&lt;/code&gt;, then the data will be passed in the next cycle. In this case, &lt;code&gt;s2_ready&lt;/code&gt; is valid, indicating that the data can flow into the next stage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;valid Signal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Maintaining ·s2_valid· is relatively simple, as it is only related to &lt;code&gt;s1_fire&lt;/code&gt; and &lt;code&gt;s2_ready&lt;/code&gt; signals. The relationship is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When &lt;code&gt;s1_fire&lt;/code&gt; is valid, indicating that data is coming in and &lt;code&gt;s2_valid&lt;/code&gt; will be valid in the next cycle.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;s2_fire&lt;/code&gt; is valid, indicating that data is flowing out and &lt;code&gt;s2_valid&lt;/code&gt; will be invalid in the next cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;fire Signal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fire signal is somewhat special, but for intermediate pipeline stages, its maintenance is straightforward. For example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;s2_fire&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_valid&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_components_ready&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_ready&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This simply requires considering the &lt;code&gt;valid&lt;/code&gt; of the current pipeline stage and the &lt;code&gt;ready&lt;/code&gt; of the next pipeline stage.&lt;/p&gt;
&lt;p&gt;However, for s0_fire, since there is no valid signal, it is directly set to &lt;code&gt;s1_components_ready &amp;amp;&amp;amp; s1_ready&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For s3_fire, as there is no ready signal for the next stage, it is directly set to &lt;code&gt;s3_valid&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;incorporating-flush-and-redirect&#34;&gt;Incorporating Flush and Redirect&lt;/h3&gt;
&lt;p&gt;When there is a different prediction result in the pipeline, a redirection signal is generated, and the pipeline needs to be flushed. &lt;code&gt;flush&lt;/code&gt; and &lt;code&gt;redirect&lt;/code&gt; handle these two tasks. &lt;code&gt;redirect&lt;/code&gt; indicates whether the current pipeline stage needs redirection, while &lt;code&gt;flush&lt;/code&gt; indicates whether the current pipeline stage needs flushing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;redirect Signal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The generation of &lt;code&gt;s2_redirect&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;s2_redirect&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_fire&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_redirect_s1_last_pred&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This means that when &lt;code&gt;s2_fire&lt;/code&gt; is valid and the prediction result of s2 is different from the prediction result saved from s1, this signal is valid. Later, this signal will be connected to the input of the sub-predictor and the output of the BPU prediction result, guiding the sub-predictor and FTQ to restore their states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;flush Signal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The flush signal is used to guide the flushing of the pipeline. For example, when s3_redirect is valid, it means that the incorrect prediction result has entered the pipeline, and both s1 and s2 are now predicting based on the wrong result. Therefore, the pipeline needs to be flushed to stop the previous stages and wait for new prediction results to enter.&lt;/p&gt;
&lt;p&gt;Specifically, the relationship between them is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_flush&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_flush&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;||&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_redirect&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#000&#34;&gt;s1_flush&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_flush&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;||&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;s2_redirect&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This means that if a pipeline stage needs redirection, all previous stages will be flushed. The purpose of flush is to guide the valid signal. If the valid signal is valid in this cycle but the fire signal is not, it means that the incorrect data has not been taken by the next pipeline stage. In this case, when flush is valid, valid will immediately become invalid in the next cycle, avoiding storing incorrect data in the pipeline for a long time.&lt;/p&gt;
&lt;p&gt;However, the effect of flush on the valid signal varies depending on each pipeline stage. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the s1 pipeline stage, although flush is valid, if &lt;code&gt;s0_fire&lt;/code&gt; is valid, indicating that new data is flowing in, valid will remain valid in the next cycle.&lt;/li&gt;
&lt;li&gt;For the s2 pipeline stage, if flush is valid, valid will definitely be invalid in the next cycle (because s1 is also flushed), indicating that valid can be directly set to invalid. However, there is a special case where &lt;code&gt;s2_redirect&lt;/code&gt; occurs but &lt;code&gt;s2_flush&lt;/code&gt; is not set to valid. In this case, if &lt;code&gt;s1_fire&lt;/code&gt; occurs, the incorrect prediction result of s1 may also flow in. In this case, &lt;code&gt;s2_valid&lt;/code&gt; needs to be determined based on the &lt;code&gt;s1_flush&lt;/code&gt; signal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The use of flush is complex, and more detailed details need to be understood by referring to the code.&lt;/p&gt;
&lt;h2 id=&#34;redirect-recovery-logic&#34;&gt;Redirect Recovery Logic&lt;/h2&gt;
&lt;p&gt;When the redirect request from FTQ to BPU takes effect, it indicates that all stages of the pipeline have incorrect predictions, and all stages should be flushed. This can be achieved by setting &lt;code&gt;s3_flush&lt;/code&gt; to be valid. Therefore, we have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Scala&#34; data-lang=&#34;Scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#000&#34;&gt;s3_flush&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;redirect_req&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;valid&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the BPU, the redirect request is delayed by one cycle before being officially used. Therefore, the response of &lt;code&gt;s1_valid&lt;/code&gt; to the &lt;code&gt;flush&lt;/code&gt; signal needs to be changed. When the redirect request (before delay) is valid, &lt;code&gt;s1_valid&lt;/code&gt; in the next cycle is immediately set to invalid, without the need to refer to the &lt;code&gt;s0_fire&lt;/code&gt; signal.&lt;/p&gt;
&lt;p&gt;At this point, generators such as &lt;code&gt;npcGen&lt;/code&gt; also need to directly use the data from the redirect request to generate, which is equivalent to redirecting the BPU&amp;rsquo;s state to the state before the error occurred. However, it is important to note that the default redirect level in BPU is &lt;code&gt;flushAfter&lt;/code&gt;, which means that the redirect request corresponds to a predicted erroneous instruction, and the BPU assumes that although this instruction was predicted incorrectly, it has been corrected and executed by the backend. Therefore, the next prediction can start directly from the next instruction.&lt;/p&gt;
&lt;p&gt;Therefore, when recovering from a redirect, it is necessary not only to restore the information from the redirect interface, but also to update the execution status of this predicted erroneous instruction in the history.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: FTB Item and Complete Prediction Result Interface</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/00_ftb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/00_ftb/</guid>
      <description>
        
        
        &lt;h2 id=&#34;ftb-item&#34;&gt;FTB item&lt;/h2&gt;
&lt;p&gt;The FTB item is the core data structure of branch prediction blocks in Xiangshan. It stores the information needed to generate a branch prediction block. When BPU performs predictions, the initial branch prediction block is first generated from a read-out FTB item. Then, this branch prediction block is passed to subsequent predictors, which read and modify the information to generate the final prediction result.&lt;/p&gt;
&lt;p&gt;Therefore, to understand the structure of a branch prediction block, we first need to understand the structure of an FTB item. An FTB item corresponds to a branch prediction block, and its general structure is as follows:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;750px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;First, it is necessary to clarify one piece of information: whether it is a branch prediction block or an FTB item, the number of instructions they can contain is set to a specific limit (16 RVC instructions in the current version of Xiangshan), called the &lt;strong&gt;maximum prediction length&lt;/strong&gt;. This means that if we need to record the position of an instruction within the branch prediction block, we can use a fixed-length bit vector to specify the offset of this instruction relative to the starting address of the prediction block.&lt;/p&gt;
&lt;p&gt;The determinant of the branch prediction block&amp;rsquo;s execution process is the information about the branch instructions. The other instructions are considered ordinary instructions and do not affect the program&amp;rsquo;s execution flow. Therefore, in a branch prediction block, we only need to record the positions of the branch instructions, while the positions of ordinary instructions are not our concern.&lt;/p&gt;
&lt;p&gt;Therefore, the FTB item defines two types of &lt;strong&gt;branch instruction slots&lt;/strong&gt;—&lt;code&gt;brSlots&lt;/code&gt; and &lt;code&gt;tailSlot&lt;/code&gt;, used to store the branch instructions within the branch prediction block. In the current version of Xiangshan, &lt;code&gt;brSlots&lt;/code&gt; contains only one slot, while &lt;code&gt;tailSlot&lt;/code&gt; is a separate slot, totaling two slots.&lt;/p&gt;
&lt;p&gt;Within the instructions of the maximum prediction length, if a branch instruction appears, the FTB item will record it in the corresponding slot and mark the slot as valid. If too many branch instructions appear, reaching the capacity limit of the FTB item, the excess branch instructions will be handed over to the next FTB item for storage. In each slot, we record the offset of a branch instruction relative to the starting address of the prediction block and information such as its jump target address.&lt;/p&gt;
&lt;h3 id=&#34;the-unique-tailslot&#34;&gt;The Unique tailSlot&lt;/h3&gt;
&lt;p&gt;In RISC-V, branch instructions are mainly divided into two types: conditional branches and unconditional jumps. Therefore, &lt;strong&gt;for a branch prediction block, it will contain at most one unconditional jump instruction&lt;/strong&gt;. Because once this instruction is executed, the program&amp;rsquo;s execution flow will change, and subsequent instructions will no longer be executed. Hence, we define a type of slot called &lt;code&gt;tailSlot&lt;/code&gt; specifically for storing this unconditional jump instruction. As for conditional branch instructions, they are stored in &lt;code&gt;brSlots&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As its name suggests, &lt;code&gt;tailSlot&lt;/code&gt; is located in the last slot of the entire prediction block. This is also because once the unconditional jump instruction is filled, the program will definitely jump, and subsequent instructions will be handled by other prediction blocks, so we do not need to care about the subsequent instructions. However, among the instructions before the unconditional jump instruction, we need to care about whether there are conditional branch instructions, because conditional branch instructions may or may not jump. Therefore, we need to record the relevant information of the conditional branch instructions.&lt;/p&gt;
&lt;h3 id=&#34;tailslot-sharing&#34;&gt;tailSlot Sharing&lt;/h3&gt;
&lt;p&gt;Consider a situation: if no unconditional jump instructions appear from the starting PC of the prediction block to the maximum prediction length, but two conditional branch instructions appear instead, the &lt;code&gt;tailSlot&lt;/code&gt; will be idle, and the second conditional branch instruction cannot be stored, causing space waste.&lt;/p&gt;
&lt;p&gt;To solve this problem, Xiangshan adopts a method of setting a &lt;code&gt;sharing&lt;/code&gt; mark. We can directly store the second branch instruction into the &lt;code&gt;tailSlot&lt;/code&gt; and set the sharing mark to true, indicating that the second conditional branch instruction shares the tailSlot of the unconditional jump instruction. This way, the space of the &lt;code&gt;tailSlot&lt;/code&gt; is effectively utilized.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;isCall&lt;/code&gt;, &lt;code&gt;isRet&lt;/code&gt;, and &lt;code&gt;isJalr&lt;/code&gt; fields in the prediction block serve the &lt;code&gt;tailSlot&lt;/code&gt;. If the tailSlot records an unconditional jump instruction, these fields will further indicate the type of the jump instruction. There is also a field in the FTB item called &lt;code&gt;always_taken&lt;/code&gt;, which records whether each conditional branch instruction stored in each slot is always predicted to jump. If so, subsequent predictors can directly adopt this prediction result.&lt;/p&gt;
&lt;p&gt;Through the FTB item, we can know the instruction situation in a branch prediction block, including the position and type of branch instructions. This information will be handed over to subsequent predictors, which will predict more accurate jump targets, whether to jump, and other information.&lt;/p&gt;
&lt;h2 id=&#34;complete-structure-of-ftb-item-ftbentry&#34;&gt;Complete Structure of FTB Item (FTBEntry)&lt;/h2&gt;
&lt;p&gt;Interface Definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FTB.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This section describes the complete structural definition of the FTB item, containing the following signals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;valid&lt;/strong&gt;: Whether the FTB entry is valid.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;brSlots&lt;/strong&gt;: Conditional branch instruction slots.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(numBrSlot, new FtbSlot(BR_OFFSET_LEN))&lt;/code&gt; （see FtbSlot for interface list）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tailSlot&lt;/strong&gt;: Unconditional jump instruction slot.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;new FtbSlot(JMP_OFFSET_LEN, Some(BR_OFFSET_LEN))&lt;/code&gt; （see FtbSlot for interface list）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pftAddr&lt;/strong&gt;: Unconditional jump instruction slot
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(log2Up(PredictWidth).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;carry&lt;/strong&gt;: Unconditional jump instruction slot.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;isCall&lt;/strong&gt;: The instruction in the unconditional jump instruction slot is a call instruction.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;isRet&lt;/strong&gt;: The instruction in the unconditional jump instruction slot is a ret instruction.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;isJalr&lt;/strong&gt;: The instruction in the unconditional jump instruction slot is a jalr instruction.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_may_be_rvi_call&lt;/strong&gt;:  The instruction in the unconditional jump instruction slot may be an RVI type call instruction signal.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;always_taken&lt;/strong&gt;: Whether each branch instruction in the prediction block is always predicted as Taken.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(numBr, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Explanation: pftAddr and carry&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;pftAddr&lt;/code&gt; stands for Partial Fallthrough Address. Fallthrough Address means that if there is no jump in the prediction block, the program will sequentially execute to the address reached. In other words, if the offset of an unconditional jump instruction is 5, then the offset corresponding to the Fallthrough Address is 6. This signal is mainly used to get the return address of the program after a function call, and this concept can be understood as the end address of the prediction block.&lt;/p&gt;
&lt;p&gt;Partial means part of the address, which is determined by the address representation method. Here, the address representation method is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  pc: | ... |&amp;lt;-- log(predictWidth) --&amp;gt;|&amp;lt;-- log(instBytes) --&amp;gt;|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           ^                         ^
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           |                         |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           carryPos                  instOffsetBits
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;pftAddr&lt;/code&gt; only records the middle offset part (the part with a length of log(predictWidth)), and a complete PC can be generated by combining it with the current PC. However, a carry might occur, so a &lt;code&gt;carry&lt;/code&gt; bit is recorded separately. carryPos is the position in the instruction address within the prediction block where a carry might occur.&lt;/p&gt;
&lt;p&gt;Additionally, &lt;code&gt;last_may_be_rvi_call&lt;/code&gt; is an auxiliary signal for this address, indicating that the instruction in the unconditional jump instruction slot is an RVI type call instruction. Since pftAddr assumes the instruction length as the compressed instruction length by default when calculating, the end address is increased by only 2 bytes. If the actual call instruction is not a compressed instruction, it will lead to an incorrect return address calculation. RAS will correct this error based on this signal.&lt;/p&gt;
&lt;h2 id=&#34;branch-prediction-slot-ftbslot&#34;&gt;Branch Prediction Slot (FTBSlot)&lt;/h2&gt;
&lt;p&gt;Interface Definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FTB.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This interface defines the slot in the FTB entry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;offset&lt;/strong&gt;: The offset of the instruction in the slot relative to the start address of the prediction block.
&lt;ul&gt;
&lt;li&gt;Interface Type: UInt(log2Ceil(PredictWidth).W)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lower&lt;/strong&gt;: The lower bits of the jump target address.
&lt;ul&gt;
&lt;li&gt;Interface Type: UInt(offsetLen.W)&lt;/li&gt;
&lt;li&gt;Note: The lower is set to 12 or 20 bits because the addressing capability of branch instructions is 12 bits, while the addressing capability of jump instructions is 20 bits.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tarStat&lt;/strong&gt;: Whether the high bits of the PC after the jump are incremented or decremented.
&lt;ul&gt;
&lt;li&gt;Interface Type: UInt(TAR_STAT_SZ.W) (TAR_STAT_SZ = 2)&lt;/li&gt;
&lt;li&gt;Note: The jump target address is calculated from the high bits of the current PC, the tarStat, and the lower field. The lower field directly stores the lower bits of the jump target address. The high bits of the current PC are adjusted according to the tarStat, then concatenated with the lower to get the actual jump target address. The tarStat can take three values: 0 - no increment or decrement, 1 - increment, 2 - decrement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sharing&lt;/strong&gt;: Indicates that a conditional branch instruction is stored in an unconditional jump instruction slot.
&lt;ul&gt;
&lt;li&gt;Interface Type: Bool()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;valid&lt;/strong&gt;: Indicates whether the slot is valid.
&lt;ul&gt;
&lt;li&gt;Interface Type: Bool()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;full-branch-prediction&#34;&gt;Full Branch Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Interface Definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This interface defines the complete branch prediction results, included in the prediction results of each pipeline stage.&lt;/p&gt;
&lt;p&gt;The full branch prediction result interface is similar to the FTB entry and is initially generated from the FTB entry. Two slots are split into individual signals: &lt;code&gt;slot_valids&lt;/code&gt;, &lt;code&gt;targets&lt;/code&gt;, &lt;code&gt;offsets&lt;/code&gt;, &lt;code&gt;is_br_sharing&lt;/code&gt;, etc. Additionally, several fields are added such as &lt;code&gt;br_taken_mask&lt;/code&gt;, &lt;code&gt;jalr_target&lt;/code&gt; to facilitate the provision of precise prediction results by subsequent predictors. The &lt;code&gt;hit&lt;/code&gt; indicates whether an FTB entry is hit, i.e., the PC in the current prediction round indexed to an FTB entry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Full Interface List：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hit&lt;/strong&gt;: Indicates whether the FTB entry is hit.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;slot_valids&lt;/strong&gt;: Indicates whether each slot in the FTB entry is valid.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(totalSlot, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;targets&lt;/strong&gt;: The jump target address corresponding to each slot.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(totalSlot, UInt(VAddrBits.W))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;offsets&lt;/strong&gt;: The offset of the instruction in each slot relative to the start address of the prediction block.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(totalSlot, UInt(log2Ceil(PredictWidth).W))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughAddr&lt;/strong&gt;: The end address of the prediction block.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(VAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughErr&lt;/strong&gt;: Indicates that the pftAddr recorded in the FTB entry is incorrect.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jal&lt;/strong&gt;: Indicates that the prediction block contains a jal instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jalr&lt;/strong&gt;: Indicates that the prediction block contains a jalr instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_call&lt;/strong&gt;: Indicates that the prediction block contains a call instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_ret&lt;/strong&gt;: Indicates that the prediction block contains a ret instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_may_be_rvi_call&lt;/strong&gt;: Indicates that the instruction in the unconditional jump instruction slot might be an RVI type call instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_br_sharing&lt;/strong&gt;: Indicates that the last slot (tailSlot) stores a conditional branch instruction.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;br_taken_mask&lt;/strong&gt;: The branch prediction result, with each bit corresponding to a branch (slot), indicating whether the branch is predicted to be taken.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Vec(numBr, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;jalr_target&lt;/strong&gt;: The jump target of the jalr instruction in the prediction block.&lt;/li&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(VAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: What is Branch Prediction</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/00_bp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/00_bp/</guid>
      <description>
        
        
        &lt;h2 id=&#34;why-do-we-need-branch-prediction&#34;&gt;Why Do We Need Branch Prediction?&lt;/h2&gt;
&lt;p&gt;There are mainly two reasons for branch prediction: one is that &lt;strong&gt;the program&amp;rsquo;s execution flow contains branch instructions&lt;/strong&gt;, and the other is that &lt;strong&gt;high-performance processors use pipeline design&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;programs-execution-flow-contains-branch-instructions&#34;&gt;Program&amp;rsquo;s Execution Flow Contains Branch Instructions&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;y&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;result&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;result&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;result&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above is a piece of C code. It first defines three variables x, y, and result, and then assigns a value to result based on the comparison of x and y. It can be observed that the program assigns values to variables in sequence in the first three lines. However, in the 5th line, due to the presence of the if instruction, the program branches, jumping directly from the 5th line to the 8th line to continue execution, which causes a &lt;strong&gt;branch&lt;/strong&gt; in the program&amp;rsquo;s execution.&lt;/p&gt;
&lt;p&gt;After translating into RISC-V assembly code, the code is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-asm&#34; data-lang=&#34;asm&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;li&lt;/span&gt;  &lt;span style=&#34;color:#000&#34;&gt;a0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;10&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# x = 10
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;li&lt;/span&gt;  &lt;span style=&#34;color:#000&#34;&gt;a1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# y = 20
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;li&lt;/span&gt;  &lt;span style=&#34;color:#000&#34;&gt;a2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;                &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# result = 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;blt&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;else_branch&lt;/span&gt;  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Jump to else_branch if x &amp;lt; y
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;add&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a1&lt;/span&gt;           &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Execute result = x + y
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;j&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;end&lt;/span&gt;                    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Jump to end
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f57900&#34;&gt;else_branch:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;sub&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a1&lt;/span&gt;           &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Execute result = x - y
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f57900&#34;&gt;end:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It can be seen that the program still maintains the previous branching behavior. In the first three lines of the code, instructions are executed in sequence. Then, in the 5th line of the program, a special instruction blt appears, which we call a branch instruction. It will determine whether to execute the next instruction based on the relationship between x and y, and the appearance of this instruction causes a branch in the program&amp;rsquo;s execution.&lt;/p&gt;
&lt;h3 id=&#34;high-performance-processors-use-pipeline-design&#34;&gt;High-performance Processors Use Pipeline Design&lt;/h3&gt;

&lt;figure&gt;
    &lt;img src=&#34;f4669877-a9f9-4f5d-9fe3-8ba976973513.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Therefore, the concept of &lt;strong&gt;branch prediction&lt;/strong&gt; arises. If we can accurately predict the address of the next instruction before the execution result is generated, the processor can continue to run efficiently.&lt;/p&gt;
&lt;h2 id=&#34;feasibility-of-branch-prediction&#34;&gt;Feasibility of Branch Prediction&lt;/h2&gt;
&lt;p&gt;Why can branch prediction be done? Let&amp;rsquo;s look at an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;data&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;128&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;sum&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;data&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Assuming that this instruction will be executed repeatedly, and data is incremented from 0, i.e., data = 0, 1, 2, 3 &amp;hellip; 128, 129&amp;hellip;, let&amp;rsquo;s analyze the behavior of executing this instruction each time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;T = branch taken
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;N = branch not taken
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data   = 0, 1, 2, 3, s, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;branch = N  N  N  N  N  ...   N    N    T    T    T  ...   T    T    T  ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT  (easy to predict)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It can be seen that in the first 128 times, the branch is always Not Taken (the condition is not met), but after 128 times, the branch is always Taken (the condition is met). If we predict whether it is Taken based on whether it was Taken last time, we will only make one prediction error throughout the prediction process.&lt;/p&gt;
&lt;p&gt;The occurrence of the above phenomenon is due to a basic fact—&lt;strong&gt;whether a branch instruction jumps is related to the past jumping behavior of that instruction&lt;/strong&gt;. By summarizing the past jumping rules, we can make a relatively accurate prediction for this jump, which also makes branch prediction possible.&lt;/p&gt;
&lt;p&gt;In fact, the jump of branch instructions is also related to factors such as the jumping situation of other branch instructions. Fully exploiting effective information to produce accurate prediction results is one of the main tasks of branch prediction.&lt;/p&gt;
&lt;h2 id=&#34;basic-types-of-branch-prediction&#34;&gt;Basic Types of Branch Prediction&lt;/h2&gt;
&lt;p&gt;In RISC-V, branch instructions include two types:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Conditional Branch Instructions (beq, bne, blt, bge, bltu, bgeu)&lt;/strong&gt; For these instructions, whether to jump is determined by the condition in the instruction, and the jump target can be obtained from the instruction. Therefore, we need to predict whether the instruction will jump.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unconditional Jump Instructions (jal, jalr)&lt;/strong&gt; For these instructions, the jump is always executed when encountered, but the jump target may be specified by a register. Therefore, we need to predict the jump target of the instruction.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fortunately, due to the concise design of the RISC-V architecture, we do not need to handle conditional jump instructions. Every jump instruction we need to predict is unconditional, which is also convenient for our design.&lt;/p&gt;
&lt;p&gt;From the above analysis, we can summarize the two basic types of branch prediction—&lt;strong&gt;direction prediction&lt;/strong&gt; and &lt;strong&gt;target address prediction&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;direction-prediction-of-branch-instructions&#34;&gt;Direction Prediction of Branch Instructions&lt;/h3&gt;
&lt;p&gt;Direction prediction of branch instructions corresponds to conditional branch instructions in RISC-V instructions. We need to predict whether it needs to jump, which is called direction prediction.&lt;/p&gt;
&lt;h4 id=&#34;two-bit-saturation-counters&#34;&gt;Two-Bit Saturation Counters&lt;/h4&gt;
&lt;p&gt;Direction prediction has a very simple and efficient prediction method called two-bit saturation counter. The basic idea can be seen in the figure below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;c7e5cf3e-8131-430b-a728-659fb7775f53.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The two-bit saturating counter is regarded as a state machine, and we maintain such a state machine for each branch instruction. When a branch instruction is taken, the corresponding state in the diagram moves to the right; otherwise, it moves to the left. So, the next time we encounter this branch instruction, we first look up its two-bit saturating counter. If the state is more biased to the right, we predict it to be taken; otherwise, we predict it not to be taken.&lt;/p&gt;
&lt;p&gt;Of course, it&amp;rsquo;s impractical to maintain a two-bit saturating counter for each branch instruction. Therefore, in practice, we usually use part of the PC or a hash method to index the two-bit saturating counter, as shown in the diagram below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;e2cf0e6d-f050-4aff-bee3-c72a2b9004e1.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;h4 id=&#34;branch-history&#34;&gt;Branch History&lt;/h4&gt;
&lt;p&gt;Branch history is a very commonly used data in branch prediction and the basis of most branch prediction algorithms because it directly shows the past jumping behavior of instructions.&lt;/p&gt;
&lt;p&gt;There are two basic types of branch history:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local Branch History&lt;/strong&gt; Maintain a set of registers for each branch instruction, recording the historical jumping behavior of that instruction.
&lt;ul&gt;
&lt;li&gt;For example: 0101000000101 (0 means not taken, 1 means taken)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;全Global Branch History&lt;/strong&gt; All instructions share a set of registers, recording the branching behavior during program execution.
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    beg a0, a1, label1          not taken  record 0
    bne a1, a2, label2          not taken  record 0
-&amp;gt;  beq a2, a3, label4          taken      record 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After executing these three different branch instructions, the global branch history becomes 001.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;branch-target-address-prediction&#34;&gt;Branch Target Address Prediction&lt;/h3&gt;
&lt;p&gt;In the RISC-V architecture, branch target address prediction refers to predicting the target address of unconditional jump instructions (e.g., jal, jalr). Since these instructions always perform a jump operation, we need to predict their target address.&lt;/p&gt;
&lt;h4 id=&#34;branch-target-buffer-btb&#34;&gt;Branch Target Buffer (BTB)&lt;/h4&gt;
&lt;p&gt;BTB is a common method for predicting target addresses. Its core idea is to use a cache to store the target addresses of past unconditional jump instructions. When encountering the same unconditional jump instruction again, the BTB can be checked to see if there is a record for that instruction. If so, the recorded target address is used as the predicted target address for the current execution.&lt;/p&gt;
&lt;p&gt;The diagram below illustrates this:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;540d418a-2760-4724-aad4-502bd0775185.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;h3 id=&#34;predicting-instruction-types&#34;&gt;Predicting Instruction Types&lt;/h3&gt;
&lt;p&gt;As we know, in branch prediction, for conditional branch instructions, we need to predict their direction, and for unconditional jump instructions, we need to predict their target. However, there is a problem: when we get a PC that needs to be predicted, we don&amp;rsquo;t know whether the corresponding instruction is a normal instruction or a branch instruction. Therefore, we cannot predict it.&lt;/p&gt;
&lt;p&gt;How to solve this? One way is to predict the behavior of the instruction after fetching it. But fetching from ICache or Memory may take several cycles, which is a major drawback of this method.&lt;/p&gt;
&lt;p&gt;A better way is to directly predict the type of instruction. After getting a PC, we can directly predict whether this instruction is a branch instruction and predict its behavior. In this way, we don&amp;rsquo;t have to wait for fetching to complete, and the predicted result can also guide the CPU to fetch from the correct location.&lt;/p&gt;
&lt;p&gt;The method of type prediction can be similar to BTB, where a field in the cache contains the type of instruction for use in the next prediction.&lt;/p&gt;
&lt;h2 id=&#34;general-steps-of-branch-prediction&#34;&gt;General Steps of Branch Prediction&lt;/h2&gt;
&lt;p&gt;Through the introduction in this section, we can summarize the general steps of branch prediction:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get the PC.&lt;/li&gt;
&lt;li&gt;Predict whether it is a branch instruction.
&lt;ol&gt;
&lt;li&gt;If it is a conditional branch instruction, predict its direction and target.&lt;/li&gt;
&lt;li&gt;If it is an unconditional jump instruction, predict its target.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that since predicting the type of instruction is required in prediction, and we haven&amp;rsquo;t obtained the specific content of the instruction, predicting the target of a conditional branch instruction also becomes our task.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Basic of the Xiangshan Branch Prediction Unit (BPU)</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/01_xsbpu_basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/01_xsbpu_basic/</guid>
      <description>
        
        
        &lt;h2 id=&#34;branch-prediction-block-concept&#34;&gt;Branch Prediction Block Concept&lt;/h2&gt;
&lt;p&gt;For a general branch predictor, it usually predicts the relevant information of an instruction corresponding to a given PC, such as whether it is a conditional branch instruction or a jump instruction. For conditional branch instructions, it predicts whether it will jump, while for jump instructions, it predicts the jump target. However, predicting instructions one by one is inefficient, leading to slow instruction supply in the frontend.&lt;/p&gt;
&lt;p&gt;In contrast, the prediction method used in Xiangshan is to predict a block of instructions each time. That is to say, &lt;strong&gt;given a PC, Xiangshan will predict a branch prediction block starting from this PC, including the situation of several subsequent instructions, such as whether there is a branch instruction, the position of the branch instruction, whether there is a jump, and the jump target.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This prediction method can predict multiple instructions at once and send the prediction results to the fetch unit (IFU) to guide the IFU to fetch instructions. In addition, since the IFU needs to consider the performance of cache lines, it can fetch multiple instructions at once based on the prediction block, thereby improving throughput efficiency.&lt;/p&gt;
&lt;p&gt;After the prediction block is generated, &lt;strong&gt;the branch prediction block will also generate the PC to which it jumps after executing this prediction block, and then the BPU will continue to generate the next branch prediction block based on this PC.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a simple example:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As shown in the above figure, when the PC reaches 0x20000118, the BPU goes through the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The BPU learns that the PC is 0x20000118.&lt;/li&gt;
&lt;li&gt;The BPU generates a branch prediction block starting from 0x20000118, with the following approximate contents:
&lt;ol&gt;
&lt;li&gt;In the next several instructions,&lt;/li&gt;
&lt;li&gt;The third instruction is a conditional branch instruction.&lt;/li&gt;
&lt;li&gt;For this conditional branch instruction, it predicts that it will be taken.&lt;/li&gt;
&lt;li&gt;The address to which it jumps is 0x20000110.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The BPU sets the PC to 0x20000110 and continues to generate the next branch prediction block.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is the basic prediction process of the Shanshan BPU using branch prediction blocks.&lt;/p&gt;
&lt;h2 id=&#34;multiple-predictors-multiple-pipeline-structure&#34;&gt;Multiple Predictors, Multiple Pipeline Structure&lt;/h2&gt;

&lt;figure&gt;
    &lt;img src=&#34;2.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The figure below shows the overall architecture of the Xiangshan BPU, where we need to focus on two main aspects:&lt;/p&gt;
&lt;h3 id=&#34;multiple-predictors&#34;&gt;Multiple Predictors&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;To ensure prediction accuracy, Xiangshan BPU uses multiple predictors, and these predictors collectively generate the BPU&amp;rsquo;s prediction results. For example, FTB generates basic prediction results for subsequent predictors to use, while TAGE produces more accurate prediction results for conditional branch instructions, and so on.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;multiple-pipelines&#34;&gt;Multiple Pipelines&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;To meet the requirements of high performance, Xiangshan BPU adopts a pipeline design. Various predictors are at different pipeline levels. Among them, the uFTB (also known as uBTB in the figure) predictor is at the first pipeline level, capable of generating prediction results in one cycle. The other predictors need 2-3 cycles to generate prediction results. Although the prediction time is longer, the prediction results are relatively more accurate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, if it takes three cycles to get the prediction result and start predicting based on the new result, this design inevitably leads to performance loss. Because of this, it takes three clock cycles to complete one prediction.&lt;/p&gt;
&lt;p&gt;To be able to get the prediction results of some predictors in the first and second cycles, we set up three prediction result channels and output the prediction results of the three pipeline levels simultaneously, as shown in the figure below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;3.png&#34; width=&#34;600px&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;fetch-target-queue-ftq&#34;&gt;Fetch Target Queue (FTQ)&lt;/h2&gt;
&lt;h3 id=&#34;storing-branch-prediction-results&#34;&gt;Storing Branch Prediction Results&lt;/h3&gt;
&lt;p&gt;Although the BPU can provide prediction results in the form of branch prediction blocks and the IFU can fetch multiple instructions at once, there is still a performance gap between them. In general, the BPU generates prediction results faster.&lt;/p&gt;
&lt;p&gt;Therefore, a Fetch Target Queue (FTQ) is added between the BPU and the IFU as a buffer. The FTQ is essentially a queue used to store individual data items. The prediction results generated by the BPU are first stored in the FTQ, and then fetched by the IFU from the FTQ, as shown in the figure below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;4.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Whenever the BPU generates a prediction block, the prediction block is placed at the head of the FTQ. When the IFU is idle, it will fetch the next prediction block from the tail of the FTQ. The diagram below illustrates this process.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;5.png&#34; width=&#34;600px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In Xiangshan, the FTQ&amp;rsquo;s functionality goes far beyond this. Referring to the FTQ&amp;rsquo;s external interface in the figure above, it is also responsible for sending prefetch information to the ICache, storing various training information of the BPU, analyzing redirection information and update information sent from the fetch module and the backend execution module, sending update requests to the BPU, and even updating the basic data structure of the FTB predictor in the FTQ.&lt;/p&gt;
&lt;h3 id=&#34;bpu-prediction-result-redirection&#34;&gt;BPU Prediction Result Redirection&lt;/h3&gt;
&lt;p&gt;As mentioned earlier, the Xiangshan branch prediction results have three channels, which simultaneously output the prediction results of stages s1, s2, and s3. How does the FTQ use the prediction results of the three stages?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start from exploring the timing of the pipeline, as shown in the figure below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;6.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;In the first cycle, a new PC 0x4 is fetched, and the predictor (called uFTB) that can produce a prediction result within one cycle outputs its prediction result at the s1 interface, indicating the next PC as 0xf, with no output from other interfaces yet.&lt;/li&gt;
&lt;li&gt;In the second cycle, the PC is set to 0xf, and uFTB also generates a prediction result of 0xf, which is sent out from the s1 channel. At the same time, the two-cycle predictor generates the prediction result for the previous address 0x4, which is sent out from the s2 channel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, a problem arises here: in the second cycle, the prediction result generated by s2 is 0x4, but the prediction result for 0x4 has already been output by s1 in the previous cycle and placed in an entry in the FTQ. In other words, the prediction result generated by s2 has already been generated by s1. The difference is that the result from s2 is generated by the two-cycle predictor, making it more accurate.&lt;/p&gt;
&lt;p&gt;Therefore, what we need to do is not to place a new FTQ entry based on the prediction result from s2 but to &lt;strong&gt;compare the prediction results from s2 and the previous cycle&amp;rsquo;s s1 prediction results. If there is a difference, then overwrite the FTQ entry placed by the previous stage&amp;rsquo;s s1 interface.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So we add two additional signal lines to the s2 and s3 channels, which we call redirect signals. If this signal is valid, it indicates that there is a difference between the prediction result of this stage and the previous prediction result, and it is necessary to overwrite an FTQ entry from before. The structure is shown in the diagram below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;7.png&#34; width=&#34;600px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;At the time corresponding to the second cycle of the pipeline in the structural diagram, the s1 channel has already placed a branch prediction block result with an address of 0x4 into the FTQ. At this time, the s2 prediction result is generated, and the BPU finds that the s2 prediction result is different from s1, so the redirect interface for this cycle is set to valid. The FTQ will use the s2 channel&amp;rsquo;s prediction result to overwrite the FTQ entry previously storing the 0x4 prediction result.&lt;/p&gt;
&lt;p&gt;At this time, although the s1 channel has also generated a branch prediction block with 0xf as the head, it is obviously an incorrect prediction result generated by s1 based on the PC of the first cycle. Therefore, at this time, the s1 result can be directly discarded.&lt;/p&gt;
&lt;p&gt;In the third cycle, s1 starts a new round of prediction with the correct prediction result indicated by s2, the new PC 0x8. After that, if no prediction errors are detected by the s2 and s3 channels, the pipeline will continue to run at full capacity.&lt;/p&gt;
&lt;h3 id=&#34;bpu-redirect-requests&#34;&gt;BPU Redirect Requests&lt;/h3&gt;
&lt;p&gt;No matter how accurate a branch predictor is, it is not always correct. This inaccuracy can lead to incorrect instructions being filled in the subsequent pipeline. Therefore, there needs to be a mechanism to correct this, and this mechanism is redirection. When an instruction is executed by the backend execution module, the true behavior of this instruction is determined. At this time, if the backend execution module detects a branch prediction error, it will issue a &lt;strong&gt;redirect request&lt;/strong&gt; to restore the processor&amp;rsquo;s state to the state before executing the incorrect instruction. For us, we only need to pay attention to how the BPU and FTQ restore the state when redirecting.&lt;/p&gt;
&lt;p&gt;In addition to redirect requests from the backend, the Shan Mountain processor will perform a simple analysis of the instruction after it is fetched by the IFU to detect the most basic prediction errors. The specific process is as follows: after the FTQ sends a fetch request to the IFU, it will wait for the IFU to return the pre-decoded information (pre-decoding is the IFU&amp;rsquo;s simple decoding of the instruction, such as whether it is a jump instruction, what is the target of the jump). The FTQ will write the pre-decoded information back to a field in the corresponding entry in the FTQ and will also analyze the pre-decoded information. If a prediction error is detected, it will generate an IFU redirect request.&lt;/p&gt;
&lt;p&gt;Redirect requests from the backend execution module do not need to be generated by the FTQ but are directly sent from the backend to the FTQ for processing. The FTQ will forward the generated IFU redirect request and the backend redirect request to the BPU&amp;rsquo;s redirect interface. If both are valid in the same cycle, the FTQ will choose to forward the backend redirect request.&lt;/p&gt;
&lt;p&gt;The BPU with the added redirect interface is shown in the diagram below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;8.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;h3 id=&#34;bpu-update-requests&#34;&gt;BPU Update Requests&lt;/h3&gt;
&lt;p&gt;The current BPU already has the ability to correct errors, but there is still a problem: the data in the BPU cannot be updated. If it is impossible to obtain information such as the location, type, whether a jump occurred, and the jump address of the instruction, the BPU will not be trained and the accuracy will be greatly reduced.&lt;/p&gt;
&lt;p&gt;To obtain this information, we still need to rely on the Fetch Target Queue (FTQ) because it can not only interact with the IFU to obtain instruction-related information but also interact with the backend to obtain execution-related information. Therefore, there will be an update request channel directly connecting the FTQ to the BPU.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;9.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;When the backend completes the execution of an entry in the FTQ, the entry is marked as committed. Next, the FTQ forwards the update information of this entry to the BPU through the Update channel.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Through this section, we have learned about all the main interfaces required for BPU external interaction and the role of FTQ in BPU. With the BPU equipped with prediction result interfaces, redirect interfaces, and update interfaces, it can already support all external interactions of the BPU. Next, we will delve deeper into the internals of the BPU.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Redirection and Update Interfaces</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/01_redirect_and_update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/01_redirect_and_update/</guid>
      <description>
        
        
        &lt;h2 id=&#34;branch-prediction-redirectionbranchpredictionredirect&#34;&gt;&lt;strong&gt;Branch Prediction Redirection（BranchPredictionRedirect）&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Interface Definition：&lt;/strong&gt;&lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interface Type：&lt;/strong&gt;&lt;code&gt;BranchPredictionRedirect&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This interface defines the redirection requests from the branch prediction unit, mainly used to redirect the state of the branch predictor.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;BranchPredictionRedirect&lt;/code&gt; interface inherits from the &lt;code&gt;Redirect&lt;/code&gt; interface and includes many signals, only a subset of which are used by the BPU redirection. The documented structure contains only the interfaces used by the BPU.&lt;/p&gt;
&lt;p&gt;Redirection requests have two sources: those generated by comparing IFU pre-decode information and those generated during the backend execution process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In redirection requests, the key information is &lt;code&gt;cfiUpdate&lt;/code&gt;, which corresponds to a control flow instruction. This information pertains to an instruction where the BPU made a prediction error&lt;/strong&gt;. For example, if the BPU indicates that the third instruction in the prediction block is a normal instruction with no jump, but the pre-decode shows it is an unconditional jump instruction, a prediction error has occurred. In this case, the FTQ generates a redirection request. The &lt;code&gt;cfiUpdate&lt;/code&gt; in the redirection request corresponds to this unconditional jump instruction.&lt;/p&gt;
&lt;p&gt;The information in &lt;code&gt;cfiUpdate&lt;/code&gt; can be broadly classified into three types:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Information about the corresponding instruction and its execution status&lt;/strong&gt;. This includes the slot (shift) and PC of the instruction in the prediction block, the type-related information of the instruction (pd), and the execution status, such as jump target and whether it jumps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;History maintenance related information&lt;/strong&gt;. The redirection request contains branch history information corresponding to the prediction block of the instruction to help the BPU restore branch history. folded_hist represents the global folded history, histPtr represents the global history pointer, and other information assists in maintaining branch history. For detailed information, refer to the BPU Top-Level Module.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAS maintenance related information&lt;/strong&gt;. For detailed meaning, refer to the RAS sub-predictor documentation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The meaning of &lt;code&gt;level&lt;/code&gt; is whether the redirection includes this instruction. If not included, the redirection request receiver will assume the instruction has been executed, and the next prediction will start from the next instruction. The &lt;code&gt;BPU&lt;/code&gt; top-level will default to not including this instruction, and upon receiving a redirection request, it will include the execution status of this instruction in the branch history.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The detailed signal list for the redirection interface is as follows:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;level&lt;/strong&gt;: Indicates whether the redirection request includes the current position. Low means redirection after this position, high means redirection at this position.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(1.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;cfiUpdate&lt;/strong&gt;: Control flow instruction update information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Interface Type: &lt;code&gt;CfiUpdateInfo&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interface List&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pc&lt;/strong&gt;: The PC of the instruction corresponding to the redirection request
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(VaddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pd&lt;/strong&gt;: Pre-decode information of the redirection instruction
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;isRVC&lt;/strong&gt;: Whether it is a compressed instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;isCall&lt;/strong&gt;: Whether it is a function call
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;isRet&lt;/strong&gt;: Whether it is a function return
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;target&lt;/strong&gt;: Target address of the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(VaddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;taken&lt;/strong&gt;: Whether the redirection request instruction is taken
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;shift&lt;/strong&gt;: Slot in which the redirection request instruction is located, 0 if it is a normal instruction.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt((log2Ceil(numBr)+1).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;addIntoHist&lt;/strong&gt;: Whether to include the execution information of the redirection request instruction in the branch history.
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;Bool&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt;: Folded history corresponding to the redirection request
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;AllFoldedHistories(foldedGHistInfos)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;afhob&lt;/strong&gt;: Oldest bit of the branch history corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;AllAheadFoldedHistoryOldestBits(foldedGHistInfos)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lastBrNumOH&lt;/strong&gt;: Last branch position corresponding to the redirection request
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt((numBr+1).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;histPtr&lt;/strong&gt;: Global history pointer to be restored by the redirection request
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ssp&lt;/strong&gt;: RAS speculative stack top pointer at the commit stack position corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(log2Up(RasSize).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sctr&lt;/strong&gt;: RAS speculative stack top recursion counter corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;UInt(log2Up(RasCtrSize).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TOSW&lt;/strong&gt;: RAS speculative stack (queue) write pointer corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TOSR&lt;/strong&gt;: RAS speculative stack (queue) read pointer corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NOS&lt;/strong&gt;: RAS stack top counter corresponding to the redirection request instruction
&lt;ul&gt;
&lt;li&gt;Interface Type: &lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;branch-prediction-updatebranchpredictionupdate&#34;&gt;&lt;strong&gt;Branch Prediction Update（BranchPredictionUpdate）&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Interface Definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This interface defines the update requests for the branch predictor, mainly used to update the state of the branch predictor. The document lists only the interfaces used in the BPU.&lt;/p&gt;
&lt;p&gt;Update requests correspond to each branch prediction block. When a branch prediction block in the FTQ has been executed, the FTQ generates an update request for this prediction block to train the predictor. Thus, an important role of the update request is to feed back the actual execution status of the instructions to the BPU. Additionally, in the Xiangshan branch prediction unit, the update request is responsible for updating FTB entries.&lt;/p&gt;
&lt;p&gt;The information in the update request can be broadly classified into four categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PC&lt;/strong&gt;: Indicates the starting address of the prediction block, specifying which prediction block the update request corresponds to&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FTB Entry Update Information&lt;/strong&gt;: The update channel contains an FTB entry structure (ftb_entry), outputs the newly generated FTB entry from the FTQ, and indicates whether it is the same as the old FTB entry (old_entry)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actual Execution Status Information of Instructions&lt;/strong&gt;: The update channel indicates the execution status of branch and unconditional jump instructions in the prediction block, and provides the address and final target of control flow instructions (i.e., instructions that jump)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictor-Related Data Corresponding to the Prediction Block&lt;/strong&gt;: Contains spec_info and meta information (refer to the BPU Global Interface Documentation for details)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The interface list of the update request is as follows:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pc&lt;/strong&gt; PC of the update request (starting address of the prediction block)
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(VAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ftb_entry&lt;/strong&gt; Updated FTB entry
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;new FTBEntry()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface List：See（&lt;code&gt;FTBEntry&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;old_entry&lt;/strong&gt; Whether the updated FTB entry is the same as the old FTB entry
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;br_taken_mask&lt;/strong&gt; Mask indicating whether each slot instruction in the prediction block jumps
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numBr, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mispred_mask&lt;/strong&gt; Mask indicating prediction errors in the prediction block. The first and second bits indicate whether the two conditional branch instructions were mispredicted, and the third bit indicates whether the unconditional jump instruction was mispredicted.
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numBr+1, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;jmp_taken&lt;/strong&gt; Unconditional jump instruction triggered in the prediction block
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cfi_idx&lt;/strong&gt; Index of the control flow instruction in the prediction block
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;ValidUndirectioned(UInt(log2Ceil(PredictWidth).W))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;full_target&lt;/strong&gt; Jump target of the prediction block (starting address of the next prediction block)
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(VAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spec_info&lt;/strong&gt; Last stage speculative information corresponding to the prediction block
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;new SpeculativeInfo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface List：（Only &lt;code&gt;foled_hist&lt;/code&gt; is used）
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt; Global folded history
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;AllFolededHistories(foldedGHistInfos)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;meta&lt;/strong&gt; Last stage meta information corresponding to the prediction block
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(MaxMetaLength.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: uFTB Branch Predictor</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/01_uftb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/01_uftb/</guid>
      <description>
        
        
        &lt;h2 id=&#34;introduction-to-uftb&#34;&gt;Introduction to uFTB&lt;/h2&gt;
&lt;p&gt;uFTB is the first predictor among all the predictors in Xiangshan, and it serves as the cornerstone for other predictors to generate prediction results. uFTB works in the s1 stage. It can generate prediction results within the current cycle after obtaining s1_pc and output them in the s1 channel, without modifying other channels. It provides the position of the branch instruction and the target of the instruction. Subsequent predictors will further predict based on this result.&lt;/p&gt;
&lt;p&gt;Its essence is an FTB item cache, which stores FTB items, and the basic prediction result will be directly generated from the read-out FTB item.&lt;/p&gt;
&lt;p&gt;Therefore, before you start reading the document, make sure you understand the FTB items and their meanings, as well as the specific details of the prediction result interface.&lt;/p&gt;
&lt;h2 id=&#34;functionality-of-uftb&#34;&gt;Functionality of uFTB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cache FTB items and generate one-cycle prediction results&lt;/strong&gt;: uFTB maintains a small FTB item cache. After receiving PC, it reads out the FTB item corresponding to the PC within one cycle and generates an s1 stage prediction result from the FTB item.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maintain two-bit saturating counters to provide basic conditional branch results&lt;/strong&gt;: uFTB maintains two-bit saturating counters for each line of the FTB item cache. The direction prediction result is reflected in the prediction result output of uFTB.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update the FTB cache and two-bit saturating counters based on update requests&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;uftb-cache-structure&#34;&gt;uFTB Cache Structure&lt;/h2&gt;
&lt;p&gt;As mentioned above, uFTB is essentially a small cache that stores FTB items. Its approximate structure is shown in the figure below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In the current version of Xiangshan, uFTB has a total of 32 cache lines, each cache line is called &lt;code&gt;FauFTBWay&lt;/code&gt;, and one FTB item can be stored in each cache line.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When s1 pipeline is valid&lt;/strong&gt;, uFTB will use &lt;code&gt;s1_pc&lt;/code&gt; to determine which item of the uFTB cache to read out. The cache is indexed based on the tag field in PC, which is defined as pc[16:1], i.e., taking 16 bits from PC as an identifier to match a certain line in the cache.&lt;/p&gt;
&lt;p&gt;Each line in the cache, i.e., the data request interface in &lt;code&gt;FauFTBWay&lt;/code&gt;, has three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;req_tag&lt;/strong&gt;: Input tag identifier extracted from pc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;resp&lt;/strong&gt;: Output the FTB item stored in this line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;resp_hit&lt;/strong&gt;: Output indicates whether the FTB item in this line matches req_tag
uFTB connects the tag to the data request interface of each line in the cache and selects the hit FTB item based on the &lt;code&gt;resp_hit&lt;/code&gt; signal. Subsequent steps will generate a complete prediction result based on this FTB item.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;two-bit-saturating-counters&#34;&gt;Two-Bit Saturating Counters&lt;/h2&gt;
&lt;p&gt;uFTB maintains two-bit saturating counters for each line of the FTB item cache. As we know, an FTB item can store up to two conditional branch instructions, so each line&amp;rsquo;s two-bit saturating counters also have two, responsible for providing rough prediction results for the conditional branch instructions in them.&lt;/p&gt;
&lt;p&gt;When indexing the FTB item, uFTB also indexes the corresponding two-bit saturating counters.&lt;/p&gt;
&lt;p&gt;When the prediction result is generated, it will be based on the FTB item and the contents of the two two-bit saturating counters corresponding to it.&lt;/p&gt;
&lt;h2 id=&#34;prediction-result-generation&#34;&gt;Prediction Result Generation&lt;/h2&gt;
&lt;p&gt;After indexing the corresponding FTB item and two two-bit saturating counters based on s1_pc, uFTB needs to generate a prediction result based on them. The prediction result generated by uFTB will be outputted through the s1 channel when s1 pipeline is valid, and will not be modified for s2 and s3 channels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The signal generation method in the s1 prediction result can refer to the following list:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hit&lt;/strong&gt;Whether the FTB item hits
&lt;ul&gt;
&lt;li&gt;Generation method: &lt;code&gt;resp_hit&lt;/code&gt; signal in &lt;code&gt;FauFTBWay&lt;/code&gt;, one of them is valid&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;slot_valids&lt;/strong&gt;: Slot valid bit, indicating whether each slot in the ftb item is valid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;targets&lt;/strong&gt;: Jump target address corresponding to each slot&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;offsets&lt;/strong&gt;: The offset of each instruction in the slot relative to the starting address of the prediction block&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jal&lt;/strong&gt;: Whether the prediction block contains a jal instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jalr&lt;/strong&gt;: Whether the prediction block contains a jalr instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_call&lt;/strong&gt;: Whether the prediction block contains a call instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_ret&lt;/strong&gt;: Whether the prediction block contains a ret instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_may_be_rvi_call&lt;/strong&gt;: Signal indicating that the last slot in the prediction block may be an RVI type call instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_br_sharing&lt;/strong&gt;: The signal stored in the last slot (tailSlot) indicates a conditional branch instruction.
&lt;ul&gt;
&lt;li&gt;Generation: Exported from the corresponding field in the FTB entry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughErr&lt;/strong&gt;: The &lt;code&gt;pftAddr&lt;/code&gt; recorded in the FTB entry is incorrect.
&lt;ul&gt;
&lt;li&gt;Generation: Compare whether the end address represented by &lt;code&gt;pftAddr&lt;/code&gt; is greater than the start address of the predicted block. If it is smaller, an error has occurred, and this signal is set to valid. This situation may occur when the PC indexes an incorrect FTB entry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughAddr&lt;/strong&gt;: The end address of the predicted block.
&lt;ul&gt;
&lt;li&gt;Generation: If &lt;code&gt;fallThroughErr&lt;/code&gt; is invalid, it is generated based on &lt;code&gt;pftAddr&lt;/code&gt;. Otherwise, it is set to the start address + prediction width.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;br_taken_mask&lt;/strong&gt;: Branch prediction result, with each branch (slot) corresponding to a bit indicating whether the branch is predicted as taken.
&lt;ul&gt;
&lt;li&gt;Generation: Generated based on the &lt;code&gt;always_taken&lt;/code&gt; field in the FTB entry and the result indicated by the two-bit saturating counter.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;jalr_target&lt;/strong&gt;: The jump target of the jalr in this predicted block.
-Generation: From the jump target in the tailSlot of the FTB entry.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;uftb-update&#34;&gt;uFTB Update&lt;/h2&gt;
&lt;p&gt;The update of uFTB involves updating the FTB entry cache and the two-bit saturating counter, with the update content obtained through the update interface.&lt;/p&gt;
&lt;p&gt;In the uFTB predictor, the reading and writing of the cache and the two-bit saturating counter do not conflict, so we do not need to consider timing conflicts between reading and updating and can consider them as two independent parts.&lt;/p&gt;
&lt;h3 id=&#34;ftb-cache-update&#34;&gt;FTB Cache Update&lt;/h3&gt;
&lt;p&gt;The update process of the FTB cache is simple. The update channel has already specified the PC and the newly generated FTB entry, so it only needs to be written to the specified position in the cache.&lt;/p&gt;
&lt;p&gt;FTB cache updating requires two cycles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the first cycle, calculate the following based on the signals in the update:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Which row in the cache corresponds to the update request&lt;/strong&gt; The PC extracted from the update request is sent to the update request channel of FauFTBWay, and the hit signal returned by each row is calculated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In the second cycle, update according to the position calculated in the first cycle. If no row is hit, uFTB will use a &lt;strong&gt;pseudo-LRU replacement algorithm&lt;/strong&gt; to select the row to be written.
Specifically, the ftb_entry signal group in the update channel contains the complete information of the new FTB entry, which is sent to the cache row that needs to be updated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-bit-saturating-counter-update&#34;&gt;Two-Bit Saturating Counter Update&lt;/h3&gt;
&lt;p&gt;The update of the two-bit saturating counter needs to be combined with the actual execution of the subsequent program and the branch instruction information recorded in the FTB entry, which can be obtained from the update channel.&lt;/p&gt;
&lt;p&gt;The update of the two-bit saturating counter also requires two cycles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the first cycle, calculate which two-bit saturating counter corresponding to the conditional branch instruction in the slot needs to be updated, which needs to meet the following conditions:
&lt;ul&gt;
&lt;li&gt;The current branch instruction slot is valid and contains a conditional branch instruction.&lt;/li&gt;
&lt;li&gt;The current branch instruction slot is not marked as always_taken. (Because after being marked as always_taken, the result of the two-bit saturating counter will not be used.)&lt;/li&gt;
&lt;li&gt;The current branch instruction slot is not after the branch instruction slot where an actual jump occurred.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In the second cycle, update the saturating counter based on the mask generated in the first cycle and the &lt;code&gt;br_taken_mask&lt;/code&gt; information in the update channel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interface-list&#34;&gt;Interface List&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;信号类型&lt;/th&gt;
&lt;th&gt;信号位&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;信号描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;clock&lt;/td&gt;
&lt;td&gt;输入时钟&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;reset&lt;/td&gt;
&lt;td&gt;复位信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[35:0]&lt;/td&gt;
&lt;td&gt;io_reset_vector&lt;/td&gt;
&lt;td&gt;用于reset时，reset s1_pc_dup_0 提供的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_0&lt;/td&gt;
&lt;td&gt;输入位s0_pc 的 第0个复制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_1&lt;/td&gt;
&lt;td&gt;同上 第1个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_2&lt;/td&gt;
&lt;td&gt;同上 第2个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_3&lt;/td&gt;
&lt;td&gt;同上 第3个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_pc_0&lt;/td&gt;
&lt;td&gt;输出s1_pc 的 第0个复制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_pc_1&lt;/td&gt;
&lt;td&gt;同上 第1个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_pc_2&lt;/td&gt;
&lt;td&gt;同上 第2个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_pc_3&lt;/td&gt;
&lt;td&gt;同上 第3个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;solt 0 是否被预测为 always taken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;solt 1 是否被预测为 always taken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;solt 0 是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;solt 1 是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;solt 0 对应的跳转目标地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;solt 1 对应的跳转目标地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_offsets_0&lt;/td&gt;
&lt;td&gt;solt 0 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_offsets_1&lt;/td&gt;
&lt;td&gt;solt 1 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;预测块的结束地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;solt 1（无条件跳转）是否被共享为有条件跳转指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;类似 io_out_s1_pc_1 io_out_s1_full_pred_0的复制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s1_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[222:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_meta&lt;/td&gt;
&lt;td&gt;输出最后阶段的元信息 io_out_last_stage_meta = {213&amp;rsquo;h0, resp_meta_pred_way_r_1, resp_meta_hit_r_1}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_ctrl_ubtb_enable&lt;/td&gt;
&lt;td&gt;控制ubtb是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_0&lt;/td&gt;
&lt;td&gt;输入s0_fire_0，与 io_out_s1_pc_0 &amp;lt;= io_in_bits_s0_pc_0 的时钟门控相关&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_1&lt;/td&gt;
&lt;td&gt;输入s0_fire_1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_2&lt;/td&gt;
&lt;td&gt;输入s0_fire_2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_3&lt;/td&gt;
&lt;td&gt;输入s0_fire_3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_0&lt;/td&gt;
&lt;td&gt;输入s1_fire_0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_0&lt;/td&gt;
&lt;td&gt;输入s2_fire_0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_valid&lt;/td&gt;
&lt;td&gt;更新有效性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_pc&lt;/td&gt;
&lt;td&gt;传回的预测块pc（用于指示更新的预测块）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_offset&lt;/td&gt;
&lt;td&gt;solt 0 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[11:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_lower&lt;/td&gt;
&lt;td&gt;跳转目标地址的低位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_tarStat&lt;/td&gt;
&lt;td&gt;跳转后的 pc 高位是否进退位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;solt 1 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[19:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_lower&lt;/td&gt;
&lt;td&gt;跳转目标地址的低位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_tarStat&lt;/td&gt;
&lt;td&gt;跳转后的 pc 高位是否进退位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;无条件跳转指令槽中存储条件分支指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_pftAddr&lt;/td&gt;
&lt;td&gt;Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_carry&lt;/td&gt;
&lt;td&gt;pc+pft时是否产生进位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;是否预测为总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;是否预测为总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;是否跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;是否跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: uFTB Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/01_uftbfeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/01_uftbfeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Support prediction based on FTB entry&lt;/li&gt;
&lt;li&gt;Support two-bit saturating counter&lt;/li&gt;
&lt;li&gt;Support basic prediction result output and meta information output for the s1 channel&lt;/li&gt;
&lt;li&gt;Support update request response, updating internal FTB and two-bit saturating counter.&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: BPU Global Interface</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/02_global_ports/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/02_global_ports/</guid>
      <description>
        
        
        &lt;h2 id=&#34;bpu-module-overall-external-interface-predirectio&#34;&gt;&lt;strong&gt;BPU Module Overall External Interface (PredirectIO)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Interface definition: &lt;code&gt;src/main/scala/xiangshan/frontend/BPU.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;PredirectIO is the overall external interface of the branch predictor (BPU). It mainly handles the interaction between the branch predictor (BPU) and the fetch target queue (FTQ), which includes the following parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bpu_to_ftq&lt;/strong&gt;: Information sent from BPU to FTQ, mainly for sending branch prediction results from BPU to FTQ
&lt;ul&gt;
&lt;li&gt;Interface type： &lt;code&gt;BpuToFtqIO&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface type：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;resp&lt;/strong&gt;: Global branch prediction information sent from BPU to FTQ
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;DecoupledIO(new BpuToFtqBundle())&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;BpuToFtqBundle&lt;/code&gt; inherits from &lt;code&gt;BranchPredictionResp&lt;/code&gt;，without additional signals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Interface type：See  (&lt;code&gt;BranchPredictionResp&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ftq_to_bpu&lt;/strong&gt;: Information sent from FTQ to BPU, mainly for handling redirect and update requests
&lt;ul&gt;
&lt;li&gt;Interface type： &lt;code&gt;FtqToBpuIO&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface type：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;redirect&lt;/strong&gt;: Redirect request sent from FTQ to BPU
&lt;ul&gt;
&lt;li&gt;Interface type： &lt;code&gt;Valid(new BranchPredictionRedirect)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list：See （&lt;code&gt;BranchPredictionRedirect&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update&lt;/strong&gt;: Update request sent from FTQ to BPU
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Valid(new BranchPredictionUpdate)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list：See （&lt;code&gt;BranchPredictionUpdate&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;enq_ptr&lt;/strong&gt;: FTQ pointer sent to BPU, indicating which FTQ entry to write to next
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;FtqPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ctrl&lt;/strong&gt;: BPU control signals, mainly for enabling various predictors
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;BPUCtrl&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ubtb_enable&lt;/strong&gt;: UBTB predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;btb_enable&lt;/strong&gt;: BTB predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bim_enable&lt;/strong&gt;: BIM predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tage_enable&lt;/strong&gt;: TAGE predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sc_enable&lt;/strong&gt;: SC predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ras_enable&lt;/strong&gt;: RAS predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;loop_enable&lt;/strong&gt;: LOOP predictor enable
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reset_vector&lt;/strong&gt;: Reset vector, which the BPU&amp;rsquo;s PC will be reset to upon reset
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt(PAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;global-branch-prediction-information-branchpredictionresp&#34;&gt;&lt;strong&gt;Global Branch Prediction Information (BranchPredictionResp)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Interface definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This interface defines all the prediction result information of the branch predictor, including the prediction results of each stage and the related information output by the last pipeline stage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s1&lt;/strong&gt; Branch prediction result of the s1 pipeline stage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s2&lt;/strong&gt; Branch prediction result of the s2 pipeline stage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s3&lt;/strong&gt; Branch prediction result of the s3 pipeline stage
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;BranchPredictionBundle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface type：See （&lt;code&gt;BranchPredictionBundle&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_stage_meta&lt;/strong&gt; Metadata of the prediction result output by the last pipeline stage. It is a bit vector output by each predictor and combined by the Composer.
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt(MaxMetaLength.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_stage_spec_info&lt;/strong&gt; Related information of the prediction result output by the last pipeline stage
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;Ftq_Redirect_SRAMEntry&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt;  Global folded history
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;AllFoldedHistories(foldedGHistInfos)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;afhob&lt;/strong&gt; Global branch history oldest bit
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;AllAheadFoldedHistoryOldestBits(foldedGHistInfos)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lastBrNumOH&lt;/strong&gt; Last branch position
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt((numBr+1).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;histPtr&lt;/strong&gt; Global branch history pointer
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ssp&lt;/strong&gt; RAS speculation stack pointer at commit stack position
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt(log2Up(RasSize).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sctr&lt;/strong&gt; RAS speculation stack recursion counter
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt(log2Up(RasCtrSize).W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TOSW&lt;/strong&gt; RAS speculation stack (queue) write pointer
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TOSR&lt;/strong&gt; RAS speculation stack (queue) read pointer
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NOS&lt;/strong&gt; RAS stack top counter
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;CGHPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topAddr&lt;/strong&gt; RAS stack top return address
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;UInt(VAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_stage_ftb_entry&lt;/strong&gt; FTB entry output by the last pipeline stage
&lt;ul&gt;
&lt;li&gt;Interface type：&lt;code&gt;FtqEntry&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list：See（&lt;code&gt;FtqEntry&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ftb-entry-output-by-the-last-pipeline-stage&#34;&gt;&lt;strong&gt;FTB entry output by the last pipeline stage&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Interface definition: &lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This interface defines the branch prediction result information output by each pipeline stage,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pc&lt;/strong&gt; Starting PC of the predicted block
&lt;ul&gt;
&lt;li&gt;Interface type: &lt;code&gt;Vec(numDup, UInt(VAddrBits.W))&lt;/code&gt; numDup is only for register replication, with identical signals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;valid&lt;/strong&gt; Whether the prediction result is valid
&lt;ul&gt;
&lt;li&gt;Interface type: &lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hasRedirect&lt;/strong&gt; Whether a redirect is needed
&lt;ul&gt;
&lt;li&gt;Interface description: Only the s2 and s3 stages will redirect, and the prediction result of this stage will override the previous pipeline stage&amp;rsquo;s result when a redirect occurs&lt;/li&gt;
&lt;li&gt;Interface type: &lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ftq_idx&lt;/strong&gt; FTQ pointer, pointing to the FTQ entry corresponding to the prediction information of this stage
&lt;ul&gt;
&lt;li&gt;Interface type: &lt;code&gt;new FtqPtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;full_pred&lt;/strong&gt; Complete branch prediction result
&lt;ul&gt;
&lt;li&gt;Interface type: &lt;code&gt;Vec(numDup, new FullBranchPrediction)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface list: See (&lt;code&gt;FullBranchPrediction&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: FTB Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/02_ftbfeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/02_ftbfeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Support FTB entry storage&lt;/li&gt;
&lt;li&gt;Support basic prediction result output and meta information output for the s2, s3 channels&lt;/li&gt;
&lt;li&gt;Support update request response, updating internal FTB entries&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Introduction to the Xiangshan Branch Prediction Unit Structure</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/02_xsbpu_structure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/02_xsbpu_structure/</guid>
      <description>
        
        
        &lt;h2 id=&#34;how-does-the-bpu-integrate-internal-sub-predictors&#34;&gt;How Does the BPU Integrate Internal Sub-predictors?&lt;/h2&gt;
&lt;p&gt;We already know that the Xiangshan BPU adopts multiple predictors and multiple pipeline schemes. To adapt to multiple pipelines, the BPU uses a three-channel result output interface. But how does it adapt to multiple predictors? This requires us to further explore the internal structure of the BPU.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;600px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The above figure is the BPU architecture diagram from the Xiangshan documentation. Currently, we only need to focus on one piece of information: all internal sub-predictors are encapsulated in a structure called &lt;code&gt;Composer&lt;/code&gt;. The BPU only needs to interact with &lt;code&gt;Composer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What is &lt;code&gt;Composer&lt;/code&gt;? Let&amp;rsquo;s first look at their definition in the Xiangshan code.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;2.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;It can be seen that &lt;code&gt;Composer&lt;/code&gt; and the five sub-predictors have a common characteristic: they all inherit from the &lt;code&gt;BasePredictor&lt;/code&gt; base class. And the interface has been defined in the &lt;code&gt;BasePredictor&lt;/code&gt; class. In other words, &lt;code&gt;Composer&lt;/code&gt; and &lt;strong&gt;the five sub-predictors all have the same interface&lt;/strong&gt;! The top-level BPU can directly regard &lt;code&gt;Composer&lt;/code&gt; as a sub-predictor, without worrying about how the internal sub-predictors are connected.&lt;/p&gt;
&lt;h2 id=&#34;sub-predictor-interface&#34;&gt;Sub-predictor Interface&lt;/h2&gt;
&lt;p&gt;Next, we will look at what the sub-predictor interface looks like. This interface will involve the interaction between &lt;code&gt;Composer&lt;/code&gt; and the top-level BPU, as well as the interaction between each sub-predictor and &lt;code&gt;Composer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take &lt;code&gt;Composer&lt;/code&gt; as an example to illustrate the structure of the sub-predictor interface.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;3.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As shown in the above figure, the three-channel prediction results of &lt;code&gt;Composer&lt;/code&gt; are directly output to the outside of the BPU. There is also a set of three-channel prediction results connected from the inside of the BPU to &lt;code&gt;Composer&lt;/code&gt;. However, since the prediction results are generated by &lt;code&gt;Composer&lt;/code&gt;, the BPU will pass an empty prediction result to &lt;code&gt;Composer&lt;/code&gt;. The significance of this is to make the sub-predictor act as a &amp;ldquo;processor.&amp;rdquo; The sub-predictor will process the input prediction results and then output the processed prediction results.&lt;/p&gt;
&lt;p&gt;Next, the top-level BPU will provide the information needed for prediction to the pipeline. First is the &lt;strong&gt;PC&lt;/strong&gt; and &lt;strong&gt;branch history records&lt;/strong&gt; (including global history and global folding history). Next, the BPU will connect some pipeline control signals between &lt;code&gt;Composer&lt;/code&gt; and the &lt;strong&gt;pipeline control signals&lt;/strong&gt;. Finally, the BPU will directly connect the externally input &lt;strong&gt;redirect request interface&lt;/strong&gt; and &lt;strong&gt;update interface&lt;/strong&gt; to &lt;code&gt;Composer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the end, a simple definition of the sub-predictor interface can be given (for detailed definitions, please refer to the interface documentation):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;in&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(s1, s2, s3)&lt;/strong&gt; Prediction information input&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s0_pc&lt;/strong&gt;         PC to be predicted&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ghist&lt;/strong&gt;         Global branch history&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt; Global folding history&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;out  (s1, s2, s3)&lt;/strong&gt; Prediction information output&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流水线控制信号&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s0_fire, s1_fire, s2_fire, s3_fire&lt;/strong&gt; Whether the corresponding pipeline stage is working&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s2_redirect, s3_redirect&lt;/strong&gt;              Redirect signals when a prediction error is discovered in the subsequent pipeline stage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s1_ready, s2_ready, s3_ready&lt;/strong&gt;    Whether the sub-predictor corresponding pipeline stage is ready&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update&lt;/strong&gt;        Update request&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;redirect&lt;/strong&gt;      Redirect request&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;connection-between-sub-predictors&#34;&gt;Connection Between Sub-predictors&lt;/h2&gt;
&lt;p&gt;We now know that the interfaces between each sub-predictor and &lt;code&gt;Composer&lt;/code&gt; are the same, and we also know how &lt;code&gt;Composer&lt;/code&gt; is connected to the top-level BPU. This section will explain how sub-predictors are connected within &lt;code&gt;Composer&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;4.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The above figure shows the connection structure of sub-predictors in &lt;code&gt;Composer&lt;/code&gt;. It can be seen that after the three-channel prediction results are input into &lt;code&gt;Composer&lt;/code&gt;, they are first processed by &lt;code&gt;uFTB&lt;/code&gt; and then output. They are then successively processed by &lt;code&gt;TAGE-SC&lt;/code&gt;, &lt;code&gt;FTB&lt;/code&gt;, &lt;code&gt;ITTAGE&lt;/code&gt;, and &lt;code&gt;RAS&lt;/code&gt;, and finally connected to the prediction result output of &lt;code&gt;Composer&lt;/code&gt;, which is then directly connected to the outside of the BPU by &lt;code&gt;Composer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For other signals, because the interfaces between &lt;code&gt;Composer&lt;/code&gt; and each sub-predictor are the same, they are directly connected to the corresponding interfaces of each predictor by &lt;code&gt;Composer&lt;/code&gt;, without much additional processing.&lt;/p&gt;
&lt;h3 id=&#34;prediction-result-interface-connection&#34;&gt;Prediction Result Interface Connection&lt;/h3&gt;
&lt;p&gt;For sub-predictors, the connection of their prediction result is that the prediction result output of one predictor is the input of the next predictor. However, it should be noted that this connection is a combinational circuit connection and is not affected by timing.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;5.png&#34; width=&#34;400px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As shown in the above figure, taking the s1 channel as an example, from input to the output of the last predictor, it is all modified by combinational circuits, unaffected by timing. Registers only exist between the s1, s2, and s3 channels.&lt;/p&gt;
&lt;p&gt;Therefore, increasing the number of sub-predictors will not increase the number of cycles required for prediction, but will only increase the delay required for prediction per cycle.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: TAGE-SC Branch Predictor</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/02_tage_sc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/02_tage_sc/</guid>
      <description>
        
        
        &lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TAGE-SC is the &lt;strong&gt;primary predictor&lt;/strong&gt; for conditional branches in the Kunming Lake architecture, classified as an Accurate Predictor (APD). TAGE-SC can be seen as two relatively independent components: the prediction part TAGE and the verification part SC.&lt;/p&gt;
&lt;p&gt;The Tagged Geometric History Length Predictor (TAGE) utilizes &lt;strong&gt;multiple prediction tables with different history lengths&lt;/strong&gt; to exploit &lt;strong&gt;extensive branch history information&lt;/strong&gt;. TAGE predicts whether a branch instruction will be taken or not taken. It consists of a base prediction table and multiple history tables. It first predicts using multiple history tables. If there is no prediction, it uses the prediction from the base table.
The Statistical Corrector (SC) is a statistical corrector. SC references the prediction results of TAGE and &lt;strong&gt;statistical bias results&lt;/strong&gt;. Based on these two results, it &lt;strong&gt;corrects&lt;/strong&gt; the final prediction result.&lt;/p&gt;
&lt;p&gt;In Kunming Lake, each prediction block can have up to 2 branch instructions, so TAGE can predict &lt;strong&gt;up to 2 conditional branch instructions simultaneously&lt;/strong&gt;. When accessing the various history tables in TAGE, the starting address of the prediction block is used as the PC, and two prediction results are retrieved based on the &lt;strong&gt;same global history&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tage-branch-predictor-in-kunming-lake&#34;&gt;TAGE Branch Predictor in Kunming Lake&lt;/h2&gt;
&lt;h3 id=&#34;basic-functionality&#34;&gt;Basic Functionality&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;img&#34; src=&#34;tage.PNG&#34;&gt;&lt;/p&gt;
&lt;p&gt;The core idea of the TAGE predictor is to provide prediction results with different history lengths and select the most appropriate result for feedback. In the TAGE predictor, there are a total of &lt;strong&gt;1+N&lt;/strong&gt; history record tables, where N is a configurable option. In Kunming Lake, N=4.&lt;/p&gt;
&lt;p&gt;The base predictor based on the T0 table is the &lt;strong&gt;baseline predictor&lt;/strong&gt;. During prediction, it directly looks up the &amp;ldquo;2-bit saturated counter representing the jump history information&amp;rdquo; corresponding to the address in the T0 table, and then makes a prediction based on the history information. The T0 table has only 2 bits per entry, so the history states it can record are limited.&lt;/p&gt;
&lt;p&gt;For tables other than T0, we use &lt;strong&gt;Tn&lt;/strong&gt; to represent them. During table lookup, in addition to the PC, it is also necessary to use the &lt;strong&gt;global jump history information H&lt;/strong&gt; for the lookup. When a match is found, the prediction is made based on the &amp;ldquo;3-bit saturated predictor&amp;rdquo; to jump or not to jump. The higher the value of n for the Tn table, the longer the history information it uses, i.e., x&amp;lt;y.&lt;/p&gt;
&lt;p&gt;For each prediction, TAGE selects the table entry with the &lt;strong&gt;longest global jump history&lt;/strong&gt; among all the hit Tn entries.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the table entry exists and the prediction result confidence is high, it is used as the final prediction result.&lt;/li&gt;
&lt;li&gt;If the confidence is low, another internal counter is used to determine whether to select that entry or T0 as the final prediction.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To save space, when retrieving the Tn table, the input jump history information H needs to be &lt;strong&gt;compressed&lt;/strong&gt;, a process also known as &lt;strong&gt;history folding&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The table entries of each prediction table include the following elements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;T0 table indexed directly by PC
&lt;ol&gt;
&lt;li&gt;2-bit pred &lt;strong&gt;unsigned&lt;/strong&gt; saturated counter (indicating prediction direction and confidence level)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Tn table indexed by XOR of PC and folded global history
&lt;ol&gt;
&lt;li&gt;1-bit valid bit&lt;/li&gt;
&lt;li&gt;3-bit pred &lt;strong&gt;unsigned&lt;/strong&gt; saturated counter&lt;/li&gt;
&lt;li&gt;8-bit tag (used for verifying whether the hit is intentional, not coincidental)&lt;/li&gt;
&lt;li&gt;1-bit useful bit for controlling expiration
For a prediction block, all tables may generate prediction results, requiring a selection process. Typically, the higher the Tn table number, the higher its priority.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;TAGE contains two pipeline stages: the first stage calculates the index, and the second stage reads the results from the SRAM table.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stage 0 (s0): Input to the first pipeline stage, usually the pc and folded history.
&lt;strong&gt;First pipeline stage operation&lt;/strong&gt;: Calculate the index. Output to s1 via registers.&lt;/li&gt;
&lt;li&gt;Stage 1 (s1): Input to the second pipeline stage, consisting of the index and other calculated data from the first stage.
&lt;strong&gt;Second pipeline stage operation&lt;/strong&gt;: Memory access to SRAM, reading the prediction result. Output to s2 via registers.&lt;/li&gt;
&lt;li&gt;Stage 2 (s2): Actual prediction result. TAGE uses 2 stages for prediction, with the prediction result ready for use in the third stage after 2 stages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;data-structures&#34;&gt;Data Structures&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In Kunming Lake&amp;rsquo;s implementation, the T0 and Tn table structures are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;预测器&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;表项构成&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;项数&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;基准预测器T0&lt;/td&gt;
&lt;td&gt;用于在其他预测器的预测结果都无效时输出预测结果&lt;/td&gt;
&lt;td&gt;2 bit ctr 饱和计数器最高位决定跳转方向&lt;/td&gt;
&lt;td&gt;2路各2048项，每路对于一条分支指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;预测表T1-T4&lt;/td&gt;
&lt;td&gt;对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为最后预测结果。历史记录长度由输入的H决定&lt;/td&gt;
&lt;td&gt;1 bit valid 有效位 3 bit ctr 饱和计数器8 bit tag 校验命中1 bit us 作为usefulness计数器&lt;/td&gt;
&lt;td&gt;4096项、奇数项对应第一条分支指令，偶数项对应第二条分支指令&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each table Tn, the length of its input &amp;ldquo;global branch history data H&amp;rdquo; varies during querying. Assuming the total prediction history length is S, Tn and Tn+1 may use the low x, low y bits of S (&lt;strong&gt;the lower bits are the newer history&lt;/strong&gt;) as query inputs. Generally, the larger the n of table Tn, the longer the history information used, i.e., x&amp;lt;y.&lt;/p&gt;
&lt;p&gt;During the query of table Tn, since the historical data H is &amp;ldquo;compressed,&amp;rdquo; it may lead to a situation where the result of one PC1^H1 matches another PC2^H2 (similar to a Hash collision), resulting in indexing into invalid data (predicting PC1 indexes to predicted PC2&amp;rsquo;s data). Therefore, TAGE provides a tag identifier for each table, using an 8-bit tag in the Kunming Lake implementation to reduce the probability of collisions. The calculation method and index method for tags are different; &lt;strong&gt;only when the tag calculation is the same, the query result is valid&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In the Tn table entry, in addition to the saturation counter ctr and tag, there is also a 1-bit &lt;strong&gt;usefulness counter&lt;/strong&gt;. When this counter is 0, it is a weak entry, indicating that the entry can be reallocated for other uses; when it is not 0, it is a strong entry, indicating that the entry cannot be reallocated for other uses.&lt;/p&gt;
&lt;p&gt;To &lt;strong&gt;try to avoid&lt;/strong&gt; the situation where all table entries are 1 and no new table entries can be allocated, TAGE expects to use the counter bankTickCtrs to clear all usefulness to 0.&lt;/p&gt;
&lt;h3 id=&#34;retrieval-method-for-t0-and-tn-tables&#34;&gt;Retrieval Method for T0 and Tn Tables&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For table T0, indexing is done using the PC[11:1] bits to index 2048 table entries, so for T0, there is no possibility of not finding a match.&lt;/li&gt;
&lt;li&gt;For table Tn, in addition to PC[11:1], retrieval also requires searching based on the global branch history. In Kunming Lake, the top-level branch predictor maintains a 256-bit global history record GH, which can fold the GH&amp;rsquo;s most recent n bits of history information based on the required number of bits x for the sub-predictor. That is, n is divided into ceil(x/n) units of length x, and then XOR is performed bitwise. This folded history is denoted as FH (Folded History), and the specific process can be found in the [Branch Folding History section](../00_bpu_top/#Branch Folding History). When the TAGE predictor searches for a table entry in Tn, it uses the index and tag, calculated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Calculation Formula&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;index = FH ^ ((pc&amp;raquo;1)低位)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tag = FH1 ^ FH2 ^ ((pc&amp;raquo;1)低位)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Where FH, FH1, FH2 represent the folded global branch history according to certain rules. For Tn, FH, FH1, and FH2 each have their own folding bit numbers, which may not be the same. In the Kunming Lake implementation, the configurations of the T0 and Tn tables are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;表名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH1长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH2长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;最近历史长度（用到GH中的位数）&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;T1&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;7比特&lt;/td&gt;
&lt;td&gt;低8位，即把最新8位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T2&lt;/td&gt;
&lt;td&gt;11比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;7比特&lt;/td&gt;
&lt;td&gt;低13位，即把最新13位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T3&lt;/td&gt;
&lt;td&gt;11比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;7比特&lt;/td&gt;
&lt;td&gt;低32位，即把最新32位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;11比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;7比特&lt;/td&gt;
&lt;td&gt;低119位，即把最新119位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note: pc&amp;raquo;1 is used because RISC-C extension is used, with 2-byte alignment, and PC itself is already aligned to 1 byte, so only 1 bit is used.&lt;/p&gt;
&lt;h3 id=&#34;alternative-predictor&#34;&gt;Alternative Predictor&lt;/h3&gt;
&lt;p&gt;Since the Tn table uses saturation counters for prediction, there may be situations where the output result is &amp;ldquo;not confident.&amp;rdquo; For example, in Kunming Lake, for a 3-bit saturation counter, both 100 and 011 indicate a &lt;strong&gt;weak prediction&lt;/strong&gt;. To provide more choices as references for this state, the TAGE predictor also provides an &amp;ldquo;alternative predictor&amp;rdquo; mechanism, which determines whether to select the prediction result of Tn or T0 when the Tn table predicts with low confidence.&lt;/p&gt;
&lt;p&gt;In the Kunming Lake implementation, the &amp;ldquo;alternative predictor&amp;rdquo; is implemented based on the register group &lt;strong&gt;useAltOnNaCtrs&lt;/strong&gt;. It consists of two paths of 128 &lt;strong&gt;4-bit saturation counters&lt;/strong&gt; each, initialized to &lt;strong&gt;0b1000&lt;/strong&gt;. When TAGE makes a prediction, it uses &lt;strong&gt;PC(7,1)&lt;/strong&gt; to index the corresponding saturation counter. If the value of this counter is greater than or equal to the preset value and the prediction result of Tn is not confident, it selects the result of T0; otherwise, it selects the result of Tn.&lt;/p&gt;
&lt;h3 id=&#34;prediction-process&#34;&gt;Prediction Process&lt;/h3&gt;
&lt;p&gt;In summary, the prediction steps of the TAGE predictor in Kunming Lake are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Parallel&lt;/strong&gt; indexing of T0 and Tn tables, selecting which table to use based on the hit result:
&lt;ol&gt;
&lt;li&gt;If a match to the tag of a Tn table is found, the potential prediction result is given by the saturation counter of the longest history Tn table.&lt;/li&gt;
&lt;li&gt;If no match to a Tn table is found, the final prediction result is given by the saturation counter of the T0 table.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;If the potential prediction result of the matched Tn table is a weak prediction (100,011), and the value of the corresponding 4-bit counter in the alternative prediction for PC is greater than or equal to a threshold, the result of the T0 table is used as the final result; otherwise, the prediction result of the Tn table is used as the final prediction result.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-process&#34;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;Since the prediction process of TAGE involves many counters and tags, they need to be updated according to certain rules, a process known as training. This training process occurs in the BPU&amp;rsquo;s update stage, where the PC, branch history, and prediction correctness information are input. The training process for branch prediction in Kunming Lake is divided into several steps based on different conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update when &lt;strong&gt;T0&lt;/strong&gt; is the final prediction result: If a jump occurs (i.e., taken), increment the ctr saturation counter indexed by the pc; otherwise, decrement it.&lt;/li&gt;
&lt;li&gt;When &lt;strong&gt;only T0&lt;/strong&gt; is hit, the following operations are performed:
&lt;ol&gt;
&lt;li&gt;If T0 is &lt;strong&gt;predicted correctly&lt;/strong&gt;, no additional update is performed.&lt;/li&gt;
&lt;li&gt;If T0 is &lt;strong&gt;predicted incorrectly&lt;/strong&gt;, attempt to randomly allocate a new table entry in a Tn table. To allocate a new table entry, the original entry&amp;rsquo;s usefulness at the corresponding index must be 0. The new entry is initialized as a weak prediction with usefulness 0, and its tag is set to the newly calculated tag.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;When &lt;strong&gt;both T0 and Tn are hit&lt;/strong&gt;, the following operations are performed:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tn&lt;/strong&gt; is always updated: If a jump occurs, increment the ctr saturation counter indexed by the pc; otherwise, decrement it. It is important to note that &amp;ldquo;hit&amp;rdquo; means that the tag of the indexed entry matches the calculated tag.&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;T0 and Tn produce the same result&lt;/strong&gt;:
&lt;ol&gt;
&lt;li&gt;If &lt;strong&gt;predicted correctly&lt;/strong&gt;, no additional update is performed.&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;predicted incorrectly&lt;/strong&gt;, attempt to allocate a new table entry in a table with a longer history than Tn. To allocate a new entry, the usefulness of the original entry at the corresponding index must be 0. The new entry is initialized as a weak prediction with usefulness 0, and its tag is set to the tag calculated using the new history information.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;T0 and Tn produce different results&lt;/strong&gt;:
&lt;ol&gt;
&lt;li&gt;If &lt;strong&gt;Tn is correct&lt;/strong&gt;, the entry&amp;rsquo;s usefulness is incremented.
&lt;ol&gt;
&lt;li&gt;If the result is still a &lt;strong&gt;weak prediction&lt;/strong&gt;, the counter in the alternative prediction for T0 is decremented.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;Tn is incorrect&lt;/strong&gt;, the entry&amp;rsquo;s usefulness is decremented, and a new entry is allocated in a table with a longer history than Tn, as in 3.2.2.
&lt;ol&gt;
&lt;li&gt;If the result is still a &lt;strong&gt;weak prediction&lt;/strong&gt;, the counter in the alternative prediction for T0 is incremented.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;When a new table &lt;strong&gt;needs to be allocated&lt;/strong&gt;, dynamic reset of the usefulness flag is performed.
&lt;ol&gt;
&lt;li&gt;Using a 7-bit bankTickCtrs register and calculating:
&lt;ol&gt;
&lt;li&gt;The number of allocatable tables &lt;strong&gt;a&lt;/strong&gt; (with a longer history length than the current and corresponding index usefulness is 0)&lt;/li&gt;
&lt;li&gt;The number of unallocatable tables &lt;strong&gt;b&lt;/strong&gt; (with a longer history length than the current and corresponding index usefulness is not 0)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; bankTickCtrs += Δ (saturated counter), Δ = b - a,&lt;/li&gt;
&lt;li&gt;When bankTickCtrs reaches its &lt;strong&gt;maximum&lt;/strong&gt; value, &lt;strong&gt;reset all usefulness to 0&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;kunming-lake-sc-branch-predictor&#34;&gt;Kunming Lake SC Branch Predictor&lt;/h2&gt;
&lt;h3 id=&#34;basic-function-introduction&#34;&gt;Basic Function Introduction&lt;/h3&gt;
&lt;p&gt;The SC (Statistics counter) branch predictor is a branch predictor based on historical statistical information. Similar to TAGE, SC typically has multiple tables Tn, each corresponding to different lengths of historical jump statistics. The difference is that in SC, when predicting based on the PC, each table Tn is accessed, and then SC adds up each hit table entry to calculate the total &amp;ldquo;saturated counter&amp;rdquo; jump information, and finally determines whether to jump based on the total jump information. Generally, SC uses &amp;ldquo;signed saturated counters&amp;rdquo;, where a counter value greater than 0 indicates a jump, and less than 0 indicates no jump. The larger the absolute value of the counter, the higher the prediction confidence.&lt;/p&gt;
&lt;p&gt;In the SC predictor, SC is also composed of multiple tables (e.g., T1, T2, T3, T4), but with fewer basic prediction tables T0 compared to the TAGE predictor. The Tn tables in SC have 6-bit signed saturated counters. The indexing method for SC tables is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Calculation Method&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Index = (FH) ^ ((pc&amp;raquo;1)低位)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For each table, the number of entries and the folded history length used are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Table&lt;/th&gt;
&lt;th&gt;Number of Entries&lt;/th&gt;
&lt;th&gt;FH Length&lt;/th&gt;
&lt;th&gt;Folded History Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;T1&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;不折叠&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T2&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;把历史信息的低4位，折叠成FH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T3&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;把历史信息的低10位，折叠成FH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;把历史信息的低16位，折叠成FH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The formula for calculating the total statistical prediction result is as follows:&lt;/p&gt;

&lt;div class=&#34;math&#34;&gt;$$scCtrSum=\sum_{i=0}^{i&lt;4}( (ctr_{sc} &lt;&lt; 1) +1)$$&lt;/div&gt;&lt;p&gt;Where ctr_sc represents the signed saturated counter for each table. Left-shifting and adding one is for weight adjustment. The accumulated scCtrSum is the final prediction result of SC. If this value is greater than zero, the prediction is a jump; if it is less than zero, the prediction is no jump. The larger the absolute value, the higher the prediction confidence.&lt;/p&gt;
&lt;p&gt;Typical data conversion results are as follows (extended to 9 bits to prevent overflow during calculation):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All are 6b100000 (strong no-jump), resulting in 9b100000100, with a value of -252.&lt;/li&gt;
&lt;li&gt;All are 6b011111 (strong jump), resulting in 9b011111100, with a value of 252.&lt;/li&gt;
&lt;li&gt;All are 6b000000 (weak jump), resulting in 9b000000100, with a value of 4.&lt;/li&gt;
&lt;li&gt;All are 6b111111 (weak no-jump), resulting in 9b111111100, with a value of -4.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;prediction-process-1&#34;&gt;Prediction Process&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Calculate the index of table Tn using the PC and historical information.&lt;/li&gt;
&lt;li&gt;Query the index to obtain the saturation counters for all tables.&lt;/li&gt;
&lt;li&gt;Sum up all the saturation counters obtained from all tables to get the final prediction result (take a jump for values greater than 0, no jump for values less than 0).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-process-1&#34;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;Update the saturation counters during the update phase.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the real instruction corresponding to PC jumps, increment the saturation counters corresponding to all tables.&lt;/li&gt;
&lt;li&gt;If the real instruction corresponding to PC does not jump, decrement the saturation counters corresponding to all tables.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;kunming-lake-tage-sc-branch-predictor&#34;&gt;Kunming Lake TAGE-SC Branch Predictor&lt;/h2&gt;
&lt;h3 id=&#34;why-sc-is-needed-with-tage&#34;&gt;Why SC is Needed with TAGE&lt;/h3&gt;
&lt;p&gt;In some applications, some branch behaviors have a weak correlation with branch history or paths, showing a statistical prediction bias. For these branches, using counters to capture statistical biases is more effective than history-based branch prediction.&lt;/p&gt;
&lt;p&gt;TAGE is very effective in predicting branches that are highly correlated with history, but it performs poorly for branches with statistical biases. For example, branches that have a small bias in one direction but are not strongly correlated with historical paths. To avoid this problem, an SC predictor can be added to the traditional TAGE predictor.&lt;/p&gt;
&lt;h3 id=&#34;tage-sc-functionality&#34;&gt;TAGE-SC Functionality&lt;/h3&gt;
&lt;p&gt;In the Kunming Lake TAGE-SC predictor, both the TAGE and SC prediction results P1 and P2 are obtained simultaneously, and then their results are accumulated P = P1 + P2. If the absolute value of P is greater than the 8-bit threshold sc_bank_thres, the predictor result P is used; otherwise, P1 is used as the final prediction result.&lt;/p&gt;
&lt;p&gt;For dynamic adaptation, the threshold sc_thres needs to be dynamically changed. Therefore, in the implementation, TAGE-SC uses a 5-bit sc_bank_ctr counter to adjust the threshold sc_bank_thres. Additionally, since Kunming Lake supports the simultaneous prediction of 2 branch instructions, the threshold register and corresponding control counter are also duplicated.&lt;/p&gt;
&lt;h3 id=&#34;pipeline-1&#34;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;The TAGE-SC predictor contains 3 pipeline stages, where the 2-stage pipeline of TAGE has been introduced, and the pipeline of the SC part is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Stage 0: Read PC and folded history into s0.
&lt;strong&gt;First Stage&lt;/strong&gt;: Calculate the index from pc and FH to obtain s0_idx.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stage 1: Read s0_idx from s0.
&lt;strong&gt;Second Stage&lt;/strong&gt;: Find the counter data corresponding to s1_idx in SCTable and output to s1_scResps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stage 2: Read s1_scResps from s1.
&lt;strong&gt;Third Stage&lt;/strong&gt;: Select whether to invert the prediction result based on s2_scResps and output to s2_disagree.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stage 3: Read the result from s2_disagree as s3_disagree.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;prediction-process-2&#34;&gt;Prediction Process&lt;/h3&gt;
&lt;p&gt;In TAGE-SC prediction, the prediction result P1 of TAGE is represented by tage_ctr, and the prediction result P2 of SC is represented by scCtrSum. The prediction is divided into four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Execute the SC predictor to get the prediction result scCtrSum.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simultaneously obtain the prediction result tage_ctr of the TAGE predictor.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Since the prediction result of TAGE is an unsigned saturation counter, and the prediction result of SC is a signed saturation counter, if they are added together, data conversion is required.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kunming Lake adopts a conversion for the result of TAGE. The converted result is represented by tageCtrCentered, and the specific conversion process is as follows:&lt;/p&gt;

&lt;div class=&#34;math&#34;&gt;$$tageCtrCentered=((((ctr_{tage} -4)&lt;&lt;1)+1)&lt;&lt;3) $$&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Conversion of a 3-bit unsigned saturation counter to an 8-bit signed saturation counter result is illustrated as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;3b100 Weak jump =&amp;gt; 8b00001000 = 8&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3b011 Weak non-jump =&amp;gt; 8b11111000 = -8&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3b111 Strong jump =&amp;gt; 8b00111000 = 56&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3b000 Strong non-jump =&amp;gt; 8b11001000 = -56&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the prediction results of TAGE and SC to get the final prediction result P, represented by totalSum.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&#34;math&#34;&gt;$$totalSum = scCtrSum + tageCtrCentered$$&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;Determine the final prediction direction based on &lt;code&gt;totalSum&lt;/code&gt; and &lt;code&gt;sc_bank_thres&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Jump if totalSum &amp;gt; 0 and its absolute value exceeds the threshold: If scCtrSum &amp;gt; sc_bank_thres - tageCtrCentered, it can also be understood as totalSum &amp;gt; sc_bank_thres. The above expression can reduce the maximum bit width (ensuring no overflow requires 10 bits to become 9 bits).&lt;/li&gt;
&lt;li&gt;No jump if totalSum &amp;lt; 0 and its absolute value exceeds the threshold: If scCtrSum &amp;lt; -sc_bank_thres - tageCtrCentered, it can also be understood as |totalSum| &amp;gt; sc_bank_thres.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-process-2&#34;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;After combining TAGE and SC, TAGE-SC adds an sc_bank_ctr counter to control the threshold sc_bank_thres. Therefore, during training, in addition to the training of TAGE and SC themselves, the newly added counter needs to be updated.&lt;/p&gt;
&lt;p&gt;During the update phase, the specific update process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;TAGE-SC uses the prediction result P (i.e., the prediction result after TAGE + SC). If |totalSum| is in the range [sc_bank_thres -4, sc_bank_thres -2], update the threshold-related register group.
&lt;ol&gt;
&lt;li&gt;Update sc_bank_ctr, the saturation counter: If the prediction is correct, sc_bank_ctr +=1; if the prediction is incorrect, sc_bank_ctr -=1.&lt;/li&gt;
&lt;li&gt;Update sc_bank_thres, limited saturation operation: If the updated value of sc_bank_ctr reaches 0b11111 and sc_bank_thres &amp;lt;= 31, then sc_bank_thres +=2; if the updated value of sc_bank_ctr is 0 and sc_bank_thres &amp;gt;=6, then sc_bank_thres -=2. For all other cases, thres remains unchanged.&lt;/li&gt;
&lt;li&gt;After the update judgment of sc_bank_thres is completed, another judgment is made on sc_bank_ctr. If the updated sc_bank_ctr is 0b11111 or 0, thres_ctr is reset to the initial value 0b10000.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;TAGE-SC uses the prediction result P1 (i.e., the prediction result of TAGE) and does not perform any operations.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;interface-list&#34;&gt;Interface List&lt;/h2&gt;
&lt;h3 id=&#34;tagesc&#34;&gt;TageSC&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;信号类型&lt;/th&gt;
&lt;th&gt;信号宽度&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;信号描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;clock&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;reset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[35:0]&lt;/td&gt;
&lt;td&gt;io_reset_vector&lt;/td&gt;
&lt;td&gt;用于reset时，reset s1_pc_dup_0 提供的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_0&lt;/td&gt;
&lt;td&gt;复制的s0_pc的dup数组的第1个，给顶层BPU的PC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_1&lt;/td&gt;
&lt;td&gt;复制的s0_pc第2个，给Tage的PC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_3&lt;/td&gt;
&lt;td&gt;复制的s0_pc的第4个，给SC的PC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_17_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的11bits 折叠历史 从多长历史范围折叠到11bit见前文所述的表 注意TageTable下标+1，此处 T2 是前文 T3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_16_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的11bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_15_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_14_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 0 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_9_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_8_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_7_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 0 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_5_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_4_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_3_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_1_hist_1_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的11bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_12_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 1 用到的 4bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_11_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 2 用到的 8bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_2_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 3 用到的 8bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_{i}&lt;em&gt;br_taken_mask&lt;/em&gt;{j} Tage 在 s2流水级输出的，复制4份 预测块中第 j 条分支指令TAGE预测结果  这里不该叫mask吧&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_{i}&lt;em&gt;br_taken_mask&lt;/em&gt;{j} Tage 在 s3流水级输出的，复制4份 预测块中第 j 条分支指令SC预测结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*[222:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_meta&lt;/td&gt;
&lt;td&gt;见附表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_ctrl_tage_enable&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_ctrl_sc_enable&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s0_fire_0&lt;/td&gt;
&lt;td&gt;s0 阶段流水线控制 相同信号复制多份，0给BPU，1给Tage，3给SC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s0_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s0_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s1_fire_0&lt;/td&gt;
&lt;td&gt;s1 阶段流水线控制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s1_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s1_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s1_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s2_fire_0&lt;/td&gt;
&lt;td&gt;s2 阶段流水线控制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s2_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s2_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s2_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_s1_ready&lt;/td&gt;
&lt;td&gt;tage的所有表，可以执行读取结果的操作&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_valid&lt;/td&gt;
&lt;td&gt;从FTQ发向BPU的后端执行结果（更新信号）是否有效&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[40:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_pc&lt;/td&gt;
&lt;td&gt;（后端执行过的）预测块的PC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_17_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的11bits 折叠历史 预测时使用的分支历史结果，没有更新，转了一圈回来了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_16_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的11bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_15_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_14_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 0 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_12_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 1 用到的 4bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_11_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 2 用到的 8bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_9_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_8_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_7_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 0 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[6:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_5_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 3 用到的7bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_4_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_3_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 2 用到的8bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_2_folded_hist&lt;/td&gt;
&lt;td&gt;SCTable 3 用到的 8bit 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[10:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_1_folded_hist&lt;/td&gt;
&lt;td&gt;TageTable 1 用到的11bits 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;FTB 表项的第一个slot是否有效（存储了跳转指令）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;FTB 表项的最后一个slot是否存储了条件分支而非无条件跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;FTB 表项的最后一个slot是否有效&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;历史上slot 0 指令总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;历史上slot 1 指令总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;solt 0 是否 taken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;solt 1 是否 taken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_mispred_mask_0&lt;/td&gt;
&lt;td&gt;solt 0 是否预测正确&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;io_update_bits_mispred_mask_1&lt;/td&gt;
&lt;td&gt;solt 1 是否预测正确&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;*[222:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_meta&lt;/td&gt;
&lt;td&gt;见附表&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;io_out_last_stage_meta&#34;&gt;io_out_last_stage_meta&lt;/h3&gt;
&lt;p&gt;需要设计参与优化！&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;信号类型&lt;/th&gt;
&lt;th&gt;信号位&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;信号描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[218:88]&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;占位，全为0，传递到composer时会忽略&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;87&lt;/td&gt;
&lt;td&gt;resp_meta_providers_1_valid_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[86:85]&lt;/td&gt;
&lt;td&gt;resp_meta_providers_1_bits_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;resp_meta_providers_0_valid_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[83:82]&lt;/td&gt;
&lt;td&gt;resp_meta_providers_0_bits_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[81:79]&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_1_r_ctr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_1_r_u&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;77&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_1_r_unconf&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[76:74]&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_0_r_ctr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_0_r_u&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;resp_meta_providerResps_0_r_unconf&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;71&lt;/td&gt;
&lt;td&gt;resp_meta_altUsed_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;resp_meta_altUsed_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;69&lt;/td&gt;
&lt;td&gt;resp_meta_altDiffers_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;resp_meta_altDiffers_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[67:66]&lt;/td&gt;
&lt;td&gt;resp_meta_basecnts_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[65:64]&lt;/td&gt;
&lt;td&gt;resp_meta_basecnts_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[63:60]&lt;/td&gt;
&lt;td&gt;resp_meta_allocates_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[59:56]&lt;/td&gt;
&lt;td&gt;resp_meta_allocates_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;resp_meta_takens_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;resp_meta_takens_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_tageTakens_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_tageTakens_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;51&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_scUsed_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_scUsed_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_scPreds_1_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;resp_meta_scMeta_scPreds_0_r&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[47:42]&lt;/td&gt;
&lt;td&gt;r_1_3&lt;/td&gt;
&lt;td&gt;scMeta(预测时的状态)中第2路的第4个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[41:36]&lt;/td&gt;
&lt;td&gt;r_1_2&lt;/td&gt;
&lt;td&gt;scMeta中第2路的第3个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[35:30]&lt;/td&gt;
&lt;td&gt;r_1_1&lt;/td&gt;
&lt;td&gt;scMeta中第2路的第2个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[29:24]&lt;/td&gt;
&lt;td&gt;r_1_0&lt;/td&gt;
&lt;td&gt;scMeta中第2路的第1个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[23:18]&lt;/td&gt;
&lt;td&gt;r_3&lt;/td&gt;
&lt;td&gt;scMeta中第1路的第4个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[17:12]&lt;/td&gt;
&lt;td&gt;r_2&lt;/td&gt;
&lt;td&gt;scMeta中第1路的第3个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[11:6]&lt;/td&gt;
&lt;td&gt;r_1&lt;/td&gt;
&lt;td&gt;scMeta中第1路的第2个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[5:0]&lt;/td&gt;
&lt;td&gt;r_0&lt;/td&gt;
&lt;td&gt;scMeta中第1路的第1个sc_ctr的值&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;io_update_bits_meta&#34;&gt;io_update_bits_meta&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;信号类型&lt;/th&gt;
&lt;th&gt;信号位&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;信号描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[218:94]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;FTB, ITAGE, RAS 模块传给 FTQ 的 META 信息，忽略&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[93:6]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_meta[87:0] 偏移 6bit 后的结果&lt;/td&gt;
&lt;td&gt;TAGE 输出给 FTQ 的 META&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[5:0]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;uFTB 输出给 FTQ 的 META&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Base Predictor Class and Sub Predictor Interface</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/03_subpredictor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/ports/03_subpredictor/</guid>
      <description>
        
        
        &lt;p&gt;In the Xiangshan branch prediction unit, all its sub-predictors and the class implementations of Composer are inherited from the sub-predictor base class (BasePredictor). The sub-predictor interface (BasePredictorIO) is also defined in the sub-predictor base class. Therefore, we can consider that Composer and all sub-predictors have the same interface.&lt;/p&gt;
&lt;p&gt;In the understanding and verification of sub-prediction, our most direct external interaction occurs in the sub-predictor interface and some variables defined in the sub-predictor base class. Therefore, before verifying the sub-predictor, it is strongly recommended that you read and understand this section of the document.&lt;/p&gt;
&lt;p&gt;The general content and usage of the sub-branch predictor interface have been introduced in the &lt;code&gt;Xiangshan Branch Prediction Unit (BPU) Basic Design&lt;/code&gt; section. This document will focus on the signal details of the interface.&lt;/p&gt;
&lt;h2 id=&#34;sub-branch-predictor-interface-basepredictorio&#34;&gt;&lt;strong&gt;Sub-Branch Predictor Interface (BasePredictorIO)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Interface Definition: &lt;code&gt;src/main/scala/xiangshan/frontend/BPU.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Each sub-branch predictor needs to implement this interface, which defines the input and output interfaces of the sub-branch predictor.&lt;/p&gt;
&lt;p&gt;Note: Some signals are defined as &lt;code&gt;numDup&lt;/code&gt; quantities, where each signal is exactly the same. Multiple identical signals are for other considerations.&lt;/p&gt;
&lt;p&gt;The detailed signal list is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;reset_vector&lt;/strong&gt; Reset vector, when reset occurs, the BPU&amp;rsquo;s pc will be reset to this value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(PAddrBits.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;in&lt;/strong&gt; Information sent from the BPU to the sub-branch predictor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;DecoupledIO(new BasePredictorInput)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s0_pc&lt;/strong&gt; PC of the s0 pipeline stage
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, UInt(VAddrBits.W))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt; Global folded history information
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, new AllFoldedHistories(foldedGHistInfos))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：See（&lt;code&gt;AllFoldedHistories&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ghist&lt;/strong&gt; Global branch history information
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(HistoryLength.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;resp_in&lt;/strong&gt;  Global branch prediction information (including s1, s2, s3 prediction result information)
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;BranchPredictionResp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：See（&lt;code&gt;BranchPredictionResp&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;out&lt;/strong&gt; Information sent from the sub-branch predictor to the BPU (including s1, s2, s3 prediction result information)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;new BasePredictorOutput&lt;/code&gt; 继承自 &lt;code&gt;BranchPredictionResp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：See（&lt;code&gt;BranchPredictionResp&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ctrl&lt;/strong&gt; BPU sub-predictor enable control signal, mainly used to control whether each predictor is enabled&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;BPUCtrl&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Interface Type：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ubtb_enable&lt;/strong&gt;: UBTB predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;btb_enable&lt;/strong&gt;: BTB predictor enable
&lt;ul&gt;
&lt;li&gt;接Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bim_enable&lt;/strong&gt;: BIM predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tage_enable&lt;/strong&gt;: TAGE predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sc_enable&lt;/strong&gt;: SC predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ras_enable&lt;/strong&gt;: RAS predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;loop_enable&lt;/strong&gt;: LOOP predictor enable
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s0_fire&lt;/strong&gt; s0 stage handshake success signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s1_fire&lt;/strong&gt; s1 stage handshake success signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s2_fire&lt;/strong&gt; s2 stage handshake success signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s3_fire&lt;/strong&gt; s3 stage handshake success signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s2_redirect&lt;/strong&gt; s2 stage redirection signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s3_redirect&lt;/strong&gt; s3 stage redirection signal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Vec(numDup, Bool())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s1_ready&lt;/strong&gt; s1 stage ready to receive information (Direction: output from the sub-predictor)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s2_ready&lt;/strong&gt; s2 stage ready to receive information (Direction: output from the sub-predictor)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;s3_ready&lt;/strong&gt; s3 stage ready to receive information (Direction: output from the sub-predictor)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt; Update request forwarded from the BPU to the sub-branch predictor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Valid(new BranchPredictionUpdate)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：See（&lt;code&gt;BranchPredictionUpdate&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;redirect&lt;/strong&gt; Redirect request forwarded from the BPU to the sub-branch predictor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;Valid(new BranchPredictionRedirect)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Signal List：See（&lt;code&gt;BranchPredictionRedirect&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pipeline control signals will be further explained in the following content.&lt;/p&gt;
&lt;h3 id=&#34;global-folding-history-allfoldedhistories&#34;&gt;Global Folding History (AllFoldedHistories)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Interface Definition：&lt;/strong&gt;&lt;code&gt;src/main/scala/xiangshan/frontend/FrontendBundle.scala&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interface Type：&lt;/strong&gt;&lt;code&gt;AllFoldedHistories(foldedGHistInfos))&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The interface information of the global folding history consists of only one &lt;code&gt;FoldedHistory&lt;/code&gt; list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hist&lt;/strong&gt; List of folded histories
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;MixedVec(gen.map{case (l, cl) =&amp;gt; new FoldedHistory(l, cl, numBr)})&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interface information of &lt;code&gt;FoldedHistory&lt;/code&gt; also has only one item.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;folded_hist&lt;/strong&gt; Single folded history, with a bit width equal to the compressed history length.
&lt;ul&gt;
&lt;li&gt;Interface Type：&lt;code&gt;UInt(compLen.W)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means that the interface of the global folding history is actually a list that stores folded histories, where each item is a folded history of a specific length.&lt;/p&gt;
&lt;h2 id=&#34;base-predictor-class&#34;&gt;Base Predictor Class&lt;/h2&gt;
&lt;p&gt;The base predictor class defines several signals, which can be accessed in each sub-predictor, and several connections are made within it.&lt;/p&gt;
&lt;p&gt;Most of the signals are relatively easy to understand. We need to pay particular attention to the pc of each pipeline, which also involves your understanding of pipeline control signals. Therefore, next, we will introduce the meanings of pipeline control signals that need to be paid attention to in sub-predictors, as well as the meanings of the s1_pc, s2_pc, s3_pc signals.&lt;/p&gt;
&lt;p&gt;There are three groups of pipeline control signals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fire signals  (s0, s1, s2, s3)&lt;/li&gt;
&lt;li&gt;redirect signals  (s2, s3)&lt;/li&gt;
&lt;li&gt;ready signals  (s1, s2, s3)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pc signals in the base predictor class are divided into four groups, &lt;code&gt;s0_pc_dup&lt;/code&gt;, &lt;code&gt;s1_pc_dup&lt;/code&gt;, &lt;code&gt;s2_pc_dup&lt;/code&gt;, &lt;code&gt;s3_pc_dup&lt;/code&gt;. Each group of signals contains multiple pc signals, which are exactly the same and are duplicated for some other reasons. Therefore, we can simply consider them as &lt;code&gt;s0_pc&lt;/code&gt;, &lt;code&gt;s1_pc&lt;/code&gt;, &lt;code&gt;s2_pc&lt;/code&gt;, &lt;code&gt;s3_pc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Their usage can be seen in the following diagram:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;700px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Their relationship with the pipeline control signals is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;s0_pc&lt;/code&gt; is directly connected from the &lt;code&gt;in.s0_pc&lt;/code&gt; in the input interface.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;s0_fire&lt;/code&gt; is active, the next cycle &lt;code&gt;s1_pc&lt;/code&gt; will output the value of &lt;code&gt;s0_pc&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;s1_fire&lt;/code&gt; is active, the next cycle &lt;code&gt;s2_pc&lt;/code&gt; will output the value of &lt;code&gt;s1_pc&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;s2_fire&lt;/code&gt; is active, the next cycle &lt;code&gt;s3_pc&lt;/code&gt; will output the value of &lt;code&gt;s2_pc&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, the &lt;code&gt;fire&lt;/code&gt; signal affects whether the data of the next cycle is valid. For example, the &lt;code&gt;s0_fire&lt;/code&gt; signal affects whether the data of the s1 pipeline is valid, and the &lt;code&gt;s1_fire&lt;/code&gt; signal affects whether the data of the s2 pipeline is valid.&lt;/p&gt;
&lt;p&gt;Whether the &lt;code&gt;fire&lt;/code&gt; signal is valid depends on whether the data of this pipeline stage is valid and whether the next pipeline stage is ready. For example, the &lt;code&gt;s1_fire&lt;/code&gt; signal is valid only if the data of the s1 stage is valid and the &lt;code&gt;s2_ready&lt;/code&gt; signal from the sub-predictor output is valid. At this point, it can be considered that the data processing of the s1 stage is completed, the s2 stage is ready, and the data of the next cycle will be directly sent to the s2 stage.&lt;/p&gt;
&lt;p&gt;Therefore, in the sub-predictor, taking the s1 stage as an example, &lt;code&gt;s1_ready&lt;/code&gt; can block data from entering the s1 stage. When &lt;code&gt;s1_ready&lt;/code&gt; is active, the data for the s1 stage will be valid in the next cycle. &lt;strong&gt;When &lt;code&gt;s1_fire&lt;/code&gt; is active, it indicates that the data in the s1 stage is already valid and the predictor has generated the result for the s1 stage&lt;/strong&gt;. The data will then be directly sent to the s2 stage in the next cycle.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;redirect&lt;/code&gt; signal is relatively clear. For example, in the s2 stage, when &lt;code&gt;s2_redirect&lt;/code&gt; is valid, it indicates that when &lt;code&gt;s2_fire&lt;/code&gt; is valid, the s2 prediction result is different from the s1 prediction result in the previous cycle.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: FTB Branch Predictor</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/03_ftb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/03_ftb/</guid>
      <description>
        
        
        &lt;h2 id=&#34;introduction-to-ftb&#34;&gt;Introduction to FTB&lt;/h2&gt;
&lt;p&gt;FTB is the third sub-predictor of the Xiangshan BPU, and it can also get the outputs of uFTB and TAGE-SC together. In the input interface of FTB, the s1 channel contains the basic prediction results of uFTB, and the s2 and s3 channels are filled with only one group of signals, &lt;code&gt;br_taken_mask&lt;/code&gt;, by TAGE-SC, without the basic prediction results generated by the FTB entry. The function of FTB is to provide basic prediction results for the s2 and s3 channels.&lt;/p&gt;
&lt;p&gt;In terms of functionality and structure, FTB is similar to uFTB. The main difference is that FTB can accommodate more FTB entries, and the prediction results of FTB are output in the s2 and s3 channels. Due to its large capacity, the readout speed of FTB is slower than that of uFTB, and it cannot be placed in the first cycle to generate prediction results. However, the large capacity enables it to obtain more accurate prediction results.&lt;/p&gt;
&lt;h2 id=&#34;function-of-uftb&#34;&gt;Function of uFTB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cache more FTB entries and provide basic prediction results for the s2 and s3 channels&lt;/strong&gt;. The FTB predictor is essentially a storage with a large capacity. It reads the corresponding FTB entry based on the current predicted PC and outputs it in the s2 stage. At the same time, this FTB entry will be saved for one more cycle to generate the s3 stage prediction result. One thing to note is to consider the &lt;code&gt;br_taken_mask&lt;/code&gt; field inputted by the previous predictor to avoid losing it during generation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update FTB entries based on update requests.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ftb-storage-structure&#34;&gt;FTB Storage Structure&lt;/h2&gt;
&lt;p&gt;FTB entries in the FTB predictor are placed in a dedicated storage structure called &lt;code&gt;FTBBank&lt;/code&gt;. Before further examining the structure of &lt;code&gt;FTBBank&lt;/code&gt;, let&amp;rsquo;s first see how &lt;code&gt;FTBBank&lt;/code&gt; is used.&lt;/p&gt;
&lt;h3 id=&#34;ftb-read-request&#34;&gt;FTB Read Request&lt;/h3&gt;
&lt;p&gt;The read request interface of &lt;code&gt;FTBBank&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;req_pc&lt;/strong&gt; Requested PC
&lt;ul&gt;
&lt;li&gt;Interface type: Flipped(DecoupledIO(UInt(VAddrBits.W)))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read_resp&lt;/strong&gt; Read out FTB entry
&lt;ul&gt;
&lt;li&gt;Interface type: FTBEntry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read_hits&lt;/strong&gt; Which way (row) is hit
&lt;ul&gt;
&lt;li&gt;Interface type: Valid(UInt(log2Ceil(numWays).W))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Among, &lt;code&gt;req_pc&lt;/code&gt; interface is Decoupled, meaning it contains valid and ready signals. FTB needs to get the PC before the s1 stage starts, so &lt;code&gt;s0_pc&lt;/code&gt; is sent to the &lt;code&gt;req_pc&lt;/code&gt; interface, &lt;code&gt;s0_fire&lt;/code&gt; signal is connected to the valid signal of &lt;code&gt;req_pc&lt;/code&gt;, and the &lt;code&gt;ready&lt;/code&gt; signal is connected to the pipeline control signal &lt;code&gt;s1_ready&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;s0_fire&lt;/code&gt; enters the s1 stage, in the next cycle, when &lt;code&gt;s0_fire&lt;/code&gt; is at the same time as &lt;code&gt;s1_fire&lt;/code&gt;, FTBBank has already outputted the readout FTB entry to the &lt;code&gt;read_resp&lt;/code&gt; interface, and calculated &lt;code&gt;read_hits&lt;/code&gt;. However, at this time, because the readout has wasted too much delay, it cannot be outputted in the s1 stage. Therefore, this readout result is saved in an internal register. It will be read out from the register in the s2 and s3 stages to generate the prediction result.&lt;/p&gt;
&lt;h3 id=&#34;ftbbank&#34;&gt;FTBBank&lt;/h3&gt;
&lt;p&gt;FTBBank defines a storage to store all FTB entries. The storage adopts a group-associative structure, with 512 groups (Sets) in total, each group has 4 ways, and can store up to 2048 FTB entries. Besides storing FTB entries, it also stores the tag corresponding to each FTB entry for matching.&lt;/p&gt;
&lt;p&gt;Specifically, the tag is defined as &lt;code&gt;pc[29:10]&lt;/code&gt;, which takes 20 bits from the PC to identify the FTB entry. The PC is divided as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Plain&#34; data-lang=&#34;Plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  pc: | ... |&amp;lt;-- tag(20 bits) --&amp;gt;|&amp;lt;-- idx(9 bits) --&amp;gt;|&amp;lt;-- instOffset(1 bit) --&amp;gt;|
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When reading, provide the group number (idx) to the storage, read out all ways in that group, and then check if there is a way whose tag matches the current tag. If there is a match, it means a hit, and the readout FTB entry is sent out through the &lt;code&gt;read_resp&lt;/code&gt; interface, and the hit way number is sent out through the &lt;code&gt;read_hits&lt;/code&gt; interface.&lt;/p&gt;
&lt;h2 id=&#34;generation-of-prediction-results&#34;&gt;Generation of Prediction Results&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, for the FTB predictor, it needs to provide basic prediction results derived from FTB entries to the s2 and s3 channels. The FTB entries have been read and saved in the s1 stage. In the s2 and s3 stages, they only need to be read out to generate the prediction results. However, one thing to note is to preserve the &lt;code&gt;br_taken_mask&lt;/code&gt; field generated by TAGE-SC in the s2 and s3 prediction results, which provides precise prediction results for conditional branch instructions. For the s1 channel, the FTB predictor does not make any changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The generation of signals in the s2 and s3 prediction results can refer to the following list:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hit&lt;/strong&gt; Whether the FTB entry is hit
&lt;ul&gt;
&lt;li&gt;Generation method: The &lt;code&gt;read_hits&lt;/code&gt; signal valid bit from &lt;code&gt;FTBBank&lt;/code&gt; is valid.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;slot_valids&lt;/strong&gt; Slot valid bit, indicating whether each slot in the ftb entry is valid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;targets&lt;/strong&gt; Jump target address corresponding to each slot&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;offsets&lt;/strong&gt; Instruction offset relative to the start address of the predicted block in each slot&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jal&lt;/strong&gt; Whether the predicted block contains a jal instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_jalr&lt;/strong&gt; Whether the predicted block contains a jalr instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_call&lt;/strong&gt; Whether the predicted block contains a call instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;is_ret&lt;/strong&gt; Whether the predicted block contains a ret instruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;last_may_be_rvi_call&lt;/strong&gt; Signal indicating that the end of the predicted block may be an RVI type call instruction&lt;/li&gt;
&lt;li&gt;**is_br_sharing Whether the last slot (tailSlot) stores a conditional branch instruction signal
&lt;ul&gt;
&lt;li&gt;Generation method**: Export from the corresponding field in the FTB entry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughErr&lt;/strong&gt; Error in the &lt;code&gt;pftAddr&lt;/code&gt; recorded in the FTB entry
&lt;ul&gt;
&lt;li&gt;Generation method: Compare whether the address represented by &lt;code&gt;pftAddr&lt;/code&gt; is greater than the start address of the predicted block. If it is less than, it indicates an error, and this signal is set to valid. This situation may occur when the PC indexes an incorrect FTB entry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fallThroughAddr&lt;/strong&gt; End address of the predicted block
&lt;ul&gt;
&lt;li&gt;Generation method: If &lt;code&gt;fallThroughErr&lt;/code&gt; is invalid, it is generated according to &lt;code&gt;pftAddr&lt;/code&gt;. Otherwise, it is set to the start address + prediction width.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;br_taken_mask&lt;/strong&gt; Branch prediction result, each branch (slot) corresponds to a bit, indicating whether the branch is predicted as taken
&lt;ul&gt;
&lt;li&gt;Generation method: Generated based on the &lt;code&gt;always_taken&lt;/code&gt; field in the FTB entry and the indication result of the two-bit saturation counter.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;jalr_target&lt;/strong&gt; Jump target of jalr in this predicted block
&lt;ul&gt;
&lt;li&gt;Generation method: Jump target in the tailSlot of the FTB entry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ftb-meta&#34;&gt;FTB meta&lt;/h3&gt;
&lt;p&gt;In the third cycle of prediction, the FTB predictor outputs some auxiliary information of this prediction to &lt;code&gt;last_stage_meta&lt;/code&gt; and also sends the read FTB entry to the &lt;code&gt;last_stage_ftrb_entry&lt;/code&gt; interface.&lt;/p&gt;
&lt;p&gt;The FTB meta contains two pieces of information, &lt;code&gt;hit&lt;/code&gt; and &lt;code&gt;writeWay&lt;/code&gt;, indicating whether the prediction hits and in which way it is read. Subsequently, the update channel generates the update information for this prediction, and these two pieces of information are also sent to guide the writing of the updated FTB entry.&lt;/p&gt;
&lt;h2 id=&#34;ftb-update&#34;&gt;FTB Update&lt;/h2&gt;
&lt;p&gt;In the update channel, the pc and the new FTB entry are already specified for us, along with the &lt;code&gt;hit&lt;/code&gt; and &lt;code&gt;writeWay&lt;/code&gt; in the meta information. If &lt;code&gt;hit&lt;/code&gt; in the meta is valid, it means that the FTB entry corresponding to this pc was stored in the memory, and we only need to write it to the corresponding way.&lt;/p&gt;
&lt;p&gt;If it is invalid, it means that there was no storage before, but we do not know whether it is stored now. It is possible that before this update request, the FTB entry corresponding to this pc was written by another update request. Therefore, we still need to send a read request to FTBBank to check if there is a corresponding FTB entry. If it exists, it can be directly written to this position in the next cycle, otherwise, FTBBank will be notified to allocate a new position.&lt;/p&gt;
&lt;p&gt;Therefore, the number of cycles required for updating FTB entries depends on the hit situation.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first look at how FTBBank handles updates.&lt;/p&gt;
&lt;h3 id=&#34;ftbbank-update&#34;&gt;FTBBank Update&lt;/h3&gt;
&lt;p&gt;FTBBank&amp;rsquo;s update interface is divided into two parts, the update read interface and the update write interface.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;u_req_pc&lt;/strong&gt;: Update read request pc
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Flipped(DecoupledIO(UInt(VAddrBits.W)))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_hits&lt;/strong&gt;: Hit information read out
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Valid(UInt(log2Ceil(numWays).W))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_access&lt;/strong&gt;: There is an update request but the meta information indicates a miss
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_pc&lt;/strong&gt;: Update write request pc
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;UInt(VAddrBits.W))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_write_data&lt;/strong&gt;: Data to be written in the update request, write when valid
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Flipped(Valid(new FTBEntryWithTag))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_write_way&lt;/strong&gt;: Way index to write in the update request
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;UInt(log2Ceil(numWays).W))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update_write_alloc&lt;/strong&gt;: Whether a new FTB entry needs to be allocated (missed before)
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;For the update read interface&lt;/strong&gt;, FTBBank obtains the update read request through &lt;code&gt;u_req_pc&lt;/code&gt; signal. This request has a higher priority than the read request during prediction. In the next cycle, FTBBank will output the hit information through the &lt;code&gt;update_hits&lt;/code&gt; interface. &lt;code&gt;update_access&lt;/code&gt; is only used for some internal status judgments of FTBBank.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For the update write interface&lt;/strong&gt;, FTBBank obtains the pc of the update write request through the &lt;code&gt;update_pc&lt;/code&gt; signal, and when &lt;code&gt;update_write_data&lt;/code&gt; is valid, it writes the data into the corresponding position specified by &lt;code&gt;update_write_way&lt;/code&gt;. If &lt;code&gt;update_write_alloc&lt;/code&gt; is valid, it means that it cannot be directly written to the position specified in the request, but a new position needs to be allocated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The allocation strategy is as follows&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If all ways are filled, use the pseudo LRU replacement algorithm to select the way to replace&lt;/li&gt;
&lt;li&gt;If there is an empty way, select the empty way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;update-request-timing&#34;&gt;Update Request Timing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Meta hit is valid&lt;/strong&gt;: If hit in the update request meta is valid, then we only need to specify the address and data to be written according to the information in the update request, and the writing only takes one cycle.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meta hit is invalid&lt;/strong&gt;: In this case, after receiving the update request, we connect the pc in the request to the read port of FTBBank. The read port will return the result in the next cycle. Due to timing issues, we save this result and use it in the next cycle. Depending on the hit status in the result, we decide whether to set &lt;code&gt;update_write_alloc&lt;/code&gt; and send a write request. The entire update process takes three cycles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interface-list&#34;&gt;Interface List&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;信号类型&lt;/th&gt;
&lt;th&gt;信号位&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;信号描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;clock&lt;/td&gt;
&lt;td&gt;输入时钟&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;reset&lt;/td&gt;
&lt;td&gt;复位信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[35:0]&lt;/td&gt;
&lt;td&gt;io_reset_vector&lt;/td&gt;
&lt;td&gt;用于reset时，reset s1_pc_dup_0 提供的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_0&lt;/td&gt;
&lt;td&gt;输入位s0_pc 的 第0个复制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_1&lt;/td&gt;
&lt;td&gt;同上 第1个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_2&lt;/td&gt;
&lt;td&gt;同上 第2个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_3&lt;/td&gt;
&lt;td&gt;同上 第3个&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;预测结果输入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;s2 阶段输出的完整预测结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;s3 阶段输出的完整预测结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[222:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_meta&lt;/td&gt;
&lt;td&gt;最后一个阶段输出的 meta 信息&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_valid&lt;/td&gt;
&lt;td&gt;最后一个阶段输出的 FTB 项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[11:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[19:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_pftAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_carry&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isCall&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isRet&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isJalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_ctrl_btb_enable&lt;/td&gt;
&lt;td&gt;使能信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_0&lt;/td&gt;
&lt;td&gt;s0 阶段流水线控制信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_ready&lt;/td&gt;
&lt;td&gt;s1 阶段流水线控制信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_0&lt;/td&gt;
&lt;td&gt;s2 阶段流水线控制信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_valid&lt;/td&gt;
&lt;td&gt;更新有效性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_pc&lt;/td&gt;
&lt;td&gt;传回的预测块pc（用于指示更新的预测块）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_valid&lt;/td&gt;
&lt;td&gt;是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_offset&lt;/td&gt;
&lt;td&gt;solt 0 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[11:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_lower&lt;/td&gt;
&lt;td&gt;跳转目标地址的低位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_tarStat&lt;/td&gt;
&lt;td&gt;跳转后的 pc 高位是否进退位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_sharing&lt;/td&gt;
&lt;td&gt;无条件跳转指令槽中存储条件分支指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;solt 1 中分支指令相对于地址块起始pc的偏移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[19:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_lower&lt;/td&gt;
&lt;td&gt;跳转目标地址的低位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_tarStat&lt;/td&gt;
&lt;td&gt;跳转后的 pc 高位是否进退位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;无条件跳转指令槽中存储条件分支指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_pftAddr&lt;/td&gt;
&lt;td&gt;Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_carry&lt;/td&gt;
&lt;td&gt;pc+pft时是否产生进位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_isCall&lt;/td&gt;
&lt;td&gt;是否是函数调用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_isRet&lt;/td&gt;
&lt;td&gt;是否是函数返回&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_isJalr&lt;/td&gt;
&lt;td&gt;是否是 jalr 指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;最后一个指令槽存储的可能是 rvi 的 call 指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;是否预测为总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;是否预测为总是跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_old_entry&lt;/td&gt;
&lt;td&gt;是否是旧的 FTB 项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[222:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_meta&lt;/td&gt;
&lt;td&gt;meta 信息&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Introduction to the Timing of Xiangshan Branch Prediction Unit</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/03_xsbpu_timing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/basic/03_xsbpu_timing/</guid>
      <description>
        
        
        &lt;h2 id=&#34;single-cycle-prediction-without-bubble&#34;&gt;Single-Cycle Prediction without Bubble&lt;/h2&gt;

&lt;figure&gt;
    &lt;img src=&#34;1.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;&lt;code&gt;uFTB&lt;/code&gt; is the only predictor in Xiangshan BPU that can generate prediction results in a single cycle. The figure below shows the prediction process of &lt;code&gt;uFTB&lt;/code&gt;. The &lt;code&gt;s0_pc&lt;/code&gt; is sent from the top level of BPU, and when the s1 stage is active, the &lt;code&gt;s1_pc&lt;/code&gt; retains the value of &lt;code&gt;s0_pc&lt;/code&gt; from the previous cycle. This means that the value of &lt;code&gt;s0_pc&lt;/code&gt; will move down the pipeline.&lt;/p&gt;
&lt;p&gt;When the s1 stage is active, &lt;code&gt;uFTB&lt;/code&gt; receives the &lt;code&gt;s1_fire&lt;/code&gt; signal from the current cycle and generates a prediction result based on the &lt;code&gt;s1_pc&lt;/code&gt; address in this cycle, which can obtain the new PC value in the prediction result.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;2.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As shown in the figure, the top level of BPU analyzes the next PC value position based on the prediction result channel s1 and sends it to &lt;code&gt;npc_Gen&lt;/code&gt; (new PC generator) for generating the s0_pc of the next cycle.&lt;/p&gt;
&lt;p&gt;In the next cycle, &lt;code&gt;uFTB&lt;/code&gt; gets the new PC value and starts generating the prediction block for the new PC value. Therefore, with only the s1 stage, the prediction block can be generated at a rate of one block per cycle.&lt;/p&gt;
&lt;h2 id=&#34;prediction-result-redirection&#34;&gt;Prediction Result Redirection&lt;/h2&gt;
&lt;p&gt;However, except for &lt;code&gt;uFTB&lt;/code&gt;, other predictors require 2-3 cycles to generate prediction results. How to utilize their prediction results? And how to generate the prediction result redirection signal?&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;3.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As shown in the figure, a &lt;code&gt;Predirector 2&lt;/code&gt; that takes two cycles to generate a prediction result can output its prediction result to the s2 prediction result channel in the s2 stage. After the top level of BPU receives the prediction result, it analyzes the jump target address &lt;code&gt;target&lt;/code&gt; of the prediction block and connects it to &lt;code&gt;npc_Gen&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At this point, the signal connected to &lt;code&gt;npc_Gen&lt;/code&gt; contains both the old PC prediction result generated by s2 and the new PC prediction result generated by s1. How to choose which one to use for the new PC?&lt;/p&gt;
&lt;p&gt;As mentioned earlier, BPU compares the prediction result of s2 with the prediction result of s1 from the previous cycle. If the prediction results are different, it indicates that s1 has made a wrong prediction, and naturally, the prediction result of the current cycle generated based on the wrong prediction result of the previous cycle is also wrong. Therefore, if the prediction result is incorrect in the current cycle, &lt;code&gt;npc_Gen&lt;/code&gt; will use the &lt;code&gt;target&lt;/code&gt; provided by s2 as the new &lt;code&gt;s0_pc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This process is shown in the pipeline structure diagram as follows:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;4.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The Diff comparator compares the prediction results of the s1 stage with those of the previous cycle to generate a diff signal, guiding &lt;code&gt;npc_Gen&lt;/code&gt; to generate the next PC. At the same time, the diff signal indicates that the prediction result of the s1 stage is incorrect and can be used directly by BPU to redirect the prediction result channel of the s2 stage in the FTQ, instructing the FTQ to overwrite the previous prediction result.&lt;/p&gt;
&lt;p&gt;The diff signal is also sent to each predictor through the s2_redirect interface to guide the predictors to update their states.&lt;/p&gt;
&lt;p&gt;Furthermore, when the prediction result redirection of the s2 stage occurs, indicating that the prediction result of the s1 channel is incorrect, the s2 stage cannot continue to predict and needs to invalidate the &lt;code&gt;s2_fire&lt;/code&gt; signal of the predictor pipeline and wait for the corrected prediction result to flow in.&lt;/p&gt;
&lt;p&gt;The prediction result redirection of the s3 stage is similar to this. Its pipeline structure diagram is as follows. The specific processing process is left for you to analyze.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;5.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;redirection-requests-and-other-information-generation&#34;&gt;Redirection Requests and Other Information Generation&lt;/h2&gt;

&lt;figure&gt;
    &lt;img src=&#34;6.png&#34; width=&#34;500px&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Only when the prediction information of all three stages is incorrect will an external redirection request occur. At this time, &lt;code&gt;npc_Gen&lt;/code&gt; will receive the PC address from the redirection request. Since when a redirection request occurs, we assume that all three stages have predicted incorrectly, so all three stages&amp;rsquo; &lt;code&gt;fire&lt;/code&gt; signals need to be invalidated. Then, &lt;code&gt;npc_Gen&lt;/code&gt; uses the PC that needs to be restored from the redirection request to restart the prediction.&lt;/p&gt;
&lt;p&gt;Other information, such as the generation of the global history and the PC, follows the same principle and is maintained based on the prediction information of each stage. The global history generates a new branch history based on the prediction results of each stage.&lt;/p&gt;
&lt;h2 id=&#34;pipeline-control-signals&#34;&gt;Pipeline Control Signals&lt;/h2&gt;
&lt;p&gt;After learning about the specific process of the pipeline, you should understand the pipeline control signals in the predictor interface, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s0_fire, s1_fire, s2_fire, s3_fire&lt;/strong&gt; Indicate whether each stage of the pipeline is working.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s2_redirect, s3_redirect&lt;/strong&gt;           Indicate whether a prediction result redirection has occurred.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s1_ready, s2_ready, s3_ready&lt;/strong&gt;    Sent from the predictor to the top level of BPU, indicating whether each stage of the pipeline is ready.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By now, you should understand the basic design principles, external interaction logic, internal structure, timing, etc., of the Xiangshan Branch Prediction Unit, and have a rough understanding of the working principle of BPU. Xiangshan&amp;rsquo;s BPU is no longer mysterious to you.&lt;/p&gt;
&lt;p&gt;Next, you can read the &lt;code&gt;Important Structures and Interfaces Document&lt;/code&gt; and combine it with the source code of Xiangshan BPU to form a more detailed understanding of BPU. When you clearly understand the working principle and signal details of BPU, you can start your verification work!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: TAGE-SC Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/03_tagescfeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/03_tagescfeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;s2 TAGE outputs prediction results&lt;/li&gt;
&lt;li&gt;s3 SC outputs prediction results&lt;/li&gt;
&lt;li&gt;s2 TAGE outputs meta information&lt;/li&gt;
&lt;li&gt;s3 SC outputs meta information&lt;/li&gt;
&lt;li&gt;TAGE performs update training&lt;/li&gt;
&lt;li&gt;Check for new table entry requests&lt;/li&gt;
&lt;li&gt;Globally reset useful&lt;/li&gt;
&lt;li&gt;SC performs update training&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ITTAGE Branch Predictor</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/04_ittage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/04_ittage/</guid>
      <description>
        
        
        &lt;h2 id=&#34;function-introduction&#34;&gt;Function Introduction&lt;/h2&gt;
&lt;p&gt;For general conditional branch instructions, only predicting whether to jump (taken) or not (not taken) is needed. However, for &lt;strong&gt;indirect jumps&lt;/strong&gt;, such as call/jump instructions, it is necessary to predict &lt;strong&gt;where to jump to&lt;/strong&gt; (Target). In order to make TAGE support predicting jump addresses, ITTAGE (Indirect Target TAGE) was introduced.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;main difference&lt;/strong&gt; between ITTAGE and TAGE is that in the T0 and Tn tables, Target PC data is added. During prediction, ITTAGE selects the Target from the matched, longest history entry as the prediction result, and uses a 2-bit saturating counter to decide whether to output this result or choose an alternative prediction result. For TAGE predictor details, please refer to &lt;a href=&#34;../02_tage_sc/&#34;&gt;TAGE-SC Branch Predictor&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kunming-lake-ittage-branch-predictor&#34;&gt;Kunming Lake ITTAGE Branch Predictor&lt;/h2&gt;
&lt;p&gt;In the BPU design of Kunming Lake, prediction is performed in a cascaded manner with multiple predictors, so the implementation of the subpredictor differs from the original predictor, mainly in the default prediction result.&lt;/p&gt;
&lt;h3 id=&#34;basic-functionality&#34;&gt;Basic Functionality&lt;/h3&gt;
&lt;p&gt;ITTAGE&amp;rsquo;s basic functionality is similar to the TAGE branch predictor, but with the following differences:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Target is added as a jump target address item in the entry to predict the jump target address.&lt;/li&gt;
&lt;li&gt;The saturating counter ctr no longer provides the prediction direction, but instead decides whether to output the result (just the prediction information).&lt;/li&gt;
&lt;li&gt;Since there is only one indirect jump instruction in each branch prediction block, ITTAGE only considers one instruction.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;ITTAGE &lt;strong&gt;contains three pipeline stages&lt;/strong&gt;, the first stage calculates the index, and the second stage reads the result from the SRAM table using the index.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cycle 0, s0: Input of the first pipeline stage, generally pc and folded history.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Operation of the first pipeline stage&lt;/strong&gt;：Calculate the index. Output through registers to s1.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cycle 1, s1: Input of the second pipeline stage, the index and other data calculated in the first stage.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Operation of the second pipeline stage&lt;/strong&gt;：Access SRAM, read prediction information. Output through registers to s2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cycle 2, s2: Input of the third pipeline stage, the original prediction information read from SRAM in the second stage.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Operation of the third pipeline stage&lt;/strong&gt;：Process the original prediction information, decide whether to output the prediction result.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cycle 3, s3: Prediction result ready, the prediction result can now be used.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;data-structure&#34;&gt;Data Structure&lt;/h3&gt;
&lt;p&gt;In the Kunming Lake implementation, the table structure of T0 and Tn is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;预测器&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;表项构成&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;项数&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;基准预测器T0&lt;/td&gt;
&lt;td&gt;用于在其他预测器的预测结果都无效时输出预测结果&lt;/td&gt;
&lt;td&gt;虚表，不存在。 直接将上级预测器FTB 的预测结果作为表项结果&lt;/td&gt;
&lt;td&gt;虚表，不存在。 直接将上级预测器FTB结果作为索引到的结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;预测表T1-T2&lt;/td&gt;
&lt;td&gt;对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为 原始预测信息。历史记录长度由输入的H决定&lt;/td&gt;
&lt;td&gt;target：41 bitsvalid 1bittag 9bitsctr 2bitsus: 1bit（usefulness计数器）&lt;/td&gt;
&lt;td&gt;256项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;预测表T3-T5&lt;/td&gt;
&lt;td&gt;512项&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;t0tntable-retrieval-method&#34;&gt;T0，TnTable Retrieval Method&lt;/h3&gt;
&lt;p&gt;The retrieval method is consistent with the TAGE branch predictor, only differing in the configuration options of each table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;表名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH1长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;FH2长度&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;最近历史长度（用到GH中的位数）&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;T1&lt;/td&gt;
&lt;td&gt;4比特&lt;/td&gt;
&lt;td&gt;4比特&lt;/td&gt;
&lt;td&gt;4比特&lt;/td&gt;
&lt;td&gt;低4位，即把最新4位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T2&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;低8位，即把最新8位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T3&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;低13位，即把最新13位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;低16位，即把最新16位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T5&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;9比特&lt;/td&gt;
&lt;td&gt;8比特&lt;/td&gt;
&lt;td&gt;低32位，即把最新32位历史，折叠成FH、FH1、FH2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other processes (&lt;strong&gt;computation method&lt;/strong&gt; and &lt;strong&gt;computation formula&lt;/strong&gt;) are similar to the &lt;a href=&#34;../02_tage_sc/#MZk7dpG3woapSUx3XO9ceJShn0e&#34;&gt;TAGE-SC branch predictor&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alternate-predictor&#34;&gt;Alternate Predictor&lt;/h3&gt;
&lt;p&gt;When the prediction result given by the Tn table has insufficient &amp;ldquo;prediction confidence,&amp;rdquo; the prediction result needs to be jumped to become an &amp;ldquo;alternate predictor.&amp;rdquo; This process is similar to TAGE. For details, please refer to the corresponding part of TAGE. Unlike TAGE, ITTAGE&amp;rsquo;s ctr does not give the prediction direction but only determines whether to output the result (prediction confidence). When ctr is 2b00, it is considered weak confidence. Choose the alternate prediction result:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;If multiple tables are hit&lt;/strong&gt;, output the Target from the second-longest history table entry.&lt;/li&gt;
&lt;li&gt;Otherwise, output the T0 Target (FTB Target).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;prediction-process&#34;&gt;Prediction Process&lt;/h3&gt;
&lt;p&gt;The prediction process is similar to TAGE, but ITTAGE has an additional step to decide whether to output the prediction result based on ctr. The specific process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When the ctr of the ITTAGE table entry is not 2b00, output Target.&lt;/li&gt;
&lt;li&gt;When the ctr of the ITTAGE table entry is 2b00, output the alternate prediction result:
&lt;ol&gt;
&lt;li&gt;If there is a second-longest history (the second table is also hit), output the Target of the second-longest.&lt;/li&gt;
&lt;li&gt;Otherwise, output the FTB Target.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;When the ITTAGE table entry is not hit, output the T0 Target (FTB Target).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-process&#34;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;This process is similar to TAGE, with the following differences:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Table entry updates (original prediction data):
&lt;ol&gt;
&lt;li&gt;ctr:
&lt;ol&gt;
&lt;li&gt;If the predicted address matches the actual address, increment the ctr counter of the corresponding provider table entry by 1.&lt;/li&gt;
&lt;li&gt;If the predicted address does not match the actual address, decrement the ctr counter of the corresponding provider table entry by 1.&lt;/li&gt;
&lt;li&gt;In ITTAGE, it is determined based on ctr whether to adopt the jump target result of this prediction. If multiple tables are hit and the ctr of the longest history table is 0, adopt the alternate prediction logic (the second-longest history table or T0). Always update the longest history table during updates, and also update the alternate prediction table if the alternate prediction is adopted.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;target:
&lt;ol&gt;
&lt;li&gt;When the ctr of the table entry to be updated is 0 during this prediction, directly store the actual final jump result in the target, overwriting it.&lt;/li&gt;
&lt;li&gt;When applying for a new table entry, directly store the actual final jump result in the target.&lt;/li&gt;
&lt;li&gt;Otherwise, do not modify the target.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;usefulness:
&lt;ol&gt;
&lt;li&gt;When the provider&amp;rsquo;s prediction is correct but the alternate prediction is incorrect, set the provider&amp;rsquo;s usefulness to 1.&lt;/li&gt;
&lt;li&gt;If the alternate prediction has weak confidence and is correct, set the provider&amp;rsquo;s usefulness to 1. If the alternate prediction has weak confidence and is incorrect, set the provider&amp;rsquo;s usefulness to 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;New table entry:
&lt;ol&gt;
&lt;li&gt;Each time the prediction from the longest history table with confidence is incorrect (not due to using the alternate prediction), try to randomly apply for a table entry from a longer history table. The condition for application is that the usefulness of the corresponding entry is 0.&lt;/li&gt;
&lt;li&gt;If all longer entries are not 0, the allocation fails.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Reset useful bit:
&lt;ol&gt;
&lt;li&gt;Each time a prediction error occurs and a new table entry is applied for, if the allocation fails, increment tickCtr (an 8-bit saturated counter used to reset all usefulness). If successful, decrement tickCtr.&lt;/li&gt;
&lt;li&gt;When tickCtr reaches its maximum value, set all usefulness in ITTAGE to 0 and reset tickCtr to 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;interface-list&#34;&gt;Interface List&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;接口类型&lt;/th&gt;
&lt;th&gt;位宽&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;clock&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;reset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_s0_pc_3&lt;/td&gt;
&lt;td&gt;用于预测的PC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_14_folded_hist&lt;/td&gt;
&lt;td&gt;T2 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_13_folded_hist&lt;/td&gt;
&lt;td&gt;T3 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_12_folded_hist&lt;/td&gt;
&lt;td&gt;T1 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_10_folded_hist&lt;/td&gt;
&lt;td&gt;T5 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_6_folded_hist&lt;/td&gt;
&lt;td&gt;T4 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_4_folded_hist&lt;/td&gt;
&lt;td&gt;T3 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_3_folded_hist&lt;/td&gt;
&lt;td&gt;T5 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_folded_hist_3_hist_2_folded_hist&lt;/td&gt;
&lt;td&gt;T4 折叠历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[222:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_meta&lt;/td&gt;
&lt;td&gt;[100:0] 有效，是ITTAGE的Meta信息&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s0_fire_3&lt;/td&gt;
&lt;td&gt;s0阶段使能信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s1_fire_3&lt;/td&gt;
&lt;td&gt;s1阶段使能信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_0&lt;/td&gt;
&lt;td&gt;s2阶段使能信号，相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_s2_fire_3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_valid&lt;/td&gt;
&lt;td&gt;是否进行更新&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_pc&lt;/td&gt;
&lt;td&gt;待更新的预测块pc索引&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_14_folded_hist&lt;/td&gt;
&lt;td&gt;T2 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_13_folded_hist&lt;/td&gt;
&lt;td&gt;T3 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_12_folded_hist&lt;/td&gt;
&lt;td&gt;T1 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_10_folded_hist&lt;/td&gt;
&lt;td&gt;T5 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[8:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_6_folded_hist&lt;/td&gt;
&lt;td&gt;T4 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_4_folded_hist&lt;/td&gt;
&lt;td&gt;T3 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_3_folded_hist&lt;/td&gt;
&lt;td&gt;T5 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[7:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_spec_info_folded_hist_hist_2_folded_hist&lt;/td&gt;
&lt;td&gt;T4 更新时传入的历史&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;待更新的FTB项offset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;待更新的FTB项是否是有条件跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;待更新的tailSlot是否启用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_isRet&lt;/td&gt;
&lt;td&gt;tailSlot是否是Ret指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_ftb_entry_isJalr&lt;/td&gt;
&lt;td&gt;tailSlot是否是Jalr指令&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_cfi_idx_valid&lt;/td&gt;
&lt;td&gt;控制流指令在预测块中的索引.valid信号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_cfi_idx_bits&lt;/td&gt;
&lt;td&gt;控制流指令在预测块中的索引&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_jmp_taken&lt;/td&gt;
&lt;td&gt;预测块内无条件跳转指令被触发&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_update_bits_mispred_mask_2&lt;/td&gt;
&lt;td&gt;是否预测错误&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[222:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_meta&lt;/td&gt;
&lt;td&gt;预测时传出 meta 信息的[222:25] 即{25h0, _ubtb_io_out_last_stage_meta[5:0] ,_tage_io_out_last_stage_meta[87:0] ,_ftb_io_out_last_stage_meta[2:0], _ittage_io_out_last_stage_meta[100:0]}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_update_bits_full_target&lt;/td&gt;
&lt;td&gt;预测块的跳转目标（下一个预测块的起始地址）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;pass-through-signals-that-do-not-have-an-impact&#34;&gt;Pass-through signals that do not have an impact&lt;/h3&gt;
&lt;details&gt;
    &lt;summary&gt; These signals do not have an impact and are not important&lt;/summary&gt;
    &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;接口类型&lt;/th&gt;
&lt;th&gt;位宽&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;从FTB输入 完全透传到输出 包括jalr_target&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;RAS 模块使用的信息，透传&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;从FTB输入 完全透传到输出 包括jalr_target  fallThroughErr  表示 FTB项 中记录的 pftAddr 有误 生成方式：比较 pftAddr 代表的预测块结束地址是否大于预测块的起始地址，如果小于，则代表出现错误，此信号置为有效。这种情况可能会发生在 pc 索引到错误的 FTB 项的情况。 FTQ使用这个变量，与ITTAGE无关&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s2_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;除了 jalr_target 可能被修改，其他都是透传&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_s3_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_valid&lt;/td&gt;
&lt;td&gt;透传到output，不做修改 来源是FTB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[11:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[19:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_pftAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_carry&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_isCall&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_isRet&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_isJalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;完全透传传入值 prefix: io_in_bits_resp_in_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_jalr_target&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s2_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;见对应prefix的输入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_0_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;见对应prefix的输入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_1_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;见对应prefix的输入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_jalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_ret&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_2_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_0&lt;/td&gt;
&lt;td&gt;见对应prefix的输入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_br_taken_mask_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_slot_valids_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_slot_valids_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_targets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_targets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_offsets_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_offsets_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_fallThroughAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_fallThroughErr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_is_br_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_s3_full_pred_3_hit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_valid&lt;/td&gt;
&lt;td&gt;完全透传传入的值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[11:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_brSlots_0_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_offset&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[19:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_lower&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[1:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_tarStat&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_sharing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_tailSlot_valid&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;[3:0]&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_pftAddr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_carry&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isCall&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isRet&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_isJalr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_last_may_be_rvi_call&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_always_taken_0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;io_out_last_stage_ftb_entry_always_taken_1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/details&gt;
&lt;h2 id=&#34;other-meta-information-can-be-found-in-the-corresponding-sub-predictor-documentation&#34;&gt;Other Meta information can be found in the corresponding sub-predictor documentation&lt;/h2&gt;
&lt;p&gt;_ubtb_io_out_last_stage_meta&lt;/p&gt;
&lt;p&gt;_tage_io_out_last_stage_meta&lt;/p&gt;
&lt;p&gt;_ftb_io_out_last_stage_meta&lt;/p&gt;
&lt;h2 id=&#34;ittage_io_out_last_stage_meta1000&#34;&gt;ittage_io_out_last_stage_meta[100:0]&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;位宽&lt;/th&gt;
&lt;th&gt;信号名&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;s3_provided&lt;/td&gt;
&lt;td&gt;是否有结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[99:97]&lt;/td&gt;
&lt;td&gt;s3_provider&lt;/td&gt;
&lt;td&gt;提供结果的表项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;s3_altProvided&lt;/td&gt;
&lt;td&gt;是否有替代预测表项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[95:93]&lt;/td&gt;
&lt;td&gt;s3_altProvider&lt;/td&gt;
&lt;td&gt;提供结果的替代预测表项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td&gt;resp_meta_altDiffers&lt;/td&gt;
&lt;td&gt;替代预测是否是弱信心的（FTB不算）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;91&lt;/td&gt;
&lt;td&gt;s3_providerU&lt;/td&gt;
&lt;td&gt;主预测的useful bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[90:89]&lt;/td&gt;
&lt;td&gt;s3_providerCtr&lt;/td&gt;
&lt;td&gt;主预测给出的置信度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[88:87]&lt;/td&gt;
&lt;td&gt;s3_altProviderCtr&lt;/td&gt;
&lt;td&gt;替代预测给出的置信度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;td&gt;resp_meta_allocate_valid_r&lt;/td&gt;
&lt;td&gt;有空余的表项可供申请&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[85:83]&lt;/td&gt;
&lt;td&gt;resp_meta_allocate_bits_r&lt;/td&gt;
&lt;td&gt;申请哪个表中的表项&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;82&lt;/td&gt;
&lt;td&gt;s3_tageTaken_dup_3&lt;/td&gt;
&lt;td&gt;在不使用FTB的情况下始为true，使用FTB也为true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[81:41]&lt;/td&gt;
&lt;td&gt;s3_providerTarget&lt;/td&gt;
&lt;td&gt;主预测给出的跳转地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[40:0]&lt;/td&gt;
&lt;td&gt;s3_altProviderTarget&lt;/td&gt;
&lt;td&gt;替代预测给出的跳转地址&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ITTAGE Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/04_ittagefeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/04_ittagefeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;s2 ITTAGE determines whether to generate a prediction result&lt;/li&gt;
&lt;li&gt;s3 ITTAGE reads the predicted target for a branch&lt;/li&gt;
&lt;li&gt;s3 ITTAGE outputs meta information&lt;/li&gt;
&lt;li&gt;ITTAGE performs update training&lt;/li&gt;
&lt;li&gt;Check for new table entry requests&lt;/li&gt;
&lt;li&gt;Globally reset useful&lt;/li&gt;
&lt;li&gt;Prediction direction&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: RAS Branch Predictor</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/05_ras/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/modules/05_ras/</guid>
      <description>
        
        
        &lt;h2 id=&#34;ras介绍&#34;&gt;RAS介绍&lt;/h2&gt;
&lt;p&gt;RAS stands for &amp;ldquo;Return Address Stack.&amp;rdquo; It helps determine branch behavior in programs by tracking return addresses. As previously mentioned, there are many branches in a program: if/else, switch/case, while/for loop, iteration, call/return, etc. The RAS branch predictor specifically targets call/return types.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;function&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;_add&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#a40000&#34;&gt;?&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt;  &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#a40000&#34;&gt;?&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;function&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;add&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;function&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;sub&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000&#34;&gt;function&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;main&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;c&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;add&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#000&#34;&gt;d&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;sub&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As shown above, the main function calls add and sub, and add calls the function _add. In this process, each call&amp;rsquo;s jump address and return address are fixed, and the return address can be obtained at the time of the call. The function call process is a &amp;ldquo;stack push and pop&amp;rdquo; process, so branch prediction can be performed using a &amp;ldquo;stack&amp;rdquo; structure: each time a call instruction is encountered, the current PC+4 (compressed instructions and regular instructions have different offsets) is pushed onto the stack; when a return instruction is encountered, a pop operation is performed, and the address obtained is the target jump address. In the block-based BPU, RAS cannot know whether the current block is a call or ret, so it relies on other predictors, using the results of previous predictors for RAS operations.&lt;/p&gt;
&lt;p&gt;Specifically, in Xiangshan&amp;rsquo;s RAS predictor, at the s2 stage, it needs to determine whether the previous stage&amp;rsquo;s s2 output predicts a call or ret (i.e., the input signal io.s2_full_pred.hit_taken_on_call/ret is valid). If it&amp;rsquo;s a call, it pushes the subsequent instruction address onto the stack; if it&amp;rsquo;s a ret, it pops the address from the stack as the prediction result. Because in the BPU predictor, the result obtained at the s3 stage is assumed to be better than the s2 stage, the RAS predictor needs to check at the s3 stage. If the previous stage&amp;rsquo;s s3 prediction result is inconsistent with s2, the s3 result is taken, and it needs to determine whether to cancel or complete the stack operations of the previous s2 stage as needed. For example, if the s2 stage predicted a call instruction and performed a push operation, but s3 predicted a regular branch instruction with no need for any operation, the push must be canceled; if s2 predicted a regular branch instruction and s3 predicted a call, a push operation must be performed to complete.&lt;/p&gt;
&lt;h2 id=&#34;ras-stack-operations&#34;&gt;RAS Stack Operations&lt;/h2&gt;
&lt;p&gt;In RAS design, function return addresses are predicted using a stack. Ideally, this section assumes that RAS can be backed up at any time, with the stack top pointer represented by sp and the predicted address represented by paddr. The basic RAS operations are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;h3 id=&#34;push&#34;&gt;PUSH&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since predictions can be wrong, the current stack state needs to be backed up (often referred to as a &amp;ldquo;snapshot&amp;rdquo; in software; this term is also used in subsequent content). When encountering a call instruction, get the return address of the call instruction addr = current pc + 4 (if it&amp;rsquo;s a compressed instruction, addr = pc+2), then push onto the stack: sp = addr; sp += 1.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;h3 id=&#34;pop&#34;&gt;POP&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the same reason, take a snapshot of the current stack, marked as s. When encountering a ret instruction, the predicted jump address is paddr = sp, then pop: sp = sp - 1. Take a snapshot of the current stack, marked as s.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;h3 id=&#34;redirect-operation&#34;&gt;Redirect Operation&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the BPU predicts program branches, there are &amp;ldquo;correct predictions&amp;rdquo; and &amp;ldquo;wrong predictions.&amp;rdquo; When the CPU backend detects a branch prediction error, it performs a redirect operation, informing the BPU where the prediction was wrong and what the correct result is. During redirection, the RAS module receives the correct branch and the RAS stack information at the time of prediction. Depending on the type of correct branch instruction, the following snapshot recovery situations arise:&lt;/p&gt;
&lt;p&gt;(1) The previously predicted instruction is actually a call instruction, and the push operation is executed based on the addr address provided in the redirect.
(2) The previously predicted instruction is actually a ret instruction, and the pop operation is executed.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;h3 id=&#34;commit-operation&#34;&gt;Commit Operation&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The commit operation is when the backend informs the frontend that the previous prediction was correct. Ideally, the RAS predictor doesn&amp;rsquo;t need to perform any operations at this time.&lt;/p&gt;
&lt;h2 id=&#34;implementation-of-ras-in-kunming-lake&#34;&gt;Implementation of RAS in Kunming Lake&lt;/h2&gt;
&lt;p&gt;In actual circuit design, an infinitely large stack is impossible, and constant backups are not feasible. Therefore, in Kunming Lake&amp;rsquo;s RAS implementation, the problems and solutions are as follows:&lt;/p&gt;
&lt;h3 id=&#34;how-to-obtain-ras-stack-snapshots-for-each-prediction&#34;&gt;&lt;strong&gt;How to obtain RAS stack snapshots for each prediction?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To achieve the function of taking snapshots of the RAS stack, Kunming Lake adopts a &lt;strong&gt;linked representation&lt;/strong&gt; based on a circular array. The design is as follows:&lt;/p&gt;
&lt;image src=&#34;Linked_RAS.png&#34; width=&#34;800px&#34;&gt;
&lt;p&gt;As shown above, a circular array is used for data management. The circular array has a starting address marked as BOS and a tail pointer marked as TOSW. The data between them are valid, and the data outside are free. Within the valid data, a linked structure represents the &amp;ldquo;RAS stack,&amp;rdquo; where each stack element records the number of its previous data. When performing stack operations, the corresponding previous element can be obtained through this number. The RAS stack&amp;rsquo;s bottom pointer shares the BOS. In the initial state S in the figure, the RAS stack elements are 0, 1, 3, 5. Element 5 records the position of its previous element 3, and element 3 records the position of its previous element 1. When a push operation is needed, the RAS stack top pointer TOSR = TOSW, the new element is stored at the new TOSR position 7, and the position of its previous element 5 is recorded in the new element, then TOSW is moved back (TOSW = TOSW+1). When a pop operation is performed, the RAS stack top pointer TOSR moves to the previous element&amp;rsquo;s position 3 based on the index saved in the stack top element. Therefore, under the condition that the stack does not overflow, the above RAS stack always allocates new data on the array through TOSW during normal Push/Pop operations, so all process states and intermediate data are saved. So, to restore to the state S, it only needs to reset the corresponding stack pointers. Therefore, in each prediction, the corresponding stack pointers (BOS, TOSR, TOSW) also need to be saved in the prediction result for later restoration in case of redirection. The advantage of this structure is that it can save complete process data, but frequent push operations can lead to large space resource consumption.&lt;/p&gt;
&lt;h3 id=&#34;chain-ra-storage-space-waste&#34;&gt;&lt;strong&gt;Chain RA storage space waste?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since the prediction result is correct after commit, the stack will not roll back. When the RAS predictor receives the commit message of the prediction block &amp;ldquo;P,&amp;rdquo; it will no longer receive the redirect message of block P, so the snapshot taken during the push operation for block P will not be used again. Therefore, the RAS stack elements can be categorized, with uncommitted elements stored in a &amp;ldquo;linked&amp;rdquo; structure and committed elements stored in a regular stack structure (the original RAS stack is split into two parts: the uncommitted part stored in a snapshot-saving linked structure and the committed part stored in a regular stack structure). The optimized RAS structure is shown below:&lt;/p&gt;
&lt;image src=&#34;RAS_Arch.png&#34; width=&#34;800px&#34;&gt;
&lt;p&gt;As shown above, based on the normal call/ret predictions and commit call/ret, the original RAS stack can be split into two independent stacks, called the &amp;ldquo;speculative stack&amp;rdquo; (spec_stack, linked structure) and the &amp;ldquo;commit stack&amp;rdquo; (commit_stack, regular structure). Due to the change in stack structure, the specific Pop/Push operations are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Encounter normal call and ret:
(1) The call predicted by the prediction block is correct, and a push operation is performed on the spec_stack, specifically the linked stack Push process mentioned above.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) The ret predicted by the prediction block is correct, and the stack top of the spec_stack is used as the prediction value, then a pop operation is performed. If the spec_stack is empty, the stack top element of the commit_stack is used as the prediction result (no pop operation is performed).&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Commit operation:
(1) The FTQ execution result is call correct, and a regular push operation is performed on the commit_stack.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) The FTQ execution result is ret correct, and a regular pop operation is performed on the commit_stack.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Redirect operation:
(1) Obtain the stack pointers (BOS, TOSR, TOSW, ssp) from the redirect message during the previous prediction and cover the current pointer values to complete the speculative stack rollback.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) This operation does not affect the commit stack.&lt;/p&gt;
&lt;h3 id=&#34;how-to-handle-when-the-s3-prediction-result-is-inconsistent-with-s2-at-the-input-end&#34;&gt;&lt;strong&gt;How to handle when the S3 prediction result is inconsistent with S2 at the input end?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since the S3 result is assumed to be better than S2, the RAS stack needs to be repaired again in case of inconsistency. The specific inconsistency and corresponding repair operations are shown in the table below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;S2 Pred. Res.&lt;/th&gt;
&lt;th&gt;S3 Pred. Res.&lt;/th&gt;
&lt;th&gt;Repair Operation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;push&lt;/td&gt;
&lt;td&gt;keep&lt;/td&gt;
&lt;td&gt;pop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;keep&lt;/td&gt;
&lt;td&gt;pop&lt;/td&gt;
&lt;td&gt;pop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pop&lt;/td&gt;
&lt;td&gt;keep&lt;/td&gt;
&lt;td&gt;push&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;keep&lt;/td&gt;
&lt;td&gt;push&lt;/td&gt;
&lt;td&gt;push&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;S2 and S3 operations do not exist, pop/push or push/pop scenarios (Why not exist?)&lt;/p&gt;
&lt;h2 id=&#34;other-optimizations&#34;&gt;Other Optimizations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Each element in the RAS stack has a counter, which is used to save repeated values (for recursive calls). For example, when the address pushed for the first time is 0xff00, and the address pushed for the second time is also 0xff00, only the counter of the top element of the stack needs to be incremented, and there is no need to actually push the address onto the stack.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;interface-description&#34;&gt;Interface Description&lt;/h2&gt;
&lt;p&gt;In the RAS predictor, the core component is the &lt;strong&gt;RASStack&lt;/strong&gt;, with the following interface:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;接口名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;功能描述&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;接口名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;功能描述&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;in.spec_push_valid&lt;/td&gt;
&lt;td&gt;预测有Call指令，Spec压栈&lt;/td&gt;
&lt;td&gt;in.s2_fire&lt;/td&gt;
&lt;td&gt;s2信号有效&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.spec_pop_valid&lt;/td&gt;
&lt;td&gt;预测有Ret指令，Spec出栈&lt;/td&gt;
&lt;td&gt;in.s3_fire&lt;/td&gt;
&lt;td&gt;s3信号有效&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.spec_push_addr&lt;/td&gt;
&lt;td&gt;Ret地址&lt;/td&gt;
&lt;td&gt;in.s3_cancel&lt;/td&gt;
&lt;td&gt;s3和s2的预测结果不一样，需要撤销s2的操作&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.spec_pop_addr&lt;/td&gt;
&lt;td&gt;RAS的栈顶数据&lt;/td&gt;
&lt;td&gt;in.s3_meta&lt;/td&gt;
&lt;td&gt;s3需要的s2时的现场信息&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.ssp&lt;/td&gt;
&lt;td&gt;commit栈顶指针&lt;/td&gt;
&lt;td&gt;in.s3_missed_pop&lt;/td&gt;
&lt;td&gt;s3判断需要再次进行pop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.sctr&lt;/td&gt;
&lt;td&gt;commit栈顶重复元素计数器&lt;/td&gt;
&lt;td&gt;in.s3_missed_push&lt;/td&gt;
&lt;td&gt;s3判断需要再次进行push&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.nsp&lt;/td&gt;
&lt;td&gt;commit栈顶，会被ssp覆盖&lt;/td&gt;
&lt;td&gt;in.s3_pushAddr&lt;/td&gt;
&lt;td&gt;需要再次push时的地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.TOSR&lt;/td&gt;
&lt;td&gt;spec栈栈顶指针&lt;/td&gt;
&lt;td&gt;in.redirect_valid&lt;/td&gt;
&lt;td&gt;需要重定向&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.TOSW&lt;/td&gt;
&lt;td&gt;spec栈数据分配指针&lt;/td&gt;
&lt;td&gt;in.redirect_isCall&lt;/td&gt;
&lt;td&gt;真实执行情况是Call&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;out.BOS&lt;/td&gt;
&lt;td&gt;spec栈栈低指针&lt;/td&gt;
&lt;td&gt;in.redirect_isRet&lt;/td&gt;
&lt;td&gt;真实执行情况是Return&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_push_valid&lt;/td&gt;
&lt;td&gt;push操作正确&lt;/td&gt;
&lt;td&gt;in.redirect_meta_ssp&lt;/td&gt;
&lt;td&gt;之前预测时的现场信息ssp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_pop_valid&lt;/td&gt;
&lt;td&gt;FTQ执行结果为Call正确&lt;/td&gt;
&lt;td&gt;in.redirect_meta_sctr&lt;/td&gt;
&lt;td&gt;之前预测时的现场信息sctr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_push_addr&lt;/td&gt;
&lt;td&gt;更新信息中的Ret地址&lt;/td&gt;
&lt;td&gt;in.redirect_meta_TOSW&lt;/td&gt;
&lt;td&gt;之前预测时的现场信息TOSW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_meta_TOSW&lt;/td&gt;
&lt;td&gt;更新信息中的TOSW&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.redirect_meta_TOSR&lt;/td&gt;
&lt;td&gt;之前预测时的现场信息TOSR&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_meta_TOSR&lt;/td&gt;
&lt;td&gt;更新信息中的TOSR&lt;/td&gt;
&lt;td&gt;in.redirect_meta_NOS&lt;/td&gt;
&lt;td&gt;之前预测时的现场信息NOS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_meta_ssp&lt;/td&gt;
&lt;td&gt;更新信息中的现场信息SSP&lt;/td&gt;
&lt;td&gt;in.redirect_callAddr&lt;/td&gt;
&lt;td&gt;重定向地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;in.commit_meta_sctr&lt;/td&gt;
&lt;td&gt;更新信息中的现场信息SCTR&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The relationship between the RASStack module and the BasePredictor interface is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;stack接口&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;转换过程&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;描述&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;s.spec_push_valid&lt;/td&gt;
&lt;td&gt;io.s2_fire(2) &amp;amp;&amp;amp; s2_full_pred.hit_taken_on_call &amp;amp;&amp;amp; !io.s3_redirect(2)&lt;/td&gt;
&lt;td&gt;s2输入有效，且上级预测为call跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.spec_pop_valid&lt;/td&gt;
&lt;td&gt;io.s2_fire(2) &amp;amp;&amp;amp; s2_full_pred.hit_taken_on_ret  &amp;amp;&amp;amp; !io.s3_redirect(2)&lt;/td&gt;
&lt;td&gt;s2输入有效，且上级预测为ret跳转&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.spec_push_addr&lt;/td&gt;
&lt;td&gt;s2_full_pred.fallThroughAddr + Mux(s2_full_pred.last_may_be_rvi_call, 2.U, 0.U)&lt;/td&gt;
&lt;td&gt;上级预测器s2预测的fallThroughAddr（即PC+2），判断是否压缩指令是否需要 +2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.redirect_isCall&lt;/td&gt;
&lt;td&gt;redirect.bits.level === 0.U &amp;amp;&amp;amp; recover_cfi.pd.isCall&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.redirect_isRet&lt;/td&gt;
&lt;td&gt;redirect.bits.level === 0.U &amp;amp;&amp;amp; recover_cfi.pd.isRet&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.redirect_meta_*&lt;/td&gt;
&lt;td&gt;redirect.bits.cfiUpdate.*&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.commit_push_valid&lt;/td&gt;
&lt;td&gt;io.update.is_call_taken&lt;/td&gt;
&lt;td&gt;call指令预测正确&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.commit_push_valid&lt;/td&gt;
&lt;td&gt;io.update.is_ret_taken&lt;/td&gt;
&lt;td&gt;ret指令预测正确&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.commit_push_addr&lt;/td&gt;
&lt;td&gt;update.ftb_entry.getFallThrough(update.pc) + Mux(update.ftb_entry.last_may_be_rvi_call, 2.U, 0.U)&lt;/td&gt;
&lt;td&gt;根据是否为压缩指令，进行地址+2或者+0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;s.commit_meta_*&lt;/td&gt;
&lt;td&gt;io.update.bits.meta.asTypeOf(new RASMeta)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;io.out.last_stage_spec_info.*&lt;/td&gt;
&lt;td&gt;s3_meta.*&lt;/td&gt;
&lt;td&gt;s3_meta = RegEnable(s2_meta, io.s2_fire(2))由s2_meta延迟一怕得到&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;io.out.last_stage_meta&lt;/td&gt;
&lt;td&gt;s3_meta&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;io.out.s2.full_pred.*.jalr_target&lt;/td&gt;
&lt;td&gt;:=stack.spec_pop_addr&lt;/td&gt;
&lt;td&gt;预测地址（栈顶地址，只预测ret）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;io.out.s3.full_pred.*.jalr_target&lt;/td&gt;
&lt;td&gt;:=RegEnable(stack.spec_pop_addr, io.s2_fire(2))&lt;/td&gt;
&lt;td&gt;由s2延迟一拍得到&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;io.out.s2/3.full_pred.targets.last&lt;/td&gt;
&lt;td&gt;:=Mux(s2/3_is_jalr, s2/3_jalr_target, s2/3_last_target_in)&lt;/td&gt;
&lt;td&gt;如果时call执行，更新targets.last的结果&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timing-description&#34;&gt;Timing Description&lt;/h2&gt;
&lt;p&gt;In RAS, there are only 2 stages, S2 and S3.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Main tasks in S2&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Based on the S2 prediction result of the previous predictor, complete the prediction through the push/pop process and obtain the result spec_pop_addr.&lt;/li&gt;
&lt;li&gt;Perform update operations based on commit signal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Main tasks in S3&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on the prediction result in the previous predictor S3 and the operations in S2, determine whether to undo the Pop/Push operation. The predictor assumes that the S3 prediction result is better than S2. If S2 and S3 predictions are inconsistent, the RAS predictor accepts the S3 result for stack operation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The prediction process in S3 is the same as in S2, but the data is different.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perform redirection operations (redirection information obtained from the previous cycle).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the RASStack appears to complete its tasks within a cycle, data bypassing is needed inside the stack to cache data for processing.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: RAS Feature List</title>
      <link>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/05_rasfeature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xs-mlvp.github.io/xs-bpu/en/docs/feature/05_rasfeature/</guid>
      <description>
        
        
        &lt;h2 id=&#34;feature-list&#34;&gt;Feature List&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Supports enabling and disabling of the predictor&lt;/li&gt;
&lt;li&gt;Supports s3 prediction result overriding s2 prediction result&lt;/li&gt;
&lt;li&gt;Supports push and pop operations of the RAS stack&lt;/li&gt;
&lt;li&gt;Supports redirect operation of the RAS stack&lt;/li&gt;
&lt;li&gt;Supports update operation of the RAS stack&lt;/li&gt;
&lt;li&gt;Supports base predictor interface&lt;/li&gt;
&lt;li&gt;Conforms to the standard RAS predictor prediction process&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
  </channel>
</rss>
