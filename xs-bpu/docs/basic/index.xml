<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>香山分支预测单元（BPU）基础设计 on 香山微架构开放验证第一期：昆明湖BPU模块UT验证实战</title>
    <link>https://open-verify.cc/xs-bpu/docs/basic/</link>
    <description>Recent content in 香山分支预测单元（BPU）基础设计 on 香山微架构开放验证第一期：昆明湖BPU模块UT验证实战</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <atom:link href="https://open-verify.cc/xs-bpu/docs/basic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>何为分支预测</title>
      <link>https://open-verify.cc/xs-bpu/docs/basic/00_bp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-verify.cc/xs-bpu/docs/basic/00_bp/</guid>
      <description>为何需要分支预测？ 分支预测主要有两方面的原因：一是程序的执行流中含有分支指令，二是高性能处理器使用流水线设计。&#xA;程序的执行流中含有分支指令 int x = 10; int y = 20; int result = 0; if (x &amp;gt;= y) { result = x + y; } else { result = x - y; } 上述是一段C语言代码，这段代码首先定义了三个变量 x, y 和 result，然后根据 x 和 y 值的大小情况对result进行赋值。可以发现，程序在前三行对变量进行赋值时是顺序往下执行的，而在第 5 行时，由于 if 指令的出现，程序产生了分支，从第 5 行直接跳转到了第 8 行继续运行，这就造成了程序执行的分支。&#xA;翻译成 RISC-V 汇编之后的代码如下：&#xA;li a0, 10 # x = 10 li a1, 20 # y = 20 li a2, 0 # result = 0 blt a0, a1, else_branch # 如果 x &amp;lt; y，则跳转到 else_branch add a2, a0, a1 # 否则执行 result = x + y j end # 跳转到 end else_branch: sub a2, a0, a1 # result = x - y end: 可以发现程序依然保持着先前的分支行为，在代码的前三行，指令顺序执行，之后，在程序的第 5 行，出现了一条特殊指令blt ，我们称之为分支指令，它会根据 x 和 y 的大小关系决定指令流顺序往下执行还是跳转到其他地方，该指令的出现导致程序的执行出现了分支。</description>
    </item>
    <item>
      <title>香山分支预测单元基础</title>
      <link>https://open-verify.cc/xs-bpu/docs/basic/01_xsbpu_basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-verify.cc/xs-bpu/docs/basic/01_xsbpu_basic/</guid>
      <description>分支预测块思想 对于一般的分支预测器来说，通常会根据给定的 PC 预测出该 PC 所对应的指令的相关信息，如是否是条件分支指令、是否是跳转指令。对于条件分支指令，会预测出是否会跳转，而对于跳转指令，则会预测出跳转目标。然而，这种一条一条地预测指令的方法效率较低，导致前端指令供给过慢。&#xA;相比之下，香山采用的预测方法是每次预测一个指令块。也就是说，给定一个 PC，香山会预测出以这个PC开始的一个分支预测块，包括了接下来若干条指令的情况，如是否存在分支指令、分支指令的位置、是否跳转以及跳转目标等信息。&#xA;这样的预测方法可以一次性地预测出多条指令，并将预测结果送往取指单元（IFU），指导IFU进行取指。此外，由于IFU需要考虑缓存行的性能问题，它可以根据预测块一次性地取出多条指令，从而提高吞吐效率。&#xA;在预测块产生后，分支预测块还会生成执行完本预测块后跳转到的 PC，接着 BPU 会根据该 PC 继续产生下一个分支预测块。&#xA;举个简单的例子&#xA;如上图所示，当 PC 执行到 0x20000118 时，BPU会经历如下几个步骤：&#xA;BPU得知PC 0x20000118 BPU产生以 0x20000118 为开始的分支预测块，内容大致如下 在接下来的若干条指令中 第三条是一个条件分支指令 对于这个条件分支指令，预测他将会跳转 跳转后的地址为 0x20000110 BPU将PC设置为 0x20000110，并继续产生下一个分支预测块 这便是采用了分支预测块的香山 BPU 的基本预测流程&#xA;多预测器、多流水线结构 上图展示了香山BPU的总体架构，其中我们需要关注两个主要方面：&#xA;多预测器 为了确保预测的准确性，香山 BPU 采用了多个预测器，并且这些预测器共同产生 BPU 的预测结果。例如，FTB 会生成基础的预测结果供后续预测器使用，而 TAGE 则对条件分支指令产生更精准的预测结果等。 多流水线 为了满足高性能的需求，香山 BPU 采用了流水线设计。各个预测器处于不同的流水线级别。其中，uFTB（即图中的uBTB）预测器位于第一流水线级别，能够在一个周期内产生预测结果。而其余预测器则需要2-3个周期才能生成预测结果，尽管预测时间较长，但预测结果相对更精确。 但是，如果在第三个周期才能获取到预测结果，并基于新的结果重新开始预测，这样的设计难免导致性能损失。因为这样一来，需要耗费三个时钟周期才可以完成一次预测。&#xA;为了在第一和第二周期就能够获取到某些预测器的预测结果，我们设置了三个预测结果通道，并将三个流水线级别的预测结果同时输出，如下图所示。&#xA;取指目标队列（FTQ） 分支预测结果暂存 尽管 BPU 可以以分支预测块的形式提供预测结果，IFU 也可以一次性取指多条指令，但它们之间仍然存在性能上的差距。通常情况下，BPU产生预测结果的速度更快。&#xA;因此，在 BPU 与 IFU 之间添加了一个取指目标队列（FTQ）作为缓冲。FTQ 本质上是一个队列，用于存储一个个数据项。BPU产生的预测结果会先暂存到FTQ中，然后由 IFU 从 FTQ 中获取预测结果，如下图所示。&#xA;每当 BPU 产生一个预测块，该预测块会被放入 FTQ 的队首。当 IFU 处于空闲状态时，它会从 FTQ 队尾获取下一个预测块。下方的示意图展示了这一过程。</description>
    </item>
    <item>
      <title>香山分支预测单元结构介绍</title>
      <link>https://open-verify.cc/xs-bpu/docs/basic/02_xsbpu_structure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-verify.cc/xs-bpu/docs/basic/02_xsbpu_structure/</guid>
      <description>BPU 是如何整合内部子预测器的？ 我们已经知道香山 BPU 采用的是多预测器，多流水线方案。其中为了适配多流水线，BPU 采用了三通道结果输出接口。但是又是如何适配多预测器的呢？这就要求我们进一步深入 BPU，探究其内部结构。&#xA;上图为香山文档中的 BPU 架构图，目前我们只需要关心这样一个信息，内部的所有子预测器都被包在了一个叫做 Composer 的结构中。BPU只需要和 Composer 交互。&#xA;Composer 是什么？我们不妨先看一下香山代码中对于他们的定义。&#xA;可以发现，Composer 以及五个子预测器有一个共同的特点，他们全部继承于 BasePredictor 基类。并且接口已经在 BasePredictor 类中定义好。换句话说就是，Composer和五个子预测器都拥有相同的接口！BPU 顶层可以直接把 Composer 也当做一个子预测器，而无需关心内部是怎么连接子预测器的。&#xA;子预测器接口 接下来我们会查看子预测器接口是怎样的。该接口将涉及到 Composer 与 BPU 顶层的交互，还会涉及到各子预测器与 Composer 的交互。&#xA;我们先以 Composer 为例，说明子预测器接口的结构&#xA;如上图所示，Composer 的三通道预测结果被直接输出至 BPU 外部，并且还有一组三通道预测结果从BPU内部连接至 Composer ，但由于预测结果是由 Composer 产生，因此 BPU 会将一个空的预测结果传递给 Composer ，这样做的意义是，使子预测器形成了一个“加工”的作用，子预测器会将输入的预测结果进行加工，然后再输出加工过后的预测结果。&#xA;接下来，BPU 顶层会为流水线提供预测所需要的信息。首先是 PC 和分支历史记录（包括全局历史和全局折叠历史），接下来 BPU 会和 Composer 之间连接一些流水线控制信号，最后 BPU 将外部输入的重定向请求接口和更新接口直接连接至 Composer。&#xA;最终可以简单给出子预测器接口的定义（详细定义请前往接口文档进行查看）：&#xA;in (s1, s2, s3) 预测信息输入 s0_pc 需要预测的PC ghist 全局分支历史 folded_hist 全局折叠历史 out (s1, s2, s3) 预测信息输出 流水线控制信号 s0_fire, s1_fire, s2_fire, s3_fire 相应流水级是否工作 s2_redirect, s3_redirect 后续流水级发现预测错误时重定向信号 s1_ready, s2_ready, s3_ready 子预测器相应流水级是否就绪 update 更新请求 redirect 重定向请求 子预测器之间的连接 我们已经清楚各个子预测器之间的接口与Composer 的接口是相同的，并且我们也已经知道了 Composer是如何连向顶层 BPU 的，本小节将会说明子预测器是如何在 Composer 内部进行连接的。</description>
    </item>
    <item>
      <title>香山分支预测单元时序介绍</title>
      <link>https://open-verify.cc/xs-bpu/docs/basic/03_xsbpu_timing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-verify.cc/xs-bpu/docs/basic/03_xsbpu_timing/</guid>
      <description>一周期无空泡预测 uFTB 是香山 BPU 中唯一一个可以在一周期便可以产生预测结果的预测器，上图中展示了 uFTB 的预测过程。s0_pc 由 BPU 顶层送入，s1 周期生效时，s1_pc 保存了上一周期 s0_pc 的值，依次类推。也就是说传入的 s0_pc 的值会随流水线向下移动。&#xA;在 s1 周期生效时，uFTB 会接收到本周期传来的 s1_fire 信号，并根据 s1_pc 指示的地址，在本周期生成预测结果，在预测结果中可以获取新的 PC 值。&#xA;如图所示，BPU 顶层根据 s1 预测结果通道，分析出下一个 PC 值的位置（即图中的 target），并将其送往 npc_Gen （即新PC生成器）中，用于产生下一个周期的 s0_pc。&#xA;于是下一周期，uFTB 获取到了新的 PC 值，并开始了新 PC 值预测块的产生。由此可见，仅凭借 s1 周期，便可以以一周期一个预测块的速度来产生预测结果。&#xA;预测结果重定向 但除了 uFTB 以外，其他预测器都需要 2-3 个周期才可以产生预测结果，如何利用起他们的预测结果？又如何生成预测结果重定向信号呢？&#xA;如图中所示，一个两周期产生预测结果的 Predirector 2 ，可以在 s2 周期，向 s2 预测结果通道内输出它的预测结果。BPU 顶层拿到预测结果后，分析出预测块的跳转目标地址 target 并连向 npc_Gen。&#xA;此时连向 npc_Gen 的信号中，既有 s2 产生的旧 PC 的预测结果，又有 s1 产生的新 PC 的预测结果，该如何抉择新 PC 用哪一个呢？</description>
    </item>
  </channel>
</rss>
