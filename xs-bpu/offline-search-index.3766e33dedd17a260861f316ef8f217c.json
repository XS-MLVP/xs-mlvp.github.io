[{"body":"In processor design, a well-designed branch predictor (BPU) is a key component for improving processor performance. It is responsible for guiding the processor’s fetch, determining where the next instruction should be fetched and executed. The BPU is the starting point of an instruction’s lifecycle, so exploring a high-performance processor from the BPU is a good starting point.\nThis is also true for Shanshan, a high-performance processor with out-of-order six-issue execution, which naturally requires a branch prediction unit with high accuracy and efficiency. The design of a branch prediction unit often needs to consider many factors, such as timing, complexity of structure, silicon area occupation, prediction accuracy, and speed of recovery from prediction errors. The branch prediction unit of the Shanshan processor has achieved a good balance among these factors through many clever designs, giving it high branch prediction efficiency and accuracy, providing a basic guarantee for the supply of instructions to the backend.\nIn this section, we will introduce the basic design of the Shanshan prediction unit. By reading this section, you can learn the following:\nBasic concepts of branch prediction Basic prediction unit of the Shanshan branch prediction unit - branch prediction block External interfaces of the Shanshan branch prediction unit Basic structure of the Shanshan branch prediction unit Basic timing of the Shanshan branch prediction unit Anyone participating in the BPU crowdsourcing verification work should read this section first to have a basic understanding of the Shanshan branch prediction unit.\n","categories":["Shanshan BPU Basic Design"],"description":"This document introduces the basic design principles of the Shanshan branch prediction unit. By reading this document, you can understand the approximate workflow of the Shanshan BPU without needing to know specific signal names and code details.","excerpt":"This document introduces the basic design principles of the Shanshan …","ref":"/xs-bpu/en/docs/basic/","tags":["Shanshan","BPU","Branch Prediction"],"title":"Basic Design of Shanshan Branch Prediction Unit (BPU)"},{"body":"Feature List upport uFTB sub-predictor Support TAGE-SC sub-predictor Support FTB sub-predictor Support ITTAGE sub-predictor Support RAS sub-predictor Support three-stage prediction result and other information output Support prediction result redirection signal generation Support pipeline control signal generation Support PC generation Support global branch history maintenance Support branch folding history maintenance Support redirection request response, state restoration Support update request response ","categories":["Feature List"],"description":"","excerpt":"Feature List upport uFTB sub-predictor Support TAGE-SC sub-predictor …","ref":"/xs-bpu/en/docs/feature/00_bpufeature/","tags":["BPU","Feature List"],"title":"BPU Top Feature List"},{"body":"The overall function and structure of the BPU top level have been roughly described in previous documents. For those verifying the BPU top level, a more detailed description might be needed. Due to the many functions of the BPU top level, this section divides the BPU into several major functional points for further description. However, since there are too many details at the BPU top level, further details need to be understood by referring to the code.\nGenerator Maintenance Method From the basic design documents of Xiangshan, we know that the BPU top level maintains various variables in the s0 cycle through generators, such as PC, branch history, etc. The core concept is to decide which pipeline level’s result to adopt through the redirection signal of the prediction result.\nThere are a total of 6 generators in the BPU top level:\nnpcGen maintains the PC ghistPtrGen maintains the global history pointer ghvBitWriteGens maintains global history write data foledGhGen maintains the folded history lastBrNumOHGen maintains the position of the last effective branch instruction in the previous cycle aheadFhObGen maintains the oldest position of the branch history Except for npcGen, the rest of the generators will be introduced in this document. In this section, we will focus on the method of generating the next prediction for the generators.\nIn the code, you can see generators defined in a similar way:\nval npcGen = new PhyPriorityMuxGenerator[UInt] Next, the code registers the data sources for the generators through multiple statements:\nnpcGen.register(true.B, reg， ...) npcGen.register(s1_valid, s1_target, ...) npcGen.register(s2_redirect, s2_target, ...) npcGen.register(s3_redirect, s3_target, ...) npcGen.register(do_redirect.valid, do_redirect.bits.cfiUpdate.target, ...) Each line is called a registration. In a registration, the first signal parameter is the data valid signal, and the second signal parameter contains the specific data. The priority of the generator is also determined in the order of registration; the later the registration, the higher the priority. Therefore, the priority at the same time, from low to high, is as follows:\ns0 blocked data Data updated based on s1 prediction results Data updated based on s2 prediction results Data updated based on s3 prediction results Data in external redirection of BPU In this way, when the redirection of the prediction result is valid, we can avoid using the earlier pipeline level’s prediction result and adopt the corrected prediction result. This allows us to handle external redirection requests with the highest priority.\nWe can conclude the method by which all generators generate s0 signals: Among all data valid signals, if only one is valid, the corresponding data is selected; if multiple data valid signals are valid, the data with the highest priority is selected.\nGlobal Branch History We know that the global branch history is maintained at the BPU top level, and the maintenance strategy is consistent with the PC maintenance strategy. That is, after the prediction result is generated at each stage of the pipeline, the global branch history is updated based on the corresponding signals.\nThe top level defines two sets of signals to maintain the global branch history:\nghv stores the global branch history (maximum length 256) ghist_ptr global branch history pointer, pointing to the current position of the global branch history Similar to s0_pc, s1_pc, s2_pc, the BPU top level also maintains signals for each stage of the global history pointer: s0_ghist_ptr, s1_ghist_ptr, s2_ghist_ptr. However, the content in ghv is fixed in position, and we only use ghist_ptr to locate where the current global branch history starts.\nCalculating the Current Global Branch History with ghist_ptr The use of ghist_ptr is only visible at the BPU top level. What we pass to the sub-predictors is the global branch history after the data in the global history register is shifted based on ghist_ptr. In the global branch history obtained by the sub-predictor, the least significant bit corresponds to the newest bit of the global branch history, and the most significant bit corresponds to the oldest bit.\nSo how is the shifting done? First, let’s see how the global history is stored in ghv.\n|===== ghist =====\u003e| =======\u003e| n ^ 0 ghist_ptr As shown in the figure above, the sequence represents the entire ghv register, and ghist_ptr points to a position in ghv. This position represents the newest bit of the global branch history. When a new global history record needs to be added, ghist_ptr is first decremented by 1, and then this bit is written to the position it points to. When ghist_ptr is decremented to 0, it will loop back to point to the highest position, thereby overwriting the previously written global branch history.\nNo matter what, starting from the position pointed to by ghist_ptr, the pointer increases and the history gets older. Therefore, when we need to calculate the current global branch history, we only need to circularly right-shift the ghv register by ghist_ptr positions.\nUpdating Global Branch History The strategy for updating the global branch history is consistent with the strategy for updating the pc. At each pipeline stage, a pointer for the current stage and an update description of ghv are generated based on the prediction result of the current stage, and all are sent to the relevant generator for processing.\nThe update strategy for the global branch history is consistent with the pc update strategy, requiring the generation of a current stage pointer and ghv update instructions based on the current stage’s prediction results at each pipeline stage. These instructions are ultimately sent to the relevant generators for processing.\nThe update description of ghv is some information used to guide the update of the ghv register. Xiangshan BPU maintains two pieces of information to fulfill this duty:\nghv_wdata the data that needs to be written into ghv ghv_wens the write bit mask For the final update, only the bits identified by ghv_wens need to be written with the corresponding bits of ghv_wdata.\nTherefore, each pipeline stage needs to generate three sets of information: ghist_ptr, ghv_wdata, ghv_wens.\nSpecifically, the prediction result can contain up to two branch instructions. We only need to set these pieces of information according to the actual situation. Here are some examples:\nOnly the first slot is valid, and the conditional branch instruction in it is predicted as not taken. Then the next position of ghv_wens is set to 0, the corresponding position of ghv_wens is set to 1, and ghist_ptr is decremented by one. Both slots contain conditional branch instructions, the first is predicted as not taken, and the second is predicted as taken. At this time, ghist_ptr should be decremented by two, and the other two pieces of information should indicate writing 01 to ghv. Here, only one piece of ghv_wdata information is maintained in the generator (maintained by the ghvBitWriteGens generator), and ghv_wens is not maintained by the generator. This is because a small trick is used here, where the final output of the generator’s ghv_wdata is the result of the selected stage, and ghv_wens is used by performing a bitwise OR operation on ghv_wens of all stages.\nThis consideration is based on:\nIf the later pipeline stage is valid, the global history pointer is restored to an older position, even if the history of newer positions is modified by the earlier pipeline’s ghv_wens. If the earlier pipeline stage is valid, the global history pointer continues to update to newer positions, and the later pipeline will not set ghv_wens due to the ineffective redirect. Branch Folded History The branch folded history passed to the predictor is also maintained by the BPU top level. To shorten the update delay of the folded history, the BPU maintains many variables to support the fast update of the branch folded history. We will focus on this strategy and introduce the function of each variable.\nBefore we start, let’s first look at how the branch folded history is defined and its structure.\nBranch Folded History If you have checked the BPU global interface documentation, you will know that the sub-predictor receives an array of bit vectors of different lengths, representing various lengths of folded history, and these folded histories are compressed from the global branch history.\nFor the global branch history, we have a register that stores the global branch history with a length of 256. For example, suppose the length of the global branch history is 15 bits, and after shifting, we get a branch history like this: the least significant bit is the newest history record, and the most significant bit is the oldest history record.\nAt this time, if we need to generate a 6-bit folded history from these 15 bits, we will use an XOR strategy for compression. The specific process is as follows:\nh[5] h[4] h[3] h[2] h[1] h[0] h[11] h[10] h[9] h[8] h[7] h[6] ^ h[14] h[13] h[12] --------------------------------------------------------------- h[5]^h[11] h[4]^h[10] ... h[0]^h[6]^h[12] That is, after arranging it as shown above, perform an XOR operation on the values at each position, and the result is the folded history of length 6.\nMethod for Updating Branch Folded History Now we want to update this branch folded history. When we insert a new history into the global branch history, it is inserted from the least significant bit, meaning the original h[0] becomes h[1]. If we want to obtain the folded history at this time, we only need to perform the XOR operation again. But such efficiency is too low, because the XOR operation may become particularly long. We can explore the impact of one update on the branch folded history.\nIn the example above, before inserting a new history, the 6-bit folded history is generated in the following arrangement:\nh[5] h[4] h[3] h[2] h[1] h[0] h[11] h[10] h[9] h[8] h[7] h[6] h[14] h[13] h[12] After inserting a new history, it becomes like this:\nh[4] h[3] h[2] h[1] h[0] h[new] h[10] h[9] h[8] h[7] h[6] h[5] (h[14]) h[13] h[12] h[11] We can notice some patterns:\nBefore insertion: h[5] {h[4] h[3] h[2] h[1] h[0] } h[11] {h[10] h[9] h[8] h[7] h[6] } { h[14] h[13] h[12]} After insertion: {h[4] h[3] h[2] h[1] h[0] } h[new] {h[10] h[9] h[8] h[7] h[6] } h[5] { (h[14]) h[13] h[12]} h[11] The content in the curly braces has undergone a complete left shift, with h[5] and h[11] moving from the most significant bit to the least significant bit. So, in the compressed history, isn’t this just a typical cyclic left shift that we often encounter!\nHowever, only two bits have changed: one is the newly inserted h[new], and the other is the discarded h[14]. h[new] must be in the first position, and the discarded position is fixed. Therefore, to complete an update, we only need to know the value of the newly inserted history and the oldest bit of the previous history. After the cyclic shift, modifying these two positions according to the actual situation will give us the updated folded history.\nImplementation of Update Method To achieve this update in the top-level BPU, two additional variables are maintained:\nahead_fh_oldest_bits: the oldest bit of the global branch history, with additional bits stored before it last_br_num_oh: the slot number of the last effective branch instruction in the previous prediction An optimization in the timing of operations occurs here because the global history pointer can only be updated based on the branch outcome when the pipeline-level prediction result is available. Updating the oldest bit after updating the global history pointer would increase the latency. Therefore, we maintain the branch outcome and update the oldest bit when it is needed in the next cycle.\nThe oldest bit also needs to be maintained further back because after updating using the branch outcome, the relatively newer bits will become the oldest bits.\nThus, there are three generators related to the folded history: foldedGhGen, lastBrNumOhGen, and aheadFhObGen.\nInformation required for each update of the folded history：\nFolded history information before the update Oldest bit of the global branch history (ahead_fh_oldest_bits) Last prediction’s branch outcome (last_br_num_oh) Whether there is a branch instruction in this update The branch outcome of this update: the slot number of the last effective branch instruction For each update of the folded history, the true oldest bit needs to be determined based on last_br_num_oh and ahead_fh_oldest_bits. Then, based on the oldest bit and the branch outcome of this update, several bits are modified, and finally, a cyclic left shift completes the update operation.\nPipeline Control Method Pipeline control is the core function of the BPU, with complex logic. All pipeline control signals in the top-level BPU are as follows:\ns1_valid, s2_valid, s3_valid: indicate the corresponding pipeline data is valid s1_ready, s2_ready, s3_ready: indicate the corresponding pipeline is ready to continue the prediction of the previous pipeline stage s1_component_ready, s2_component_ready, s3_component_ready: indicate the readiness of the corresponding pipeline sub-predictor s0_fire, s1_fire, s2_fire, s3_fire: successful handshake signals, indicating that the pipeline data is valid and has been successfully passed to the next pipeline s1_flush, s2_flush, s3_flush: indicate whether the current pipeline needs flushing s2_redirect, s3_redirect: indicate whether the current pipeline needs to redirect due to a different prediction result valid, ready 与 fire We will introduce the purpose of each signal step by step. First, let’s look at the fire signal, which indicates a successful handshake in the pipeline, meaning that the data has been successfully passed to the next pipeline. This signifies the end of the current cycle and the end of the prediction for this pipeline stage, with the prediction for the next pipeline stage about to begin in the next cycle.\nThis requires two conditions:\nvalid: The data in the current pipeline stage is valid. ready: Indicates whether the next pipeline stage in the BPU top level and the predictor are ready. When these two signals are high simultaneously, the fire signal is valid, indicating a successful handshake. If we isolate a single prediction, the timing should look like this (in reality, most of the time, each pipeline is valid continuously):\nOf the four sets of signals mentioned earlier, component_ready is generated by the predictor, while the rest are maintained by the BPU top level, with only the fire set of signals exposed to the sub-predictor.\nNext, let’s take s2 as an example to see how each signal is maintained.\nready Signal\ns2_ready := s2_fire || !s2_valid This assignment statement is a combinational circuit assignment, meaning that s2_ready is directly related to s2_fire and s2_valid for this cycle, with two possible situations:\nIf s2_valid is invalid for this cycle, indicating that the s2 pipeline stage is empty and can accept new data, then s2_ready is valid. If s2_valid is valid for this cycle, indicating that the s2 pipeline stage has data that has not been passed to the next stage yet, but if s2_fire, then the data will be passed in the next cycle. In this case, s2_ready is valid, indicating that the data can flow into the next stage. valid Signal\nMaintaining ·s2_valid· is relatively simple, as it is only related to s1_fire and s2_ready signals. The relationship is as follows:\nWhen s1_fire is valid, indicating that data is coming in and s2_valid will be valid in the next cycle. When s2_fire is valid, indicating that data is flowing out and s2_valid will be invalid in the next cycle. fire Signal\nThe fire signal is somewhat special, but for intermediate pipeline stages, its maintenance is straightforward. For example,\ns2_fire := s2_valid \u0026\u0026 s3_components_ready \u0026\u0026 s3_ready This simply requires considering the valid of the current pipeline stage and the ready of the next pipeline stage.\nHowever, for s0_fire, since there is no valid signal, it is directly set to s1_components_ready \u0026\u0026 s1_ready.\nFor s3_fire, as there is no ready signal for the next stage, it is directly set to s3_valid.\nIncorporating Flush and Redirect When there is a different prediction result in the pipeline, a redirection signal is generated, and the pipeline needs to be flushed. flush and redirect handle these two tasks. redirect indicates whether the current pipeline stage needs redirection, while flush indicates whether the current pipeline stage needs flushing.\nredirect Signal\nThe generation of s2_redirect is as follows:\ns2_redirect := s2_fire \u0026\u0026 s2_redirect_s1_last_pred This means that when s2_fire is valid and the prediction result of s2 is different from the prediction result saved from s1, this signal is valid. Later, this signal will be connected to the input of the sub-predictor and the output of the BPU prediction result, guiding the sub-predictor and FTQ to restore their states.\nflush Signal\nThe flush signal is used to guide the flushing of the pipeline. For example, when s3_redirect is valid, it means that the incorrect prediction result has entered the pipeline, and both s1 and s2 are now predicting based on the wrong result. Therefore, the pipeline needs to be flushed to stop the previous stages and wait for new prediction results to enter.\nSpecifically, the relationship between them is:\ns2_flush := s3_flush || s3_redirect s1_flush := s2_flush || s2_redirect This means that if a pipeline stage needs redirection, all previous stages will be flushed. The purpose of flush is to guide the valid signal. If the valid signal is valid in this cycle but the fire signal is not, it means that the incorrect data has not been taken by the next pipeline stage. In this case, when flush is valid, valid will immediately become invalid in the next cycle, avoiding storing incorrect data in the pipeline for a long time.\nHowever, the effect of flush on the valid signal varies depending on each pipeline stage. For example:\nFor the s1 pipeline stage, although flush is valid, if s0_fire is valid, indicating that new data is flowing in, valid will remain valid in the next cycle. For the s2 pipeline stage, if flush is valid, valid will definitely be invalid in the next cycle (because s1 is also flushed), indicating that valid can be directly set to invalid. However, there is a special case where s2_redirect occurs but s2_flush is not set to valid. In this case, if s1_fire occurs, the incorrect prediction result of s1 may also flow in. In this case, s2_valid needs to be determined based on the s1_flush signal. The use of flush is complex, and more detailed details need to be understood by referring to the code.\nRedirect Recovery Logic When the redirect request from FTQ to BPU takes effect, it indicates that all stages of the pipeline have incorrect predictions, and all stages should be flushed. This can be achieved by setting s3_flush to be valid. Therefore, we have:\ns3_flush := redirect_req.valid In the BPU, the redirect request is delayed by one cycle before being officially used. Therefore, the response of s1_valid to the flush signal needs to be changed. When the redirect request (before delay) is valid, s1_valid in the next cycle is immediately set to invalid, without the need to refer to the s0_fire signal.\nAt this point, generators such as npcGen also need to directly use the data from the redirect request to generate, which is equivalent to redirecting the BPU’s state to the state before the error occurred. However, it is important to note that the default redirect level in BPU is flushAfter, which means that the redirect request corresponds to a predicted erroneous instruction, and the BPU assumes that although this instruction was predicted incorrectly, it has been corrected and executed by the backend. Therefore, the next prediction can start directly from the next instruction.\nTherefore, when recovering from a redirect, it is necessary not only to restore the information from the redirect interface, but also to update the execution status of this predicted erroneous instruction in the history.\n","categories":["Xiangshan BPU Submodule"],"description":"","excerpt":"The overall function and structure of the BPU top level have been …","ref":"/xs-bpu/en/docs/modules/00_bpu_top/","tags":["Xiangshan","BPU Top Level","Global Branch History","Branch Folding History","Pipeline Control"],"title":"BPU Top Module"},{"body":"功能列表（Feature List） 支持 uFTB 子预测器 支持 TAGE-SC 子预测器 支持 FTB 子预测器 支持 ITTAGE 子预测器 支持 RAS 子预测器 支持三阶段预测结果及其他信息输出 支持预测结果重定向信号生成 支持流水线控制信号生成 支持 PC 生成 支持全局分支历史维护 支持分支折叠历史维护 支持重定向请求响应，恢复状态 支持更新请求响应 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） 支持 uFTB 子预测器 支持 TAGE-SC 子预测器 支持 FTB 子预测器 支持 ITTAGE  …","ref":"/xs-bpu/docs/feature/00_bpufeature/","tags":["BPU","Feature List"],"title":"BPU Top 功能列表"},{"body":"BPU 顶层整体的功能和结构已经在之前的文档中粗略的描述，对于验证 BPU 顶层的同学来说，可能还需要更加细致的描述。由于 BPU 顶层功能较多，本节将 BPU 划分为了几大功能点进行更进一步的描述。但由于 BPU 顶层细节过多，更进一步的细节需参照代码进行理解。\n生成器维护方法 通过香山的基础设计文档，我们知道，BPU 顶层是通过一个个生成器来维护 s0 周期的各类变量的，例如 PC、分支历史等，并且其核心概念是，通过预测结果重定向信号决定采用哪个流水级的结果。\nBPU 顶层中所有的生成器共有 6 个：\nnpcGen 维护 pc ghistPtrGen 维护全局历史指针 ghvBitWriteGens 维护全局历史写入数据 foledGhGen 维护折叠历史 lastBrNumOHGen 维护上周期最后一个生效的分支指令位置 aheadFhObGen 维护分支历史最老位 其中，除了 npcGen 以外，其余生成器都会在本文档中进行介绍。本节中我们着重探讨一下生成器的产生下一次预测的方法。\n在代码中你可以看到生成器用类似下面这种方式进行定义：\nval npcGen = new PhyPriorityMuxGenerator[UInt] 接下来，代码通过多条语句对生成器的数据来源进行注册：\nnpcGen.register(true.B, reg， ...) npcGen.register(s1_valid, s1_target, ...) npcGen.register(s2_redirect, s2_target, ...) npcGen.register(s3_redirect, s3_target, ...) npcGen.register(do_redirect.valid, do_redirect.bits.cfiUpdate.target, ...) 每一行被称作一次注册，在一次注册中第一个信号参数是数据有效信号，第二个信号参数包含具体的数据。 生成器的优先级也是按照注册的顺序来决定，越往后优先级越高，因此，同一时刻的优先级从低到高依次为：\ns0 阻塞的数据 根据 s1 预测结果更新后的数据 根据 s2 预测结果更新后的数据 根据 s3 预测结果更新后的数据 BPU 外部重定向中的数据 这样一来，我们就可以在预测结果重定向有效时，避免采用较早流水级的预测结果，而采用纠正后的预测结果。也使得我们可以将外部重定向请求作为最高优先级去处理。\n我们可以得出所有生成器产生 s0 信号的方法：在所有数据有效信号中，如果只有一个有效的，则选取它对应的数据，如果有多个数据有效信号生效，则选取优先级最高的数据。\n全局分支历史 我们知道，全局分支历史在 BPU 顶层进行维护，维护的策略与 PC 的维护策略一致。即在每个阶段流水级预测结果产生之后，会根据相应信号对全局分支历史进行更新。\n顶层为了维护全局分支历史定义了两组信号\nghv 存储了全局分支历史（最大长度 256） ghist_ptr 全局分支历史指针，指向全局分支历史当前的位置 与 s0_pc, s1_pc, s2_pc 一样，BPU 顶层为全局历史指针也维护了每一阶段的信号 s0_ghist_ptr, s1_ghist_ptr, s2_ghist_ptr ，但 ghv 中的内容是位置固定的，我们仅通过 ghist_ptr 来定位当前的全局分支历史从哪里开始。\n通过 ghist_ptr 计算当前全局分支历史 ghist_ptr 的使用仅在 BPU 顶层可见，而我们向子预测器传入的，是全局历史寄存器中的数据根据 ghist_ptr 所移位之后的全局分支历史。在子预测器拿到的全局分支历史中，最低位对应全局分支历史的最新位，最高位对应全局分支历史的最老位。\n那么是怎样进行移位的呢，我们首先来看一下全局历史是怎样在 ghv 中进行存储的。\n|===== ghist =====\u003e| =======\u003e| n ^ 0 ghist_ptr 如上图所示，序列表示整个 ghv 寄存器，ghist_ptr 指向 ghv 中的某个位置，这个位置代表了全局分支历史的最新位。当需要添加一位新的全局历史记录时，首先将 ghist_ptr 减 1，然后将该位写在其所指向的位置。当 ghist_ptr 减到 0 后，又会循环回来指向最高位，因此会覆盖之前写入的全局分支历史。\n但不管怎样，从 ghist_ptr 所指向的位置开始，指针越增加，历史越老。因此，当我们需要计算当前全局分支历史时，只需要将 ghv 寄存器循环右移 ghist_ptr 位即可。\n全局分支历史的更新 全局分支历史的更新策略与 pc 更新的策略一致，在每一个流水级都需要根据当前流水级的预测结果生成一个 当前流水级的指针及 ghv 的更新说明，最终都送给相关生成器来处理。\nghv 的更新说明即 用于指导 ghv 寄存器的更新的某些信息。香山 BPU 中维护了两个信息来完成这一职责：\nghv_wdata 需要向 ghv 中写入的数据 ghv_wens 写入位掩码 最终更新时，只需要将 ghv_wens 所标识的位写入 ghv_wdata 的对应位即可。\n因此每个流水级需要负责产生三组信息：ghist_ptr,ghv_wdata, ghv_wens。\n具体地，预测结果中最多含有两条分支指令，我们只需将实际情况来设置这几个信息即可，举几种情况的例子：\n只有第一个槽有效，并且其中条件分支指令被预测为不跳转。则将 ghv_wens 的下一个位置置 0 ，ghv_wens 的对应位置置 1, 同时 ghist_ptr 减一。 两个槽都存放了条件分支指令，第一条被预测为不跳转, 第二条被预测为跳转。此时 ghist_ptr 应该减二，并且其他两个信息应该指示向 ghv 中写入 01。 此处在生成器中只维护了一个 ghv_wdata 信息（通过 ghvBitWriteGens 生成器维护），ghv_wens 并没有通过生成器来维护。这是因为此处使用了一个小技巧，使用了生成器的 ghv_wdata 最终输出的是被选中阶段的结果，而 ghv_wens 将所有阶段的 ghv_wens 进行按位或来使用。\n这是基于如下考虑的：\n如果较晚的流水线阶段有效。全局历史指针被恢复到较老的位置，即便被早期流水的 ghv_wens 修改了较新位置的历史也没关系。 如果较早的流水线阶段有效。全局历史指针继续向较新的位置更新，而后期流水线会因为 redirect 未生效而不把 ghv_wens 置位。 分支折叠历史 送入预测器的分支折叠历史也是由顶层 BPU 来维护的，BPU 为了缩短折叠历史的更新延迟，维护了很多变量，来支持分支折叠历史的快速更新，我们将会重点介绍这一策略，并介绍每一个变量的作用。\n在开始之前，我们先来看一下分支折叠历史是怎样定义的，结构又是怎样的。\n分支折叠历史 如果你查看了 BPU 全局接口的文档，你就会知道，子预测器拿到的是一个不同长度位向量的数组，代表了各种长度的折叠历史，而这些折叠历史都是由全局分支历史压缩而成。\n对于全局分支历史，我们有一个存放全局分支历史的寄存器，长度为 256。为了方便举例，我们假设全局分支历史的长度为 15 位，并且经过移位之后，我们可以拿到一个这样的分支历史：最低位是最新的历史记录，最高位是最老的历史记录。\n此时如果我们需要用这 15 位，产生一个 6 位的折叠历史，会使用异或的策略进行压缩，具体过程是这样的：\nh[5] h[4] h[3] h[2] h[1] h[0] h[11] h[10] h[9] h[8] h[7] h[6] ^ h[14] h[13] h[12] --------------------------------------------------------------- h[5]^h[11] h[4]^h[10] ... h[0]^h[6]^h[12] 即将其按照上面的方式排列之后，将每一位上的值进行异或，结果便是求出的长度为 6 的折叠历史。\n分支折叠历史更新方法 此时我们想要对这一分支折叠历史进行更新，当我们向全局分支历史插入一位新历史时，是从最低位插入的，也就是说原来的 h[0] 变为了 h[1]，如果我们想求此时的分支折叠历史，只需要再进行一遍异或运算。但这样的效率太低了，因为异或的操作有可能变得特别长，我们可以来探寻一下一次更新对分支折叠历史的影响。\n上述例子中，插入一位新历史之前，6 位折叠历史的生成是按照下面这种排列生成的\nh[5] h[4] h[3] h[2] h[1] h[0] h[11] h[10] h[9] h[8] h[7] h[6] h[14] h[13] h[12] 插入一位新历史之后变成了下面这样\nh[4] h[3] h[2] h[1] h[0] h[new] h[10] h[9] h[8] h[7] h[6] h[5] (h[14]) h[13] h[12] h[11] 我们可以发现一些规律\n插入前： h[5] {h[4] h[3] h[2] h[1] h[0] } h[11] {h[10] h[9] h[8] h[7] h[6] } { h[14] h[13] h[12]} 插入后： {h[4] h[3] h[2] h[1] h[0] } h[new] {h[10] h[9] h[8] h[7] h[6] } h[5] { (h[14]) h[13] h[12]} h[11] 大括号中的内容发生了整体的左移，h[5] 和 h[11]，由最高位变到了最低位。那么表现在压缩后的历史上不就是我们常见的循环左移吗！\n但其中有且仅有两个位的值发生了变化，一个是新加入的 h[new]，一个是被舍弃掉的 h[14]。h[new] 肯定在第一位，被舍弃的位置也是固定的。因此我们想要完成一次更新，只需要知道 新插入历史的值 和 前一次历史的最老位即可。循环移位后，将这两个位置根据实际情况进行一次修改便可拿到更新后的折叠历史。\n更新方法实现 BPU 顶层为了实现这种更新，正是通过维护最老位，这通过两个额外的变量来实现：\nahead_fh_oldest_bits 全局分支历史的最老位，还额外往前存储了若干位 last_br_num_oh 上一次预测最后一个生效的分支指令在第几个槽 在这里有一处为时序所优化的点，因为当流水级的预测结果出来时，全局历史指针才能通过跳转情况进行更新，等到全局历史指针更新完再来更新最老位会增加时延。因此我们将跳转情况维护起来，等到下一周期用的时候再来用跳转情况更新最老位。\n此时的最老位也需要多往前维护几位，因为在使用时，利用跳转情况更新后，前面较新的几位就会变成最老位了。\n所以与折叠历史相关的生成器共有三个：foldedGhGen, lastBrNumOhGen, aheadFhObGen\n每次折叠历史更新时需要的信息分别是：\n更新前的折叠历史信息 全局分支历史最老位(ahead_fh_oldest_bits） 上次预测的跳转情况(last_br_num_oh) 本次更新是否有指令跳转 本次更新的跳转情况：最后一个生效的分支指令在第几个槽 每次折叠历史更新时，都需要根据 last_br_num_oh 和 ahead_fh_oldest_bits 求出真正的最老位，然后通过最老位与本次更新的跳转情况将其中的若干位进行修改，最后进行循环左移，便完成了更新操作。\n流水线控制方法 流水线控制是 BPU 功能的核心，逻辑也最为复杂，BPU 顶层中所有的流水线控制信号如下：\ns1_valid, s2_valid, s3_valid 表示对应流水数据生效 s1_ready, s2_ready, s3_ready 表示对应流水已准备好继续上一流水级的预测 s1_component_ready, s2_component_ready, s3_component_ready 表示对应流水子预测器的 ready 情况 s0_fire, s1_fire, s2_fire, s3_fire 握手成功信号，表示该流水数据生效，并成功传递给了下一流水 s1_flush, s2_flush, s3_flush 表示当前流水是否需要冲刷 s2_redirect, s3_redirect 表示当前流水在 fire 的同时，是否预测结果不同，需要产生预测结果重定向 valid, ready 与 fire 我们会逐步来介绍每个信号的作用，首先我们来看 fire 信号，这一信号表示的含义是流水线握手成功，数据成功传给了下一流水。这标志着本周期结束时，本流水级的预测也随之结束，下周期开始时，下一流水级的预测即将开始。\n这需要两个条件：\nvalid 本流水级的数据是有效的。 ready 与 component_ready 分别指示了 BPU 顶层与预测器的下一流水级是否就绪。 当这两个信号同时置高时，fire 信号有效，表示握手成功。如果我们单独把一次预测拿出来，那么时序应该是这样的（实际中，大多数时间每个流水线都是一直有效的）：\n上文中提到的四组信号，除了 component_ready 是由预测器输出，其余信号皆需 BPU 顶层来维护，而最终暴露给子预测器的，只有 fire 一组信号。\n我们接下来以 s2 为例分别来看每个信号是如何维护的。\nready 信号\ns2_ready := s2_fire || !s2_valid 该赋值语句是一个组合电路赋值，也就是说，s2_ready 信号是与本周期的 s2_fire 和 s2_valid 直接相关联的，分为以下两种情况：\ns2_valid 信号在本周期无效，说明 s2 流水级目前是空的，自然可以接受新的数据，则 s2_ready 有效 s2_valid 信号在本周期有效，说明 s2 流水级目前有数据还未传递给下一级，但如果 s2_fire ，那么本周期就会传递给下一级。此时 s2_ready 有效，刚好指示数据可以在下一拍流入。 valid 信号\ns2_valid 信号目前为止维护是相对简单的，与 s1_fire 信号和 s2_ready 信号相关。其关系为：\n当 s1_fire 有效，说明数据传进来，下一周期 s2_valid 有效。 当 s2_fire 有效，说明数据流出去，下一周期 s2_valid 无效。 fire 信号\nfire 信号相对特殊，但对于中间的流水级来说，维护非常简单，例如\ns2_fire := s2_valid \u0026\u0026 s3_components_ready \u0026\u0026 s3_ready 只需考虑当前流水级的 valid 和下一流水级的 ready 即可。\n但对 s0_fire 来说，没有 valid 信号，因此其直接等于 s1_components_ready \u0026\u0026 s1_ready\n对于 s3_fire 来说，没有下一级的 ready 信号，因此其直接等于 s3_valid\n加入 flush 和 redirect 我们知道，当流水线出现预测结果不同时，需要产生预测结果重定向信号，并且将之前的流水线清空。flush 和 redirect 正是在做这两项工作。redirect 表示当前流水级是否需要重定向，flush 则表示当前流水级是否需要冲刷。\nredirect 信号\ns2_redirect 的产生方式如下：\ns2_redirect := s2_fire \u0026\u0026 s2_redirect_s1_last_pred 也就是说，当 s2_fire 时，并且 s2 的预测结果与上一周期保存的 s1 预测结果不同时，这个信号便有效。之后该信号将会连接到子预测器的输入，与 BPU 预测结果的输出，分别指导子预测器和 FTQ 的状态恢复。\nflush 信号\nflush 信号是用于指导流水线冲刷的，例如 s3 重定向有效时，说明错误的预测结果已经流入流水线， s1 和 s2 此时全都是基于错误的结果来预测的，因此需要进行流水线冲刷，使之前的流水级都暂停工作，等待新的预测结果流入。\n具体地，他们之间有如下关系：\ns2_flush := s3_flush || s3_redirect s1_flush := s2_flush || s2_redirect 也就是说，某个流水级 redirect 有效，之前的流水级的 flush 全都会被置为有效。那么 flush 具有什么作用呢？答案是指导 valid 信号，如果本周期 valid 信号有效，但 fire 信号未生效，说明错误的数据没有被下一流水取走，此时 flush 有效后，在下一周期 valid 就会立即变为无效，以这种方式来避免错误数据长期存储在流水线中。\n但 flush 信号对 valid 信号的影响，也根据每一个流水级的不同而有一定差异。例如\ns1 流水级。虽然 flush 有效，但是如果此时 s0_fire 有效，说明新数据流入，那么下周期 valid 依然有效。 s2 流水级。flush 有效，那么必定下周期不会 valid（因为 s1 也肯定被 flush），此时就可以直接将 valid 置为无效。但还存在一种特殊情况，s2_redirect 发生时，s2_flush 并没有被置为有效，此时如果发生 s1_fire，s1 的错误预测结果也可能流入，此时还需根据 s1_flush 信号来决定 s2_valid 是否有效。 flush 的使用较复杂，更详细的细节还需参考代码进行理解。\n重定向恢复逻辑 当 FTQ 发往 BPU 的重定向请求生效时就说明所有流水级的预测结果都是不正确的，此时应该将所有流水级进行冲刷，这可以通过将 s3_flush 置为有效来实现。因此有\ns3_flush := redirect_req.valid 在 BPU 中，重定向请求送入后被延迟一周期才正式使用，因此 s1_valid 的信号也需要对 flush 信号的响应做出一些改变。当重定向请求（延迟前）有效时，s1_valid 下周期立即被置为无效，不需要再去参考 s0_fire 信号了。\n此时 npcGen 等生成器也需要直接去采用重定向请求中的数据来生成，这就相当于将 BPU 的状态重定向到出错之前状态的过程。但注意 BPU 默认的重定向等级为 flushAfter ，即重定向请求会对应一条预测错误的指令，而 BPU 会认为这条指令虽然预测错了，但是已经被纠正并且交由后端执行了，因此下一次预测可以直接从下一条指令开始。\n所以在重定向恢复时，不仅需要将重定向接口中的信息恢复，还需要将这条预测错误的指令的执行情况也更新到历史中去。\n","categories":["香山 BPU 子模块"],"description":"","excerpt":"BPU 顶层整体的功能和结构已经在之前的文档中粗略的描述，对于验证 BPU 顶层的同学来说，可能还需要更加细致的描述。由于 BPU 顶层功能 …","ref":"/xs-bpu/docs/modules/00_bpu_top/","tags":["香山","BPU 顶层","全局分支历史","分支折叠历史","流水线控制"],"title":"BPU 顶层模块"},{"body":"FTB item The FTB item is the core data structure of branch prediction blocks in Xiangshan. It stores the information needed to generate a branch prediction block. When BPU performs predictions, the initial branch prediction block is first generated from a read-out FTB item. Then, this branch prediction block is passed to subsequent predictors, which read and modify the information to generate the final prediction result.\nTherefore, to understand the structure of a branch prediction block, we first need to understand the structure of an FTB item. An FTB item corresponds to a branch prediction block, and its general structure is as follows:\nFirst, it is necessary to clarify one piece of information: whether it is a branch prediction block or an FTB item, the number of instructions they can contain is set to a specific limit (16 RVC instructions in the current version of Xiangshan), called the maximum prediction length. This means that if we need to record the position of an instruction within the branch prediction block, we can use a fixed-length bit vector to specify the offset of this instruction relative to the starting address of the prediction block.\nThe determinant of the branch prediction block’s execution process is the information about the branch instructions. The other instructions are considered ordinary instructions and do not affect the program’s execution flow. Therefore, in a branch prediction block, we only need to record the positions of the branch instructions, while the positions of ordinary instructions are not our concern.\nTherefore, the FTB item defines two types of branch instruction slots—brSlots and tailSlot, used to store the branch instructions within the branch prediction block. In the current version of Xiangshan, brSlots contains only one slot, while tailSlot is a separate slot, totaling two slots.\nWithin the instructions of the maximum prediction length, if a branch instruction appears, the FTB item will record it in the corresponding slot and mark the slot as valid. If too many branch instructions appear, reaching the capacity limit of the FTB item, the excess branch instructions will be handed over to the next FTB item for storage. In each slot, we record the offset of a branch instruction relative to the starting address of the prediction block and information such as its jump target address.\nThe Unique tailSlot In RISC-V, branch instructions are mainly divided into two types: conditional branches and unconditional jumps. Therefore, for a branch prediction block, it will contain at most one unconditional jump instruction. Because once this instruction is executed, the program’s execution flow will change, and subsequent instructions will no longer be executed. Hence, we define a type of slot called tailSlot specifically for storing this unconditional jump instruction. As for conditional branch instructions, they are stored in brSlots.\nAs its name suggests, tailSlot is located in the last slot of the entire prediction block. This is also because once the unconditional jump instruction is filled, the program will definitely jump, and subsequent instructions will be handled by other prediction blocks, so we do not need to care about the subsequent instructions. However, among the instructions before the unconditional jump instruction, we need to care about whether there are conditional branch instructions, because conditional branch instructions may or may not jump. Therefore, we need to record the relevant information of the conditional branch instructions.\ntailSlot Sharing Consider a situation: if no unconditional jump instructions appear from the starting PC of the prediction block to the maximum prediction length, but two conditional branch instructions appear instead, the tailSlot will be idle, and the second conditional branch instruction cannot be stored, causing space waste.\nTo solve this problem, Xiangshan adopts a method of setting a sharing mark. We can directly store the second branch instruction into the tailSlot and set the sharing mark to true, indicating that the second conditional branch instruction shares the tailSlot of the unconditional jump instruction. This way, the space of the tailSlot is effectively utilized.\nThe isCall, isRet, and isJalr fields in the prediction block serve the tailSlot. If the tailSlot records an unconditional jump instruction, these fields will further indicate the type of the jump instruction. There is also a field in the FTB item called always_taken, which records whether each conditional branch instruction stored in each slot is always predicted to jump. If so, subsequent predictors can directly adopt this prediction result.\nThrough the FTB item, we can know the instruction situation in a branch prediction block, including the position and type of branch instructions. This information will be handed over to subsequent predictors, which will predict more accurate jump targets, whether to jump, and other information.\nComplete Structure of FTB Item (FTBEntry) Interface Definition: src/main/scala/xiangshan/frontend/FTB.scala\nThis section describes the complete structural definition of the FTB item, containing the following signals:\nvalid: Whether the FTB entry is valid. Interface Type: Bool brSlots: Conditional branch instruction slots. Interface Type: Vec(numBrSlot, new FtbSlot(BR_OFFSET_LEN)) （see FtbSlot for interface list） tailSlot: Unconditional jump instruction slot. Interface Type: new FtbSlot(JMP_OFFSET_LEN, Some(BR_OFFSET_LEN)) （see FtbSlot for interface list） pftAddr: Unconditional jump instruction slot Interface Type: UInt(log2Up(PredictWidth).W) carry: Unconditional jump instruction slot. Interface Type: Bool() isCall: The instruction in the unconditional jump instruction slot is a call instruction. Interface Type: Bool() isRet: The instruction in the unconditional jump instruction slot is a ret instruction. Interface Type: Bool() isJalr: The instruction in the unconditional jump instruction slot is a jalr instruction. Interface Type: Bool() last_may_be_rvi_call: The instruction in the unconditional jump instruction slot may be an RVI type call instruction signal. Interface Type: Bool() always_taken: Whether each branch instruction in the prediction block is always predicted as Taken. Interface Type: Vec(numBr, Bool()) Explanation: pftAddr and carry\nHere, pftAddr stands for Partial Fallthrough Address. Fallthrough Address means that if there is no jump in the prediction block, the program will sequentially execute to the address reached. In other words, if the offset of an unconditional jump instruction is 5, then the offset corresponding to the Fallthrough Address is 6. This signal is mainly used to get the return address of the program after a function call, and this concept can be understood as the end address of the prediction block.\nPartial means part of the address, which is determined by the address representation method. Here, the address representation method is as follows:\npc: | ... |\u003c-- log(predictWidth) --\u003e|\u003c-- log(instBytes) --\u003e| ^ ^ | | carryPos instOffsetBits pftAddr only records the middle offset part (the part with a length of log(predictWidth)), and a complete PC can be generated by combining it with the current PC. However, a carry might occur, so a carry bit is recorded separately. carryPos is the position in the instruction address within the prediction block where a carry might occur.\nAdditionally, last_may_be_rvi_call is an auxiliary signal for this address, indicating that the instruction in the unconditional jump instruction slot is an RVI type call instruction. Since pftAddr assumes the instruction length as the compressed instruction length by default when calculating, the end address is increased by only 2 bytes. If the actual call instruction is not a compressed instruction, it will lead to an incorrect return address calculation. RAS will correct this error based on this signal.\nBranch Prediction Slot (FTBSlot) Interface Definition: src/main/scala/xiangshan/frontend/FTB.scala\nThis interface defines the slot in the FTB entry:\noffset: The offset of the instruction in the slot relative to the start address of the prediction block. Interface Type: UInt(log2Ceil(PredictWidth).W) lower: The lower bits of the jump target address. Interface Type: UInt(offsetLen.W) Note: The lower is set to 12 or 20 bits because the addressing capability of branch instructions is 12 bits, while the addressing capability of jump instructions is 20 bits. tarStat: Whether the high bits of the PC after the jump are incremented or decremented. Interface Type: UInt(TAR_STAT_SZ.W) (TAR_STAT_SZ = 2) Note: The jump target address is calculated from the high bits of the current PC, the tarStat, and the lower field. The lower field directly stores the lower bits of the jump target address. The high bits of the current PC are adjusted according to the tarStat, then concatenated with the lower to get the actual jump target address. The tarStat can take three values: 0 - no increment or decrement, 1 - increment, 2 - decrement. sharing: Indicates that a conditional branch instruction is stored in an unconditional jump instruction slot. Interface Type: Bool() valid: Indicates whether the slot is valid. Interface Type: Bool() Full Branch Prediction Interface Definition: src/main/scala/xiangshan/frontend/FrontendBundle.scala This interface defines the complete branch prediction results, included in the prediction results of each pipeline stage.\nThe full branch prediction result interface is similar to the FTB entry and is initially generated from the FTB entry. Two slots are split into individual signals: slot_valids, targets, offsets, is_br_sharing, etc. Additionally, several fields are added such as br_taken_mask, jalr_target to facilitate the provision of precise prediction results by subsequent predictors. The hit indicates whether an FTB entry is hit, i.e., the PC in the current prediction round indexed to an FTB entry.\nFull Interface List：\nhit: Indicates whether the FTB entry is hit. Interface Type: Bool() slot_valids: Indicates whether each slot in the FTB entry is valid. Interface Type: Vec(totalSlot, Bool()) targets: The jump target address corresponding to each slot. Interface Type: Vec(totalSlot, UInt(VAddrBits.W)) offsets: The offset of the instruction in each slot relative to the start address of the prediction block. Interface Type: Vec(totalSlot, UInt(log2Ceil(PredictWidth).W)) fallThroughAddr: The end address of the prediction block. Interface Type: UInt(VAddrBits.W) fallThroughErr: Indicates that the pftAddr recorded in the FTB entry is incorrect. Interface Type: Bool() is_jal: Indicates that the prediction block contains a jal instruction. Interface Type: Bool() is_jalr: Indicates that the prediction block contains a jalr instruction. Interface Type: Bool() is_call: Indicates that the prediction block contains a call instruction. Interface Type: Bool() is_ret: Indicates that the prediction block contains a ret instruction. Interface Type: Bool() last_may_be_rvi_call: Indicates that the instruction in the unconditional jump instruction slot might be an RVI type call instruction. Interface Type: Bool() is_br_sharing: Indicates that the last slot (tailSlot) stores a conditional branch instruction. Interface Type: Bool() br_taken_mask: The branch prediction result, with each bit corresponding to a branch (slot), indicating whether the branch is predicted to be taken. Interface Type: Vec(numBr, Bool()) jalr_target: The jump target of the jalr instruction in the prediction block. Interface Type: UInt(VAddrBits.W) ","categories":["Xiangshan BPU Interface"],"description":"The basic prediction unit of BPU is the branch prediction block, but what is the structure of a branch prediction block? Why can a branch prediction block guide the execution of so many subsequent instructions? This section will describe the core data structure for generating branch prediction blocks—the FTB item, and the complete prediction result structure generated from the FTB item.","excerpt":"The basic prediction unit of BPU is the branch prediction block, but …","ref":"/xs-bpu/en/docs/ports/00_ftb/","tags":["FTB"],"title":"FTB Item and Complete Prediction Result Interface"},{"body":"FTB 项 FTB 项是香山中分支预测块的核心数据结构，它存储了生成一个分支预测块所需的信息。在BPU进行预测时，初始的分支预测块首先由一个读出的 FTB 项生成。然后，这个分支预测块会传递到后续的预测器中，后续的预测器会读取其中的信息并加以修改，生成最终的预测结果。\n因此，要理解分支预测块的结构，首先需要理解FTB项的结构。一个FTB项对应着一个分支预测块，其大致结构如下：\n首先，需要明确一个信息：无论是分支预测块还是FTB项，它们可以容纳的指令数量都被设定为一个特定的上限（在香山当前版本中为16条RVC指令），被称为最大预测长度。这意味着，如果我们需要记录分支预测块内某一条指令的位置，可以使用一个固定长度的位向量，来指定这条指令相对于预测块起始地址的偏移量。\n分支预测块执行流程的决定因素是其中的分支指令信息。其余的指令都被视为普通指令，不影响程序的执行流。因此，在一个分支预测块中，我们只需记录下其中的分支指令位置即可，而普通指令的位置我们并不关心。\n因此，FTB项中定义了两种类型的分支指令槽（slot）——brSlots和tailSlot，用于存储分支预测块中的分支指令。目前香山版本中，brSlots只含有一个 slot，而 tailSlot 则是一个单独的 slot，总计含有两个 slot。\n在最大预测长度内的指令中，如果出现了分支指令，FTB项会将其记录在对应的slot内，并将该slot置为有效。如果分支指令出现过多，达到了FTB项的容纳上限，那么超出上限的分支指令则会交给下一个FTB项来存储。在每个 slot中，我们会记录一条分支指令相对于预测块起始地址的偏移量（offset），同时还会记录其跳转目标地址等信息。\n唯一的 tailSlot 在RISC-V中，分支指令主要分为两种类型：条件分支和无条件跳转。因此，对于一个分支预测块来说，最多只会包含一条无条件跳转指令。因为一旦执行到这条指令，程序的执行流就会发生改变，后续的指令将不再执行。因此，我们定义了一种类型的 slot 叫做 tailSlot，专门用来存储这条无条件跳转指令。而对于条件分支指令，则将它们存储在brSlots中。\ntailSlot如其名，位于整个预测块中的最后一个 slot。这也是因为一旦填充了无条件跳转指令，程序必定跳转，之后的指令就由其他预测块进行处理，因此后续的指令情况我们无需关心。但在无条件跳转指令之前的指令中，我们需要关心是否存在条件分支指令，因为条件分支指令既有可能跳转，也有可能不跳转。因此，我们需要记录下条件分支指令的相关信息。\ntailSlot 共享 考虑一种情况：如果从预测块起始PC开始直到最大预测长度，都没有出现无条件跳转指令，反而出现了两条条件分支指令，这样一来，tailSlot 就会空闲，而第二条条件分支指令又无法存储，造成空间的浪费。\n为了解决这个问题，香山采用了一种方法：设置一个 sharing 标记。我们可以直接将第二条分支指令存储到tailSlot 中，并将sharing标记设置为真，表示第二条条件分支指令共享了无条件跳转指令的 tailSlot。这样，就有效利用了tailSlot的空间。\n预测块中的 isCall、isRet和isJalr字段是为tailSlot服务的。如果tailSlot中记录的是无条件跳转指令，这几个字段会进一步指示该跳转指令的类型。在FTB项中还有一个字段 always_taken，用来记录每个Slot中存储的条件分支指令是否总是被预测为跳转。如果是的话，后续的预测器可以直接采用这一预测结果。\n通过FTB项，我们可以得知一个分支预测块中的指令情况，包括分支指令的位置、类型等。这些信息会交给后续的预测器，由后续的预测器来预测更为精确的跳转目标、是否跳转等信息。\nFTB 项完整结构 （FTBEntry） 接口定义: src/main/scala/xiangshan/frontend/FTB.scala\n本节中描述了 FTB 项的完整结构定义，其所含的信号如下：\nvalid: FTB表项是否有效。 接口类型: Bool brSlots: 条件分支指令槽。 接口类型: Vec(numBrSlot, new FtbSlot(BR_OFFSET_LEN)) （接口列表详见FtbSlot） tailSlot: 无条件跳转指令槽。 接口类型: new FtbSlot(JMP_OFFSET_LEN, Some(BR_OFFSET_LEN)) （接口列表详见FtbSlot） pftAddr: 预测块的结束地址。 接口类型: UInt(log2Up(PredictWidth).W) carry: 预测块的结束地址进位。 接口类型: Bool() isCall: 无条件跳转指令槽中的指令为 call 指令。 接口类型: Bool() isRet: 无条件跳转指令槽中的指令为 ret 指令。 接口类型: Bool() isJalr: 无条件跳转指令槽中的指令为 jalr 指令。 接口类型: Bool() last_may_be_rvi_call: 无条件跳转指令槽中的指令可能为一个 RVI 类型的 call 指令信号。 接口类型: Bool() always_taken: 该预测块中的每个分支指令是否总是被预测为 Taken。 接口类型: Vec(numBr, Bool()) 说明：pftAddr 和 carry\n在这里，pftAddr是Partial Fallthrough Addr的缩写。Fallthrough Addr 表示，如果预测块中没有跳转，那么程序将会顺序执行到达的地址。换句话说，如果一条无条件跳转指令的偏移为5，那么 Fallthrough Addr 对应的偏移便是6。这一信号主要用于获取函数调用后程序的返回地址，可以将这一概念理解为预测块的结束地址。\nPartial 表示部分地址，这是由地址的表示方法决定的。在这里，地址的表示方法如下：\npc: | ... |\u003c-- log(predictWidth) --\u003e|\u003c-- log(instBytes) --\u003e| ^ ^ | | carryPos instOffsetBits pftAddr 便是只记录了中间的偏移部分（即长度为 log(predictWidth) 的部分），之后结合当前 PC 才能生成完整 PC。但由于有可能产生进位，因此单独记录一个 carry 位，carryPos 是预测块中的指令地址中 会产生进位的位置\n另外，last_may_be_rvi_call 也是对该地址的一个辅助信号，表示无条件跳转指令槽中的指令是一个RVI类型的 call 指令。由于pftAddr在计算时默认认为指令长度为压缩指令长度，因此计算结束地址只会加 2 字节。但如果实际的call指令并不是压缩指令，就会导致返回地址计算错误。RAS会根据这一信号修正这个错误。\n分支预测槽 （FTBSlot） 接口定义: src/main/scala/xiangshan/frontend/FTB.scala\n该接口定义了 FTB 表项中 slot：\noffset: 该 slot 中指令相对于预测块起始地址的偏移 接口类型：UInt(log2Ceil(PredictWidth).W) lower: 跳转目标地址的低位。 接口类型：UInt(offsetLen.W) 说明：这里设置 lower 为 12 位或 20 位，因为分支指令的寻址能力为 12 位，而跳转指令的寻址能力为 20 位 tarStat: 跳转后的 pc 高位是否进退位 接口类型：UInt(TAR_STAT_SZ.W)（TAR_STAT_SZ = 2） 说明：跳转目标地址由当前 PC 的高位、tarState 以及 lower 字段共同计算得出，其中，lower 字段直接存储了跳转目标地址的低位。当前的 PC 高位经过 tarStat 指导进行进退位之后，直接与 lower 拼接便可得到真正的跳转目标地址。tarStat 共有三个可能的取值：0 - 不进不退, 1 - 进位, 2 - 退位。 sharing: 无条件跳转指令槽中存储了一条条件分支指令。 接口类型：Bool() valid: slot 有效位。 接口类型：Bool() 完整分支预测结果 (FullBranchPrediction) 接口定义: src/main/scala/xiangshan/frontend/FrontendBundle.scala 该接口定义了完整的分支预测结果，每流水级预测结果中都会包含该接口。\n完整分支预测结果接口与 FTB 项相差无几，并最初由 FTB 项生成。两个Slot被拆分成了单独的信号：slot_valids, targets, offsets, is_br_sharing 等，另外增加了若干字段例如 br_taken_mask, jar_target 便于后续预测器提供精确预测结果，另外 hit 表示 FTB 项是否命中，即本轮预测中的 PC 索引到了某个 FTB 项。\n完整接口列表如下：\nhit FTB表项是否命中\n接口类型：Bool() slot_valids slot有效位，表示 ftb 项中的每个 slot 是否有效\n接口类型：Vec(totalSlot, Bool()) targets 每个 slot 对应的跳转目标地址\n接口类型：Vec(totalSlot, UInt(VAddrBits.W)) offsets 每个 slot 中的指令相对于预测块起始地址的偏移\n接口类型：Vec(totalSlot, UInt(log2Ceil(PredictWidth).W)) fallThroughAddr 预测块的结束地址\n接口类型：UInt(VAddrBits.W) fallThroughErr FTB项中记录的 pftAddr 有误\n接口类型：Bool() is_jal 预测块内包含jal指令\n接口类型：Bool() is_jalr 预测块内包含jalr指令\n接口类型：Bool() is_call 预测块内包含call指令\n接口类型：Bool() is_ret 预测块内包含ret指令\n接口类型：Bool() last_may_be_rvi_call 无条件跳转指令槽中的指令可能为一个 RVI 类型的 call 指令信号。\n接口类型：Bool() is_br_sharing 最后一个slot (tailSlot) 中存储的是条件分支指令信号\n接口类型：Bool() br_taken_mask 分支预测结果，每个分支(slot)对应一位，表示该分支是否被预测为 taken\n接口类型：Vec(numBr, Bool()) jalr_target 本预测块中的 jalr 的跳转目标\n接口类型：UInt(VAddrBits.W) ","categories":["香山 BPU 接口"],"description":"BPU 的基本预测单位是分支预测块，然而分支预测块的结构是怎样的呢？为什么一个分支预测块可以指导接下来如此多条指令的执行呢？这一部分将讲述生成分支预测块的核心数据结构——FTB项，以及由 FTB 项生成的完整预测结果结构。","excerpt":"BPU 的基本预测单位是分支预测块，然而分支预测块的结构是怎样的呢？为什么一个分支预测块可以指导接下来如此多条指令的执行呢？这一部分将讲述生 …","ref":"/xs-bpu/docs/ports/00_ftb/","tags":["FTB 项"],"title":"FTB项与完整预测结果接口"},{"body":"Why Do We Need Branch Prediction? There are mainly two reasons for branch prediction: one is that the program’s execution flow contains branch instructions, and the other is that high-performance processors use pipeline design.\nProgram’s Execution Flow Contains Branch Instructions int x = 10; int y = 20; int result = 0; if (x \u003e= y) { result = x + y; } else { result = x - y; } The above is a piece of C code. It first defines three variables x, y, and result, and then assigns a value to result based on the comparison of x and y. It can be observed that the program assigns values to variables in sequence in the first three lines. However, in the 5th line, due to the presence of the if instruction, the program branches, jumping directly from the 5th line to the 8th line to continue execution, which causes a branch in the program’s execution.\nAfter translating into RISC-V assembly code, the code is as follows:\nli a0, 10 # x = 10 li a1, 20 # y = 20 li a2, 0 # result = 0 blt a0, a1, else_branch # Jump to else_branch if x \u003c y add a2, a0, a1 # Execute result = x + y j end # Jump to end else_branch: sub a2, a0, a1 # Execute result = x - y end: It can be seen that the program still maintains the previous branching behavior. In the first three lines of the code, instructions are executed in sequence. Then, in the 5th line of the program, a special instruction blt appears, which we call a branch instruction. It will determine whether to execute the next instruction based on the relationship between x and y, and the appearance of this instruction causes a branch in the program’s execution.\nHigh-performance Processors Use Pipeline Design Therefore, the concept of branch prediction arises. If we can accurately predict the address of the next instruction before the execution result is generated, the processor can continue to run efficiently.\nFeasibility of Branch Prediction Why can branch prediction be done? Let’s look at an example:\nif (data \u003e= 128) sum += data; Assuming that this instruction will be executed repeatedly, and data is incremented from 0, i.e., data = 0, 1, 2, 3 … 128, 129…, let’s analyze the behavior of executing this instruction each time.\nT = branch taken N = branch not taken data = 0, 1, 2, 3, s, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ... branch = N N N N N ... N N T T T ... T T T ... = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT (easy to predict) It can be seen that in the first 128 times, the branch is always Not Taken (the condition is not met), but after 128 times, the branch is always Taken (the condition is met). If we predict whether it is Taken based on whether it was Taken last time, we will only make one prediction error throughout the prediction process.\nThe occurrence of the above phenomenon is due to a basic fact—whether a branch instruction jumps is related to the past jumping behavior of that instruction. By summarizing the past jumping rules, we can make a relatively accurate prediction for this jump, which also makes branch prediction possible.\nIn fact, the jump of branch instructions is also related to factors such as the jumping situation of other branch instructions. Fully exploiting effective information to produce accurate prediction results is one of the main tasks of branch prediction.\nBasic Types of Branch Prediction In RISC-V, branch instructions include two types:\nConditional Branch Instructions (beq, bne, blt, bge, bltu, bgeu) For these instructions, whether to jump is determined by the condition in the instruction, and the jump target can be obtained from the instruction. Therefore, we need to predict whether the instruction will jump. Unconditional Jump Instructions (jal, jalr) For these instructions, the jump is always executed when encountered, but the jump target may be specified by a register. Therefore, we need to predict the jump target of the instruction. Fortunately, due to the concise design of the RISC-V architecture, we do not need to handle conditional jump instructions. Every jump instruction we need to predict is unconditional, which is also convenient for our design.\nFrom the above analysis, we can summarize the two basic types of branch prediction—direction prediction and target address prediction.\nDirection Prediction of Branch Instructions Direction prediction of branch instructions corresponds to conditional branch instructions in RISC-V instructions. We need to predict whether it needs to jump, which is called direction prediction.\nTwo-Bit Saturation Counters Direction prediction has a very simple and efficient prediction method called two-bit saturation counter. The basic idea can be seen in the figure below.\nThe two-bit saturating counter is regarded as a state machine, and we maintain such a state machine for each branch instruction. When a branch instruction is taken, the corresponding state in the diagram moves to the right; otherwise, it moves to the left. So, the next time we encounter this branch instruction, we first look up its two-bit saturating counter. If the state is more biased to the right, we predict it to be taken; otherwise, we predict it not to be taken.\nOf course, it’s impractical to maintain a two-bit saturating counter for each branch instruction. Therefore, in practice, we usually use part of the PC or a hash method to index the two-bit saturating counter, as shown in the diagram below.\nBranch History Branch history is a very commonly used data in branch prediction and the basis of most branch prediction algorithms because it directly shows the past jumping behavior of instructions.\nThere are two basic types of branch history:\nLocal Branch History Maintain a set of registers for each branch instruction, recording the historical jumping behavior of that instruction. For example: 0101000000101 (0 means not taken, 1 means taken) 全Global Branch History All instructions share a set of registers, recording the branching behavior during program execution. For example:\n$$ beg a0, a1, label1 not taken record 0 bne a1, a2, label2 not taken record 0 -\u003e beq a2, a3, label4 taken record 1$$After executing these three different branch instructions, the global branch history becomes 001.\nBranch Target Address Prediction In the RISC-V architecture, branch target address prediction refers to predicting the target address of unconditional jump instructions (e.g., jal, jalr). Since these instructions always perform a jump operation, we need to predict their target address.\nBranch Target Buffer (BTB) BTB is a common method for predicting target addresses. Its core idea is to use a cache to store the target addresses of past unconditional jump instructions. When encountering the same unconditional jump instruction again, the BTB can be checked to see if there is a record for that instruction. If so, the recorded target address is used as the predicted target address for the current execution.\nThe diagram below illustrates this:\nPredicting Instruction Types As we know, in branch prediction, for conditional branch instructions, we need to predict their direction, and for unconditional jump instructions, we need to predict their target. However, there is a problem: when we get a PC that needs to be predicted, we don’t know whether the corresponding instruction is a normal instruction or a branch instruction. Therefore, we cannot predict it.\nHow to solve this? One way is to predict the behavior of the instruction after fetching it. But fetching from ICache or Memory may take several cycles, which is a major drawback of this method.\nA better way is to directly predict the type of instruction. After getting a PC, we can directly predict whether this instruction is a branch instruction and predict its behavior. In this way, we don’t have to wait for fetching to complete, and the predicted result can also guide the CPU to fetch from the correct location.\nThe method of type prediction can be similar to BTB, where a field in the cache contains the type of instruction for use in the next prediction.\nGeneral Steps of Branch Prediction Through the introduction in this section, we can summarize the general steps of branch prediction:\nGet the PC. Predict whether it is a branch instruction. If it is a conditional branch instruction, predict its direction and target. If it is an unconditional jump instruction, predict its target. Note that since predicting the type of instruction is required in prediction, and we haven’t obtained the specific content of the instruction, predicting the target of a conditional branch instruction also becomes our task.\n","categories":["Shanshan BPU Basic Design"],"description":"The branch prediction unit, as the name suggests, needs to perform a basic task—branch prediction. Before delving into the branch prediction unit, it is necessary to understand what branch prediction is.","excerpt":"The branch prediction unit, as the name suggests, needs to perform a …","ref":"/xs-bpu/en/docs/basic/00_bp/","tags":["Shanshan","BPU","Branch Prediction"],"title":"What is Branch Prediction"},{"body":"为何需要分支预测？ 分支预测主要有两方面的原因：一是程序的执行流中含有分支指令，二是高性能处理器使用流水线设计。\n程序的执行流中含有分支指令 int x = 10; int y = 20; int result = 0; if (x \u003e= y) { result = x + y; } else { result = x - y; } 上述是一段C语言代码，这段代码首先定义了三个变量 x, y 和 result，然后根据 x 和 y 值的大小情况对result进行赋值。可以发现，程序在前三行对变量进行赋值时是顺序往下执行的，而在第 5 行时，由于 if 指令的出现，程序产生了分支，从第 5 行直接跳转到了第 8 行继续运行，这就造成了程序执行的分支。\n翻译成 RISC-V 汇编之后的代码如下：\nli a0, 10 # x = 10 li a1, 20 # y = 20 li a2, 0 # result = 0 blt a0, a1, else_branch # 如果 x \u003c y，则跳转到 else_branch add a2, a0, a1 # 否则执行 result = x + y j end # 跳转到 end else_branch: sub a2, a0, a1 # result = x - y end: 可以发现程序依然保持着先前的分支行为，在代码的前三行，指令顺序执行，之后，在程序的第 5 行，出现了一条特殊指令blt ，我们称之为分支指令，它会根据 x 和 y 的大小关系决定指令流顺序往下执行还是跳转到其他地方，该指令的出现导致程序的执行出现了分支。\n高性能处理器使用流水线设计 因此有了分支预测的概念，如果我们可以在执行结果产生之前，精准的预测出下一条指令的地址，便可以使处理器一直高效的运行下去。\n分支预测的可行性 为什么可以进行分支预测呢？我们看接下来一个例子\nif (data \u003e= 128) sum += data; 假设该条指令将被重复执行，并且 data 从 0 开始递增，即 data = 0, 1, 2, 3 … 128, 129…，接下来我们可以分析每次执行这条指令的行为。\nT = branch taken N = branch not taken data = 0, 1, 2, 3, s, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ... branch = N N N N N ... N N T T T ... T T T ... = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT (easy to predict) 可以看出，在前 128 次，该分支都是 Not Taken （条件不成立）的，但在 128 次之后，分支一直都是 Taken （条件成立）的。如果我们用最简单的方法根据上一次是否 Taken，来预测本次是否 Taken，在整个预测过程中，我们只会产生一次预测错误。\n上述现象的产生是源于一个基本事实的——分支指令是否跳转与该条指令以往的跳转行为有关联，我们通过总结以往跳转的规律，便可以对本次跳转产生较为精准的预测，这也使得分支预测变得可能。\n事实上，分支指令的跳转还会与其他分支指令的跳转情况等因素相关，充分发掘有效信息，产生精准预测结果，是分支预测的主要任务之一。\n分支预测的基本类型 在RISC-V 中，分支指令包含两种类型：\n条件分支指令 (beq, bne, blt, bge, bltu, bgeu) 对于这类指令，是否跳转时候其中的条件决定，跳转目标可以在指令中获取，因此我们需要预测该指令是否跳转 无条件跳转指令 (jal, jalr) 对于这类指令，每当执行到它时是总是跳转，但跳转目标有可能由寄存器执行，因此我们需要预测该指令的跳转地址。 幸运的是，由于RISC-V架构设计的简洁，我们不需要处理条件跳转指令，每一个需要我们预测的跳转指令都是无条件的，这也给我们的设计提供了便利。\n上述分析中，我们可以总结出分支预测的两大基本类型——方向预测 和 目标地址预测。\n分支指令的方向预测 分支指令的方向预测对应到 RISC-V 指令就是条件分支指令，我们需要预测它是否需要跳转，也就是分支的方向，因此叫做方向预测。\n两位饱和计数器 方向预测有一种非常简单并且十分高效的预测方法，叫做叫做饱和计数器法，它的大致思想可以参考下图。\n两位饱和计数器被视作一个状态机，我们为每一个分支指令都维护这样一个状态机器，当一条分支指令发生跳转时，对应上图中的状态向右移动，否则状态向左移动。这样在下次遇到这条分支指令时，我们首先查找到它的两位饱和计数器，如果状态更偏右则预测其跳转，否则预测其不跳转。\n当然，为每一条分支指令都维护一个两位饱和计数器是不现实的，因此在实际中，我们通常会采取PC部分位，或哈希的方法来索引两位饱和计数器，如下图所示。\n分支历史 分支历史是方向预测中非常常用的数据，也是大多分支预测算法的基础，因为分支历史是指令以往跳转行为最直接的展示。\n分支历史有以下两种基本类型：\n局部分支历史 为每一条分支指令维护一组寄存器，记录该条指令的历史跳转情况 例如： 0101000000101 （0代表不跳转，1代表跳转） 全局分支历史 所有指令共用一组寄存器，记录程序执行过程中的分支跳转情况 例如：\n$$ beg a0, a1, label1 不跳转 记录0 bne a1, a2, label2 不跳转 记录0 -\u003e beq a2, a3, label4 跳转 记录1$$​执行完三条不同的分支指令后，全局分支历史变为 001\n分支指令的目标地址预测 RISC-V架构中，分支指令的目标地址预测是指对无条件跳转指令（如jal、jalr）的跳转目标地址进行预测。由于这类指令总是执行跳转操作，我们需要预测其跳转的目标地址。\n分支目标缓存（BTB） BTB 是目标地址预测的一种常用方法，它的核心思想是使用一个缓存来存储以往无条件跳转指令的跳转目标，之后如果再次执行到这一条无条件跳转指令，就可以查看BTB中是存在该指令的记录，将记录的跳转目标作为本次预测的跳转目标。\n示意图如下\n预测指令类型 我们已经知道，在分支预测中，对于条件分支指令来说，我们需要预测其方向，对于无条件跳转指令来说，我们需要预测其跳转目标。但是出现了一个问题，拿到一个需要预测的PC之后，我们并不清楚这个PC所对应的指令是什么，也就是说我们根本不知道当前指令到底是一条普通指令还是一条分支指令，也就无法进行预测了。\n如何解决呢？可以采取的一种方法是，在取出指令之后，对这条指令的行为进行预测。但取指需要从 ICache 或 Memory 中去取，有可能会耗费多个周期，这是这种方法的一大弊端。\n更好的一种方法是，直接预测指令的类型，拿到一个PC之后，可以直接预测出这条指令是否是分支指令，并对指令行为进行预测。这样一来，我们就没有必要等待取指完成，并且预测出来的结果还可以指导 CPU 到什么地方去取指。\n类型预测的方法，可以与BTB相似，在缓存中的某个字段中加入指令的类型，以供下一次预测使用。\n分支预测的一般步骤 通过本节知识的介绍，我们可以总结出分支预测的一般步骤\n获取 PC 预测是否是分支指令 如果是条件分支指令，预测其跳转方向和跳转目标 如果是无条件跳转指令 ，预测其跳转目标 注意，由于在预测中需要预测指令类型，没有获取到指令具体内容，所以对于条件分支指令来说，预测它的跳转目标也变成了我们的工作。\n","categories":["香山 BPU 基础设计"],"description":"分支预测单元，顾名思义需要完成一件基本的任务——分支预测。在深入探讨分支预测单元之前，必须要了解分支预测是什么。","excerpt":"分支预测单元，顾名思义需要完成一件基本的任务——分支预测。在深入探讨分支预测单元之前，必须要了解分支预测是什么。","ref":"/xs-bpu/docs/basic/00_bp/","tags":["香山","BPU","分支预测"],"title":"何为分支预测"},{"body":"在处理器设计中，一个设计优秀的分支预测器（BPU）是提高处理器性能的关键部件，它负责指导处理器的取指，确定处理器下一步要到哪里取出指令并执行。BPU 是一条指令生命周期最开始的地方，因此想要深入探究一个高性能处理器，从BPU开始是一个很好的选择。\n在香山中亦是如此，香山作为一个乱序六发射的高性能处理器，自然需要一个准确度高、预测效率高的分支预测单元。而一个分支预测单元的设计实际中往往需要考虑很多因素，例如时序、结构的复杂度、占用的硅片面积、预测的准确度、预测错误时恢复的速度等等。香山处理器的分支预测单元在这些因素之间都做到了比较好的权衡，通过许多巧妙的设计使其拥有了很高的分支预测效率与准确度，为后端的指令供给提供了基础的保障。\n在本节中，我们将会介绍香山预测单元的基础设计，通过阅读本节，可以掌握如下知识：\n分支预测的基本概念 香山分支预测单元的基本预测单位——分支预测块 香山分支预测单元的对外交互接口 香山分支预测单元的基本结构 香山分支预测单元的基础时序 任何参与BPU众包验证工作的同学，都需要率先阅读本节，以便对香山分支预测单元有基础了解。\n","categories":["香山 BPU 基础设计"],"description":"本文档介绍了香山分支预测单元的基础设计思想，通过阅读本文档，你不需要得知具体的信号名称和代码细节便可理解香山分支预测单元的大致工作流程。","excerpt":"本文档介绍了香山分支预测单元的基础设计思想，通过阅读本文档，你不需要得知具体的信号名称和代码细节便可理解香山分支预测单元的大致工作流程。","ref":"/xs-bpu/docs/basic/","tags":["香山","BPU","分支预测"],"title":"香山分支预测单元（BPU）基础设计"},{"body":"Branch Prediction Block Concept For a general branch predictor, it usually predicts the relevant information of an instruction corresponding to a given PC, such as whether it is a conditional branch instruction or a jump instruction. For conditional branch instructions, it predicts whether it will jump, while for jump instructions, it predicts the jump target. However, predicting instructions one by one is inefficient, leading to slow instruction supply in the frontend.\nIn contrast, the prediction method used in Xiangshan is to predict a block of instructions each time. That is to say, given a PC, Xiangshan will predict a branch prediction block starting from this PC, including the situation of several subsequent instructions, such as whether there is a branch instruction, the position of the branch instruction, whether there is a jump, and the jump target.\nThis prediction method can predict multiple instructions at once and send the prediction results to the fetch unit (IFU) to guide the IFU to fetch instructions. In addition, since the IFU needs to consider the performance of cache lines, it can fetch multiple instructions at once based on the prediction block, thereby improving throughput efficiency.\nAfter the prediction block is generated, the branch prediction block will also generate the PC to which it jumps after executing this prediction block, and then the BPU will continue to generate the next branch prediction block based on this PC.\nHere’s a simple example:\nAs shown in the above figure, when the PC reaches 0x20000118, the BPU goes through the following steps:\nThe BPU learns that the PC is 0x20000118. The BPU generates a branch prediction block starting from 0x20000118, with the following approximate contents: In the next several instructions, The third instruction is a conditional branch instruction. For this conditional branch instruction, it predicts that it will be taken. The address to which it jumps is 0x20000110. The BPU sets the PC to 0x20000110 and continues to generate the next branch prediction block. This is the basic prediction process of the Shanshan BPU using branch prediction blocks.\nMultiple Predictors, Multiple Pipeline Structure The figure below shows the overall architecture of the Xiangshan BPU, where we need to focus on two main aspects:\nMultiple Predictors To ensure prediction accuracy, Xiangshan BPU uses multiple predictors, and these predictors collectively generate the BPU’s prediction results. For example, FTB generates basic prediction results for subsequent predictors to use, while TAGE produces more accurate prediction results for conditional branch instructions, and so on. Multiple Pipelines To meet the requirements of high performance, Xiangshan BPU adopts a pipeline design. Various predictors are at different pipeline levels. Among them, the uFTB (also known as uBTB in the figure) predictor is at the first pipeline level, capable of generating prediction results in one cycle. The other predictors need 2-3 cycles to generate prediction results. Although the prediction time is longer, the prediction results are relatively more accurate. However, if it takes three cycles to get the prediction result and start predicting based on the new result, this design inevitably leads to performance loss. Because of this, it takes three clock cycles to complete one prediction.\nTo be able to get the prediction results of some predictors in the first and second cycles, we set up three prediction result channels and output the prediction results of the three pipeline levels simultaneously, as shown in the figure below.\nFetch Target Queue (FTQ) Storing Branch Prediction Results Although the BPU can provide prediction results in the form of branch prediction blocks and the IFU can fetch multiple instructions at once, there is still a performance gap between them. In general, the BPU generates prediction results faster.\nTherefore, a Fetch Target Queue (FTQ) is added between the BPU and the IFU as a buffer. The FTQ is essentially a queue used to store individual data items. The prediction results generated by the BPU are first stored in the FTQ, and then fetched by the IFU from the FTQ, as shown in the figure below.\nWhenever the BPU generates a prediction block, the prediction block is placed at the head of the FTQ. When the IFU is idle, it will fetch the next prediction block from the tail of the FTQ. The diagram below illustrates this process.\nIn Xiangshan, the FTQ’s functionality goes far beyond this. Referring to the FTQ’s external interface in the figure above, it is also responsible for sending prefetch information to the ICache, storing various training information of the BPU, analyzing redirection information and update information sent from the fetch module and the backend execution module, sending update requests to the BPU, and even updating the basic data structure of the FTB predictor in the FTQ.\nBPU Prediction Result Redirection As mentioned earlier, the Xiangshan branch prediction results have three channels, which simultaneously output the prediction results of stages s1, s2, and s3. How does the FTQ use the prediction results of the three stages?\nLet’s start from exploring the timing of the pipeline, as shown in the figure below.\nIn the first cycle, a new PC 0x4 is fetched, and the predictor (called uFTB) that can produce a prediction result within one cycle outputs its prediction result at the s1 interface, indicating the next PC as 0xf, with no output from other interfaces yet. In the second cycle, the PC is set to 0xf, and uFTB also generates a prediction result of 0xf, which is sent out from the s1 channel. At the same time, the two-cycle predictor generates the prediction result for the previous address 0x4, which is sent out from the s2 channel. However, a problem arises here: in the second cycle, the prediction result generated by s2 is 0x4, but the prediction result for 0x4 has already been output by s1 in the previous cycle and placed in an entry in the FTQ. In other words, the prediction result generated by s2 has already been generated by s1. The difference is that the result from s2 is generated by the two-cycle predictor, making it more accurate.\nTherefore, what we need to do is not to place a new FTQ entry based on the prediction result from s2 but to compare the prediction results from s2 and the previous cycle’s s1 prediction results. If there is a difference, then overwrite the FTQ entry placed by the previous stage’s s1 interface.\nSo we add two additional signal lines to the s2 and s3 channels, which we call redirect signals. If this signal is valid, it indicates that there is a difference between the prediction result of this stage and the previous prediction result, and it is necessary to overwrite an FTQ entry from before. The structure is shown in the diagram below.\nAt the time corresponding to the second cycle of the pipeline in the structural diagram, the s1 channel has already placed a branch prediction block result with an address of 0x4 into the FTQ. At this time, the s2 prediction result is generated, and the BPU finds that the s2 prediction result is different from s1, so the redirect interface for this cycle is set to valid. The FTQ will use the s2 channel’s prediction result to overwrite the FTQ entry previously storing the 0x4 prediction result.\nAt this time, although the s1 channel has also generated a branch prediction block with 0xf as the head, it is obviously an incorrect prediction result generated by s1 based on the PC of the first cycle. Therefore, at this time, the s1 result can be directly discarded.\nIn the third cycle, s1 starts a new round of prediction with the correct prediction result indicated by s2, the new PC 0x8. After that, if no prediction errors are detected by the s2 and s3 channels, the pipeline will continue to run at full capacity.\nBPU Redirect Requests No matter how accurate a branch predictor is, it is not always correct. This inaccuracy can lead to incorrect instructions being filled in the subsequent pipeline. Therefore, there needs to be a mechanism to correct this, and this mechanism is redirection. When an instruction is executed by the backend execution module, the true behavior of this instruction is determined. At this time, if the backend execution module detects a branch prediction error, it will issue a redirect request to restore the processor’s state to the state before executing the incorrect instruction. For us, we only need to pay attention to how the BPU and FTQ restore the state when redirecting.\nIn addition to redirect requests from the backend, the Shan Mountain processor will perform a simple analysis of the instruction after it is fetched by the IFU to detect the most basic prediction errors. The specific process is as follows: after the FTQ sends a fetch request to the IFU, it will wait for the IFU to return the pre-decoded information (pre-decoding is the IFU’s simple decoding of the instruction, such as whether it is a jump instruction, what is the target of the jump). The FTQ will write the pre-decoded information back to a field in the corresponding entry in the FTQ and will also analyze the pre-decoded information. If a prediction error is detected, it will generate an IFU redirect request.\nRedirect requests from the backend execution module do not need to be generated by the FTQ but are directly sent from the backend to the FTQ for processing. The FTQ will forward the generated IFU redirect request and the backend redirect request to the BPU’s redirect interface. If both are valid in the same cycle, the FTQ will choose to forward the backend redirect request.\nThe BPU with the added redirect interface is shown in the diagram below.\nBPU Update Requests The current BPU already has the ability to correct errors, but there is still a problem: the data in the BPU cannot be updated. If it is impossible to obtain information such as the location, type, whether a jump occurred, and the jump address of the instruction, the BPU will not be trained and the accuracy will be greatly reduced.\nTo obtain this information, we still need to rely on the Fetch Target Queue (FTQ) because it can not only interact with the IFU to obtain instruction-related information but also interact with the backend to obtain execution-related information. Therefore, there will be an update request channel directly connecting the FTQ to the BPU.\nWhen the backend completes the execution of an entry in the FTQ, the entry is marked as committed. Next, the FTQ forwards the update information of this entry to the BPU through the Update channel.\nSummary Through this section, we have learned about all the main interfaces required for BPU external interaction and the role of FTQ in BPU. With the BPU equipped with prediction result interfaces, redirect interfaces, and update interfaces, it can already support all external interactions of the BPU. Next, we will delve deeper into the internals of the BPU.\n","categories":["Xiangshan BPU Basic Design"],"description":"This section introduces the basic ideas and working principles of the Xiangshan Branch Prediction Unit (BPU), including the use of branch prediction block concepts, multiple predictors, multiple pipeline structures, and the role of the Fetch Target Queue (FTQ), explaining the main interfaces of BPU for external interaction.","excerpt":"This section introduces the basic ideas and working principles of the …","ref":"/xs-bpu/en/docs/basic/01_xsbpu_basic/","tags":["Branch Prediction Block","Redirect Request","Update Request","Fetch Target Queue (FTQ)"],"title":"Basic of the Xiangshan Branch Prediction Unit (BPU)"},{"body":"This document will describe the important structures and external interfaces in the BPU, with a description granularity that delves into the code level. The structures described in the document will be consistent with the Chisel version of the Xiangshan branch prediction unit code, and the signal structures and names also come from the Chisel version code.\nThis document is intended for those who have already understood the basic design of the Xiangshan branch prediction unit and want to delve into the signal details. You can selectively read according to the content needed for verification or refer to it at any time.\nAmong them, the FTB entry and the full prediction result interface involve the way the BPU generates prediction results, which is recommended for every reader to read.\n","categories":["Xiangshan BPU Interface"],"description":"This document will describe the important structures and external interfaces in the BPU, with a description granularity that delves into the code level. By reading this document, you can understand the role of each signal in the Xiangshan branch prediction unit, comprehend the specific implementation methods of various requests, and gain a functional understanding in conjunction with the code.","excerpt":"This document will describe the important structures and external …","ref":"/xs-bpu/en/docs/ports/","tags":["Xiangshan","BPU"],"title":"Important Structures and Interface Documentation"},{"body":"Branch Prediction Redirection（BranchPredictionRedirect） Interface Definition：src/main/scala/xiangshan/frontend/FrontendBundle.scala\nInterface Type：BranchPredictionRedirect\nThis interface defines the redirection requests from the branch prediction unit, mainly used to redirect the state of the branch predictor.\nThe BranchPredictionRedirect interface inherits from the Redirect interface and includes many signals, only a subset of which are used by the BPU redirection. The documented structure contains only the interfaces used by the BPU.\nRedirection requests have two sources: those generated by comparing IFU pre-decode information and those generated during the backend execution process.\nIn redirection requests, the key information is cfiUpdate, which corresponds to a control flow instruction. This information pertains to an instruction where the BPU made a prediction error. For example, if the BPU indicates that the third instruction in the prediction block is a normal instruction with no jump, but the pre-decode shows it is an unconditional jump instruction, a prediction error has occurred. In this case, the FTQ generates a redirection request. The cfiUpdate in the redirection request corresponds to this unconditional jump instruction.\nThe information in cfiUpdate can be broadly classified into three types:\nInformation about the corresponding instruction and its execution status. This includes the slot (shift) and PC of the instruction in the prediction block, the type-related information of the instruction (pd), and the execution status, such as jump target and whether it jumps. History maintenance related information. The redirection request contains branch history information corresponding to the prediction block of the instruction to help the BPU restore branch history. folded_hist represents the global folded history, histPtr represents the global history pointer, and other information assists in maintaining branch history. For detailed information, refer to the BPU Top-Level Module. RAS maintenance related information. For detailed meaning, refer to the RAS sub-predictor documentation. The meaning of level is whether the redirection includes this instruction. If not included, the redirection request receiver will assume the instruction has been executed, and the next prediction will start from the next instruction. The BPU top-level will default to not including this instruction, and upon receiving a redirection request, it will include the execution status of this instruction in the branch history.\nThe detailed signal list for the redirection interface is as follows:\nlevel: Indicates whether the redirection request includes the current position. Low means redirection after this position, high means redirection at this position.\nInterface Type: UInt(1.W) cfiUpdate: Control flow instruction update information\nInterface Type: CfiUpdateInfo\nInterface List\npc: The PC of the instruction corresponding to the redirection request Interface Type: UInt(VaddrBits.W) pd: Pre-decode information of the redirection instruction isRVC: Whether it is a compressed instruction Interface Type: Bool isCall: Whether it is a function call Interface Type: Bool isRet: Whether it is a function return Interface Type: Bool target: Target address of the redirection request instruction Interface Type: UInt(VaddrBits.W) taken: Whether the redirection request instruction is taken Interface Type: Bool shift: Slot in which the redirection request instruction is located, 0 if it is a normal instruction. Interface Type: UInt((log2Ceil(numBr)+1).W) addIntoHist: Whether to include the execution information of the redirection request instruction in the branch history. Interface Type: Bool folded_hist: Folded history corresponding to the redirection request Interface Type: AllFoldedHistories(foldedGHistInfos) afhob: Oldest bit of the branch history corresponding to the redirection request instruction Interface Type: AllAheadFoldedHistoryOldestBits(foldedGHistInfos) lastBrNumOH: Last branch position corresponding to the redirection request Interface Type: UInt((numBr+1).W) histPtr: Global history pointer to be restored by the redirection request Interface Type: CGHPtr ssp: RAS speculative stack top pointer at the commit stack position corresponding to the redirection request instruction Interface Type: UInt(log2Up(RasSize).W) sctr: RAS speculative stack top recursion counter corresponding to the redirection request instruction Interface Type: UInt(log2Up(RasCtrSize).W) TOSW: RAS speculative stack (queue) write pointer corresponding to the redirection request instruction Interface Type: CGHPtr TOSR: RAS speculative stack (queue) read pointer corresponding to the redirection request instruction Interface Type: CGHPtr NOS: RAS stack top counter corresponding to the redirection request instruction Interface Type: CGHPtr Branch Prediction Update（BranchPredictionUpdate） Interface Definition: src/main/scala/xiangshan/frontend/FrontendBundle.scala\nThis interface defines the update requests for the branch predictor, mainly used to update the state of the branch predictor. The document lists only the interfaces used in the BPU.\nUpdate requests correspond to each branch prediction block. When a branch prediction block in the FTQ has been executed, the FTQ generates an update request for this prediction block to train the predictor. Thus, an important role of the update request is to feed back the actual execution status of the instructions to the BPU. Additionally, in the Xiangshan branch prediction unit, the update request is responsible for updating FTB entries.\nThe information in the update request can be broadly classified into four categories:\nPC: Indicates the starting address of the prediction block, specifying which prediction block the update request corresponds to FTB Entry Update Information: The update channel contains an FTB entry structure (ftb_entry), outputs the newly generated FTB entry from the FTQ, and indicates whether it is the same as the old FTB entry (old_entry) Actual Execution Status Information of Instructions: The update channel indicates the execution status of branch and unconditional jump instructions in the prediction block, and provides the address and final target of control flow instructions (i.e., instructions that jump) Predictor-Related Data Corresponding to the Prediction Block: Contains spec_info and meta information (refer to the BPU Global Interface Documentation for details) The interface list of the update request is as follows:\npc PC of the update request (starting address of the prediction block) Interface Type：UInt(VAddrBits.W) ftb_entry Updated FTB entry Interface Type：new FTBEntry() Interface List：See（FTBEntry） old_entry Whether the updated FTB entry is the same as the old FTB entry Interface Type：Bool() br_taken_mask Mask indicating whether each slot instruction in the prediction block jumps Interface Type：Vec(numBr, Bool()) mispred_mask Mask indicating prediction errors in the prediction block. The first and second bits indicate whether the two conditional branch instructions were mispredicted, and the third bit indicates whether the unconditional jump instruction was mispredicted. Interface Type：Vec(numBr+1, Bool()) jmp_taken Unconditional jump instruction triggered in the prediction block Interface Type：Bool() cfi_idx Index of the control flow instruction in the prediction block Interface Type：ValidUndirectioned(UInt(log2Ceil(PredictWidth).W)) full_target Jump target of the prediction block (starting address of the next prediction block) Interface Type：UInt(VAddrBits.W) spec_info Last stage speculative information corresponding to the prediction block Interface Type：new SpeculativeInfo Interface List：（Only foled_hist is used） folded_hist Global folded history Interface Type：AllFolededHistories(foldedGHistInfos) meta Last stage meta information corresponding to the prediction block Interface Type：UInt(MaxMetaLength.W) ","categories":["Xiangshan BPU Interfaces"],"description":"Redirection and update requests are the two main types of information sent from the FTQ to the BPU. This section details the specific structure of these request interfaces.","excerpt":"Redirection and update requests are the two main types of information …","ref":"/xs-bpu/en/docs/ports/01_redirect_and_update/","tags":["Redirection Request","Update Request"],"title":"Redirection and Update Interfaces"},{"body":"Introduction to uFTB uFTB is the first predictor among all the predictors in Xiangshan, and it serves as the cornerstone for other predictors to generate prediction results. uFTB works in the s1 stage. It can generate prediction results within the current cycle after obtaining s1_pc and output them in the s1 channel, without modifying other channels. It provides the position of the branch instruction and the target of the instruction. Subsequent predictors will further predict based on this result.\nIts essence is an FTB item cache, which stores FTB items, and the basic prediction result will be directly generated from the read-out FTB item.\nTherefore, before you start reading the document, make sure you understand the FTB items and their meanings, as well as the specific details of the prediction result interface.\nFunctionality of uFTB Cache FTB items and generate one-cycle prediction results: uFTB maintains a small FTB item cache. After receiving PC, it reads out the FTB item corresponding to the PC within one cycle and generates an s1 stage prediction result from the FTB item. Maintain two-bit saturating counters to provide basic conditional branch results: uFTB maintains two-bit saturating counters for each line of the FTB item cache. The direction prediction result is reflected in the prediction result output of uFTB. Update the FTB cache and two-bit saturating counters based on update requests uFTB Cache Structure As mentioned above, uFTB is essentially a small cache that stores FTB items. Its approximate structure is shown in the figure below.\nIn the current version of Xiangshan, uFTB has a total of 32 cache lines, each cache line is called FauFTBWay, and one FTB item can be stored in each cache line.\nWhen s1 pipeline is valid, uFTB will use s1_pc to determine which item of the uFTB cache to read out. The cache is indexed based on the tag field in PC, which is defined as pc[16:1], i.e., taking 16 bits from PC as an identifier to match a certain line in the cache.\nEach line in the cache, i.e., the data request interface in FauFTBWay, has three items:\nreq_tag: Input tag identifier extracted from pc resp: Output the FTB item stored in this line resp_hit: Output indicates whether the FTB item in this line matches req_tag uFTB connects the tag to the data request interface of each line in the cache and selects the hit FTB item based on the resp_hit signal. Subsequent steps will generate a complete prediction result based on this FTB item. Two-Bit Saturating Counters uFTB maintains two-bit saturating counters for each line of the FTB item cache. As we know, an FTB item can store up to two conditional branch instructions, so each line’s two-bit saturating counters also have two, responsible for providing rough prediction results for the conditional branch instructions in them.\nWhen indexing the FTB item, uFTB also indexes the corresponding two-bit saturating counters.\nWhen the prediction result is generated, it will be based on the FTB item and the contents of the two two-bit saturating counters corresponding to it.\nPrediction Result Generation After indexing the corresponding FTB item and two two-bit saturating counters based on s1_pc, uFTB needs to generate a prediction result based on them. The prediction result generated by uFTB will be outputted through the s1 channel when s1 pipeline is valid, and will not be modified for s2 and s3 channels.\nThe signal generation method in the s1 prediction result can refer to the following list:\nhitWhether the FTB item hits Generation method: resp_hit signal in FauFTBWay, one of them is valid slot_valids: Slot valid bit, indicating whether each slot in the ftb item is valid targets: Jump target address corresponding to each slot offsets: The offset of each instruction in the slot relative to the starting address of the prediction block is_jal: Whether the prediction block contains a jal instruction is_jalr: Whether the prediction block contains a jalr instruction is_call: Whether the prediction block contains a call instruction is_ret: Whether the prediction block contains a ret instruction last_may_be_rvi_call: Signal indicating that the last slot in the prediction block may be an RVI type call instruction is_br_sharing: The signal stored in the last slot (tailSlot) indicates a conditional branch instruction. Generation: Exported from the corresponding field in the FTB entry. fallThroughErr: The pftAddr recorded in the FTB entry is incorrect. Generation: Compare whether the end address represented by pftAddr is greater than the start address of the predicted block. If it is smaller, an error has occurred, and this signal is set to valid. This situation may occur when the PC indexes an incorrect FTB entry. fallThroughAddr: The end address of the predicted block. Generation: If fallThroughErr is invalid, it is generated based on pftAddr. Otherwise, it is set to the start address + prediction width. br_taken_mask: Branch prediction result, with each branch (slot) corresponding to a bit indicating whether the branch is predicted as taken. Generation: Generated based on the always_taken field in the FTB entry and the result indicated by the two-bit saturating counter. jalr_target: The jump target of the jalr in this predicted block. -Generation: From the jump target in the tailSlot of the FTB entry. uFTB Update The update of uFTB involves updating the FTB entry cache and the two-bit saturating counter, with the update content obtained through the update interface.\nIn the uFTB predictor, the reading and writing of the cache and the two-bit saturating counter do not conflict, so we do not need to consider timing conflicts between reading and updating and can consider them as two independent parts.\nFTB Cache Update The update process of the FTB cache is simple. The update channel has already specified the PC and the newly generated FTB entry, so it only needs to be written to the specified position in the cache.\nFTB cache updating requires two cycles:\nIn the first cycle, calculate the following based on the signals in the update: Which row in the cache corresponds to the update request The PC extracted from the update request is sent to the update request channel of FauFTBWay, and the hit signal returned by each row is calculated. In the second cycle, update according to the position calculated in the first cycle. If no row is hit, uFTB will use a pseudo-LRU replacement algorithm to select the row to be written. Specifically, the ftb_entry signal group in the update channel contains the complete information of the new FTB entry, which is sent to the cache row that needs to be updated. Two-Bit Saturating Counter Update The update of the two-bit saturating counter needs to be combined with the actual execution of the subsequent program and the branch instruction information recorded in the FTB entry, which can be obtained from the update channel.\nThe update of the two-bit saturating counter also requires two cycles:\nIn the first cycle, calculate which two-bit saturating counter corresponding to the conditional branch instruction in the slot needs to be updated, which needs to meet the following conditions: The current branch instruction slot is valid and contains a conditional branch instruction. The current branch instruction slot is not marked as always_taken. (Because after being marked as always_taken, the result of the two-bit saturating counter will not be used.) The current branch instruction slot is not after the branch instruction slot where an actual jump occurred. In the second cycle, update the saturating counter based on the mask generated in the first cycle and the br_taken_mask information in the update channel. Interface List 信号类型 信号位 信号名 信号描述 input clock 输入时钟 input reset 复位信号 input [35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input [40:0] io_in_bits_s0_pc_0 输入位s0_pc 的 第0个复制 input [40:0] io_in_bits_s0_pc_1 同上 第1个 input [40:0] io_in_bits_s0_pc_2 同上 第2个 input [40:0] io_in_bits_s0_pc_3 同上 第3个 output [40:0] io_out_s1_pc_0 输出s1_pc 的 第0个复制 output [40:0] io_out_s1_pc_1 同上 第1个 output [40:0] io_out_s1_pc_2 同上 第2个 output [40:0] io_out_s1_pc_3 同上 第3个 output io_out_s1_full_pred_0_br_taken_mask_0 solt 0 是否被预测为 always taken output io_out_s1_full_pred_0_br_taken_mask_1 solt 1 是否被预测为 always taken output io_out_s1_full_pred_0_slot_valids_0 solt 0 是否启用 output io_out_s1_full_pred_0_slot_valids_1 solt 1 是否启用 output [40:0] io_out_s1_full_pred_0_targets_0 solt 0 对应的跳转目标地址 output [40:0] io_out_s1_full_pred_0_targets_1 solt 1 对应的跳转目标地址 output [3:0] io_out_s1_full_pred_0_offsets_0 solt 0 中分支指令相对于地址块起始pc的偏移 output [3:0] io_out_s1_full_pred_0_offsets_1 solt 1 中分支指令相对于地址块起始pc的偏移 output [40:0] io_out_s1_full_pred_0_fallThroughAddr 预测块的结束地址 output io_out_s1_full_pred_0_is_br_sharing solt 1（无条件跳转）是否被共享为有条件跳转指令 output io_out_s1_full_pred_0_hit output io_out_s1_full_pred_1_br_taken_mask_0 类似 io_out_s1_pc_1 io_out_s1_full_pred_0的复制 output io_out_s1_full_pred_1_br_taken_mask_1 output io_out_s1_full_pred_1_slot_valids_0 output io_out_s1_full_pred_1_slot_valids_1 output [40:0] io_out_s1_full_pred_1_targets_0 output [40:0] io_out_s1_full_pred_1_targets_1 output [3:0] io_out_s1_full_pred_1_offsets_0 output [3:0] io_out_s1_full_pred_1_offsets_1 output [40:0] io_out_s1_full_pred_1_fallThroughAddr output io_out_s1_full_pred_1_is_br_sharing output io_out_s1_full_pred_1_hit output io_out_s1_full_pred_2_br_taken_mask_0 同上 output io_out_s1_full_pred_2_br_taken_mask_1 output io_out_s1_full_pred_2_slot_valids_0 output io_out_s1_full_pred_2_slot_valids_1 output [40:0] io_out_s1_full_pred_2_targets_0 output [40:0] io_out_s1_full_pred_2_targets_1 output [3:0] io_out_s1_full_pred_2_offsets_0 output [3:0] io_out_s1_full_pred_2_offsets_1 output [40:0] io_out_s1_full_pred_2_fallThroughAddr output io_out_s1_full_pred_2_is_br_sharing output io_out_s1_full_pred_2_hit output io_out_s1_full_pred_3_br_taken_mask_0 同上 output io_out_s1_full_pred_3_br_taken_mask_1 output io_out_s1_full_pred_3_slot_valids_0 output io_out_s1_full_pred_3_slot_valids_1 output [40:0] io_out_s1_full_pred_3_targets_0 output [40:0] io_out_s1_full_pred_3_targets_1 output [3:0] io_out_s1_full_pred_3_offsets_0 output [3:0] io_out_s1_full_pred_3_offsets_1 output [40:0] io_out_s1_full_pred_3_fallThroughAddr output io_out_s1_full_pred_3_fallThroughErr output io_out_s1_full_pred_3_is_br_sharing output io_out_s1_full_pred_3_hit output [222:0] io_out_last_stage_meta 输出最后阶段的元信息 io_out_last_stage_meta = {213’h0, resp_meta_pred_way_r_1, resp_meta_hit_r_1} input io_ctrl_ubtb_enable 控制ubtb是否启用 input io_s0_fire_0 输入s0_fire_0，与 io_out_s1_pc_0 \u003c= io_in_bits_s0_pc_0 的时钟门控相关 input io_s0_fire_1 输入s0_fire_1 input io_s0_fire_2 输入s0_fire_2 input io_s0_fire_3 输入s0_fire_3 input io_s1_fire_0 输入s1_fire_0 input io_s2_fire_0 输入s2_fire_0 input io_update_valid 更新有效性 input [40:0] io_update_bits_pc 传回的预测块pc（用于指示更新的预测块） input [3:0] io_update_bits_ftb_entry_brSlots_0_offset solt 0 中分支指令相对于地址块起始pc的偏移 input [11:0] io_update_bits_ftb_entry_brSlots_0_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_brSlots_0_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_brSlots_0_valid 是否启用 input [3:0] io_update_bits_ftb_entry_tailSlot_offset solt 1 中分支指令相对于地址块起始pc的偏移 input [19:0] io_update_bits_ftb_entry_tailSlot_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_tailSlot_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_tailSlot_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_tailSlot_valid 是否启用 input [3:0] io_update_bits_ftb_entry_pftAddr Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。 input io_update_bits_ftb_entry_carry pc+pft时是否产生进位 input io_update_bits_ftb_entry_always_taken_0 是否预测为总是跳转 input io_update_bits_ftb_entry_always_taken_1 是否预测为总是跳转 input io_update_bits_br_taken_mask_0 是否跳转 input io_update_bits_br_taken_mask_1 是否跳转 ","categories":["Branch Prediction Unit (BPU)"],"description":"","excerpt":"Introduction to uFTB uFTB is the first predictor among all the …","ref":"/xs-bpu/en/docs/modules/01_uftb/","tags":["Sub-predictor","uFTB"],"title":"uFTB Branch Predictor"},{"body":"Feature List Support prediction based on FTB entry Support two-bit saturating counter Support basic prediction result output and meta information output for the s1 channel Support update request response, updating internal FTB and two-bit saturating counter. ","categories":["Feature List"],"description":"","excerpt":"Feature List Support prediction based on FTB entry Support two-bit …","ref":"/xs-bpu/en/docs/feature/01_uftbfeature/","tags":["uFTB","Feature List"],"title":"uFTB Feature List"},{"body":"uFTB 简介 uFTB 是香山所有预测器中第一个预测器，也是其他预测器产生预测结果的基石。uFTB 工作在 s1 阶段，它获取到 s1_pc 后能在本周期内产生预测结果，并在 s1 通道输出，其他通道不予更改。分支指令的位置、指令的跳转目标都是由它来提供，后续预测器也将会根据这一预测结果进行进一步的预测。\n而其本质则是一个 FTB 项缓存，其中存储了 FTB 项，基础的预测结果将直接由读出的 FTB 项生成。\n因此，在你开始文档的阅读之前，请确保你理解了 FTB 项和含义以及预测结果接口的具体细节。\nuFTB 功能简介 缓存FTB项，生成一周期预测结果 uFTB 中维护了一个小型的 FTB 项缓存，拿到 PC 之后，会在一周期之内读出 PC 所对应的 FTB 项，并从 FTB 项生成 s1 阶段预测结果。 维护两比特饱和计数器，提供基础条件分支结果 uFTB 中为 FTB项缓存的每一行都维护了对应的两比特饱和计数器，其方向预测结果会反映在 uFTB 的预测结果输出中。 根据更新请求更新 FTB 缓存和两比特饱和计数器 uFTB 缓存结构 如上所述，uFTB 本质上是一个存储了 FTB 项的小型缓存，其大致结构如下图所示。\n在当前版本的香山中，uFTB 共有 32 个缓存行，每个缓存行被称作 FauFTBWay，一个缓存行中可以存储一个 FTB 项。\n当 s1 流水有效时，uFTB 会使用 s1_pc 来决定读出 uFTB 缓存的哪一项。缓存是根据 PC 中的 tag 字段来索引，tag 字段被定义为 pc[16: 1]，即在 PC 中截取了 16 比特作为标识来匹配缓存中的某一行。\n缓存中的每一行，即 FauFTBWay 中的数据请求接口有三项：\nreq_tag 输入 pc 中截取的 tag 标识 resp 输出本行所存储的 FTB 项 resp_hit 输出指示本行的 FTB 项是否与 req_tag 匹配 uFTB 会将 tag 连接到缓存的每一行的数据请求接口，并根据 resp_hit 信号，选出命中的那一个 FTB 项。后续将会根据这一 FTB 项生成完整预测结果。\n两比特饱和计数器 uFTB 中为 FTB 项缓存的每一行都维护了对应的两比特饱和计数器。我们知道，在一个 FTB 项最多可以存储两个条件分支指令，因此每一行对应的两比特饱和计数器也有两个，负责为其中的条件分支指令提供粗略的预测结果。\nuFTB 在索引到 FTB 项时，也会索引到对应的两比特饱和计数器。\n当预测结果生成时，会根据 FTB 项与其对应的两个两比特饱和计数器中的内容来产生。\n预测结果生成 uFTB 通过 s1_pc 索引到对应的 FTB 项以及两个两比饱和计数器之后，需要根据他们产生预测结果。uFTB 产生的预测结果，将在 s1 流水有效时，通过 s1 通道进行输出，对于 s2 以及 s3 通道，uFTB 不予更改。\ns1 预测结果中的信号产生方式可参考以下列表：\nhit FTB表项是否命中\n生成方式：FauFTBWay 中的 resp_hit 信号，存在一个有效 slot_valids slot有效位，表示 ftb 项中的每个 slot 是否有效\ntargets 每个slot对应的跳转目标地址\noffsets 每个slot中的指令相对于预测块起始地址的偏移\nis_jal 预测块内包含jal指令\nis_jalr 预测块内包含jalr指令\nis_call 预测块内包含call指令\nis_ret 预测块内包含ret指令\nlast_may_be_rvi_call 预测结果预测块末尾可能为一个RVI类型的call指令信号\nis_br_sharing 最后一个slot (tailSlot) 中存储的是条件分支指令信号\n生成方式： FTB 项中对应字段导出 fallThroughErr FTB项中记录的 pftAddr 有误\n生成方式：比较 pftAddr 代表的预测块结束地址是否大于预测块的起始地址，如果小于，则代表出现错误，此信号置为有效。这种情况可能会发生在 pc 索引到错误的 FTB 项的情况。 fallThroughAddr 预测块的结束地址\n生成方式：如果 fallThroughErr 无效，则根据 pftAddr 生成，否则将其设置为起始地址 + 预测宽度。 br_taken_mask 分支预测结果，每个分支(slot)对应一个 bit，表示该分支是否被预测为 taken\n生成方式：根据 FTB 项中的 always_taken 字段和两比特饱和计数器指示结果生成。 jalr_target 本预测块中的 jalr 的跳转目标\n生成方式：FTB 项中 tailSlot 中的跳转目标。 uFTB 更新 uFTB 的更新涉及 FTB 项缓存的更新，以及两比特饱和计数器的更新，而更新的内容都是通过更新接口来获取的。\n在 uFTB 预测器中，缓存和两比特饱和计数器的读与写并不冲突，因此我们无需考虑读和更新之间的时序冲突问题，可以将他们看做是独立的两部分。\nFTB 缓存更新 FTB 缓存的更新过程很简单，更新通道中已经为我们指定好了 pc 以及新生成的 FTB 项，只需将其写入到缓存的指定位置即可。\nFTB 缓存的更新需要两个周期：\n在第一周期，通过 update 中的信号计算出如下内容： 更新请求对应的预测块在缓存的哪一行 更新请求中的 pc 截取 tag 后被发送到 FauFTBWay 的更新请求通道，根据每一行所返回的 hit 信号计算 在第二周期，根据第一周期求出的需要更新的位置进行更新，如果无任何一行命中，则 uFTB 会使用 伪 LRU 替换算法 选出需要写入的行。 具体的，更新通道中的 ftb_entry 信号组包含了新 FTB 项的完整信息，将该信息发送到需要更新的某一个缓存行即可。 两位饱和计数器更新 两位饱和计数器的更新则需要结合后续程序的真实执行情况，以及 FTB 项中记录的分支指令信息来进行更新了，而这些信息都可以从更新通道中进行获取。\n两位饱和计数器的更新也需要两个周期：\n在第一周期，计算出槽中哪些条件分支指令 对应的两位饱和计数器 需要被更新，这需要满足以下条件： 当前分支指令槽有效，并且存放了一条条件分支指令 当前分支指令槽并没有被标识为 always_taken。（因为被标识为 always_taken 后不会采用两位饱和计数器的结果） 当前分支指令槽不在实际发生了跳转的分支指令槽之后。 在第二周期，更新饱和计数器 根据第一周期生成的需要更新的掩码，以及更新通道中的 br_taken_mask 信息进行更新。 接口列表 信号类型 信号位 信号名 信号描述 input clock 输入时钟 input reset 复位信号 input [35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input [40:0] io_in_bits_s0_pc_0 输入位s0_pc 的 第0个复制 input [40:0] io_in_bits_s0_pc_1 同上 第1个 input [40:0] io_in_bits_s0_pc_2 同上 第2个 input [40:0] io_in_bits_s0_pc_3 同上 第3个 output [40:0] io_out_s1_pc_0 输出s1_pc 的 第0个复制 output [40:0] io_out_s1_pc_1 同上 第1个 output [40:0] io_out_s1_pc_2 同上 第2个 output [40:0] io_out_s1_pc_3 同上 第3个 output io_out_s1_full_pred_0_br_taken_mask_0 solt 0 是否被预测为 always taken output io_out_s1_full_pred_0_br_taken_mask_1 solt 1 是否被预测为 always taken output io_out_s1_full_pred_0_slot_valids_0 solt 0 是否启用 output io_out_s1_full_pred_0_slot_valids_1 solt 1 是否启用 output [40:0] io_out_s1_full_pred_0_targets_0 solt 0 对应的跳转目标地址 output [40:0] io_out_s1_full_pred_0_targets_1 solt 1 对应的跳转目标地址 output [3:0] io_out_s1_full_pred_0_offsets_0 solt 0 中分支指令相对于地址块起始pc的偏移 output [3:0] io_out_s1_full_pred_0_offsets_1 solt 1 中分支指令相对于地址块起始pc的偏移 output [40:0] io_out_s1_full_pred_0_fallThroughAddr 预测块的结束地址 output io_out_s1_full_pred_0_is_br_sharing solt 1（无条件跳转）是否被共享为有条件跳转指令 output io_out_s1_full_pred_0_hit output io_out_s1_full_pred_1_br_taken_mask_0 类似 io_out_s1_pc_1 io_out_s1_full_pred_0的复制 output io_out_s1_full_pred_1_br_taken_mask_1 output io_out_s1_full_pred_1_slot_valids_0 output io_out_s1_full_pred_1_slot_valids_1 output [40:0] io_out_s1_full_pred_1_targets_0 output [40:0] io_out_s1_full_pred_1_targets_1 output [3:0] io_out_s1_full_pred_1_offsets_0 output [3:0] io_out_s1_full_pred_1_offsets_1 output [40:0] io_out_s1_full_pred_1_fallThroughAddr output io_out_s1_full_pred_1_is_br_sharing output io_out_s1_full_pred_1_hit output io_out_s1_full_pred_2_br_taken_mask_0 同上 output io_out_s1_full_pred_2_br_taken_mask_1 output io_out_s1_full_pred_2_slot_valids_0 output io_out_s1_full_pred_2_slot_valids_1 output [40:0] io_out_s1_full_pred_2_targets_0 output [40:0] io_out_s1_full_pred_2_targets_1 output [3:0] io_out_s1_full_pred_2_offsets_0 output [3:0] io_out_s1_full_pred_2_offsets_1 output [40:0] io_out_s1_full_pred_2_fallThroughAddr output io_out_s1_full_pred_2_is_br_sharing output io_out_s1_full_pred_2_hit output io_out_s1_full_pred_3_br_taken_mask_0 同上 output io_out_s1_full_pred_3_br_taken_mask_1 output io_out_s1_full_pred_3_slot_valids_0 output io_out_s1_full_pred_3_slot_valids_1 output [40:0] io_out_s1_full_pred_3_targets_0 output [40:0] io_out_s1_full_pred_3_targets_1 output [3:0] io_out_s1_full_pred_3_offsets_0 output [3:0] io_out_s1_full_pred_3_offsets_1 output [40:0] io_out_s1_full_pred_3_fallThroughAddr output io_out_s1_full_pred_3_fallThroughErr output io_out_s1_full_pred_3_is_br_sharing output io_out_s1_full_pred_3_hit output [222:0] io_out_last_stage_meta 输出最后阶段的元信息 io_out_last_stage_meta = {213’h0, resp_meta_pred_way_r_1, resp_meta_hit_r_1} input io_ctrl_ubtb_enable 控制ubtb是否启用 input io_s0_fire_0 输入s0_fire_0，与 io_out_s1_pc_0 \u003c= io_in_bits_s0_pc_0 的时钟门控相关 input io_s0_fire_1 输入s0_fire_1 input io_s0_fire_2 输入s0_fire_2 input io_s0_fire_3 输入s0_fire_3 input io_s1_fire_0 输入s1_fire_0 input io_s2_fire_0 输入s2_fire_0 input io_update_valid 更新有效性 input [40:0] io_update_bits_pc 传回的预测块pc（用于指示更新的预测块） input [3:0] io_update_bits_ftb_entry_brSlots_0_offset solt 0 中分支指令相对于地址块起始pc的偏移 input [11:0] io_update_bits_ftb_entry_brSlots_0_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_brSlots_0_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_brSlots_0_valid 是否启用 input [3:0] io_update_bits_ftb_entry_tailSlot_offset solt 1 中分支指令相对于地址块起始pc的偏移 input [19:0] io_update_bits_ftb_entry_tailSlot_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_tailSlot_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_tailSlot_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_tailSlot_valid 是否启用 input [3:0] io_update_bits_ftb_entry_pftAddr Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。 input io_update_bits_ftb_entry_carry pc+pft时是否产生进位 input io_update_bits_ftb_entry_always_taken_0 是否预测为总是跳转 input io_update_bits_ftb_entry_always_taken_1 是否预测为总是跳转 input io_update_bits_br_taken_mask_0 是否跳转 input io_update_bits_br_taken_mask_1 是否跳转 ","categories":["分支预测单元（BPU）"],"description":"","excerpt":"uFTB 简介 uFTB 是香山所有预测器中第一个预测器，也是其他预测器产生预测结果的基石。uFTB 工作在 s1 阶段， …","ref":"/xs-bpu/docs/modules/01_uftb/","tags":["子预测器","uFTB"],"title":"uFTB 分支预测器"},{"body":"功能列表（Feature List） 支持基于 FTB 项的预测 支持两比特饱和计数器 支持 s1 通道基础预测结果输出及 meta 信息输出 支持更新请求响应，更新内部 FTB 及两比特饱和计数器。 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） 支持基于 FTB 项的预测 支持两比特饱和计数器 支持 s1 通道基础预测结果输出及 meta 信息输 …","ref":"/xs-bpu/docs/feature/01_uftbfeature/","tags":["uFTB","Feature List"],"title":"uFTB 功能列表"},{"body":"分支预测块思想 对于一般的分支预测器来说，通常会根据给定的 PC 预测出该 PC 所对应的指令的相关信息，如是否是条件分支指令、是否是跳转指令。对于条件分支指令，会预测出是否会跳转，而对于跳转指令，则会预测出跳转目标。然而，这种一条一条地预测指令的方法效率较低，导致前端指令供给过慢。\n相比之下，香山采用的预测方法是每次预测一个指令块。也就是说，给定一个 PC，香山会预测出以这个PC开始的一个分支预测块，包括了接下来若干条指令的情况，如是否存在分支指令、分支指令的位置、是否跳转以及跳转目标等信息。\n这样的预测方法可以一次性地预测出多条指令，并将预测结果送往取指单元（IFU），指导IFU进行取指。此外，由于IFU需要考虑缓存行的性能问题，它可以根据预测块一次性地取出多条指令，从而提高吞吐效率。\n在预测块产生后，分支预测块还会生成执行完本预测块后跳转到的 PC，接着 BPU 会根据该 PC 继续产生下一个分支预测块。\n举个简单的例子\n如上图所示，当 PC 执行到 0x20000118 时，BPU会经历如下几个步骤：\nBPU得知PC 0x20000118 BPU产生以 0x20000118 为开始的分支预测块，内容大致如下 在接下来的若干条指令中 第三条是一个条件分支指令 对于这个条件分支指令，预测他将会跳转 跳转后的地址为 0x20000110 BPU将PC设置为 0x20000110，并继续产生下一个分支预测块 这便是采用了分支预测块的香山 BPU 的基本预测流程\n多预测器、多流水线结构 上图展示了香山BPU的总体架构，其中我们需要关注两个主要方面：\n多预测器 为了确保预测的准确性，香山 BPU 采用了多个预测器，并且这些预测器共同产生 BPU 的预测结果。例如，FTB 会生成基础的预测结果供后续预测器使用，而 TAGE 则对条件分支指令产生更精准的预测结果等。 多流水线 为了满足高性能的需求，香山 BPU 采用了流水线设计。各个预测器处于不同的流水线级别。其中，uFTB（即图中的uBTB）预测器位于第一流水线级别，能够在一个周期内产生预测结果。而其余预测器则需要2-3个周期才能生成预测结果，尽管预测时间较长，但预测结果相对更精确。 但是，如果在第三个周期才能获取到预测结果，并基于新的结果重新开始预测，这样的设计难免导致性能损失。因为这样一来，需要耗费三个时钟周期才可以完成一次预测。\n为了在第一和第二周期就能够获取到某些预测器的预测结果，我们设置了三个预测结果通道，并将三个流水线级别的预测结果同时输出，如下图所示。\n取指目标队列（FTQ） 分支预测结果暂存 尽管 BPU 可以以分支预测块的形式提供预测结果，IFU 也可以一次性取指多条指令，但它们之间仍然存在性能上的差距。通常情况下，BPU产生预测结果的速度更快。\n因此，在 BPU 与 IFU 之间添加了一个取指目标队列（FTQ）作为缓冲。FTQ 本质上是一个队列，用于存储一个个数据项。BPU产生的预测结果会先暂存到FTQ中，然后由 IFU 从 FTQ 中获取预测结果，如下图所示。\n每当 BPU 产生一个预测块，该预测块会被放入 FTQ 的队首。当 IFU 处于空闲状态时，它会从 FTQ 队尾获取下一个预测块。下方的示意图展示了这一过程。\n在香山中，FTQ 的功能远远不止于此。参考上文图中 FTQ 的对外交互接口，可以看出它还负责向 ICache 发送预取信息，存储 BPU 的各类训练信息，负责分析取指模块、后端执行模块中发来的重定向信息、更新信息，还会向 BPU 发送更新请求，甚至 FTB 预测器中的基本数据结构——FTB项都是由 FTQ 来更新的。\nBPU 预测结果重定向 上文中提到，香山分支预测结果共有三个通道，会同时输出 s1, s2, s3 流水级的预测结果，那 FTQ 如何使用三个阶段的预测结果呢？\n我们不妨从探讨流水线的时序出发，如下图所示\n在第一个周期，一个新的PC 0x4 被送入，能在一周期内产生预测结果的预测器（叫做 uFTB）在 s1 接口输出了他的预测结果，并且结果指示下一个 PC 为 0xf，其他接口还无输出 在第二周期，PC 被设置为了 0xf，而 uFTB 也产生了 0xf 的预测结果，并从 s1 通道送出。与此同时，两周期预测器产生了上一轮 0x4 地址的预测结果，并从 s2 通道送出。 那么在这里产生了一个问题，第二周期 s2 产生的是 0x4 预测结果，而 0x4 的预测结果已经在上一周期被 s1 所输出，并放置在了 FTQ 中的某个项中。也就是说 s2 产生的预测结果，已经在 s1 通道被产生过了。但不同点是 s2 结果是由两周期预测器产生而来，结果会更加准确。\n因此我们需要做的并不是根据 s2 的预测结果，放置一个新的 FTQ 项，而是 对比 s2 预测结果和上一周期 s1 预测结果是否有差异，如果有差异则覆盖上一阶段 s1 接口放置的 FTQ 项\n于是我们给 s2 和 s3 通道增加两个额外的信号线，我们称之为 redirect 信号，如果该信号有效说明此时该阶段的预测结果与之前的预测结果产生差异，并且需要覆盖之前某个位置的 FTQ 项。结构如下图所示。\n结构示意图中的时刻，对应流水线的第二个周期，此时 s1 通道已经往 FTQ 中放入一个地址为 0x4 的分支预测块结果，而此时 s2 预测结果产生，BPU 发现 s2 预测结果与 s1 不同，则本周期 redirect 接口被置为有效。FTQ 会使用 s2 通道的预测结果覆盖之前存放 0x4 预测结果的 FTQ 项。\n此时，s1 通道虽然也产生了以 0xf 为首的分支预测块，但显然，0xf 是 s1 基于第一周期的错误预测结果产生的 PC，因此在这时候，s1 结果可直接弃之不用。\n在第三周期，s1 又使用 s2 所产生的正确预测结果所指示的新PC 0x8 开始了新一轮预测。之后，如果 s2, s3 通道始终未检测到有预测错误发生，则流水线将以满状态持续运行下去。\nBPU 重定向请求 无论再精确的分支预测器也不总是正确的，这种不正确会导致后续流水线中填充错误的指令，因此需要有一种机制来纠正这种错误，这种机制就是重定向。指令在被后端执行模块执行时，这条指令真正的行为才会被确定，因此在这个时候，后端执行模块如果发现分支预测错误，就会发出 重定向请求 （注意此处重定向请求与上文预测结果重定向是不同的概念），让处理器的状态恢复到执行错误指令之前的状态，对于我们来说，只需要关注BPU和FTQ在重定向时是怎样恢复状态的即可。\n除了后端会产生重定向请求，香山处理器中会在 IFU 取到指令之后对指令进行简单分析，检测出最基本的预测错误。具体的过程是这样的，FTQ 向 IFU 发送取指请求之后，会等待 IFU 返回预译码信息（预译码是IFU对指令的简单解码，例如是否是跳转指令、跳转的目标是什么），FTQ 会将预译码信息写回到对应 FTQ项 中的某个字段，同时会分析预译码信息，如果发现预测错误，便会生成一个 IFU 重定向请求。\n来自后端执行模块的重定向请求不需要 FTQ 生成，是直接由后端发往 FTQ 进行处理。FTQ 会将生成的 IFU 重定向请求与后端重定向请求转发到 BPU 的重定向接口中去，如果二者在同一周期有效，那么FTQ会选择转发后端重定向请求。\n添加了重定向接口的 BPU 如下图所示。\nBPU 的更新请求 目前的 BPU 已经有了纠错的能力，但是还有一个问题，BPU 中的数据没有办法得到更新。如果无法获取指令的位置、类型、是否发生跳转以及跳转地址等信息，BPU 将得不到训练，并导致准确度大幅降低。\n为了获得这些信息，我们仍需依赖于取指目标队列（FTQ），因为它不仅能与 IFU 交互获取指令相关的信息，还可以与后端交互，获取执行的相关信息。 因此会有一个更新请求通道由 FTQ 直接连接至 BPU。\n当后端执行完 FTQ 中的某一项时，该项会被标识为已提交，接下来 FTQ 将该项的更新信息，通过 Update 通道发往BPU。\n总结 通过本节，我们了解到 BPU 对外交互所需的所有主要接口，以及 FTQ 对于 BPU 的作用。拥有了预测结果接口、重定向接口、更新接口的 BPU已经可以支撑 BPU 的所有对外交互了。接下来我们会将目光深入 BPU 内部。\n","categories":["香山 BPU 基础设计"],"description":"本节介绍了香山分支预测单元（BPU）的基础思想和工作原理，包括采用的分支预测块思想、多预测器、多流水线结构以及取指目标队列（FTQ）的作用，说明了 BPU 对外交互的主要接口。","excerpt":"本节介绍了香山分支预测单元（BPU）的基础思想和工作原理，包括采用的分支预测块思想、多预测器、多流水线结构以及取指目标队列（FTQ）的作用， …","ref":"/xs-bpu/docs/basic/01_xsbpu_basic/","tags":["分支预测块","重定向请求","更新请求","取指目标队列（FTQ）"],"title":"香山分支预测单元基础"},{"body":"分支预测重定向（BranchPredictionRedirect） 接口定义：src/main/scala/xiangshan/frontend/FrontendBundle.scala\n接口类型：BranchPredictionRedirect\n该接口定义了分支预测单元的重定向请求，主要用于重定向分支预测器的状态。\nBranchPredictionRedirct 接口继承自 Redirct 接口，所含信号较多，BPU 重定向只会用到其中一部分接口，文档描述的结构中只包含 BPU 用到的接口内容。\n重定向请求有两种来源，与 IFU 预译码信息对比生成的重定向请求，后端执行过程中的重定向请求。\n在重定向请求中，关键信息是cfiUpdate，即控制流指令信息。这一信息对应着一条指令，也就是BPU发生预测错误的指令。举例来说，如果 BPU 在预测块中指示第三条指令为普通指令不发生跳转，但预译码显示第三条指令是一条无条件跳转指令，必定会跳转。在这种情况下，BPU 发生了预测错误，FTQ 产生了重定向请求。在重定向请求中，cfiUpdate 将会对应这条无条件跳转指令。\ncfiUpdate 中的信息可大致分为三种类型的信息：\n对应指令的信息及执行情况。其中会说明该指令在预测块中位于那个槽 (shift) 及 pc； 会指示这条指令的类型相关信息 (pd)；还有这条指令的执行情况，例如跳转目标、是否跳转等 历史维护相关信息。重定向请求中包含了该条指令所处的预测块对应的分支历史信息，以帮助BPU恢复分支历史信息。foleded_hist 代表全局折叠历史，histPtr 代表全局历史指针，而其余信息则是维护分支历史的辅助信息。要详细了解这些信息的含义，请参阅 BPU顶层模块。 RAS维护相关信息。详细含义请参阅 RAS 子预测器文档。 level 信息的含义是，重定向是否包含本条指令，如果不包含，重定向请求的接收方将认为这条指令已经被执行了，下次预测将从下一条指令开始。BPU 顶层会默认不包含本条指令，收到重定向请求后，会将本条指令的执行情况统计到分支历史当中。\n以下为重定向接口的详细信号列表：\nlevel: 重定向请求是否包括本位置，低表示在本位置后重定向，高表示在本位置重定向。\n接口类型: UInt(1.W) cfiUpdate: 控制流指令更新信息\n接口类型: CfiUpdateInfo\n接口列表\npc: 重定向请求对应的指令 PC 接口类型: UInt(VaddrBits.W) pd: 重定向指令的预译码信息 isRVC: 是否是压缩指令 接口类型: Bool isCall: 是否是函数调用 接口类型: Bool isRet: 是否是函数返回 接口类型: Bool target: 重定向请求指令的目标地址。 接口类型: UInt(VaddrBits.W) taken: 重定向请求指令是否发生跳转。 接口类型: Bool shift: 重定向请求指令位于哪个槽，如果是普通指令则为0. 接口类型: UInt((log2Ceil(numBr)+1).W) addIntoHist: 重定向请求指令执行信息是否要加入分支历史。 接口类型: Bool folded_hist: 重定向请求对应的折叠历史。 接口类型: AllFoldedHistories(foldedGHistInfos) afhob: 重定向请求指令对应的分支历史最老位。 接口类型: AllAheadFoldedHistoryOldestBits(foldedGHistInfos) lastBrNumOH: 重定向请求对应的上次跳转位置。 接口类型: UInt((numBr+1).W) histPtr: 重定向请求需要恢复的全局历史指针。 接口类型: CGHPtr ssp: 重定向请求指令对应的 RAS 推测栈栈顶在提交栈位置的指针。 接口类型: UInt(log2Up(RasSize).W) sctr: 重定向请求指令对应的 RAS 推测栈栈顶递归计数 Counter。 接口类型: UInt(log2Up(RasCtrSize).W) TOSW: 重定向请求指令对应的 RAS 推测栈（队列）写指针。 接口类型: CGHPtr TOSR: 重定向请求指令对应的 RAS 推测栈（队列）读指针。 接口类型: CGHPtr NOS: 重定向请求指令的 RAS 栈顶 Counter。 接口类型: CGHPtr 分支预测更新（BranchPredictionUpdate） 接口定义: src/main/scala/xiangshan/frontend/FrontendBundle.scala\n该接口定义了分支预测器的更新请求，主要用于更新分支预测器的状态，文档只列出了在 BPU 中使用的接口。\n更新请求与每一个分支预测块一一对应，当 FTQ 中的一个分支预测块被执行过以后，FTQ 将会为这个分支预测块产生一个更新请求，来指导预测器进行训练，因此更新请求中的一个重要职责就是向 BPU 反馈指令的真实执行情况。当然在香山分支预测单元中，更新请求还会负责 FTB 项的更新\n更新请求中的信息可大致分为四类：\nPC 表示预测块的起始地址，指示了该更新请求对应哪个分支预测块 FTB 项更新信息 更新通道中会含有一个 FTB 项结构 (ftb_entry)，输出 FTQ 新生成的 FTB 项，并且还会指示是否与旧的 FTB 项相同 (old_entry) 指令真实执行情况信息 更新通道中会指示该分支预测块中分支指令和无条件跳转指令的执行情况，还会给出控制流指令（即发生跳转的指令）的地址以及最终跳转目标。 与该预测块对应的预测器相关数据 包含 spec_info 以及 meta 信息。（具体请参阅BPU全局接口文档） 更新请求的接口列表如下：\npc 更新请求的pc (预测块起始地址) 接口类型：UInt(VAddrBits.W) ftb_entry 更新后的ftb项 接口类型：new FTBEntry() 接口列表：详见（FTBEntry） old_entry 更新后的 FTB 项是否与旧的 FTB 项相同 接口类型：Bool() br_taken_mask 预测块内每个slot内指令是否跳转 接口类型：Vec(numBr, Bool()) mispred_mask 预测块内预测错误的掩码。第一、二位分别代表两个条件分支指令是否预测错误，第三位指示无条件跳转指令是否预测错误。 接口类型：Vec(numBr+1, Bool()) jmp_taken 预测块内无条件跳转指令被触发 接口类型：Bool() cfi_idx 控制流指令在预测块中的索引 接口类型：ValidUndirectioned(UInt(log2Ceil(PredictWidth).W)) full_target 预测块的跳转目标（下一个预测块的起始地址） 接口类型：UInt(VAddrBits.W) spec_info 该预测块对应的 last_stage_spec_info 接口类型：new SpeculativeInfo 接口列表：（只用到了 foled_hist 一项） folded_hist 全局折叠历史 接口类型：AllFolededHistories(foldedGHistInfos) meta 该预测块对应的 last_stage_meta 信息 接口类型：UInt(MaxMetaLength.W) ","categories":["香山 BPU 接口"],"description":"重定向与更新请求是 FTQ 发往 BPU 的两大信息类型，本节中讲述了这两种请求接口的具体结构。","excerpt":"重定向与更新请求是 FTQ 发往 BPU 的两大信息类型，本节中讲述了这两种请求接口的具体结构。","ref":"/xs-bpu/docs/ports/01_redirect_and_update/","tags":["重定向请求","更新请求"],"title":"重定向与更新接口"},{"body":"本文档将会对 BPU 中的重要结构以及对外接口进行描述，描述粒度将深入代码级别，文档中描述的结构将与香山分支预测单元 Chisel 版本代码保持一致，信号结构及名称亦来自于 Chisel 版本代码。\n文档适用于已经了解了香山分支预测单元基础设计，并想要深入了解信号细节的同学使用。可以根据验证需要的内容有选择的进行阅读，或随时进行查阅。\n其中，FTB项与完整预测结果接口涉及 BPU 产生预测结果的方式，建议每位同学进行阅读。\n","categories":["香山 BPU 接口"],"description":"本文档将会对 BPU 中的重要结构以及对外接口进行描述，描述粒度将深入代码级别，通过阅读本文档，你可以理解香山分支预测单元中每一个信号是什么作用，理解各种请求的具体实现方式，也将使你能够结合代码进行功能的理解。","excerpt":"本文档将会对 BPU 中的重要结构以及对外接口进行描述，描述粒度将深入代码级别，通过阅读本文档，你可以理解香山分支预测单元中每一个信号是什么 …","ref":"/xs-bpu/docs/ports/","tags":["香山","BPU"],"title":"重要结构及接口文档"},{"body":"BPU Module Overall External Interface (PredirectIO) Interface definition: src/main/scala/xiangshan/frontend/BPU.scala\nPredirectIO is the overall external interface of the branch predictor (BPU). It mainly handles the interaction between the branch predictor (BPU) and the fetch target queue (FTQ), which includes the following parts:\nbpu_to_ftq: Information sent from BPU to FTQ, mainly for sending branch prediction results from BPU to FTQ Interface type： BpuToFtqIO Interface type： resp: Global branch prediction information sent from BPU to FTQ Interface type：DecoupledIO(new BpuToFtqBundle()) BpuToFtqBundle inherits from BranchPredictionResp，without additional signals Interface type：See (BranchPredictionResp) ftq_to_bpu: Information sent from FTQ to BPU, mainly for handling redirect and update requests Interface type： FtqToBpuIO Interface type： redirect: Redirect request sent from FTQ to BPU Interface type： Valid(new BranchPredictionRedirect) Interface list：See （BranchPredictionRedirect） update: Update request sent from FTQ to BPU Interface type：Valid(new BranchPredictionUpdate) Interface list：See （BranchPredictionUpdate） enq_ptr: FTQ pointer sent to BPU, indicating which FTQ entry to write to next Interface type：FtqPtr ctrl: BPU control signals, mainly for enabling various predictors Interface type：BPUCtrl Interface list： ubtb_enable: UBTB predictor enable Interface type：Bool() btb_enable: BTB predictor enable Interface type：Bool() bim_enable: BIM predictor enable Interface type：Bool() tage_enable: TAGE predictor enable Interface type：Bool() sc_enable: SC predictor enable Interface type：Bool() ras_enable: RAS predictor enable Interface type：Bool() loop_enable: LOOP predictor enable Interface type：Bool() reset_vector: Reset vector, which the BPU’s PC will be reset to upon reset Interface type：UInt(PAddrBits.W) Global Branch Prediction Information (BranchPredictionResp) Interface definition: src/main/scala/xiangshan/frontend/FrontendBundle.scala\nThis interface defines all the prediction result information of the branch predictor, including the prediction results of each stage and the related information output by the last pipeline stage.\ns1 Branch prediction result of the s1 pipeline stage s2 Branch prediction result of the s2 pipeline stage s3 Branch prediction result of the s3 pipeline stage Interface type：BranchPredictionBundle Interface type：See （BranchPredictionBundle） last_stage_meta Metadata of the prediction result output by the last pipeline stage. It is a bit vector output by each predictor and combined by the Composer. Interface type：UInt(MaxMetaLength.W) last_stage_spec_info Related information of the prediction result output by the last pipeline stage Interface type：Ftq_Redirect_SRAMEntry Interface list： folded_hist Global folded history Interface type：AllFoldedHistories(foldedGHistInfos) afhob Global branch history oldest bit Interface type：AllAheadFoldedHistoryOldestBits(foldedGHistInfos) lastBrNumOH Last branch position Interface type：UInt((numBr+1).W) histPtr Global branch history pointer Interface type：CGHPtr ssp RAS speculation stack pointer at commit stack position Interface type：UInt(log2Up(RasSize).W) sctr RAS speculation stack recursion counter Interface type：UInt(log2Up(RasCtrSize).W) TOSW RAS speculation stack (queue) write pointer Interface type：CGHPtr TOSR RAS speculation stack (queue) read pointer Interface type：CGHPtr NOS RAS stack top counter Interface type：CGHPtr topAddr RAS stack top return address Interface type：UInt(VAddrBits.W) last_stage_ftb_entry FTB entry output by the last pipeline stage Interface type：FtqEntry Interface list：See（FtqEntry） FTB entry output by the last pipeline stage Interface definition: src/main/scala/xiangshan/frontend/FrontendBundle.scala\nThis interface defines the branch prediction result information output by each pipeline stage,\npc Starting PC of the predicted block Interface type: Vec(numDup, UInt(VAddrBits.W)) numDup is only for register replication, with identical signals valid Whether the prediction result is valid Interface type: Vec(numDup, Bool()) hasRedirect Whether a redirect is needed Interface description: Only the s2 and s3 stages will redirect, and the prediction result of this stage will override the previous pipeline stage’s result when a redirect occurs Interface type: Vec(numDup, Bool()) ftq_idx FTQ pointer, pointing to the FTQ entry corresponding to the prediction information of this stage Interface type: new FtqPtr full_pred Complete branch prediction result Interface type: Vec(numDup, new FullBranchPrediction) Interface list: See (FullBranchPrediction) ","categories":["Xiangshan BPU Interface"],"description":"This section introduces the definition of the overall external interaction interface of the Xiangshan Branch Prediction Unit, including the presentation of global branch prediction results and single pipeline stage prediction results.","excerpt":"This section introduces the definition of the overall external …","ref":"/xs-bpu/en/docs/ports/02_global_ports/","tags":["BPU"],"title":"BPU Global Interface"},{"body":"BPU 模块整体对外接口 (PredirectIO) 接口定义: src/main/scala/xiangshan/frontend/BPU.scala\nPredirectIO 是分支预测器整体的对外接口，它主要处理了分支预测器（BPU）与取指目标队列（FTQ）之间的交互，主要包含以下几个部分：\nbpu_to_ftq: BPU 向 FTQ 发送的信息，主要用于 BPU 向 FTQ 发送分支预测结果 接口类型： BpuToFtqIO 信号列表： resp: BPU 向 FTQ 发送的全局分支预测信息 接口类型：DecoupledIO(new BpuToFtqBundle()) BpuToFtqBundle 继承自 BranchPredictionResp，没有额外的信号 信号列表：详见 (BranchPredictionResp) ftq_to_bpu: FTQ 向 BPU 发送的信息，主要用于处理重定向及更新请求 接口类型： FtqToBpuIO 信号列表： redirect: FTQ 向 BPU 发送的重定向请求 接口类型： Valid(new BranchPredictionRedirect) 接口列表：详见（BranchPredictionRedirect） update: FTQ 向 BPU 发送的更新请求 接口类型：Valid(new BranchPredictionUpdate) 接口列表：详见（BranchPredictionUpdate） enq_ptr: FTQ发送给BPU的FTQ指针，告诉BPU下一次需要写到哪个FTQ表项 接口类型：FtqPtr ctrl: BPU 控制信号，主要用于控制各预测器的使能 接口类型：BPUCtrl 接口列表： ubtb_enable: UBTB 预测器使能 接口类型：Bool() btb_enable: BTB 预测器使能 接口类型：Bool() bim_enable: BIM 预测器使能 接口类型：Bool() tage_enable: TAGE 预测器使能 接口类型：Bool() sc_enable: SC 预测器使能 接口类型：Bool() ras_enable: RAS 预测器使能 接口类型：Bool() loop_enable: LOOP 预测器使能 接口类型：Bool() reset_vector: 重置向量，reset 时 BPU 的 pc 会被重置为该值 接口类型：UInt(PAddrBits.W) 全局分支预测信息 (BranchPredictionResp) 接口定义: src/main/scala/xiangshan/frontend/FrontendBundle.scala\n该接口定义了分支预测器预测的所有结果信息，包含了每一阶段的预测结果，以及最后一个流水级输出的相关信息。\ns1 s1流水级的分支预测结果 s2 s2流水级的分支预测结果 s3 s3流水级的分支预测结果 接口类型：BranchPredictionBundle 信号列表：详见（BranchPredictionBundle） last_stage_meta 最后一个流水级的输出的本轮预测的预测器元数据。是一个位向量，由每个预测器输出、并由 Composer 合并为一个。 接口类型：UInt(MaxMetaLength.W) last_stage_spec_info 最后一个流水级的输出的本轮预测的相关信息 接口类型：Ftq_Redirect_SRAMEntry 接口列表： folded_hist 全局折叠历史 接口类型：AllFoldedHistories(foldedGHistInfos) afhob 全局分支历史最老位 接口类型：AllAheadFoldedHistoryOldestBits(foldedGHistInfos) lastBrNumOH 上次跳转位置。 接口类型：UInt((numBr+1).W) histPtr 全局分支历史指针 接口类型：CGHPtr ssp RAS推测栈栈顶在提交栈位置的指针 接口类型：UInt(log2Up(RasSize).W) sctr RAS推测栈栈顶递归计数 Counter 接口类型：UInt(log2Up(RasCtrSize).W) TOSW RAS推测栈（队列）写指针 接口类型：CGHPtr TOSR RAS推测栈（队列）读指针 接口类型：CGHPtr NOS RAS栈顶 Counter 接口类型：CGHPtr topAddr RAS栈顶返回地址 接口类型：UInt(VAddrBits.W) last_stage_ftb_entry 最后一个流水级的输出的FTB表项 接口类型：FtqEntry 接口列表：详见（FtqEntry） 单流水级分支预测结果 (BranchPredictionBundle) 接口定义: src/main/scala/xiangshan/frontend/FrontendBundle.scala\n该接口定义了每个流水级对外输出的分支预测结果信息，\npc 预测块的起始 pc 接口类型：Vec(numDup, UInt(VAddrBits.W)) numDup仅为寄存器复制，其中信号完全相同 valid 预测结果是否有效 接口类型：Vec(numDup, Bool()) hasRedirect 是否需要重定向 接口说明：只有s2, s3阶段会重定向，重定向发生时本阶段预测结果会覆盖之前流水级的结果 接口类型：Vec(numDup, Bool()) ftq_idx FTQ指针，指向该阶段对应预测信息的FTQ表项 接口类型：new FtqPtr full_pred 完整的分支预测结果 接口类型：Vec(numDup, new FullBranchPrediction) 接口列表：详见（FullBranchPrediction） ","categories":["香山 BPU 接口"],"description":"本节中介绍了香山分支预测单元整体对外交互的接口的定义，包括全局分支预测结果和单流水级预测结果的呈现形式。","excerpt":"本节中介绍了香山分支预测单元整体对外交互的接口的定义，包括全局分支预测结果和单流水级预测结果的呈现形式。","ref":"/xs-bpu/docs/ports/02_global_ports/","tags":["BPU"],"title":"BPU 全局接口"},{"body":"Feature List Support FTB entry storage Support basic prediction result output and meta information output for the s2, s3 channels Support update request response, updating internal FTB entries ","categories":["Feature List"],"description":"","excerpt":"Feature List Support FTB entry storage Support basic prediction result …","ref":"/xs-bpu/en/docs/feature/02_ftbfeature/","tags":["FTB","Feature List"],"title":"FTB Feature List"},{"body":"功能列表（Feature List） 支持 FTB 项存储 支持 s2, s3 通道基础预测结果输出以及 meta 信息输出 支持更新请求响应，更新内部 FTB 项 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） 支持 FTB 项存储 支持 s2, s3 通道基础预测结果输出以及 meta 信息输出 支持更新请求响 …","ref":"/xs-bpu/docs/feature/02_ftbfeature/","tags":["FTB","Feature List"],"title":"FTB 功能列表"},{"body":"How Does the BPU Integrate Internal Sub-predictors? We already know that the Xiangshan BPU adopts multiple predictors and multiple pipeline schemes. To adapt to multiple pipelines, the BPU uses a three-channel result output interface. But how does it adapt to multiple predictors? This requires us to further explore the internal structure of the BPU.\nThe above figure is the BPU architecture diagram from the Xiangshan documentation. Currently, we only need to focus on one piece of information: all internal sub-predictors are encapsulated in a structure called Composer. The BPU only needs to interact with Composer.\nWhat is Composer? Let’s first look at their definition in the Xiangshan code.\nIt can be seen that Composer and the five sub-predictors have a common characteristic: they all inherit from the BasePredictor base class. And the interface has been defined in the BasePredictor class. In other words, Composer and the five sub-predictors all have the same interface! The top-level BPU can directly regard Composer as a sub-predictor, without worrying about how the internal sub-predictors are connected.\nSub-predictor Interface Next, we will look at what the sub-predictor interface looks like. This interface will involve the interaction between Composer and the top-level BPU, as well as the interaction between each sub-predictor and Composer.\nLet’s take Composer as an example to illustrate the structure of the sub-predictor interface.\nAs shown in the above figure, the three-channel prediction results of Composer are directly output to the outside of the BPU. There is also a set of three-channel prediction results connected from the inside of the BPU to Composer. However, since the prediction results are generated by Composer, the BPU will pass an empty prediction result to Composer. The significance of this is to make the sub-predictor act as a “processor.” The sub-predictor will process the input prediction results and then output the processed prediction results.\nNext, the top-level BPU will provide the information needed for prediction to the pipeline. First is the PC and branch history records (including global history and global folding history). Next, the BPU will connect some pipeline control signals between Composer and the pipeline control signals. Finally, the BPU will directly connect the externally input redirect request interface and update interface to Composer.\nIn the end, a simple definition of the sub-predictor interface can be given (for detailed definitions, please refer to the interface documentation):\nin (s1, s2, s3) Prediction information input s0_pc PC to be predicted ghist Global branch history folded_hist Global folding history out (s1, s2, s3) Prediction information output 流水线控制信号 s0_fire, s1_fire, s2_fire, s3_fire Whether the corresponding pipeline stage is working s2_redirect, s3_redirect Redirect signals when a prediction error is discovered in the subsequent pipeline stage s1_ready, s2_ready, s3_ready Whether the sub-predictor corresponding pipeline stage is ready update Update request redirect Redirect request Connection Between Sub-predictors We now know that the interfaces between each sub-predictor and Composer are the same, and we also know how Composer is connected to the top-level BPU. This section will explain how sub-predictors are connected within Composer.\nThe above figure shows the connection structure of sub-predictors in Composer. It can be seen that after the three-channel prediction results are input into Composer, they are first processed by uFTB and then output. They are then successively processed by TAGE-SC, FTB, ITTAGE, and RAS, and finally connected to the prediction result output of Composer, which is then directly connected to the outside of the BPU by Composer.\nFor other signals, because the interfaces between Composer and each sub-predictor are the same, they are directly connected to the corresponding interfaces of each predictor by Composer, without much additional processing.\nPrediction Result Interface Connection For sub-predictors, the connection of their prediction result is that the prediction result output of one predictor is the input of the next predictor. However, it should be noted that this connection is a combinational circuit connection and is not affected by timing.\nAs shown in the above figure, taking the s1 channel as an example, from input to the output of the last predictor, it is all modified by combinational circuits, unaffected by timing. Registers only exist between the s1, s2, and s3 channels.\nTherefore, increasing the number of sub-predictors will not increase the number of cycles required for prediction, but will only increase the delay required for prediction per cycle.\n","categories":["Xiangshan BPU Basic Design"],"description":"This section introduces the structure of the Xiangshan Branch Prediction Unit (BPU), including the integration of multiple predictors and multiple pipeline schemes, as well as the organization structure and interface design of internal sub-predictors, demonstrating how the BPU interacts with the Composer, and explaining the connection methods between sub-predictors.","excerpt":"This section introduces the structure of the Xiangshan Branch …","ref":"/xs-bpu/en/docs/basic/02_xsbpu_structure/","tags":["Xiangshan BPU Structure","Sub-predictors"],"title":"Introduction to the Xiangshan Branch Prediction Unit Structure"},{"body":"This section of the documentation will provide a detailed introduction to each module of the Xiangshan Branch Prediction Unit, including the BPU top-level and five sub-predictors.\nIn each module’s documentation, we will provide a detailed explanation of the module’s role in the Xiangshan Branch Prediction Unit, as well as the module’s algorithm principles, structure, and timing.\nStudents responsible for verifying a specific module should read the corresponding documentation thoroughly and understand it in conjunction with the code. Other documents can also be read to help you understand the overall functionality of the Xiangshan Branch Prediction Unit. During the understanding process, you may need to constantly review the basic design concepts and interface signals described in previous documents.\n","categories":["Xiangshan BPU Submodules"],"description":"This section of the documentation will provide a detailed introduction to each module of the Xiangshan Branch Prediction Unit, including the BPU top-level and five sub-predictors.","excerpt":"This section of the documentation will provide a detailed introduction …","ref":"/xs-bpu/en/docs/modules/","tags":["Xiangshan","BPU","Sub-Predictors"],"title":"Submodule Documentation"},{"body":"Introduction TAGE-SC is the primary predictor for conditional branches in the Kunming Lake architecture, classified as an Accurate Predictor (APD). TAGE-SC can be seen as two relatively independent components: the prediction part TAGE and the verification part SC.\nThe Tagged Geometric History Length Predictor (TAGE) utilizes multiple prediction tables with different history lengths to exploit extensive branch history information. TAGE predicts whether a branch instruction will be taken or not taken. It consists of a base prediction table and multiple history tables. It first predicts using multiple history tables. If there is no prediction, it uses the prediction from the base table. The Statistical Corrector (SC) is a statistical corrector. SC references the prediction results of TAGE and statistical bias results. Based on these two results, it corrects the final prediction result.\nIn Kunming Lake, each prediction block can have up to 2 branch instructions, so TAGE can predict up to 2 conditional branch instructions simultaneously. When accessing the various history tables in TAGE, the starting address of the prediction block is used as the PC, and two prediction results are retrieved based on the same global history.\nTAGE Branch Predictor in Kunming Lake Basic Functionality The core idea of the TAGE predictor is to provide prediction results with different history lengths and select the most appropriate result for feedback. In the TAGE predictor, there are a total of 1+N history record tables, where N is a configurable option. In Kunming Lake, N=4.\nThe base predictor based on the T0 table is the baseline predictor. During prediction, it directly looks up the “2-bit saturated counter representing the jump history information” corresponding to the address in the T0 table, and then makes a prediction based on the history information. The T0 table has only 2 bits per entry, so the history states it can record are limited.\nFor tables other than T0, we use Tn to represent them. During table lookup, in addition to the PC, it is also necessary to use the global jump history information H for the lookup. When a match is found, the prediction is made based on the “3-bit saturated predictor” to jump or not to jump. The higher the value of n for the Tn table, the longer the history information it uses, i.e., x\u003cy.\nFor each prediction, TAGE selects the table entry with the longest global jump history among all the hit Tn entries.\nIf the table entry exists and the prediction result confidence is high, it is used as the final prediction result. If the confidence is low, another internal counter is used to determine whether to select that entry or T0 as the final prediction. To save space, when retrieving the Tn table, the input jump history information H needs to be compressed, a process also known as history folding.\nThe table entries of each prediction table include the following elements:\nT0 table indexed directly by PC 2-bit pred unsigned saturated counter (indicating prediction direction and confidence level) Tn table indexed by XOR of PC and folded global history 1-bit valid bit 3-bit pred unsigned saturated counter 8-bit tag (used for verifying whether the hit is intentional, not coincidental) 1-bit useful bit for controlling expiration For a prediction block, all tables may generate prediction results, requiring a selection process. Typically, the higher the Tn table number, the higher its priority. Pipeline TAGE contains two pipeline stages: the first stage calculates the index, and the second stage reads the results from the SRAM table.\nStage 0 (s0): Input to the first pipeline stage, usually the pc and folded history. First pipeline stage operation: Calculate the index. Output to s1 via registers. Stage 1 (s1): Input to the second pipeline stage, consisting of the index and other calculated data from the first stage. Second pipeline stage operation: Memory access to SRAM, reading the prediction result. Output to s2 via registers. Stage 2 (s2): Actual prediction result. TAGE uses 2 stages for prediction, with the prediction result ready for use in the third stage after 2 stages. Data Structures In Kunming Lake’s implementation, the T0 and Tn table structures are as follows:\n预测器 作用 表项构成 项数 基准预测器T0 用于在其他预测器的预测结果都无效时输出预测结果 2 bit ctr 饱和计数器最高位决定跳转方向 2路各2048项，每路对于一条分支指令 预测表T1-T4 对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为最后预测结果。历史记录长度由输入的H决定 1 bit valid 有效位 3 bit ctr 饱和计数器8 bit tag 校验命中1 bit us 作为usefulness计数器 4096项、奇数项对应第一条分支指令，偶数项对应第二条分支指令 For each table Tn, the length of its input “global branch history data H” varies during querying. Assuming the total prediction history length is S, Tn and Tn+1 may use the low x, low y bits of S (the lower bits are the newer history) as query inputs. Generally, the larger the n of table Tn, the longer the history information used, i.e., x\u003cy.\nDuring the query of table Tn, since the historical data H is “compressed,” it may lead to a situation where the result of one PC1^H1 matches another PC2^H2 (similar to a Hash collision), resulting in indexing into invalid data (predicting PC1 indexes to predicted PC2’s data). Therefore, TAGE provides a tag identifier for each table, using an 8-bit tag in the Kunming Lake implementation to reduce the probability of collisions. The calculation method and index method for tags are different; only when the tag calculation is the same, the query result is valid.\nIn the Tn table entry, in addition to the saturation counter ctr and tag, there is also a 1-bit usefulness counter. When this counter is 0, it is a weak entry, indicating that the entry can be reallocated for other uses; when it is not 0, it is a strong entry, indicating that the entry cannot be reallocated for other uses.\nTo try to avoid the situation where all table entries are 1 and no new table entries can be allocated, TAGE expects to use the counter bankTickCtrs to clear all usefulness to 0.\nRetrieval Method for T0 and Tn Tables For table T0, indexing is done using the PC[11:1] bits to index 2048 table entries, so for T0, there is no possibility of not finding a match. For table Tn, in addition to PC[11:1], retrieval also requires searching based on the global branch history. In Kunming Lake, the top-level branch predictor maintains a 256-bit global history record GH, which can fold the GH’s most recent n bits of history information based on the required number of bits x for the sub-predictor. That is, n is divided into ceil(x/n) units of length x, and then XOR is performed bitwise. This folded history is denoted as FH (Folded History), and the specific process can be found in the [Branch Folding History section](../00_bpu_top/#Branch Folding History). When the TAGE predictor searches for a table entry in Tn, it uses the index and tag, calculated as follows: Calculation Formula index = FH ^ ((pc»1)低位) tag = FH1 ^ FH2 ^ ((pc»1)低位) Where FH, FH1, FH2 represent the folded global branch history according to certain rules. For Tn, FH, FH1, and FH2 each have their own folding bit numbers, which may not be the same. In the Kunming Lake implementation, the configurations of the T0 and Tn tables are as follows:\n表名称 FH长度 FH1长度 FH2长度 最近历史长度（用到GH中的位数） T1 8比特 8比特 7比特 低8位，即把最新8位历史，折叠成FH、FH1、FH2 T2 11比特 8比特 7比特 低13位，即把最新13位历史，折叠成FH、FH1、FH2 T3 11比特 8比特 7比特 低32位，即把最新32位历史，折叠成FH、FH1、FH2 T4 11比特 8比特 7比特 低119位，即把最新119位历史，折叠成FH、FH1、FH2 Note: pc»1 is used because RISC-C extension is used, with 2-byte alignment, and PC itself is already aligned to 1 byte, so only 1 bit is used.\nAlternative Predictor Since the Tn table uses saturation counters for prediction, there may be situations where the output result is “not confident.” For example, in Kunming Lake, for a 3-bit saturation counter, both 100 and 011 indicate a weak prediction. To provide more choices as references for this state, the TAGE predictor also provides an “alternative predictor” mechanism, which determines whether to select the prediction result of Tn or T0 when the Tn table predicts with low confidence.\nIn the Kunming Lake implementation, the “alternative predictor” is implemented based on the register group useAltOnNaCtrs. It consists of two paths of 128 4-bit saturation counters each, initialized to 0b1000. When TAGE makes a prediction, it uses PC(7,1) to index the corresponding saturation counter. If the value of this counter is greater than or equal to the preset value and the prediction result of Tn is not confident, it selects the result of T0; otherwise, it selects the result of Tn.\nPrediction Process In summary, the prediction steps of the TAGE predictor in Kunming Lake are as follows:\nParallel indexing of T0 and Tn tables, selecting which table to use based on the hit result: If a match to the tag of a Tn table is found, the potential prediction result is given by the saturation counter of the longest history Tn table. If no match to a Tn table is found, the final prediction result is given by the saturation counter of the T0 table. If the potential prediction result of the matched Tn table is a weak prediction (100,011), and the value of the corresponding 4-bit counter in the alternative prediction for PC is greater than or equal to a threshold, the result of the T0 table is used as the final result; otherwise, the prediction result of the Tn table is used as the final prediction result. Training Process Since the prediction process of TAGE involves many counters and tags, they need to be updated according to certain rules, a process known as training. This training process occurs in the BPU’s update stage, where the PC, branch history, and prediction correctness information are input. The training process for branch prediction in Kunming Lake is divided into several steps based on different conditions:\nUpdate when T0 is the final prediction result: If a jump occurs (i.e., taken), increment the ctr saturation counter indexed by the pc; otherwise, decrement it. When only T0 is hit, the following operations are performed: If T0 is predicted correctly, no additional update is performed. If T0 is predicted incorrectly, attempt to randomly allocate a new table entry in a Tn table. To allocate a new table entry, the original entry’s usefulness at the corresponding index must be 0. The new entry is initialized as a weak prediction with usefulness 0, and its tag is set to the newly calculated tag. When both T0 and Tn are hit, the following operations are performed: Tn is always updated: If a jump occurs, increment the ctr saturation counter indexed by the pc; otherwise, decrement it. It is important to note that “hit” means that the tag of the indexed entry matches the calculated tag. If T0 and Tn produce the same result: If predicted correctly, no additional update is performed. If predicted incorrectly, attempt to allocate a new table entry in a table with a longer history than Tn. To allocate a new entry, the usefulness of the original entry at the corresponding index must be 0. The new entry is initialized as a weak prediction with usefulness 0, and its tag is set to the tag calculated using the new history information. If T0 and Tn produce different results: If Tn is correct, the entry’s usefulness is incremented. If the result is still a weak prediction, the counter in the alternative prediction for T0 is decremented. If Tn is incorrect, the entry’s usefulness is decremented, and a new entry is allocated in a table with a longer history than Tn, as in 3.2.2. If the result is still a weak prediction, the counter in the alternative prediction for T0 is incremented. When a new table needs to be allocated, dynamic reset of the usefulness flag is performed. Using a 7-bit bankTickCtrs register and calculating: The number of allocatable tables a (with a longer history length than the current and corresponding index usefulness is 0) The number of unallocatable tables b (with a longer history length than the current and corresponding index usefulness is not 0) Update bankTickCtrs += Δ (saturated counter), Δ = b - a, When bankTickCtrs reaches its maximum value, reset all usefulness to 0. Kunming Lake SC Branch Predictor Basic Function Introduction The SC (Statistics counter) branch predictor is a branch predictor based on historical statistical information. Similar to TAGE, SC typically has multiple tables Tn, each corresponding to different lengths of historical jump statistics. The difference is that in SC, when predicting based on the PC, each table Tn is accessed, and then SC adds up each hit table entry to calculate the total “saturated counter” jump information, and finally determines whether to jump based on the total jump information. Generally, SC uses “signed saturated counters”, where a counter value greater than 0 indicates a jump, and less than 0 indicates no jump. The larger the absolute value of the counter, the higher the prediction confidence.\nIn the SC predictor, SC is also composed of multiple tables (e.g., T1, T2, T3, T4), but with fewer basic prediction tables T0 compared to the TAGE predictor. The Tn tables in SC have 6-bit signed saturated counters. The indexing method for SC tables is as follows:\nCalculation Method Index = (FH) ^ ((pc»1)低位) For each table, the number of entries and the folded history length used are as follows:\nTable Number of Entries FH Length Folded History Range T1 512 0 不折叠 T2 512 4 把历史信息的低4位，折叠成FH T3 512 8 把历史信息的低10位，折叠成FH T4 512 8 把历史信息的低16位，折叠成FH The formula for calculating the total statistical prediction result is as follows:\n$$scCtrSum=\\sum_{i=0}^{i\u003c4}( (ctr_{sc} \u003c\u003c 1) +1)$$Where ctr_sc represents the signed saturated counter for each table. Left-shifting and adding one is for weight adjustment. The accumulated scCtrSum is the final prediction result of SC. If this value is greater than zero, the prediction is a jump; if it is less than zero, the prediction is no jump. The larger the absolute value, the higher the prediction confidence.\nTypical data conversion results are as follows (extended to 9 bits to prevent overflow during calculation):\nAll are 6b100000 (strong no-jump), resulting in 9b100000100, with a value of -252. All are 6b011111 (strong jump), resulting in 9b011111100, with a value of 252. All are 6b000000 (weak jump), resulting in 9b000000100, with a value of 4. All are 6b111111 (weak no-jump), resulting in 9b111111100, with a value of -4. Prediction Process Calculate the index of table Tn using the PC and historical information. Query the index to obtain the saturation counters for all tables. Sum up all the saturation counters obtained from all tables to get the final prediction result (take a jump for values greater than 0, no jump for values less than 0). Training Process Update the saturation counters during the update phase.\nIf the real instruction corresponding to PC jumps, increment the saturation counters corresponding to all tables. If the real instruction corresponding to PC does not jump, decrement the saturation counters corresponding to all tables. Kunming Lake TAGE-SC Branch Predictor Why SC is Needed with TAGE In some applications, some branch behaviors have a weak correlation with branch history or paths, showing a statistical prediction bias. For these branches, using counters to capture statistical biases is more effective than history-based branch prediction.\nTAGE is very effective in predicting branches that are highly correlated with history, but it performs poorly for branches with statistical biases. For example, branches that have a small bias in one direction but are not strongly correlated with historical paths. To avoid this problem, an SC predictor can be added to the traditional TAGE predictor.\nTAGE-SC Functionality In the Kunming Lake TAGE-SC predictor, both the TAGE and SC prediction results P1 and P2 are obtained simultaneously, and then their results are accumulated P = P1 + P2. If the absolute value of P is greater than the 8-bit threshold sc_bank_thres, the predictor result P is used; otherwise, P1 is used as the final prediction result.\nFor dynamic adaptation, the threshold sc_thres needs to be dynamically changed. Therefore, in the implementation, TAGE-SC uses a 5-bit sc_bank_ctr counter to adjust the threshold sc_bank_thres. Additionally, since Kunming Lake supports the simultaneous prediction of 2 branch instructions, the threshold register and corresponding control counter are also duplicated.\nPipeline The TAGE-SC predictor contains 3 pipeline stages, where the 2-stage pipeline of TAGE has been introduced, and the pipeline of the SC part is as follows:\nStage 0: Read PC and folded history into s0. First Stage: Calculate the index from pc and FH to obtain s0_idx.\nStage 1: Read s0_idx from s0. Second Stage: Find the counter data corresponding to s1_idx in SCTable and output to s1_scResps.\nStage 2: Read s1_scResps from s1. Third Stage: Select whether to invert the prediction result based on s2_scResps and output to s2_disagree.\nStage 3: Read the result from s2_disagree as s3_disagree.\nPrediction Process In TAGE-SC prediction, the prediction result P1 of TAGE is represented by tage_ctr, and the prediction result P2 of SC is represented by scCtrSum. The prediction is divided into four steps:\nExecute the SC predictor to get the prediction result scCtrSum.\nSimultaneously obtain the prediction result tage_ctr of the TAGE predictor.\nSince the prediction result of TAGE is an unsigned saturation counter, and the prediction result of SC is a signed saturation counter, if they are added together, data conversion is required.\nKunming Lake adopts a conversion for the result of TAGE. The converted result is represented by tageCtrCentered, and the specific conversion process is as follows:\n$$tageCtrCentered=((((ctr_{tage} -4)\u003c\u003c1)+1)\u003c\u003c3) $$ Conversion of a 3-bit unsigned saturation counter to an 8-bit signed saturation counter result is illustrated as follows:\n3b100 Weak jump =\u003e 8b00001000 = 8\n3b011 Weak non-jump =\u003e 8b11111000 = -8\n3b111 Strong jump =\u003e 8b00111000 = 56\n3b000 Strong non-jump =\u003e 8b11001000 = -56\nAdd the prediction results of TAGE and SC to get the final prediction result P, represented by totalSum.\n$$totalSum = scCtrSum + tageCtrCentered$$ Determine the final prediction direction based on totalSum and sc_bank_thres\nJump if totalSum \u003e 0 and its absolute value exceeds the threshold: If scCtrSum \u003e sc_bank_thres - tageCtrCentered, it can also be understood as totalSum \u003e sc_bank_thres. The above expression can reduce the maximum bit width (ensuring no overflow requires 10 bits to become 9 bits). No jump if totalSum \u003c 0 and its absolute value exceeds the threshold: If scCtrSum \u003c -sc_bank_thres - tageCtrCentered, it can also be understood as |totalSum| \u003e sc_bank_thres. Training Process After combining TAGE and SC, TAGE-SC adds an sc_bank_ctr counter to control the threshold sc_bank_thres. Therefore, during training, in addition to the training of TAGE and SC themselves, the newly added counter needs to be updated.\nDuring the update phase, the specific update process is as follows:\nTAGE-SC uses the prediction result P (i.e., the prediction result after TAGE + SC). If |totalSum| is in the range [sc_bank_thres -4, sc_bank_thres -2], update the threshold-related register group. Update sc_bank_ctr, the saturation counter: If the prediction is correct, sc_bank_ctr +=1; if the prediction is incorrect, sc_bank_ctr -=1. Update sc_bank_thres, limited saturation operation: If the updated value of sc_bank_ctr reaches 0b11111 and sc_bank_thres \u003c= 31, then sc_bank_thres +=2; if the updated value of sc_bank_ctr is 0 and sc_bank_thres \u003e=6, then sc_bank_thres -=2. For all other cases, thres remains unchanged. After the update judgment of sc_bank_thres is completed, another judgment is made on sc_bank_ctr. If the updated sc_bank_ctr is 0b11111 or 0, thres_ctr is reset to the initial value 0b10000. TAGE-SC uses the prediction result P1 (i.e., the prediction result of TAGE) and does not perform any operations. Interface List TageSC 信号类型 信号宽度 信号名 信号描述 input * clock input * reset input *[35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input *[40:0] io_in_bits_s0_pc_0 复制的s0_pc的dup数组的第1个，给顶层BPU的PC input *[40:0] io_in_bits_s0_pc_1 复制的s0_pc第2个，给Tage的PC input *[40:0] io_in_bits_s0_pc_3 复制的s0_pc的第4个，给SC的PC input *[10:0] io_in_bits_folded_hist_1_hist_17_folded_hist TageTable 2 用到的11bits 折叠历史 从多长历史范围折叠到11bit见前文所述的表 注意TageTable下标+1，此处 T2 是前文 T3 input *[10:0] io_in_bits_folded_hist_1_hist_16_folded_hist TageTable 3 用到的11bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_15_folded_hist TageTable 1 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_14_folded_hist TageTable 0 用到的8bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_9_folded_hist TageTable 2 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_8_folded_hist TageTable 3 用到的8bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_7_folded_hist TageTable 0 用到的7bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_5_folded_hist TageTable 3 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_4_folded_hist TageTable 1 用到的8bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_3_folded_hist TageTable 2 用到的8bits 折叠历史 input *[10:0] io_in_bits_folded_hist_1_hist_1_folded_hist TageTable 1 用到的11bits 折叠历史 input *[3:0] io_in_bits_folded_hist_3_hist_12_folded_hist SCTable 1 用到的 4bit 折叠历史 input *[7:0] io_in_bits_folded_hist_3_hist_11_folded_hist SCTable 2 用到的 8bit 折叠历史 input *[7:0] io_in_bits_folded_hist_3_hist_2_folded_hist SCTable 3 用到的 8bit 折叠历史 output * io_out_s2_full_pred_0_br_taken_mask_0 io_out_s2_full_pred_{i}br_taken_mask{j} Tage 在 s2流水级输出的，复制4份 预测块中第 j 条分支指令TAGE预测结果 这里不该叫mask吧 output * io_out_s2_full_pred_0_br_taken_mask_1 output * io_out_s2_full_pred_1_br_taken_mask_0 output * io_out_s2_full_pred_1_br_taken_mask_1 output * io_out_s2_full_pred_2_br_taken_mask_0 output * io_out_s2_full_pred_2_br_taken_mask_1 output * io_out_s2_full_pred_3_br_taken_mask_0 output * io_out_s2_full_pred_3_br_taken_mask_1 output * io_out_s3_full_pred_0_br_taken_mask_0 io_out_s3_full_pred_{i}br_taken_mask{j} Tage 在 s3流水级输出的，复制4份 预测块中第 j 条分支指令SC预测结果 output * io_out_s3_full_pred_0_br_taken_mask_1 output * io_out_s3_full_pred_1_br_taken_mask_0 output * io_out_s3_full_pred_1_br_taken_mask_1 output * io_out_s3_full_pred_2_br_taken_mask_0 output * io_out_s3_full_pred_2_br_taken_mask_1 output * io_out_s3_full_pred_3_br_taken_mask_0 output * io_out_s3_full_pred_3_br_taken_mask_1 output *[222:0] io_out_last_stage_meta 见附表 input * io_ctrl_tage_enable input * io_ctrl_sc_enable input * io_s0_fire_0 s0 阶段流水线控制 相同信号复制多份，0给BPU，1给Tage，3给SC input * io_s0_fire_1 input * io_s0_fire_3 input * io_s1_fire_0 s1 阶段流水线控制 input * io_s1_fire_1 input * io_s1_fire_2 input * io_s1_fire_3 input * io_s2_fire_0 s2 阶段流水线控制 input * io_s2_fire_1 input * io_s2_fire_2 input * io_s2_fire_3 output * io_s1_ready tage的所有表，可以执行读取结果的操作 input * io_update_valid 从FTQ发向BPU的后端执行结果（更新信号）是否有效 input *[40:0] io_update_bits_pc （后端执行过的）预测块的PC input *[10:0] io_update_bits_spec_info_folded_hist_hist_17_folded_hist TageTable 2 用到的11bits 折叠历史 预测时使用的分支历史结果，没有更新，转了一圈回来了 input *[10:0] io_update_bits_spec_info_folded_hist_hist_16_folded_hist TageTable 3 用到的11bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_15_folded_hist TageTable 1 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_14_folded_hist TageTable 0 用到的8bits 折叠历史 input *[3:0] io_update_bits_spec_info_folded_hist_hist_12_folded_hist SCTable 1 用到的 4bit 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_11_folded_hist SCTable 2 用到的 8bit 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_9_folded_hist TageTable 2 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_8_folded_hist TageTable 3 用到的8bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_7_folded_hist TageTable 0 用到的7bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_5_folded_hist TageTable 3 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_4_folded_hist TageTable 1 用到的8bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_3_folded_hist TageTable 2 用到的8bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_2_folded_hist SCTable 3 用到的 8bit 折叠历史 input *[10:0] io_update_bits_spec_info_folded_hist_hist_1_folded_hist TageTable 1 用到的11bits 折叠历史 input * io_update_bits_ftb_entry_brSlots_0_valid FTB 表项的第一个slot是否有效（存储了跳转指令） input * io_update_bits_ftb_entry_tailSlot_sharing FTB 表项的最后一个slot是否存储了条件分支而非无条件跳转 input * io_update_bits_ftb_entry_tailSlot_valid FTB 表项的最后一个slot是否有效 input * io_update_bits_ftb_entry_always_taken_0 历史上slot 0 指令总是跳转 input * io_update_bits_ftb_entry_always_taken_1 历史上slot 1 指令总是跳转 input * io_update_bits_br_taken_mask_0 solt 0 是否 taken input * io_update_bits_br_taken_mask_1 solt 1 是否 taken input * io_update_bits_mispred_mask_0 solt 0 是否预测正确 input * io_update_bits_mispred_mask_1 solt 1 是否预测正确 input *[222:0] io_update_bits_meta 见附表 io_out_last_stage_meta 需要设计参与优化！\n信号类型 信号位 信号名 信号描述 output [218:88] 0 占位，全为0，传递到composer时会忽略 87 resp_meta_providers_1_valid_r [86:85] resp_meta_providers_1_bits_r 84 resp_meta_providers_0_valid_r [83:82] resp_meta_providers_0_bits_r [81:79] resp_meta_providerResps_1_r_ctr 78 resp_meta_providerResps_1_r_u 77 resp_meta_providerResps_1_r_unconf [76:74] resp_meta_providerResps_0_r_ctr 73 resp_meta_providerResps_0_r_u 72 resp_meta_providerResps_0_r_unconf 71 resp_meta_altUsed_1_r 70 resp_meta_altUsed_0_r 69 resp_meta_altDiffers_1_r 68 resp_meta_altDiffers_0_r [67:66] resp_meta_basecnts_1_r [65:64] resp_meta_basecnts_0_r [63:60] resp_meta_allocates_1_r [59:56] resp_meta_allocates_0_r 55 resp_meta_takens_1_r 54 resp_meta_takens_0_r 53 resp_meta_scMeta_tageTakens_1_r 52 resp_meta_scMeta_tageTakens_0_r 51 resp_meta_scMeta_scUsed_1_r 50 resp_meta_scMeta_scUsed_0_r 49 resp_meta_scMeta_scPreds_1_r 48 resp_meta_scMeta_scPreds_0_r [47:42] r_1_3 scMeta(预测时的状态)中第2路的第4个sc_ctr的值 [41:36] r_1_2 scMeta中第2路的第3个sc_ctr的值 [35:30] r_1_1 scMeta中第2路的第2个sc_ctr的值 [29:24] r_1_0 scMeta中第2路的第1个sc_ctr的值 [23:18] r_3 scMeta中第1路的第4个sc_ctr的值 [17:12] r_2 scMeta中第1路的第3个sc_ctr的值 [11:6] r_1 scMeta中第1路的第2个sc_ctr的值 [5:0] r_0 scMeta中第1路的第1个sc_ctr的值 io_update_bits_meta 信号类型 信号位 信号名 信号描述 input [218:94] FTB, ITAGE, RAS 模块传给 FTQ 的 META 信息，忽略 [93:6] io_out_last_stage_meta[87:0] 偏移 6bit 后的结果 TAGE 输出给 FTQ 的 META [5:0] uFTB 输出给 FTQ 的 META ","categories":["Branch Prediction Unit (BPU)"],"description":"","excerpt":"Introduction TAGE-SC is the primary predictor for conditional branches …","ref":"/xs-bpu/en/docs/modules/02_tage_sc/","tags":["Subpredictor","TAGE-SC"],"title":"TAGE-SC Branch Predictor"},{"body":"功能介绍 TAGE-SC 是南湖架构条件分支的主预测器，属于精确预测器（Accurate Predictor，简称 APD）。TAGE-SC 可以看作两个功能相对独立的组件：预测部分 TAGE 和 校验部分 SC。\n标记几何历史长度预测器 TAGE （Tagged Geometric History Length Predictor）利用历史长度不同的多个预测表，可以挖掘极长的分支历史信息。TAGE功能是预测一个跳转指令是跳转（Taken），还是不跳转（不Taken）， 它由一个基预测表和多个历史表组成，首先通过多个历史表进行分支预测，如果没有预测结果，则再采用基础预测表的预测结果。 SC (Statistical Corrector) 是统计校正器。当 SC 会参考 TAGE 的预测结果和统计偏向的结果。并根据这两个结果，矫正最终的预测结果。 在昆明湖中，由于每个预测块，最多可以有2条跳转指令，因此TAGE在每次预测最多同时预测 2 条条件分支指令。在访问 TAGE 的各个历史表时，用预测块的起始地址作为 PC，同时取出两个预测结果，并基于相同的全局历史进行预测。\n昆明湖Tage分支预测器 基本功能 TAGE预测器的核心思路是提供不同历史长度的预测结果，并选择最合适的结果进行反馈。在TAGE预测器中，一共有 1+N 个表历史记录表，N 为可配置选项。以昆明湖为例，N=4。\n基于 T0 表的预测器为基础预测器。在预测时，直接通过预测块的 PC 地址在 T0 表中查找到该地址对应的 “2 bit 饱和计数器表示的跳转历史信息” ，然后根据历史信息做出预测结果。T0表的表项只有2bit，所以能记录的历史状态有限。\n对于 T0 以外的表，我们用 Tn 进行表示。在查表时除了 PC 外，还需要用到全局跳转历史信息H进行查找。查找命中后，根据 “3bit饱和预测器” 进行跳转或者不跳转的预测。Tn 表的n越大，其用到的H中的位数越多。\n对于每一次预测，TAGE 从所有 Tn 命中表项中选择全局跳转历史最长的表项。\n如果该表项存在，且预测结果置信度高就用该项作为最终预测结果。 如果置信度低，就通过另外一个内部计数器来判断选择该项还是 T0 作为最后预测。 在实现上，为了节约空间，检索 Tn 表时，输入的跳转历史信息H需要进行压缩，该过程也称为历史折叠。\n各预测表的表项包含如下元素：\nT0 表项通过 pc 直接索引 2 bit pred 无符号饱和计数器（给出预测方向和信心强度） Tn 表项通过 异或 pc 与 折叠全局历史 索引 1 bit valid 有效位 3 bit pred 无符号饱和计数器 8 bit tag （基于Hash校验是否命中，而非巧合） 1 bit useful 用于控制有效期 对于一个预测块，有可能所有的表都会产生预测结果，此时就需要进行选择。如上图所示，一般情况下，Tn表的编号越大，优先级越高。\n流水线 TAGE 内含两个流水级，第一级计算索引，第二级通过索引读出SRAM表中的结果\n第0拍，s0：第一个流水级的输入，一般是pc和折叠历史。 第一流水级运行：计算index。通过寄存器输出到s1\n第1拍，s1：第二个流水级的输入，为第一级阶段计算好的index等数据。 第二流水级运行：访存SRAM，读取预测结果。通过寄存器输出到s2\n第2拍，s2：实际预测结果。TAGE用了2拍进行预测，在2拍之后的第3拍预测结果就绪可用。 数据结构 在昆明湖的实现中，T0与Tn的表结构如下所示：\n预测器 作用 表项构成 项数 基准预测器T0 用于在其他预测器的预测结果都无效时输出预测结果 2 bit ctr 饱和计数器最高位决定跳转方向 2路各2048项，每路对于一条分支指令 预测表T1-T4 对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为最后预测结果。历史记录长度由输入的H决定 1 bit valid 有效位 3 bit ctr 饱和计数器8 bit tag 校验命中1 bit us 作为usefulness计数器 4096项、奇数项对应第一条分支指令，偶数项对应第二条分支指令 对于每个表Tn，在查询时，其输入“全局分支历史数据H”的长度是不同的。假如总的预测历史长度为S，Tn、Tn+1可能用S的低x，低y位 （越低的位是越新的历史）作为查询输入。一般情况下，Tn表的n越大，其用到的历史信息越长，即 x\u003cy 。\n在进行 Tn 表的查询时，由于历史数据H进行了“压缩”，可能导致一个PC1^H1的结果与另一个PC2^H2的结果相同（类似Hash表的Hash冲突），进而导致索引到无效数据（预测PC1索引到预测PC2的数据）。所以TAGE为每个表提供了一个tag标识，昆明湖中采用了8bit tag，来减小冲突发生的概率，tag的计算方法和查询的索引方法不同，只有当tag计算相同时查询结果才有效。\n在Tn表项中，除了饱和计数器ctr和tag外，还有1bit的usefulness计数器，该计数器为0时，作弱表项，表示可以当作该项可以被重新分配当作他用；不为0时，作强表项，该项不能被分配当作他用。\n为了尽力避免所有表项全为1，无法分配新表项的情况发生，TAGE预期使用计数器bankTickCtrs 对所有usefulness进行清零操作。\nT0，Tn表的检索方法 对于T0表，在查找时用 PC[11:1] 位进行索引2048表项，因此对于T0来说，不可能存在检索不到的情况。 对于表Tn，除了 PC[11:1] 外，还需要基于全局分支历史进行检索。在昆明湖中，顶层分支预测器维护了一个256bit的全局历史记录GH，可以根据子预测器需要将GH的最近n位历史信息，按目标位数x进行折叠。 即把n，以长度x为基本单元，拆分成ceil(x/n)个长为x的数，然后按位进行异或得到。通常用FH（Folded History）表示，具体流程请参考 分支折叠历史章节。TAGE预测器查找Tn的表项时，用到了index和tag，他们的计算公式如下表所示： 计算方法 公式 index = FH ^ ((pc»1)低位) tag = FH1 ^ FH2 ^ ((pc»1)低位) 其中FH、FH1、FH2表示按一定规范折叠后的全局分支历史。对于Tn而言，其FH、FH1、FH2都有各自的对于折叠位数，不一定相同。在昆明湖实现中，T0和Tn表的配置如下：\n表名称 FH长度 FH1长度 FH2长度 最近历史长度（用到GH中的位数） T1 8比特 8比特 7比特 低8位，即把最新8位历史，折叠成FH、FH1、FH2 T2 11比特 8比特 7比特 低13位，即把最新13位历史，折叠成FH、FH1、FH2 T3 11比特 8比特 7比特 低32位，即把最新32位历史，折叠成FH、FH1、FH2 T4 11比特 8比特 7比特 低119位，即把最新119位历史，折叠成FH、FH1、FH2 注：pc»1是因为使用了RISC-C拓展，2Byte对齐，而PC本身已经省略了1B对齐，所以只用1b\n替代预测器 由于 Tn 表使用饱和计数器进行预测，因此其输出结果可能存在“信心不足”的情况。例如在昆明湖中，对3比特饱和计数器，100、011时都是弱预测。为了为该状态提供更多的选择参考，TAGE 预测器还提供了一个“替代预测器”机制，他的作用是告诉 TAGE 在 Tn 表预测的结果 “信心不足” 时，是该选择 Tn 的预测结果，还是选择 T0 的预测结果。\n在昆明湖的具体实现中，“替代预测器” 基于 useAltOnNaCtrs 的寄存器组实现。它由两路128个 4-bit饱和计数器 构成，每个计数器都被初始化为 0b1000。在TAGE进行预测时，使用 PC(7,1) 进行索引得到对应的饱和计数器。如果该计数器的值大于等于预设值，且Tn预测结果信心不高，则选择T0的结果，否则选择Tn的结果。\n预测过程 综上所述，昆明湖中TAGE预测器的预测步骤如下：\n并行索引 T0 表以及 Tn 表，根据命中结果选择使用哪一个表给出预测结果 如果有命中到tag匹配的Tn表，由历史记录最长Tn表的ctr饱和计数器给出潜在预测结果。 如果没有命中Tn表，则由T0表的饱和计数器给出最终预测结果。 如果匹配到的Tn表的潜在预测结果为弱预测(100,011)，且替代预测中PC对应4-bit计数器的值大于等于阈值，则采用T0表的结果为最终结果，否则采用Tn表的预测结果为最终预测结果。 训练过程 由于TAGE的预测过程用到了很多计数器和tag，他们需要根据一定的规则进行更新，该过程称为训练。该训练过程发生在BPU的update阶段，该阶段会输入PC，分支历史、以及否预测正确等信息。香山昆明湖的分支预测进行训练的流程根据不同情况，分为如下几个步骤：\nT0作为最终使用的预测结果时更新：发生跳转（即taken）则 pc 索引的 ctr 饱和计数器 +1，否则 -1 在只有T0命中时，进行如下操作 T0预测正确则不额外更新 T0预测错误则尝试随机在Tn中的某个表申请一个新表项 申请新表项时需要对应index位置的原表项useful为0 新表项默认是弱预测，useful为0，并设置其tag为计算出来的新tag 在T0和Tn同时命中时，进行如下操作 Tn 始终更新：taken 则 pc 索引的 ctr 饱和计数器 +1，否则 -1 需要注意，”命中“ 表示 index 索引到的表项的 tag 要和计算出的 tag 匹配 若T0和Tn结果相同 预测正确则不额外更新 预测错误则尝试在比Tn对应历史更长的表中随机申请一个新表项 申请新表项时需要对应index位置的原表项useful为0 新表项默认是弱预测，useful为0，tag设置为用新历史信息计算出来的tag 若T0和Tn结果不同 若Tn正确则表项 useful +1 若结果还同时为弱预测，则选用T0的替代预测计数器 -1 若Tn错误则表项 useful -1，同时如 3.2.2 在更长历史表中申请新表项 若结果还同时为弱预测，则选用T0的替代预测计数器 +1 在每次需要分配表时，进行动态重置usefulness标志位 使用 7bit 的 bankTickCtrs 寄存器，并计算 可分配的表数 a（历史长度比当前更长，且对应索引的useful为 0 ） 不可分配的表数 b（历史长度比当前更长，且对应索引的useful 不为 0 ） 更新 bankTickCtrs += Δ（饱和计数）， Δ = b - a， 当 bankTickCtrs当为最大值时, 重置所有useful=0 昆明湖SC分支预测器 基本功能介绍 SC（Statistics counter）分支预测器是一种基于历史统计信息的分支预测器。\n与TAGE类似，在SC中通常有多个表Tn，他们对应了不同历史长度的跳转统计。不同点在于，在预测按PC检索时，每个表Tn都会命中，然后SC把每个命中的表项进行求和，计算总的“饱和计数器”跳转信息，最后根据总的跳转信息判断是否跳转。一般情况下，SC采用“有符号饱和计数器”，计数器值大于0时表示跳转，小于0时表示不跳转。计数器的绝对值越大，表示其预测置信度越高。\n在SC预测器中，SC也由多个表组成（例如T1，T2，T3，T4），但相对TAGE预测器少了基础预测表T0。SC中Tn表有6 bit的有符号饱和计数器。SC表的索引方式如下：\n计算方式 Index = (FH) ^ ((pc»1)低位) 对应每个表，其表项数和用到的折叠历史长度如下：\n表名 表项数 FH长度 折叠历史范围 T1 512 0 不折叠 T2 512 4 把历史信息的低4位，折叠成FH T3 512 8 把历史信息的低10位，折叠成FH T4 512 8 把历史信息的低16位，折叠成FH 在计算统计预测结果时，总得统计结果计算公式如下：\n$$scCtrSum=\\sum_{i=0}^{i\u003c4}( (ctr_{sc} \u003c\u003c 1) +1)$$其中ctr_sc表示每个表的有符号饱和计数器。对其进行左移加一是进行权重调整。累加后的scCtrSum就是SC的最终预测结果，如果为该值大于零，则预测跳转，小于零则预测不跳转。其绝对值越大，表示预测置信度越高。\n典型数据转换结果如下（在计算时为了不溢出，扩展到了9位）：\n均为 6b100000（强不跳转） ，求得 9b100000100，值为 -252 均为 6b011111（强跳转），求得 9b011111100，值为 252 均为 6b000000（弱跳转），求得 9b000000100，值为 4 均为 6b111111（弱不跳转），求得 9b111111100，值为 -4 预测过程 通过PC和历史信息结算表Tn的索引 Index 根据Index查询，得到所有表对应的饱和计数器 所有所有表查询到的饱和计数器进行求和得到最终预测结果（大于0跳转，小于0不跳转） 训练过程 在update阶段进行饱和计数器的更新。\n如果PC对应的真实指令跳转，这所有表中对应的饱和计数器+1 如果PC对应的真实指令不跳转，这所有表中对应的饱和计数器-1 昆明湖TAGE-SC分支预测器 有了TAGE为何还需要SC 一些应用上，一些分支行为与分支历史或路径相关性较弱，表现出一个统计上的预测偏向性。对于这些分支，相比基于历史的分支预测，使用计数器捕捉统计偏向的方法更为有效。\nTAGE在预测与历史非常相关的分支时非常有效，但对有统计偏向的分支则支持不佳。例如只对一个方向有小偏差，但与历史路径没有强相关性的分支。为了避免该问题，可以在传统TAGE预测器上增加SC预测器。\nTAGE-SC功能介绍 在昆明湖 TAGE-SC 预测器中，会同时得到 TAGE 和 SC 的预测结果 P1 和 P2，然后对他们的结果进行累加 P = P1+P2 ，如果 P 的绝对值大于 8bit 的阈值 sc_bank_thres，则采用预测器结果 P，否则则采用 P1 作为最终预测结果。\n为了进行动态自适应，阈值sc_thres是需要是动态变化的。为此在实现上TAGE-SC使用了一个5bit的sc_bank_ctr计数器对阈值sc_bank_thres进行调整。另外，由于昆明湖支持同时预测2条分支指令，因此阈值寄存器和对应的控制计数器也是两份。\n流水线 TAGE-SC 内含 3 个流水级，TAGE的2级流水已经介绍过，SC部分的流水线如下：\n第0拍，s0 读入 pc 和折叠历史 第一流水级：通过pc和FH计算出index，并输出到s0_idx寄存器\n第1拍，s1 从s0寄存器读出s0_idx作为s1_idx 第二流水级：找到SCTable中s1_idx对应的计数器数据，输出到s1_scResps寄存器\n第2拍，s2 从s1寄存器读出s1_scResps作为s2_scResps 第三流水级：根据s2_scResps选择是否需要反转预测结果，输出到s2_disagree寄存器\n第3拍，s3 从s2_disagree中读出结果，作为s3_disagree输出 预测过程 在TAGE-SC预测时，TAGE的预测结果P1用tage_ctr表示，SC的预测结果P2用scCtrSum表示。在预测时，分为如下四步。\n执行SC预测器，得到预测结果scCtrSum\n并行得到TAGE预测器的预测结果 tage_ctr。\n由于TAGE的预测结果是无符号饱和计数器，而SC的预测结果是有符号饱和计数器，如果对他们进行相加，需要进行数据转换。\n昆明湖在实现上，采用了对TAGE的结果进行转换。转换后的结果用 tageCtrCentered 表示，具体转换过程如下：\n$$tageCtrCentered=((((ctr_{tage} -4)\u003c\u003c1)+1)\u003c\u003c3) $$ 3比特的无符号饱和计数器转换为8比特的有符号饱和计数器结果举例如下所示：\n3b100 弱跳转 =\u003e 8b00001000 = 8\n3b011 弱不跳转 =\u003e 8b11111000 = -8\n3b111 强跳转 =\u003e 8b00111000 = 56\n3b000 强不跳转 =\u003e 8b11001000 = -56\n对TAGE和SC的预测结果进行相加得到最终预测结果P，在实现上用totalSum进行表示。\n$$totalSum = scCtrSum + tageCtrCentered$$ 根据 totalSum 以及 sc_bank_thres 决定最终预测方向\ntotalSum \u003e 0 且绝对值超过阈值则跳转： 如果 scCtrSum \u003e sc_bank_thres - tageCtrCentered 也可以理解成 totalSum \u003e sc_bank_thres 上面的写法可以降低最大位宽（把保证不溢出需要10bit变成9bit） totalSum \u003c 0 且绝对值超过阈值则不跳转： 如果 scCtrSum \u003c -sc_bank_thres - tageCtrCentered 也可以理解成 |totalSum| \u003e sc_bank_thres 训练过程 在对 TAGE 和 SC 进行组合后，TAGE-SC 添加了 sc_bank_ctr 计数器用来控制阈值sc_bank_thres。因此在训练时，除了 TAGE 和 SC 本身的训练外，还需要对新增加的计数器进行更新。\n在update阶段，其具体更新流程如下：\nTAGE-SC采用了预测结果P（即TAGE+SC后的预测结果），如果 |totalSum| 在 [sc_bank_thres -4, sc_bank_thres -2] 的范围内，则对阈值相关寄存器组进行更新 更新 sc_bank_ctr，饱和计数 若预测正确，则 sc_bank_ctr +=1 若预测错误，则 sc_bank_ctr -=1 更新 sc_bank_thres ，受限制的饱和运算， 若 sc_bank_ctr 更新后的值已达 0b11111 且 sc_bank_thres \u003c= 31，则 sc_bank_thres +=2 若 sc_bank_ctr 更新后的值为 0 且 sc_bank_thres \u003e=6，则 sc_bank_thres -=2 其余情况thres不变。 sc_bank_thres 更新判断结束后，会对 sc_bank_ctr 再做一次判断 若更新后的sc_bank_ctr若为0b11111或0，则thres_ctr会被置回初始值0b10000。 TAGE-SC采用了预测结果P1（即TAGE的预测结果）不进行任何操作 接口列表 TageSC 信号类型 信号宽度 信号名 信号描述 input * clock input * reset input *[35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input *[40:0] io_in_bits_s0_pc_0 复制的s0_pc的dup数组的第1个，给顶层BPU的PC input *[40:0] io_in_bits_s0_pc_1 复制的s0_pc第2个，给Tage的PC input *[40:0] io_in_bits_s0_pc_3 复制的s0_pc的第4个，给SC的PC input *[10:0] io_in_bits_folded_hist_1_hist_17_folded_hist TageTable 2 用到的11bits 折叠历史 从多长历史范围折叠到11bit见前文所述的表 注意TageTable下标+1，此处 T2 是前文 T3 input *[10:0] io_in_bits_folded_hist_1_hist_16_folded_hist TageTable 3 用到的11bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_15_folded_hist TageTable 1 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_14_folded_hist TageTable 0 用到的8bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_9_folded_hist TageTable 2 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_8_folded_hist TageTable 3 用到的8bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_7_folded_hist TageTable 0 用到的7bits 折叠历史 input *[6:0] io_in_bits_folded_hist_1_hist_5_folded_hist TageTable 3 用到的7bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_4_folded_hist TageTable 1 用到的8bits 折叠历史 input *[7:0] io_in_bits_folded_hist_1_hist_3_folded_hist TageTable 2 用到的8bits 折叠历史 input *[10:0] io_in_bits_folded_hist_1_hist_1_folded_hist TageTable 1 用到的11bits 折叠历史 input *[3:0] io_in_bits_folded_hist_3_hist_12_folded_hist SCTable 1 用到的 4bit 折叠历史 input *[7:0] io_in_bits_folded_hist_3_hist_11_folded_hist SCTable 2 用到的 8bit 折叠历史 input *[7:0] io_in_bits_folded_hist_3_hist_2_folded_hist SCTable 3 用到的 8bit 折叠历史 output * io_out_s2_full_pred_0_br_taken_mask_0 io_out_s2_full_pred_{i}br_taken_mask{j} Tage 在 s2流水级输出的，复制4份 预测块中第 j 条分支指令TAGE预测结果 这里不该叫mask吧 output * io_out_s2_full_pred_0_br_taken_mask_1 output * io_out_s2_full_pred_1_br_taken_mask_0 output * io_out_s2_full_pred_1_br_taken_mask_1 output * io_out_s2_full_pred_2_br_taken_mask_0 output * io_out_s2_full_pred_2_br_taken_mask_1 output * io_out_s2_full_pred_3_br_taken_mask_0 output * io_out_s2_full_pred_3_br_taken_mask_1 output * io_out_s3_full_pred_0_br_taken_mask_0 io_out_s3_full_pred_{i}br_taken_mask{j} Tage 在 s3流水级输出的，复制4份 预测块中第 j 条分支指令SC预测结果 output * io_out_s3_full_pred_0_br_taken_mask_1 output * io_out_s3_full_pred_1_br_taken_mask_0 output * io_out_s3_full_pred_1_br_taken_mask_1 output * io_out_s3_full_pred_2_br_taken_mask_0 output * io_out_s3_full_pred_2_br_taken_mask_1 output * io_out_s3_full_pred_3_br_taken_mask_0 output * io_out_s3_full_pred_3_br_taken_mask_1 output *[222:0] io_out_last_stage_meta 见附表 input * io_ctrl_tage_enable input * io_ctrl_sc_enable input * io_s0_fire_0 s0 阶段流水线控制 相同信号复制多份，0给BPU，1给Tage，3给SC input * io_s0_fire_1 input * io_s0_fire_3 input * io_s1_fire_0 s1 阶段流水线控制 input * io_s1_fire_1 input * io_s1_fire_2 input * io_s1_fire_3 input * io_s2_fire_0 s2 阶段流水线控制 input * io_s2_fire_1 input * io_s2_fire_2 input * io_s2_fire_3 output * io_s1_ready tage的所有表，可以执行读取结果的操作 input * io_update_valid 从FTQ发向BPU的后端执行结果（更新信号）是否有效 input *[40:0] io_update_bits_pc （后端执行过的）预测块的PC input *[10:0] io_update_bits_spec_info_folded_hist_hist_17_folded_hist TageTable 2 用到的11bits 折叠历史 预测时使用的分支历史结果，没有更新，转了一圈回来了 input *[10:0] io_update_bits_spec_info_folded_hist_hist_16_folded_hist TageTable 3 用到的11bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_15_folded_hist TageTable 1 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_14_folded_hist TageTable 0 用到的8bits 折叠历史 input *[3:0] io_update_bits_spec_info_folded_hist_hist_12_folded_hist SCTable 1 用到的 4bit 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_11_folded_hist SCTable 2 用到的 8bit 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_9_folded_hist TageTable 2 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_8_folded_hist TageTable 3 用到的8bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_7_folded_hist TageTable 0 用到的7bits 折叠历史 input *[6:0] io_update_bits_spec_info_folded_hist_hist_5_folded_hist TageTable 3 用到的7bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_4_folded_hist TageTable 1 用到的8bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_3_folded_hist TageTable 2 用到的8bits 折叠历史 input *[7:0] io_update_bits_spec_info_folded_hist_hist_2_folded_hist SCTable 3 用到的 8bit 折叠历史 input *[10:0] io_update_bits_spec_info_folded_hist_hist_1_folded_hist TageTable 1 用到的11bits 折叠历史 input * io_update_bits_ftb_entry_brSlots_0_valid FTB 表项的第一个slot是否有效（存储了跳转指令） input * io_update_bits_ftb_entry_tailSlot_sharing FTB 表项的最后一个slot是否存储了条件分支而非无条件跳转 input * io_update_bits_ftb_entry_tailSlot_valid FTB 表项的最后一个slot是否有效 input * io_update_bits_ftb_entry_always_taken_0 历史上slot 0 指令总是跳转 input * io_update_bits_ftb_entry_always_taken_1 历史上slot 1 指令总是跳转 input * io_update_bits_br_taken_mask_0 solt 0 是否 taken input * io_update_bits_br_taken_mask_1 solt 1 是否 taken input * io_update_bits_mispred_mask_0 solt 0 是否预测正确 input * io_update_bits_mispred_mask_1 solt 1 是否预测正确 input *[222:0] io_update_bits_meta 见附表 io_out_last_stage_meta 需要设计参与优化！\n信号类型 信号位 信号名 信号描述 output [218:88] 0 占位，全为0，传递到composer时会忽略 87 resp_meta_providers_1_valid_r [86:85] resp_meta_providers_1_bits_r 84 resp_meta_providers_0_valid_r [83:82] resp_meta_providers_0_bits_r [81:79] resp_meta_providerResps_1_r_ctr 78 resp_meta_providerResps_1_r_u 77 resp_meta_providerResps_1_r_unconf [76:74] resp_meta_providerResps_0_r_ctr 73 resp_meta_providerResps_0_r_u 72 resp_meta_providerResps_0_r_unconf 71 resp_meta_altUsed_1_r 70 resp_meta_altUsed_0_r 69 resp_meta_altDiffers_1_r 68 resp_meta_altDiffers_0_r [67:66] resp_meta_basecnts_1_r [65:64] resp_meta_basecnts_0_r [63:60] resp_meta_allocates_1_r [59:56] resp_meta_allocates_0_r 55 resp_meta_takens_1_r 54 resp_meta_takens_0_r 53 resp_meta_scMeta_tageTakens_1_r 52 resp_meta_scMeta_tageTakens_0_r 51 resp_meta_scMeta_scUsed_1_r 50 resp_meta_scMeta_scUsed_0_r 49 resp_meta_scMeta_scPreds_1_r 48 resp_meta_scMeta_scPreds_0_r [47:42] r_1_3 scMeta(预测时的状态)中第2路的第4个sc_ctr的值 [41:36] r_1_2 scMeta中第2路的第3个sc_ctr的值 [35:30] r_1_1 scMeta中第2路的第2个sc_ctr的值 [29:24] r_1_0 scMeta中第2路的第1个sc_ctr的值 [23:18] r_3 scMeta中第1路的第4个sc_ctr的值 [17:12] r_2 scMeta中第1路的第3个sc_ctr的值 [11:6] r_1 scMeta中第1路的第2个sc_ctr的值 [5:0] r_0 scMeta中第1路的第1个sc_ctr的值 io_update_bits_meta 信号类型 信号位 信号名 信号描述 input [218:94] FTB, ITAGE, RAS 模块传给 FTQ 的 META 信息，忽略 [93:6] io_out_last_stage_meta[87:0] 偏移 6bit 后的结果 TAGE 输出给 FTQ 的 META [5:0] uFTB 输出给 FTQ 的 META ","categories":["分支预测单元（BPU）"],"description":"","excerpt":"功能介绍 TAGE-SC 是南湖架构条件分支的主预测器，属于精确预测器（Accurate Predictor，简称 APD）。TAGE-SC …","ref":"/xs-bpu/docs/modules/02_tage_sc/","tags":["子预测器","TAGE-SC"],"title":"TAGE-SC 分支预测器"},{"body":"BPU 是如何整合内部子预测器的？ 我们已经知道香山 BPU 采用的是多预测器，多流水线方案。其中为了适配多流水线，BPU 采用了三通道结果输出接口。但是又是如何适配多预测器的呢？这就要求我们进一步深入 BPU，探究其内部结构。\n上图为香山文档中的 BPU 架构图，目前我们只需要关心这样一个信息，内部的所有子预测器都被包在了一个叫做 Composer 的结构中。BPU只需要和 Composer 交互。\nComposer 是什么？我们不妨先看一下香山代码中对于他们的定义。\n可以发现，Composer 以及五个子预测器有一个共同的特点，他们全部继承于 BasePredictor 基类。并且接口已经在 BasePredictor 类中定义好。换句话说就是，Composer和五个子预测器都拥有相同的接口！BPU 顶层可以直接把 Composer 也当做一个子预测器，而无需关心内部是怎么连接子预测器的。\n子预测器接口 接下来我们会查看子预测器接口是怎样的。该接口将涉及到 Composer 与 BPU 顶层的交互，还会涉及到各子预测器与 Composer 的交互。\n我们先以 Composer 为例，说明子预测器接口的结构\n如上图所示，Composer 的三通道预测结果被直接输出至 BPU 外部，并且还有一组三通道预测结果从BPU内部连接至 Composer ，但由于预测结果是由 Composer 产生，因此 BPU 会将一个空的预测结果传递给 Composer ，这样做的意义是，使子预测器形成了一个“加工”的作用，子预测器会将输入的预测结果进行加工，然后再输出加工过后的预测结果。\n接下来，BPU 顶层会为流水线提供预测所需要的信息。首先是 PC 和分支历史记录（包括全局历史和全局折叠历史），接下来 BPU 会和 Composer 之间连接一些流水线控制信号，最后 BPU 将外部输入的重定向请求接口和更新接口直接连接至 Composer。\n最终可以简单给出子预测器接口的定义（详细定义请前往接口文档进行查看）：\nin (s1, s2, s3) 预测信息输入 s0_pc 需要预测的PC ghist 全局分支历史 folded_hist 全局折叠历史 out (s1, s2, s3) 预测信息输出 流水线控制信号 s0_fire, s1_fire, s2_fire, s3_fire 相应流水级是否工作 s2_redirect, s3_redirect 后续流水级发现预测错误时重定向信号 s1_ready, s2_ready, s3_ready 子预测器相应流水级是否就绪 update 更新请求 redirect 重定向请求 子预测器之间的连接 我们已经清楚各个子预测器之间的接口与Composer 的接口是相同的，并且我们也已经知道了 Composer是如何连向顶层 BPU 的，本小节将会说明子预测器是如何在 Composer 内部进行连接的。\n上图便是子预测器在 Composer 中连接的结构图。可以看出，三通道预测结果送入Composer后，首先经过 uFTB 的“加工”后输出，再依次由 TAGE-SC、FTB、ITTAGE、RAS进行“加工”，最终连接到 Composer 的预测结果输出，并由Composer 再直接连往 BPU 外部。\n而对于其他信号，因为Composer和各子预测器的接口相同，都被Composer直接连接到了每个预测器的相应接口，而没有做非常多额外处理。\n预测结果接口连接 对于子预测器来说，他们的预测结果的连接方式是，预测结果输出是下一个预测器的输入。但需要注意的是，这种连接是组合电路的连接，而没有时序的影响。\n如上图所示，以 s1 通道为例，从输入到最后一个预测器输出，中间全都是组合电路在做修改，不受时序影响。寄存器只在 s1, s2 和 s3 通道之间存在。\n因此增加子预测器的数量，并不会导致预测所需的周期数量增多，仅仅会导致每个周期预测所需要的时延增大。\n","categories":["香山 BPU 基础设计"],"description":"本节介绍了香山分支预测单元（BPU）的结构，包括多预测器、多流水线方案的整合，以及内部子预测器的组织结构和接口设计，展示了BPU如何与Composer交互，并说明了子预测器之间的连接方式。","excerpt":"本节介绍了香山分支预测单元（BPU）的结构，包括多预测器、多流水线方案的整合，以及内部子预测器的组织结构和接口设计，展示了BPU如何 …","ref":"/xs-bpu/docs/basic/02_xsbpu_structure/","tags":["香山 BPU 结构","子预测器"],"title":"香山分支预测单元结构介绍"},{"body":"本部分文档将会详细介绍香山分支预测单元的每一个模块，包括 BPU 顶层和五个子预测器。\n在每一个模块的文档中，我们将会详细介绍该模块在香山分支预测单元中所发挥的作用，以及该模块的算法原理、模块结构和时序等。\n负责一个模块验证的同学需要重点阅读对应文档并结合代码进行理解，其他文档亦可进行阅读以帮助你对香山分支预测单元整体功能的理解。理解过程中，你可能需要时刻回顾之前文档中所描述的基础设计思想及接口信号的含义。\n","categories":["香山 BPU 子模块"],"description":"本部分文档将会详细介绍香山分支预测单元的每一个模块，包括 BPU 顶层和五个子预测器。","excerpt":"本部分文档将会详细介绍香山分支预测单元的每一个模块，包括 BPU 顶层和五个子预测器。","ref":"/xs-bpu/docs/modules/","tags":["香山","BPU","子预测器"],"title":"子模块文档"},{"body":"In the Xiangshan branch prediction unit, all its sub-predictors and the class implementations of Composer are inherited from the sub-predictor base class (BasePredictor). The sub-predictor interface (BasePredictorIO) is also defined in the sub-predictor base class. Therefore, we can consider that Composer and all sub-predictors have the same interface.\nIn the understanding and verification of sub-prediction, our most direct external interaction occurs in the sub-predictor interface and some variables defined in the sub-predictor base class. Therefore, before verifying the sub-predictor, it is strongly recommended that you read and understand this section of the document.\nThe general content and usage of the sub-branch predictor interface have been introduced in the Xiangshan Branch Prediction Unit (BPU) Basic Design section. This document will focus on the signal details of the interface.\nSub-Branch Predictor Interface (BasePredictorIO) Interface Definition: src/main/scala/xiangshan/frontend/BPU.scala\nEach sub-branch predictor needs to implement this interface, which defines the input and output interfaces of the sub-branch predictor.\nNote: Some signals are defined as numDup quantities, where each signal is exactly the same. Multiple identical signals are for other considerations.\nThe detailed signal list is as follows:\nreset_vector Reset vector, when reset occurs, the BPU’s pc will be reset to this value.\nInterface Type：UInt(PAddrBits.W) in Information sent from the BPU to the sub-branch predictor\nInterface Type：DecoupledIO(new BasePredictorInput) Signal List： s0_pc PC of the s0 pipeline stage Interface Type：Vec(numDup, UInt(VAddrBits.W)) folded_hist Global folded history information Interface Type：Vec(numDup, new AllFoldedHistories(foldedGHistInfos)) Signal List：See（AllFoldedHistories） ghist Global branch history information Interface Type：UInt(HistoryLength.W) resp_in Global branch prediction information (including s1, s2, s3 prediction result information) Interface Type：BranchPredictionResp Signal List：See（BranchPredictionResp） out Information sent from the sub-branch predictor to the BPU (including s1, s2, s3 prediction result information)\nInterface Type：new BasePredictorOutput 继承自 BranchPredictionResp Signal List：See（BranchPredictionResp） ctrl BPU sub-predictor enable control signal, mainly used to control whether each predictor is enabled\nInterface Type：BPUCtrl Interface Type： ubtb_enable: UBTB predictor enable Interface Type：Bool() btb_enable: BTB predictor enable 接Interface Type：Bool() bim_enable: BIM predictor enable Interface Type：Bool() tage_enable: TAGE predictor enable Interface Type：Bool() sc_enable: SC predictor enable Interface Type：Bool() ras_enable: RAS predictor enable Interface Type：Bool() loop_enable: LOOP predictor enable Interface Type：Bool() s0_fire s0 stage handshake success signal\nInterface Type：Vec(numDup, Bool()) s1_fire s1 stage handshake success signal\nInterface Type：Vec(numDup, Bool()) s2_fire s2 stage handshake success signal\nInterface Type：Vec(numDup, Bool()) s3_fire s3 stage handshake success signal\nInterface Type：Vec(numDup, Bool()) s2_redirect s2 stage redirection signal\nInterface Type：Vec(numDup, Bool()) s3_redirect s3 stage redirection signal\nInterface Type：Vec(numDup, Bool()) s1_ready s1 stage ready to receive information (Direction: output from the sub-predictor)\nInterface Type：Bool() s2_ready s2 stage ready to receive information (Direction: output from the sub-predictor)\nInterface Type：Bool() s3_ready s3 stage ready to receive information (Direction: output from the sub-predictor)\nInterface Type：Bool() update Update request forwarded from the BPU to the sub-branch predictor\nInterface Type：Valid(new BranchPredictionUpdate) Signal List：See（BranchPredictionUpdate） redirect Redirect request forwarded from the BPU to the sub-branch predictor\nInterface Type：Valid(new BranchPredictionRedirect) Signal List：See（BranchPredictionRedirect） The pipeline control signals will be further explained in the following content.\nGlobal Folding History (AllFoldedHistories) Interface Definition：src/main/scala/xiangshan/frontend/FrontendBundle.scala\nInterface Type：AllFoldedHistories(foldedGHistInfos))\nThe interface information of the global folding history consists of only one FoldedHistory list.\nhist List of folded histories Interface Type：MixedVec(gen.map{case (l, cl) =\u003e new FoldedHistory(l, cl, numBr)}) The interface information of FoldedHistory also has only one item.\nfolded_hist Single folded history, with a bit width equal to the compressed history length. Interface Type：UInt(compLen.W) This means that the interface of the global folding history is actually a list that stores folded histories, where each item is a folded history of a specific length.\nBase Predictor Class The base predictor class defines several signals, which can be accessed in each sub-predictor, and several connections are made within it.\nMost of the signals are relatively easy to understand. We need to pay particular attention to the pc of each pipeline, which also involves your understanding of pipeline control signals. Therefore, next, we will introduce the meanings of pipeline control signals that need to be paid attention to in sub-predictors, as well as the meanings of the s1_pc, s2_pc, s3_pc signals.\nThere are three groups of pipeline control signals:\nfire signals (s0, s1, s2, s3) redirect signals (s2, s3) ready signals (s1, s2, s3) The pc signals in the base predictor class are divided into four groups, s0_pc_dup, s1_pc_dup, s2_pc_dup, s3_pc_dup. Each group of signals contains multiple pc signals, which are exactly the same and are duplicated for some other reasons. Therefore, we can simply consider them as s0_pc, s1_pc, s2_pc, s3_pc.\nTheir usage can be seen in the following diagram:\nTheir relationship with the pipeline control signals is as follows:\ns0_pc is directly connected from the in.s0_pc in the input interface. When s0_fire is active, the next cycle s1_pc will output the value of s0_pc. When s1_fire is active, the next cycle s2_pc will output the value of s1_pc. When s2_fire is active, the next cycle s3_pc will output the value of s2_pc. In other words, the fire signal affects whether the data of the next cycle is valid. For example, the s0_fire signal affects whether the data of the s1 pipeline is valid, and the s1_fire signal affects whether the data of the s2 pipeline is valid.\nWhether the fire signal is valid depends on whether the data of this pipeline stage is valid and whether the next pipeline stage is ready. For example, the s1_fire signal is valid only if the data of the s1 stage is valid and the s2_ready signal from the sub-predictor output is valid. At this point, it can be considered that the data processing of the s1 stage is completed, the s2 stage is ready, and the data of the next cycle will be directly sent to the s2 stage.\nTherefore, in the sub-predictor, taking the s1 stage as an example, s1_ready can block data from entering the s1 stage. When s1_ready is active, the data for the s1 stage will be valid in the next cycle. When s1_fire is active, it indicates that the data in the s1 stage is already valid and the predictor has generated the result for the s1 stage. The data will then be directly sent to the s2 stage in the next cycle.\nThe redirect signal is relatively clear. For example, in the s2 stage, when s2_redirect is valid, it indicates that when s2_fire is valid, the s2 prediction result is different from the s1 prediction result in the previous cycle.\n","categories":["Xiangshan BPU Interface"],"description":"This document introduces the sub-predictor interface in the Xiangshan BPU, as well as the use of the sub-predictor base class. Reading this document can help you understand the external interactions of sub-predictors and the use of signals in the sub-predictor base class.","excerpt":"This document introduces the sub-predictor interface in the Xiangshan …","ref":"/xs-bpu/en/docs/ports/03_subpredictor/","tags":["Sub-predictor"],"title":"Base Predictor Class and Sub Predictor Interface"},{"body":"The Feature List in hardware verification lists various functions and characteristics of the design that need to be verified. Before starting verification, we generally list the features of the design to clarify which features need to be verified in this task.\nTo verify a feature in the feature list, we will build the necessary test points for this feature and write test cases to cover these test points. Therefore, the feature list is not only a description of the features of the design to be verified but also a guide for our verification direction and a simple summary of the verification task.\nIn this verification, we provide a basic feature list for each verification task (i.e., the top-level BPU and each sub-predictor), which only covers the basic functions of the corresponding module. In your verification, you must ensure that this verification covers these features. Additionally, you can supplement features not covered in the list to ensure the final delivery quality.\nIn this document section, you only need to refer to the document corresponding to the module you will verify.\n","categories":["Example","Tutorial"],"description":"The Feature List in hardware verification lists various functions and characteristics of the design that need to be verified. This section provides a basic feature list for each verification task for reference.","excerpt":"The Feature List in hardware verification lists various functions and …","ref":"/xs-bpu/en/docs/feature/","tags":["examples","docs"],"title":"Feature List"},{"body":"Introduction to FTB FTB is the third sub-predictor of the Xiangshan BPU, and it can also get the outputs of uFTB and TAGE-SC together. In the input interface of FTB, the s1 channel contains the basic prediction results of uFTB, and the s2 and s3 channels are filled with only one group of signals, br_taken_mask, by TAGE-SC, without the basic prediction results generated by the FTB entry. The function of FTB is to provide basic prediction results for the s2 and s3 channels.\nIn terms of functionality and structure, FTB is similar to uFTB. The main difference is that FTB can accommodate more FTB entries, and the prediction results of FTB are output in the s2 and s3 channels. Due to its large capacity, the readout speed of FTB is slower than that of uFTB, and it cannot be placed in the first cycle to generate prediction results. However, the large capacity enables it to obtain more accurate prediction results.\nFunction of uFTB Cache more FTB entries and provide basic prediction results for the s2 and s3 channels. The FTB predictor is essentially a storage with a large capacity. It reads the corresponding FTB entry based on the current predicted PC and outputs it in the s2 stage. At the same time, this FTB entry will be saved for one more cycle to generate the s3 stage prediction result. One thing to note is to consider the br_taken_mask field inputted by the previous predictor to avoid losing it during generation. Update FTB entries based on update requests. FTB Storage Structure FTB entries in the FTB predictor are placed in a dedicated storage structure called FTBBank. Before further examining the structure of FTBBank, let’s first see how FTBBank is used.\nFTB Read Request The read request interface of FTBBank is as follows:\nreq_pc Requested PC Interface type: Flipped(DecoupledIO(UInt(VAddrBits.W))) read_resp Read out FTB entry Interface type: FTBEntry read_hits Which way (row) is hit Interface type: Valid(UInt(log2Ceil(numWays).W)) Among, req_pc interface is Decoupled, meaning it contains valid and ready signals. FTB needs to get the PC before the s1 stage starts, so s0_pc is sent to the req_pc interface, s0_fire signal is connected to the valid signal of req_pc, and the ready signal is connected to the pipeline control signal s1_ready.\nWhen s0_fire enters the s1 stage, in the next cycle, when s0_fire is at the same time as s1_fire, FTBBank has already outputted the readout FTB entry to the read_resp interface, and calculated read_hits. However, at this time, because the readout has wasted too much delay, it cannot be outputted in the s1 stage. Therefore, this readout result is saved in an internal register. It will be read out from the register in the s2 and s3 stages to generate the prediction result.\nFTBBank FTBBank defines a storage to store all FTB entries. The storage adopts a group-associative structure, with 512 groups (Sets) in total, each group has 4 ways, and can store up to 2048 FTB entries. Besides storing FTB entries, it also stores the tag corresponding to each FTB entry for matching.\nSpecifically, the tag is defined as pc[29:10], which takes 20 bits from the PC to identify the FTB entry. The PC is divided as follows:\npc: | ... |\u003c-- tag(20 bits) --\u003e|\u003c-- idx(9 bits) --\u003e|\u003c-- instOffset(1 bit) --\u003e| When reading, provide the group number (idx) to the storage, read out all ways in that group, and then check if there is a way whose tag matches the current tag. If there is a match, it means a hit, and the readout FTB entry is sent out through the read_resp interface, and the hit way number is sent out through the read_hits interface.\nGeneration of Prediction Results As mentioned earlier, for the FTB predictor, it needs to provide basic prediction results derived from FTB entries to the s2 and s3 channels. The FTB entries have been read and saved in the s1 stage. In the s2 and s3 stages, they only need to be read out to generate the prediction results. However, one thing to note is to preserve the br_taken_mask field generated by TAGE-SC in the s2 and s3 prediction results, which provides precise prediction results for conditional branch instructions. For the s1 channel, the FTB predictor does not make any changes.\nThe generation of signals in the s2 and s3 prediction results can refer to the following list:\nhit Whether the FTB entry is hit Generation method: The read_hits signal valid bit from FTBBank is valid. slot_valids Slot valid bit, indicating whether each slot in the ftb entry is valid targets Jump target address corresponding to each slot offsets Instruction offset relative to the start address of the predicted block in each slot is_jal Whether the predicted block contains a jal instruction is_jalr Whether the predicted block contains a jalr instruction is_call Whether the predicted block contains a call instruction is_ret Whether the predicted block contains a ret instruction last_may_be_rvi_call Signal indicating that the end of the predicted block may be an RVI type call instruction **is_br_sharing Whether the last slot (tailSlot) stores a conditional branch instruction signal Generation method**: Export from the corresponding field in the FTB entry fallThroughErr Error in the pftAddr recorded in the FTB entry Generation method: Compare whether the address represented by pftAddr is greater than the start address of the predicted block. If it is less than, it indicates an error, and this signal is set to valid. This situation may occur when the PC indexes an incorrect FTB entry. fallThroughAddr End address of the predicted block Generation method: If fallThroughErr is invalid, it is generated according to pftAddr. Otherwise, it is set to the start address + prediction width. br_taken_mask Branch prediction result, each branch (slot) corresponds to a bit, indicating whether the branch is predicted as taken Generation method: Generated based on the always_taken field in the FTB entry and the indication result of the two-bit saturation counter. jalr_target Jump target of jalr in this predicted block Generation method: Jump target in the tailSlot of the FTB entry. FTB meta In the third cycle of prediction, the FTB predictor outputs some auxiliary information of this prediction to last_stage_meta and also sends the read FTB entry to the last_stage_ftrb_entry interface.\nThe FTB meta contains two pieces of information, hit and writeWay, indicating whether the prediction hits and in which way it is read. Subsequently, the update channel generates the update information for this prediction, and these two pieces of information are also sent to guide the writing of the updated FTB entry.\nFTB Update In the update channel, the pc and the new FTB entry are already specified for us, along with the hit and writeWay in the meta information. If hit in the meta is valid, it means that the FTB entry corresponding to this pc was stored in the memory, and we only need to write it to the corresponding way.\nIf it is invalid, it means that there was no storage before, but we do not know whether it is stored now. It is possible that before this update request, the FTB entry corresponding to this pc was written by another update request. Therefore, we still need to send a read request to FTBBank to check if there is a corresponding FTB entry. If it exists, it can be directly written to this position in the next cycle, otherwise, FTBBank will be notified to allocate a new position.\nTherefore, the number of cycles required for updating FTB entries depends on the hit situation.\nLet’s first look at how FTBBank handles updates.\nFTBBank Update FTBBank’s update interface is divided into two parts, the update read interface and the update write interface.\nu_req_pc: Update read request pc Flipped(DecoupledIO(UInt(VAddrBits.W))) update_hits: Hit information read out Valid(UInt(log2Ceil(numWays).W)) update_access: There is an update request but the meta information indicates a miss Bool() update_pc: Update write request pc UInt(VAddrBits.W)) update_write_data: Data to be written in the update request, write when valid Flipped(Valid(new FTBEntryWithTag)) update_write_way: Way index to write in the update request UInt(log2Ceil(numWays).W)) update_write_alloc: Whether a new FTB entry needs to be allocated (missed before) Bool() For the update read interface, FTBBank obtains the update read request through u_req_pc signal. This request has a higher priority than the read request during prediction. In the next cycle, FTBBank will output the hit information through the update_hits interface. update_access is only used for some internal status judgments of FTBBank.\nFor the update write interface, FTBBank obtains the pc of the update write request through the update_pc signal, and when update_write_data is valid, it writes the data into the corresponding position specified by update_write_way. If update_write_alloc is valid, it means that it cannot be directly written to the position specified in the request, but a new position needs to be allocated.\nThe allocation strategy is as follows:\nIf all ways are filled, use the pseudo LRU replacement algorithm to select the way to replace If there is an empty way, select the empty way. Update Request Timing Meta hit is valid: If hit in the update request meta is valid, then we only need to specify the address and data to be written according to the information in the update request, and the writing only takes one cycle. Meta hit is invalid: In this case, after receiving the update request, we connect the pc in the request to the read port of FTBBank. The read port will return the result in the next cycle. Due to timing issues, we save this result and use it in the next cycle. Depending on the hit status in the result, we decide whether to set update_write_alloc and send a write request. The entire update process takes three cycles. Interface List 信号类型 信号位 信号名 信号描述 input clock 输入时钟 input reset 复位信号 input [35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input [40:0] io_in_bits_s0_pc_0 输入位s0_pc 的 第0个复制 input [40:0] io_in_bits_s0_pc_1 同上 第1个 input [40:0] io_in_bits_s0_pc_2 同上 第2个 input [40:0] io_in_bits_s0_pc_3 同上 第3个 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0 预测结果输入 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_0_br_taken_mask_0 s2 阶段输出的完整预测结果 output io_out_s2_full_pred_0_br_taken_mask_1 output io_out_s2_full_pred_0_slot_valids_0 output io_out_s2_full_pred_0_slot_valids_1 output [40:0] io_out_s2_full_pred_0_targets_0 output [40:0] io_out_s2_full_pred_0_targets_1 output [40:0] io_out_s2_full_pred_0_jalr_target output [3:0] io_out_s2_full_pred_0_offsets_0 output [3:0] io_out_s2_full_pred_0_offsets_1 output [40:0] io_out_s2_full_pred_0_fallThroughAddr output io_out_s2_full_pred_0_is_br_sharing output io_out_s2_full_pred_0_hit output io_out_s2_full_pred_1_br_taken_mask_0 同上 output io_out_s2_full_pred_1_br_taken_mask_1 output io_out_s2_full_pred_1_slot_valids_0 output io_out_s2_full_pred_1_slot_valids_1 output [40:0] io_out_s2_full_pred_1_targets_0 output [40:0] io_out_s2_full_pred_1_targets_1 output [40:0] io_out_s2_full_pred_1_jalr_target output [3:0] io_out_s2_full_pred_1_offsets_0 output [3:0] io_out_s2_full_pred_1_offsets_1 output [40:0] io_out_s2_full_pred_1_fallThroughAddr output io_out_s2_full_pred_1_is_br_sharing output io_out_s2_full_pred_1_hit output io_out_s2_full_pred_2_br_taken_mask_0 同上 output io_out_s2_full_pred_2_br_taken_mask_1 output io_out_s2_full_pred_2_slot_valids_0 output io_out_s2_full_pred_2_slot_valids_1 output [40:0] io_out_s2_full_pred_2_targets_0 output [40:0] io_out_s2_full_pred_2_targets_1 output [40:0] io_out_s2_full_pred_2_jalr_target output [3:0] io_out_s2_full_pred_2_offsets_0 output [3:0] io_out_s2_full_pred_2_offsets_1 output [40:0] io_out_s2_full_pred_2_fallThroughAddr output io_out_s2_full_pred_2_is_jalr output io_out_s2_full_pred_2_is_call output io_out_s2_full_pred_2_is_ret output io_out_s2_full_pred_2_last_may_be_rvi_call output io_out_s2_full_pred_2_is_br_sharing output io_out_s2_full_pred_2_hit output io_out_s2_full_pred_3_br_taken_mask_0 同上 output io_out_s2_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_3_slot_valids_0 output io_out_s2_full_pred_3_slot_valids_1 output [40:0] io_out_s2_full_pred_3_targets_0 output [40:0] io_out_s2_full_pred_3_targets_1 output [40:0] io_out_s2_full_pred_3_jalr_target output [3:0] io_out_s2_full_pred_3_offsets_0 output [3:0] io_out_s2_full_pred_3_offsets_1 output [40:0] io_out_s2_full_pred_3_fallThroughAddr output io_out_s2_full_pred_3_fallThroughErr output io_out_s2_full_pred_3_is_br_sharing output io_out_s2_full_pred_3_hit output io_out_s3_full_pred_0_br_taken_mask_0 s3 阶段输出的完整预测结果 output io_out_s3_full_pred_0_br_taken_mask_1 output io_out_s3_full_pred_0_slot_valids_0 output io_out_s3_full_pred_0_slot_valids_1 output [40:0] io_out_s3_full_pred_0_targets_0 output [40:0] io_out_s3_full_pred_0_targets_1 output [40:0] io_out_s3_full_pred_0_jalr_target output [40:0] io_out_s3_full_pred_0_fallThroughAddr output io_out_s3_full_pred_0_fallThroughErr output io_out_s3_full_pred_0_is_br_sharing output io_out_s3_full_pred_0_hit output io_out_s3_full_pred_1_br_taken_mask_0 同上 output io_out_s3_full_pred_1_br_taken_mask_1 output io_out_s3_full_pred_1_slot_valids_0 output io_out_s3_full_pred_1_slot_valids_1 output [40:0] io_out_s3_full_pred_1_targets_0 output [40:0] io_out_s3_full_pred_1_targets_1 output [40:0] io_out_s3_full_pred_1_jalr_target output [40:0] io_out_s3_full_pred_1_fallThroughAddr output io_out_s3_full_pred_1_fallThroughErr output io_out_s3_full_pred_1_is_br_sharing output io_out_s3_full_pred_1_hit output io_out_s3_full_pred_2_br_taken_mask_0 同上 output io_out_s3_full_pred_2_br_taken_mask_1 output io_out_s3_full_pred_2_slot_valids_0 output io_out_s3_full_pred_2_slot_valids_1 output [40:0] io_out_s3_full_pred_2_targets_0 output [40:0] io_out_s3_full_pred_2_targets_1 output [40:0] io_out_s3_full_pred_2_jalr_target output [40:0] io_out_s3_full_pred_2_fallThroughAddr output io_out_s3_full_pred_2_fallThroughErr output io_out_s3_full_pred_2_is_jalr output io_out_s3_full_pred_2_is_call output io_out_s3_full_pred_2_is_ret output io_out_s3_full_pred_2_is_br_sharing output io_out_s3_full_pred_2_hit output io_out_s3_full_pred_3_br_taken_mask_0 同上 output io_out_s3_full_pred_3_br_taken_mask_1 output io_out_s3_full_pred_3_slot_valids_0 output io_out_s3_full_pred_3_slot_valids_1 output [40:0] io_out_s3_full_pred_3_targets_0 output [40:0] io_out_s3_full_pred_3_targets_1 output [40:0] io_out_s3_full_pred_3_jalr_target output [3:0] io_out_s3_full_pred_3_offsets_0 output [3:0] io_out_s3_full_pred_3_offsets_1 output [40:0] io_out_s3_full_pred_3_fallThroughAddr output io_out_s3_full_pred_3_fallThroughErr output io_out_s3_full_pred_3_is_br_sharing output io_out_s3_full_pred_3_hit output [222:0] io_out_last_stage_meta 最后一个阶段输出的 meta 信息 output io_out_last_stage_ftb_entry_valid 最后一个阶段输出的 FTB 项 output [3:0] io_out_last_stage_ftb_entry_brSlots_0_offset output [11:0] io_out_last_stage_ftb_entry_brSlots_0_lower output [1:0] io_out_last_stage_ftb_entry_brSlots_0_tarStat output io_out_last_stage_ftb_entry_brSlots_0_sharing output io_out_last_stage_ftb_entry_brSlots_0_valid output [3:0] io_out_last_stage_ftb_entry_tailSlot_offset output [19:0] io_out_last_stage_ftb_entry_tailSlot_lower output [1:0] io_out_last_stage_ftb_entry_tailSlot_tarStat output io_out_last_stage_ftb_entry_tailSlot_sharing output io_out_last_stage_ftb_entry_tailSlot_valid output [3:0] io_out_last_stage_ftb_entry_pftAddr output io_out_last_stage_ftb_entry_carry output io_out_last_stage_ftb_entry_isCall output io_out_last_stage_ftb_entry_isRet output io_out_last_stage_ftb_entry_isJalr output io_out_last_stage_ftb_entry_last_may_be_rvi_call output io_out_last_stage_ftb_entry_always_taken_0 output io_out_last_stage_ftb_entry_always_taken_1 input io_ctrl_btb_enable 使能信号 input io_s0_fire_0 s0 阶段流水线控制信号 input io_s0_fire_1 input io_s0_fire_2 input io_s0_fire_3 output io_s1_ready s1 阶段流水线控制信号 input io_s1_fire_0 input io_s1_fire_1 input io_s1_fire_2 input io_s1_fire_3 input io_s2_fire_0 s2 阶段流水线控制信号 input io_s2_fire_1 input io_s2_fire_2 input io_s2_fire_3 input io_update_valid 更新有效性 input [40:0] io_update_bits_pc 传回的预测块pc（用于指示更新的预测块） input io_update_bits_ftb_entry_valid 是否启用 input [3:0] io_update_bits_ftb_entry_brSlots_0_offset solt 0 中分支指令相对于地址块起始pc的偏移 input [11:0] io_update_bits_ftb_entry_brSlots_0_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_brSlots_0_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_brSlots_0_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_brSlots_0_valid 是否启用 input [3:0] io_update_bits_ftb_entry_tailSlot_offset solt 1 中分支指令相对于地址块起始pc的偏移 input [19:0] io_update_bits_ftb_entry_tailSlot_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_tailSlot_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_tailSlot_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_tailSlot_valid 是否启用 input [3:0] io_update_bits_ftb_entry_pftAddr Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。 input io_update_bits_ftb_entry_carry pc+pft时是否产生进位 input io_update_bits_ftb_entry_isCall 是否是函数调用 input io_update_bits_ftb_entry_isRet 是否是函数返回 input io_update_bits_ftb_entry_isJalr 是否是 jalr 指令 input io_update_bits_ftb_entry_last_may_be_rvi_call 最后一个指令槽存储的可能是 rvi 的 call 指令 input io_update_bits_ftb_entry_always_taken_0 是否预测为总是跳转 input io_update_bits_ftb_entry_always_taken_1 是否预测为总是跳转 input io_update_bits_old_entry 是否是旧的 FTB 项 input [222:0] io_update_bits_meta meta 信息 ","categories":["Xiangshan BPU Submodule"],"description":"","excerpt":"Introduction to FTB FTB is the third sub-predictor of the Xiangshan …","ref":"/xs-bpu/en/docs/modules/03_ftb/","tags":["Sub-predictor","FTB"],"title":"FTB Branch Predictor"},{"body":"FTB 简介 FTB 是香山 BPU 的第三个子预测器，它也能一并获取到 uFTB 和 TAGE-SC 的输出。在 FTB 的输入接口中，s1 通道含有 uFTB 的基础预测结果，s2 通道和 s3 通道中仅有 br_taken_mask 一组信号被 TAGE-SC 填充，并无 FTB 项生成的基础预测结果。FTB 的工作便是为 s2 和 s3 通道提供基础预测结果。\nFTB 在功能和结构上都与 uFTB 类似，其主要区别就是 FTB 能够容纳更多的 FTB 项，并且 FTB 的预测结果是在 s2 与 s3 通道输出。正是由于容量大，其读出的速度上会比 uFTB 慢，无法被放置在第一周期产生预测结果，但大容量也使它能够获得更加精准的预测结果。\nuFTB 功能简介 缓存更多 FTB 项，为 s2 和 s3 通道提供基础预测结果。 FTB 预测器的本质是一个较大容量的存储器，其会根据当前预测的 PC 读出对应的 FTB 项，并在 s2 阶段产出预测结果。与此同时该 FTB 项还会被再保存一个周期，生成 s3 阶段预测结果。生成结果需要注意的点是要考虑到上一预测器输入的 br_taken_mask 字段，避免在生成时丢失。 根据更新请求，更新存储中的 FTB 项。 FTB 存储结构 FTB 预测器中 FTB 项被放置在了一个专门的存储结构中，叫做 FTBBank ，在进一步查看 FTBBank 的结构之前，我们先来看一下 FTBBank 是怎样使用的。\nFTB 读请求 FTBBank 的读请求接口如下：\nreq_pc 请求的 pc 接口类型：Flipped(DecoupledIO(UInt(VAddrBits.W))) read_resp 读出来的FTB项 接口类型：FTBEntry read_hits 命中了哪一路 (way) 接口类型：Valid(UInt(log2Ceil(numWays).W)) 其中 req_pc 的接口是 Decoupled 的，也就是说含有 valid 和 ready 信号。 FTB 需要在 s1 阶段开始之前便获取到 pc，因此 s0_pc 被送入 req_pc 接口，s0_fire 信号被连接至 req_pc 的 valid 信号，ready 信号被连接至预测器的流水控制信号 s1_ready 。\n当 s0_fire 之后进入 s1 阶段，s0_fire 的下一个周期，也就是在 s1_fire 的同时，FTBBank 已经将读出的 FTB 项输出至 read_resp 接口，并计算好了 read_hits 。但此时因为读出已经浪费了过多时延，无法在 s1 阶段进行输出了，因此该读出结果被保存在内部寄存器中。在 s2 和 s3 阶段会在寄存器中读出该 FTB 项并生成预测结果。\nFTBBank FTBBank 中定义了一个存储器来存储所有的 FTB 项，存储器采用组相联的结构（如果你不清楚组相联的含义，请自行 RTFM），共含 512 个组（Set），每组有 4 路（Way），最多能存储 2048 个 FTB 项，存储 FTB 的同时还会存储与 FTB 项对应的 tag 用于匹配 FTB 项。\n具体地，tag 被定义为 pc[29:10]，即在 pc 中截取了 20 位用于标识 FTB 项，pc 的具体划分方式如下：\npc: | ... |\u003c-- tag(20 bits) --\u003e|\u003c-- idx(9 bits) --\u003e|\u003c-- instOffset(1 bit) --\u003e| 读出时，向存储器提供组序号(idx)，读出该组中所有路，然后检测是否有某一路的 tag 与当前 tag 匹配，如果匹配，则表示命中，读出的 FTB 项通过 read_resp 接口送出，并把命中的路编号通过 read_hits 接口送出。\n预测结果生成 正如我们之前提到的，对于 FTB 预测器，它需要向 s2, s3 通道提供由 FTB 项转化来的基础预测结果，FTB 项已经在 s1 阶段被读出并保存。在 s2 和 s3 阶段只需直接读出便生成即可，但需要注意的一点就是保护 TAGE-SC 在 s2 及 s3 通道中产生的预测结果 br_taken_mask 字段，这一字段为条件分支指令提供了精确的预测结果。对于 s1 通道，FTB 预测器不予更改。\ns2 和 s3预测结果中的信号产生方式可参考以下列表：\nhit FTB表项是否命中\n生成方式：FTBBank 输出的 read_hits 信号 valid 位有效。 slot_valids slot有效位，表示 ftb 项中的每个 slot 是否有效\ntargets 每个slot对应的跳转目标地址\noffsets 每个slot中的指令相对于预测块起始地址的偏移\nis_jal 预测块内包含jal指令\nis_jalr 预测块内包含jalr指令\nis_call 预测块内包含call指令\nis_ret 预测块内包含ret指令\nlast_may_be_rvi_call 预测结果预测块末尾可能为一个RVI类型的call指令信号\nis_br_sharing 最后一个slot (tailSlot) 中存储的是条件分支指令信号\n生成方式： FTB 项中对应字段导出 fallThroughErr FTB项中记录的 pftAddr 有误\n生成方式：比较 pftAddr 代表的预测块结束地址是否大于预测块的起始地址，如果小于，则代表出现错误，此信号置为有效。这种情况可能会发生在 pc 索引到错误的 FTB 项的情况。 fallThroughAddr 预测块的结束地址\n生成方式：如果 fallThroughErr 无效，则根据 pftAddr 生成，否则将其设置为起始地址 + 预测宽度。 br_taken_mask 分支预测结果，每个分支(slot)对应一位，表示该分支是否被预测为 taken\n生成方式：若 FTB 项命中并且 always_taken 字段有效，则预测为 taken；否则由相应通道输入接口中提供的 br_taken_mask 信号来决定。 jalr_target 本预测块中的 jalr 的跳转目标\n生成方式：FTB 项中 tailSlot 中的跳转目标。 FTB meta 在预测的第三周期，FTB 预测器会将本次预测的一些辅助信息输出至 last_stage_meta 中，还会将读出的 FTB 项送到 last_stage_ftrb_entry 接口中。\nFTB meta 中含有 hit 和 writeWay 两个信息，分别表示本次预测是否命中，以及在哪一路读出。之后更新通道中会产生本次预测的更新信息，这两个信息也会随之送入，来指导更新后的 FTB 项写入的位置。\nFTB 更新 update 通道中已经为我们指定好了 pc 以及新 FTB 项，并且还有 meta 信息中的 hit 和 writeWay 。如果 meta 中的 hit 有效，说明之前这一 pc 对应的 FTB 项在存储器中是有存储的，我们只需将它写入到 writeWay 对应的路中即可。\n如果无效，说明以前没有存储，但现在是否存储我们并不知道，有可能在此更新请求之前，该 pc 对应的 FTB 项被另一个更新请求写入了。因此我们还需要给 FTBBank 发送一个读请求，来判断其中是否存在对应的 FTB 项。如果存在，下周期可直接写入到该位置，否则，通知 FTBBank 为其重新分配一个位置。\n因此 FTB 项的更新需要的周期数需随 hit 情况而定。\n我们首先来看一下 FTBBank是怎样处理更新的。\nFTBBank 更新 FTBBank 的更新接口如下，分为更新读接口和更新写接口两部分。\nu_req_pc 更新读请求 pc\nFlipped(DecoupledIO(UInt(VAddrBits.W))) update_hits 读出的命中情况\nValid(UInt(log2Ceil(numWays).W)) update_access 有更新请求但 meta 信息指示未命中\nBool() update_pc 更新写请求 pc\nUInt(VAddrBits.W)) update_write_data 更新请求写入的数据，valid 有效的时候写\nFlipped(Valid(new FTBEntryWithTag)) update_write_way 更新请求写入的路索引\nUInt(log2Ceil(numWays).W)) update_write_alloc 是否需要分配FTB项（以前未命中）\nBool() 对于更新读接口，FTBBank 通过 u_req_pc 信号获取更新读请求，该请求要比预测时的读请求优先级更高，在下一个周期，FTBBank 会将命中情况通过 update_hits 接口送出。update_access 仅用于 FTBBank 内部某些状态判断。\n对于更新写接口，FTBBank 通过 update_pc 信号获取更新写请求的 pc，并在 update_write_data 有效时向 update_write_way 对应的位置写入其中的数据。如果 update_write_alloc 有效，说明不能直接写入请求中指定的位置，而是需要重新分配一个位置。\n分配的策略如下：\n如果所有路均已填充，则使用伪 LRU 替换算法选取要替换的路 若存在空路，否则选取空路。 更新请求时序 meta hit 有效 若更新请求中 meta 指示为 hit，则我们直接根据更新请求中的信息，指定好写入的地址和数据即可，写入仅需一周期 meta hit 无效 此时，在接收到更新请求后，我们将请求中的 pc 连接至 FTBBank 的读端口，读端口在下一周期将结果返回。由于时延问题，我们将这一结果保存，并在下一周期使用。下一周期根据结果中的命中情况指定是否需要设置 update_write_alloc，并发出写请求。这时的更新过程总共花费了三个周期。 接口列表 信号类型 信号位 信号名 信号描述 input clock 输入时钟 input reset 复位信号 input [35:0] io_reset_vector 用于reset时，reset s1_pc_dup_0 提供的值 input [40:0] io_in_bits_s0_pc_0 输入位s0_pc 的 第0个复制 input [40:0] io_in_bits_s0_pc_1 同上 第1个 input [40:0] io_in_bits_s0_pc_2 同上 第2个 input [40:0] io_in_bits_s0_pc_3 同上 第3个 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0 预测结果输入 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_0_br_taken_mask_0 s2 阶段输出的完整预测结果 output io_out_s2_full_pred_0_br_taken_mask_1 output io_out_s2_full_pred_0_slot_valids_0 output io_out_s2_full_pred_0_slot_valids_1 output [40:0] io_out_s2_full_pred_0_targets_0 output [40:0] io_out_s2_full_pred_0_targets_1 output [40:0] io_out_s2_full_pred_0_jalr_target output [3:0] io_out_s2_full_pred_0_offsets_0 output [3:0] io_out_s2_full_pred_0_offsets_1 output [40:0] io_out_s2_full_pred_0_fallThroughAddr output io_out_s2_full_pred_0_is_br_sharing output io_out_s2_full_pred_0_hit output io_out_s2_full_pred_1_br_taken_mask_0 同上 output io_out_s2_full_pred_1_br_taken_mask_1 output io_out_s2_full_pred_1_slot_valids_0 output io_out_s2_full_pred_1_slot_valids_1 output [40:0] io_out_s2_full_pred_1_targets_0 output [40:0] io_out_s2_full_pred_1_targets_1 output [40:0] io_out_s2_full_pred_1_jalr_target output [3:0] io_out_s2_full_pred_1_offsets_0 output [3:0] io_out_s2_full_pred_1_offsets_1 output [40:0] io_out_s2_full_pred_1_fallThroughAddr output io_out_s2_full_pred_1_is_br_sharing output io_out_s2_full_pred_1_hit output io_out_s2_full_pred_2_br_taken_mask_0 同上 output io_out_s2_full_pred_2_br_taken_mask_1 output io_out_s2_full_pred_2_slot_valids_0 output io_out_s2_full_pred_2_slot_valids_1 output [40:0] io_out_s2_full_pred_2_targets_0 output [40:0] io_out_s2_full_pred_2_targets_1 output [40:0] io_out_s2_full_pred_2_jalr_target output [3:0] io_out_s2_full_pred_2_offsets_0 output [3:0] io_out_s2_full_pred_2_offsets_1 output [40:0] io_out_s2_full_pred_2_fallThroughAddr output io_out_s2_full_pred_2_is_jalr output io_out_s2_full_pred_2_is_call output io_out_s2_full_pred_2_is_ret output io_out_s2_full_pred_2_last_may_be_rvi_call output io_out_s2_full_pred_2_is_br_sharing output io_out_s2_full_pred_2_hit output io_out_s2_full_pred_3_br_taken_mask_0 同上 output io_out_s2_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_3_slot_valids_0 output io_out_s2_full_pred_3_slot_valids_1 output [40:0] io_out_s2_full_pred_3_targets_0 output [40:0] io_out_s2_full_pred_3_targets_1 output [40:0] io_out_s2_full_pred_3_jalr_target output [3:0] io_out_s2_full_pred_3_offsets_0 output [3:0] io_out_s2_full_pred_3_offsets_1 output [40:0] io_out_s2_full_pred_3_fallThroughAddr output io_out_s2_full_pred_3_fallThroughErr output io_out_s2_full_pred_3_is_br_sharing output io_out_s2_full_pred_3_hit output io_out_s3_full_pred_0_br_taken_mask_0 s3 阶段输出的完整预测结果 output io_out_s3_full_pred_0_br_taken_mask_1 output io_out_s3_full_pred_0_slot_valids_0 output io_out_s3_full_pred_0_slot_valids_1 output [40:0] io_out_s3_full_pred_0_targets_0 output [40:0] io_out_s3_full_pred_0_targets_1 output [40:0] io_out_s3_full_pred_0_jalr_target output [40:0] io_out_s3_full_pred_0_fallThroughAddr output io_out_s3_full_pred_0_fallThroughErr output io_out_s3_full_pred_0_is_br_sharing output io_out_s3_full_pred_0_hit output io_out_s3_full_pred_1_br_taken_mask_0 同上 output io_out_s3_full_pred_1_br_taken_mask_1 output io_out_s3_full_pred_1_slot_valids_0 output io_out_s3_full_pred_1_slot_valids_1 output [40:0] io_out_s3_full_pred_1_targets_0 output [40:0] io_out_s3_full_pred_1_targets_1 output [40:0] io_out_s3_full_pred_1_jalr_target output [40:0] io_out_s3_full_pred_1_fallThroughAddr output io_out_s3_full_pred_1_fallThroughErr output io_out_s3_full_pred_1_is_br_sharing output io_out_s3_full_pred_1_hit output io_out_s3_full_pred_2_br_taken_mask_0 同上 output io_out_s3_full_pred_2_br_taken_mask_1 output io_out_s3_full_pred_2_slot_valids_0 output io_out_s3_full_pred_2_slot_valids_1 output [40:0] io_out_s3_full_pred_2_targets_0 output [40:0] io_out_s3_full_pred_2_targets_1 output [40:0] io_out_s3_full_pred_2_jalr_target output [40:0] io_out_s3_full_pred_2_fallThroughAddr output io_out_s3_full_pred_2_fallThroughErr output io_out_s3_full_pred_2_is_jalr output io_out_s3_full_pred_2_is_call output io_out_s3_full_pred_2_is_ret output io_out_s3_full_pred_2_is_br_sharing output io_out_s3_full_pred_2_hit output io_out_s3_full_pred_3_br_taken_mask_0 同上 output io_out_s3_full_pred_3_br_taken_mask_1 output io_out_s3_full_pred_3_slot_valids_0 output io_out_s3_full_pred_3_slot_valids_1 output [40:0] io_out_s3_full_pred_3_targets_0 output [40:0] io_out_s3_full_pred_3_targets_1 output [40:0] io_out_s3_full_pred_3_jalr_target output [3:0] io_out_s3_full_pred_3_offsets_0 output [3:0] io_out_s3_full_pred_3_offsets_1 output [40:0] io_out_s3_full_pred_3_fallThroughAddr output io_out_s3_full_pred_3_fallThroughErr output io_out_s3_full_pred_3_is_br_sharing output io_out_s3_full_pred_3_hit output [222:0] io_out_last_stage_meta 最后一个阶段输出的 meta 信息 output io_out_last_stage_ftb_entry_valid 最后一个阶段输出的 FTB 项 output [3:0] io_out_last_stage_ftb_entry_brSlots_0_offset output [11:0] io_out_last_stage_ftb_entry_brSlots_0_lower output [1:0] io_out_last_stage_ftb_entry_brSlots_0_tarStat output io_out_last_stage_ftb_entry_brSlots_0_sharing output io_out_last_stage_ftb_entry_brSlots_0_valid output [3:0] io_out_last_stage_ftb_entry_tailSlot_offset output [19:0] io_out_last_stage_ftb_entry_tailSlot_lower output [1:0] io_out_last_stage_ftb_entry_tailSlot_tarStat output io_out_last_stage_ftb_entry_tailSlot_sharing output io_out_last_stage_ftb_entry_tailSlot_valid output [3:0] io_out_last_stage_ftb_entry_pftAddr output io_out_last_stage_ftb_entry_carry output io_out_last_stage_ftb_entry_isCall output io_out_last_stage_ftb_entry_isRet output io_out_last_stage_ftb_entry_isJalr output io_out_last_stage_ftb_entry_last_may_be_rvi_call output io_out_last_stage_ftb_entry_always_taken_0 output io_out_last_stage_ftb_entry_always_taken_1 input io_ctrl_btb_enable 使能信号 input io_s0_fire_0 s0 阶段流水线控制信号 input io_s0_fire_1 input io_s0_fire_2 input io_s0_fire_3 output io_s1_ready s1 阶段流水线控制信号 input io_s1_fire_0 input io_s1_fire_1 input io_s1_fire_2 input io_s1_fire_3 input io_s2_fire_0 s2 阶段流水线控制信号 input io_s2_fire_1 input io_s2_fire_2 input io_s2_fire_3 input io_update_valid 更新有效性 input [40:0] io_update_bits_pc 传回的预测块pc（用于指示更新的预测块） input io_update_bits_ftb_entry_valid 是否启用 input [3:0] io_update_bits_ftb_entry_brSlots_0_offset solt 0 中分支指令相对于地址块起始pc的偏移 input [11:0] io_update_bits_ftb_entry_brSlots_0_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_brSlots_0_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_brSlots_0_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_brSlots_0_valid 是否启用 input [3:0] io_update_bits_ftb_entry_tailSlot_offset solt 1 中分支指令相对于地址块起始pc的偏移 input [19:0] io_update_bits_ftb_entry_tailSlot_lower 跳转目标地址的低位 input [1:0] io_update_bits_ftb_entry_tailSlot_tarStat 跳转后的 pc 高位是否进退位 input io_update_bits_ftb_entry_tailSlot_sharing 无条件跳转指令槽中存储条件分支指令 input io_update_bits_ftb_entry_tailSlot_valid 是否启用 input [3:0] io_update_bits_ftb_entry_pftAddr Partial Fallthrough Addr 如果预测块中没有跳转，那么程序将会顺序执行到达的地址，预测块的结束地址。 input io_update_bits_ftb_entry_carry pc+pft时是否产生进位 input io_update_bits_ftb_entry_isCall 是否是函数调用 input io_update_bits_ftb_entry_isRet 是否是函数返回 input io_update_bits_ftb_entry_isJalr 是否是 jalr 指令 input io_update_bits_ftb_entry_last_may_be_rvi_call 最后一个指令槽存储的可能是 rvi 的 call 指令 input io_update_bits_ftb_entry_always_taken_0 是否预测为总是跳转 input io_update_bits_ftb_entry_always_taken_1 是否预测为总是跳转 input io_update_bits_old_entry 是否是旧的 FTB 项 input [222:0] io_update_bits_meta meta 信息 ","categories":["香山 BPU 子模块"],"description":"","excerpt":"FTB 简介 FTB 是香山 BPU 的第三个子预测器，它也能一并获取到 uFTB 和 TAGE-SC 的输出。在 FTB 的输入接口 …","ref":"/xs-bpu/docs/modules/03_ftb/","tags":["子预测器","FTB"],"title":"FTB 分支预测器"},{"body":"Single-Cycle Prediction without Bubble uFTB is the only predictor in Xiangshan BPU that can generate prediction results in a single cycle. The figure below shows the prediction process of uFTB. The s0_pc is sent from the top level of BPU, and when the s1 stage is active, the s1_pc retains the value of s0_pc from the previous cycle. This means that the value of s0_pc will move down the pipeline.\nWhen the s1 stage is active, uFTB receives the s1_fire signal from the current cycle and generates a prediction result based on the s1_pc address in this cycle, which can obtain the new PC value in the prediction result.\nAs shown in the figure, the top level of BPU analyzes the next PC value position based on the prediction result channel s1 and sends it to npc_Gen (new PC generator) for generating the s0_pc of the next cycle.\nIn the next cycle, uFTB gets the new PC value and starts generating the prediction block for the new PC value. Therefore, with only the s1 stage, the prediction block can be generated at a rate of one block per cycle.\nPrediction Result Redirection However, except for uFTB, other predictors require 2-3 cycles to generate prediction results. How to utilize their prediction results? And how to generate the prediction result redirection signal?\nAs shown in the figure, a Predirector 2 that takes two cycles to generate a prediction result can output its prediction result to the s2 prediction result channel in the s2 stage. After the top level of BPU receives the prediction result, it analyzes the jump target address target of the prediction block and connects it to npc_Gen.\nAt this point, the signal connected to npc_Gen contains both the old PC prediction result generated by s2 and the new PC prediction result generated by s1. How to choose which one to use for the new PC?\nAs mentioned earlier, BPU compares the prediction result of s2 with the prediction result of s1 from the previous cycle. If the prediction results are different, it indicates that s1 has made a wrong prediction, and naturally, the prediction result of the current cycle generated based on the wrong prediction result of the previous cycle is also wrong. Therefore, if the prediction result is incorrect in the current cycle, npc_Gen will use the target provided by s2 as the new s0_pc.\nThis process is shown in the pipeline structure diagram as follows:\nThe Diff comparator compares the prediction results of the s1 stage with those of the previous cycle to generate a diff signal, guiding npc_Gen to generate the next PC. At the same time, the diff signal indicates that the prediction result of the s1 stage is incorrect and can be used directly by BPU to redirect the prediction result channel of the s2 stage in the FTQ, instructing the FTQ to overwrite the previous prediction result.\nThe diff signal is also sent to each predictor through the s2_redirect interface to guide the predictors to update their states.\nFurthermore, when the prediction result redirection of the s2 stage occurs, indicating that the prediction result of the s1 channel is incorrect, the s2 stage cannot continue to predict and needs to invalidate the s2_fire signal of the predictor pipeline and wait for the corrected prediction result to flow in.\nThe prediction result redirection of the s3 stage is similar to this. Its pipeline structure diagram is as follows. The specific processing process is left for you to analyze.\nRedirection Requests and Other Information Generation Only when the prediction information of all three stages is incorrect will an external redirection request occur. At this time, npc_Gen will receive the PC address from the redirection request. Since when a redirection request occurs, we assume that all three stages have predicted incorrectly, so all three stages’ fire signals need to be invalidated. Then, npc_Gen uses the PC that needs to be restored from the redirection request to restart the prediction.\nOther information, such as the generation of the global history and the PC, follows the same principle and is maintained based on the prediction information of each stage. The global history generates a new branch history based on the prediction results of each stage.\nPipeline Control Signals After learning about the specific process of the pipeline, you should understand the pipeline control signals in the predictor interface, as follows:\ns0_fire, s1_fire, s2_fire, s3_fire Indicate whether each stage of the pipeline is working. s2_redirect, s3_redirect Indicate whether a prediction result redirection has occurred. s1_ready, s2_ready, s3_ready Sent from the predictor to the top level of BPU, indicating whether each stage of the pipeline is ready. Conclusion By now, you should understand the basic design principles, external interaction logic, internal structure, timing, etc., of the Xiangshan Branch Prediction Unit, and have a rough understanding of the working principle of BPU. Xiangshan’s BPU is no longer mysterious to you.\nNext, you can read the Important Structures and Interfaces Document and combine it with the source code of Xiangshan BPU to form a more detailed understanding of BPU. When you clearly understand the working principle and signal details of BPU, you can start your verification work!\n","categories":["Xiangshan BPU Basic Design"],"description":"The timing design of the three-stage pipeline is the essence of the Xiangshan BPU. This section will introduce how the prediction result redirection signal is generated, how a new PC is generated based on the prediction result, and how the prediction results of the three channels are handled.","excerpt":"The timing design of the three-stage pipeline is the essence of the …","ref":"/xs-bpu/en/docs/basic/03_xsbpu_timing/","tags":["Xiangshan BPU Timing","Pipeline Control","Prediction Result Redirection"],"title":"Introduction to the Timing of Xiangshan Branch Prediction Unit"},{"body":"Feature List s2 TAGE outputs prediction results s3 SC outputs prediction results s2 TAGE outputs meta information s3 SC outputs meta information TAGE performs update training Check for new table entry requests Globally reset useful SC performs update training ","categories":["Feature List"],"description":"","excerpt":"Feature List s2 TAGE outputs prediction results s3 SC outputs …","ref":"/xs-bpu/en/docs/feature/03_tagescfeature/","tags":["TAGE-SC","Feature List"],"title":"TAGE-SC Feature List"},{"body":"功能列表（Feature List） s2 TAGE 输出预测结果 s3 SC 输出预测结果 s2 TAGE 输出meta信息 s3 SC 输出meta信息 TAGE 进行更新训练 检查申请新表项 全局重置useful SC 进行更新训练 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） s2 TAGE 输出预测结果 s3 SC 输出预测结果 s2 TAGE 输出meta信息 s3 SC  …","ref":"/xs-bpu/docs/feature/03_tagescfeature/","tags":["TAGE-SC","Feature List"],"title":"TAGE-SC 功能列表"},{"body":"硬件验证中的 功能列表(Feature List) 列出了需要验证的设计的各种功能、特性。在开始验证之前，我们一般都会先列出该设计的功能列表，来明确我们本次任务需要对哪些特性进行验证。\n如果要验证功能列表中的一个特征，我们将会针对这一特征构建所需的测试点，并编写测试用例来覆盖这些测试点。因此功能列表不仅仅是对待验证设计的一份功能描述，还是对我们一次验证方向的指引，也是对一次验证的任务量的简单概括。\n在本次验证中，我们为每个验证任务（即 BPU 顶层及每个子预测器）提供了一份基础的功能列表，这些特征仅涵盖了对应模块的基础功能，在你的验证中，必须保证本次验证可以覆盖到这些特征。在此基础上，你还可以对列表中没有涵盖到的特征进行补充，以保证最后的交付质量。\n在本节文档中，你只需要查看你将要验证模块所对应的文档即可。\n","categories":["示例","教程"],"description":"硬件验证中的 功能列表(Feature List) 列出了需要验证的设计的各种功能、特性。本节提供了每个验证任务的基础功能列表，以供参考。","excerpt":"硬件验证中的 功能列表(Feature List) 列出了需要验证的设计的各种功能、特性。本节提供了每个验证任务的基础功能列表，以供参考。","ref":"/xs-bpu/docs/feature/","tags":["examples","docs"],"title":"功能列表"},{"body":"一周期无空泡预测 uFTB 是香山 BPU 中唯一一个可以在一周期便可以产生预测结果的预测器，上图中展示了 uFTB 的预测过程。s0_pc 由 BPU 顶层送入，s1 周期生效时，s1_pc 保存了上一周期 s0_pc 的值，依次类推。也就是说传入的 s0_pc 的值会随流水线向下移动。\n在 s1 周期生效时，uFTB 会接收到本周期传来的 s1_fire 信号，并根据 s1_pc 指示的地址，在本周期生成预测结果，在预测结果中可以获取新的 PC 值。\n如图所示，BPU 顶层根据 s1 预测结果通道，分析出下一个 PC 值的位置（即图中的 target），并将其送往 npc_Gen （即新PC生成器）中，用于产生下一个周期的 s0_pc。\n于是下一周期，uFTB 获取到了新的 PC 值，并开始了新 PC 值预测块的产生。由此可见，仅凭借 s1 周期，便可以以一周期一个预测块的速度来产生预测结果。\n预测结果重定向 但除了 uFTB 以外，其他预测器都需要 2-3 个周期才可以产生预测结果，如何利用起他们的预测结果？又如何生成预测结果重定向信号呢？\n如图中所示，一个两周期产生预测结果的 Predirector 2 ，可以在 s2 周期，向 s2 预测结果通道内输出它的预测结果。BPU 顶层拿到预测结果后，分析出预测块的跳转目标地址 target 并连向 npc_Gen。\n此时连向 npc_Gen 的信号中，既有 s2 产生的旧 PC 的预测结果，又有 s1 产生的新 PC 的预测结果，该如何抉择新 PC 用哪一个呢？\n通过之前的介绍，我们已经知道，BPU 会将 s2 的预测结果与上周期 s1 的预测结果进行比较，如果预测结果不同，那么说明 s1 预测错了，自然 s1 基于上周期错误预测结果产生的本周期预测结果也是错误的，因此在本周期如果预测结果产生错误，npc_Gen 将使用 s2 所提供的 target 作为新的 s0_pc。\n这个过程画在流水线结构图中是这样的：\nDiff 比较器通过获取 s1 周期的预测结果，与 s1 周期的预测结果进行对比产生 diff 信号，指导 npc_Gen 进行下一条 pc 的生成。与此同时，diff 信号指示了 s1 阶段的预测结果发生错误，可直接用于 BPU 发往 FTQ 的预测结果中 s2 通道的预测结果重定向通道，指导 FTQ 覆盖之前的预测结果。\nDiff 信号还会通过 s2_redirect 接口送往每个子预测器，指导子预测器进行状态的更新。\n除此之外，当 s2 预测结果重定向发生时，说明 s1 通道预测结果已经发生错误，s2 阶段不能继续进行预测，需及时将子预测器流水线控制信号 s2_fire 置为无效，并等待纠正后的预测结果流入。\ns3 预测结果的重定向与此类似，其流水线结构图如下所示。具体的处理过程留给大家来分析。\n重定向请求与其他信息生成 只有当三个阶段的预测信息都错误时，才会产生有外部重定向请求发生，此时 npc_Gen 会接收到来自重定向请求中的 pc 地址。由于当重定向请求发生时，我们认为三阶段均预测错误，因此需要将三个阶段的 fire 信号全部置为无效，然后 npc_Gen 采用重定向请求中需要恢复的 PC，重新开始预测。\n其他信息，例如全局历史的生成方式与PC的生成方式一致，都是在顶层通过三阶段预测信息来维护，全局历史会根据每一阶段的预测结果来产生新的分支历史。\n流水线控制信号 学习了流水线的具体流程，我们应该可以理解子预测器接口中的流水线控制信号了，如下\ns0_fire, s1_fire, s2_fire, s3_fire 指示每个流水级是否工作 s2_redirect, s3_redirect 指示是否有预测结果重定向发生 s1_ready, s2_ready, s3_ready 子预测器发给BPU顶层，表示相应流水级是否就绪 总结 至此，你应该懂得了香山分支预测单元的基础设计思想、对外交互逻辑、内部结构、时序等内容，并对 BPU 的工作原理有了大致的理解，香山的 BPU 对你来说已经不再神秘。\n接下来你可以阅读重要结构及接口文档，并结合香山 BPU 源代码对 BPU 形成更为细致的理解，当你能清楚的明白 BPU 的工作原理和信号细节时，便可以开始你的验证工作了！\n","categories":["香山 BPU 基础设计"],"description":"三级流水线的时序设计是香山 BPU 的灵魂所在。预测结果重定向信号如何产生，如何根据预测结果生成新的PC，如何处理三个通道的预测结果，都会在本节中进行介绍。","excerpt":"三级流水线的时序设计是香山 BPU 的灵魂所在。预测结果重定向信号如何产生，如何根据预测结果生成新的PC，如何处理三个通道的预测结果，都会在 …","ref":"/xs-bpu/docs/basic/03_xsbpu_timing/","tags":["香山 BPU 时序","流水线控制","预测结果重定向"],"title":"香山分支预测单元时序介绍"},{"body":"在香山分支预测单元中，其所有子预测器以及 Composer 的类实现都是继承自子预测器基类（BasePredictor），并且子预测器接口（BasePredictorIO）也是在子预测器基类中进行定义。因此我们可以认为，Compser 和所有子预测器都含有相同的接口。\n在子预测的理解及验证当中，我们最直接的外界交互是发生在子预测器接口，以及子预测器基类中定义的一些变量，因此在子预测器的验证之前，强烈建议你阅读并理解本节文档。\n子分支预测器接口的大致内容及使用方法，已在香山分支预测单元（BPU）基础设计中进行了介绍，本节文档将专注于接口的信号细节。\n子分支预测器接口 (BasePredictorIO) 接口定义: src/main/scala/xiangshan/frontend/BPU.scala\n每个子分支预测器都需要实现该接口，该接口定义了子分支预测器的输入输出接口。\n注意：其中某些信号被定义为了 numDup 个数量，其中每个信号完全一样，多个相同信号是为其他因素考虑。\n详细的信号列表如下：\nreset_vector 重置向量，reset 时 BPU 的 pc 会被重置为该值\n接口类型：UInt(PAddrBits.W) in BPU 向子分支预测器发送的信息\n接口类型：DecoupledIO(new BasePredictorInput) 信号列表： s0_pc s0流水级的pc 接口类型：Vec(numDup, UInt(VAddrBits.W)) folded_hist 全局折叠历史信息 接口类型：Vec(numDup, new AllFoldedHistories(foldedGHistInfos)) 信号列表：详见（AllFoldedHistories） ghist 全局分支历史信息 接口类型：UInt(HistoryLength.W) resp_in 全局分支预测信息（包含s1, s2, s3 预测结果信息） 接口类型：BranchPredictionResp 信号列表：详见（BranchPredictionResp） out 子分支预测器向 BPU 发送的信息（包含s1, s2, s3 预测结果信息）\n接口类型：new BasePredictorOutput 继承自 BranchPredictionResp 信号列表：详见（BranchPredictionResp） ctrl BPU 子预测器使能控制信号，主要用于控制各预测器是否开启\n接口类型：BPUCtrl 接口列表： ubtb_enable: UBTB 预测器使能 接口类型：Bool() btb_enable: BTB 预测器使能 接口类型：Bool() bim_enable: BIM 预测器使能 接口类型：Bool() tage_enable: TAGE 预测器使能 接口类型：Bool() sc_enable: SC 预测器使能 接口类型：Bool() ras_enable: RAS 预测器使能 接口类型：Bool() loop_enable: LOOP 预测器使能 接口类型：Bool() s0_fire s0阶段握手成功信号\n接口类型：Vec(numDup, Bool()) s1_fire s1阶段握手成功信号\n接口类型：Vec(numDup, Bool()) s2_fire s2阶段握手成功信号\n接口类型：Vec(numDup, Bool()) s3_fire s3阶段握手成功信号\n接口类型：Vec(numDup, Bool()) s2_redirect s2阶段重定向信号\n接口类型：Vec(numDup, Bool()) s3_redirect s3阶段重定向信号\n接口类型：Vec(numDup, Bool()) s1_ready s1阶段是否准备好接收信息 （方向：由子预测器输出）\n接口类型：Bool() s2_ready s2阶段是否准备好接收信息 （方向：由子预测器输出）\n接口类型：Bool() s3_ready s3阶段是否准备好接收信息 （方向：由子预测器输出）\n接口类型：Bool() update BPU 向子分支预测器转发的更新请求\n接口类型：Valid(new BranchPredictionUpdate) 信号列表：详见（BranchPredictionUpdate） redirect BPU 向子分支预测器转发的重定向请求\n接口类型：Valid(new BranchPredictionRedirect) 信号列表：详见（BranchPredictionRedirect） 其中，流水线控制信号将会在下面内容中进行进一步的说明。\n全局折叠历史 (AllFoldedHistories) 接口定义：src/main/scala/xiangshan/frontend/FrontendBundle.scala\n接口类型：AllFoldedHistories(foldedGHistInfos))\n全局折叠历史的接口信息仅为一个 FoldedHistory 的列表\nhist 折叠历史列表 接口类型：MixedVec(gen.map{case (l, cl) =\u003e new FoldedHistory(l, cl, numBr)}) 而 FoldedHistory 的接口信息也仅有一项\nfolded_hist 单项折叠历史，其位宽为历史压缩后长度。 接口类型：UInt(compLen.W) 也就是说，全局折叠历史接口实际上是一个存储了折叠后历史的列表，其中每一项都是一个特定长度的折叠历史。\n子预测器基类 在子预测器基类中定义了若干信号，并在每个子预测器中都可以访问到，并在其中还进行了若干连线。\n其中多数信号都较容易理解 ，我们需要重点关注的是其中每个流水的 pc，这还会涉及到你对于流水线控制信号的掌握。因此接下来，我们将会介绍在子预测器中需要关注的流水线控制信号的含义，以及 s1_pc, s2_pc, s3_pc 信号的含义。\n其中流水线控制信号共分为 3 组：\nfire 信号 (s0, s1, s2, s3) redirect 信号 (s2, s3) ready 信号 (s1, s2, s3) 子预测器基类中的 pc 信号共有四组，s0_pc_dup, s1_pc_dup, s2_pc_dup, s3_pc_dup。每组信号中有多个 pc 信号，他们完全相同，为考虑一些其他因素所复制。因此我们可单纯把他们看做 s0_pc, s1_pc, s2_pc, s3_pc。\n他们的用法可参照下图：\n他们与流水线控制信号的关系是：\ns0_pc 直接由输入接口中的 in.s0_pc 连接而来 当 s0_fire 生效时，下一周期 s1_pc 会输出 s0_pc 的值 当 s1_fire 生效时，下一周期 s2_pc 会输出 s1_pc 的值 当 s2_fire 生效时，下一周期 s3_pc 会输出 s2_pc 的值 也就是说，fire 信号是会影响到下一个周期的数据是否有效，比如 s0_fire 信号会影响 s1 流水的数据是否有效，s1_fire 信号会影响到 s2 流水的数据是否有效。\n而 fire 信号是否有效取决于本流水级数据是否有效 以及 下一流水级是否 ready。例如 s1_fire 信号有效的条件必须包含 s1 阶段数据有效，以及子预测器输出的 s2_ready 信号为有效，此时便可认为，s1 阶段数据处理完成，并且 s2 阶段就绪，下一周期数据将被直接送入 s2 阶段。\n因此在子预测器中，以 s1 阶段为例，s1_ready 可以阻塞数据进入 s1 阶段，当 s1_ready 生效时，下一周期 s1 阶段的数据便会生效。而当 s1_fire 有效的同时，说明 s1 阶段数据已经生效，预测器也已经将 s1 阶段的结果生成，下一周期将直接被送入 s2 阶段。\nredirect 信号则相对明确。以 s2 为例，当 s2_redirect 有效时，说明 s2_fire 生效的同时 s2 预测结果与上周期 s1 预测结果不同。\n","categories":["香山 BPU 接口"],"description":"本文档介绍了香山BPU中的子预测器接口，以及子预测器基类的使用，阅读此文档可帮助你了解子预测器的对外交互，和子预测器基类中信号的使用。","excerpt":"本文档介绍了香山BPU中的子预测器接口，以及子预测器基类的使用，阅读此文档可帮助你了解子预测器的对外交互，和子预测器基类中信号的使用。","ref":"/xs-bpu/docs/ports/03_subpredictor/","tags":["子预测器"],"title":"子预测器基类与子预测器接口"},{"body":"Function Introduction For general conditional branch instructions, only predicting whether to jump (taken) or not (not taken) is needed. However, for indirect jumps, such as call/jump instructions, it is necessary to predict where to jump to (Target). In order to make TAGE support predicting jump addresses, ITTAGE (Indirect Target TAGE) was introduced.\nThe main difference between ITTAGE and TAGE is that in the T0 and Tn tables, Target PC data is added. During prediction, ITTAGE selects the Target from the matched, longest history entry as the prediction result, and uses a 2-bit saturating counter to decide whether to output this result or choose an alternative prediction result. For TAGE predictor details, please refer to TAGE-SC Branch Predictor.\nKunming Lake ITTAGE Branch Predictor In the BPU design of Kunming Lake, prediction is performed in a cascaded manner with multiple predictors, so the implementation of the subpredictor differs from the original predictor, mainly in the default prediction result.\nBasic Functionality ITTAGE’s basic functionality is similar to the TAGE branch predictor, but with the following differences:\nThe Target is added as a jump target address item in the entry to predict the jump target address. The saturating counter ctr no longer provides the prediction direction, but instead decides whether to output the result (just the prediction information). Since there is only one indirect jump instruction in each branch prediction block, ITTAGE only considers one instruction. Pipeline ITTAGE contains three pipeline stages, the first stage calculates the index, and the second stage reads the result from the SRAM table using the index.\nCycle 0, s0: Input of the first pipeline stage, generally pc and folded history. Operation of the first pipeline stage：Calculate the index. Output through registers to s1.\nCycle 1, s1: Input of the second pipeline stage, the index and other data calculated in the first stage. Operation of the second pipeline stage：Access SRAM, read prediction information. Output through registers to s2.\nCycle 2, s2: Input of the third pipeline stage, the original prediction information read from SRAM in the second stage. Operation of the third pipeline stage：Process the original prediction information, decide whether to output the prediction result.\nCycle 3, s3: Prediction result ready, the prediction result can now be used. Data Structure In the Kunming Lake implementation, the table structure of T0 and Tn is as follows:\n预测器 作用 表项构成 项数 基准预测器T0 用于在其他预测器的预测结果都无效时输出预测结果 虚表，不存在。 直接将上级预测器FTB 的预测结果作为表项结果 虚表，不存在。 直接将上级预测器FTB结果作为索引到的结果 预测表T1-T2 对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为 原始预测信息。历史记录长度由输入的H决定 target：41 bitsvalid 1bittag 9bitsctr 2bitsus: 1bit（usefulness计数器） 256项 预测表T3-T5 512项 T0，TnTable Retrieval Method The retrieval method is consistent with the TAGE branch predictor, only differing in the configuration options of each table.\n表名称 FH长度 FH1长度 FH2长度 最近历史长度（用到GH中的位数） T1 4比特 4比特 4比特 低4位，即把最新4位历史，折叠成FH、FH1、FH2 T2 8比特 8比特 8比特 低8位，即把最新8位历史，折叠成FH、FH1、FH2 T3 9比特 9比特 8比特 低13位，即把最新13位历史，折叠成FH、FH1、FH2 T4 9比特 9比特 8比特 低16位，即把最新16位历史，折叠成FH、FH1、FH2 T5 9比特 9比特 8比特 低32位，即把最新32位历史，折叠成FH、FH1、FH2 Other processes (computation method and computation formula) are similar to the TAGE-SC branch predictor.\nAlternate Predictor When the prediction result given by the Tn table has insufficient “prediction confidence,” the prediction result needs to be jumped to become an “alternate predictor.” This process is similar to TAGE. For details, please refer to the corresponding part of TAGE. Unlike TAGE, ITTAGE’s ctr does not give the prediction direction but only determines whether to output the result (prediction confidence). When ctr is 2b00, it is considered weak confidence. Choose the alternate prediction result:\nIf multiple tables are hit, output the Target from the second-longest history table entry. Otherwise, output the T0 Target (FTB Target). Prediction Process The prediction process is similar to TAGE, but ITTAGE has an additional step to decide whether to output the prediction result based on ctr. The specific process is as follows:\nWhen the ctr of the ITTAGE table entry is not 2b00, output Target. When the ctr of the ITTAGE table entry is 2b00, output the alternate prediction result: If there is a second-longest history (the second table is also hit), output the Target of the second-longest. Otherwise, output the FTB Target. When the ITTAGE table entry is not hit, output the T0 Target (FTB Target). Training Process This process is similar to TAGE, with the following differences:\nTable entry updates (original prediction data): ctr: If the predicted address matches the actual address, increment the ctr counter of the corresponding provider table entry by 1. If the predicted address does not match the actual address, decrement the ctr counter of the corresponding provider table entry by 1. In ITTAGE, it is determined based on ctr whether to adopt the jump target result of this prediction. If multiple tables are hit and the ctr of the longest history table is 0, adopt the alternate prediction logic (the second-longest history table or T0). Always update the longest history table during updates, and also update the alternate prediction table if the alternate prediction is adopted. target: When the ctr of the table entry to be updated is 0 during this prediction, directly store the actual final jump result in the target, overwriting it. When applying for a new table entry, directly store the actual final jump result in the target. Otherwise, do not modify the target. usefulness: When the provider’s prediction is correct but the alternate prediction is incorrect, set the provider’s usefulness to 1. If the alternate prediction has weak confidence and is correct, set the provider’s usefulness to 1. If the alternate prediction has weak confidence and is incorrect, set the provider’s usefulness to 0. New table entry: Each time the prediction from the longest history table with confidence is incorrect (not due to using the alternate prediction), try to randomly apply for a table entry from a longer history table. The condition for application is that the usefulness of the corresponding entry is 0. If all longer entries are not 0, the allocation fails. Reset useful bit: Each time a prediction error occurs and a new table entry is applied for, if the allocation fails, increment tickCtr (an 8-bit saturated counter used to reset all usefulness). If successful, decrement tickCtr. When tickCtr reaches its maximum value, set all usefulness in ITTAGE to 0 and reset tickCtr to 0. Interface List 接口类型 位宽 信号名 备注 input clock input reset input [40:0] io_in_bits_s0_pc_3 用于预测的PC input [7:0] io_in_bits_folded_hist_3_hist_14_folded_hist T2 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_13_folded_hist T3 折叠历史 input [3:0] io_in_bits_folded_hist_3_hist_12_folded_hist T1 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_10_folded_hist T5 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_6_folded_hist T4 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_4_folded_hist T3 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_3_folded_hist T5 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_2_folded_hist T4 折叠历史 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_jalr_target input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_jalr_target input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_jalr_target input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_jalr_target output [40:0] io_out_s3_full_pred_0_jalr_target output [40:0] io_out_s3_full_pred_1_jalr_target output [40:0] io_out_s3_full_pred_2_jalr_target output [40:0] io_out_s3_full_pred_3_jalr_target output [222:0] io_out_last_stage_meta [100:0] 有效，是ITTAGE的Meta信息 input io_s0_fire_3 s0阶段使能信号 input io_s1_fire_3 s1阶段使能信号 input io_s2_fire_0 s2阶段使能信号，相同 input io_s2_fire_1 input io_s2_fire_2 input io_s2_fire_3 input io_update_valid 是否进行更新 input [40:0] io_update_bits_pc 待更新的预测块pc索引 input [7:0] io_update_bits_spec_info_folded_hist_hist_14_folded_hist T2 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_13_folded_hist T3 更新时传入的历史 input [3:0] io_update_bits_spec_info_folded_hist_hist_12_folded_hist T1 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_10_folded_hist T5 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_6_folded_hist T4 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_4_folded_hist T3 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_3_folded_hist T5 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_2_folded_hist T4 更新时传入的历史 input [3:0] io_update_bits_ftb_entry_tailSlot_offset 待更新的FTB项offset input io_update_bits_ftb_entry_tailSlot_sharing 待更新的FTB项是否是有条件跳转 input io_update_bits_ftb_entry_tailSlot_valid 待更新的tailSlot是否启用 input io_update_bits_ftb_entry_isRet tailSlot是否是Ret指令 input io_update_bits_ftb_entry_isJalr tailSlot是否是Jalr指令 input io_update_bits_cfi_idx_valid 控制流指令在预测块中的索引.valid信号 input [3:0] io_update_bits_cfi_idx_bits 控制流指令在预测块中的索引 input io_update_bits_jmp_taken 预测块内无条件跳转指令被触发 input io_update_bits_mispred_mask_2 是否预测错误 input [222:0] io_update_bits_meta 预测时传出 meta 信息的[222:25] 即{25h0, _ubtb_io_out_last_stage_meta[5:0] ,_tage_io_out_last_stage_meta[87:0] ,_ftb_io_out_last_stage_meta[2:0], _ittage_io_out_last_stage_meta[100:0]} input [40:0] io_update_bits_full_target 预测块的跳转目标（下一个预测块的起始地址） Pass-through signals that do not have an impact These signals do not have an impact and are not important 接口类型 位宽 信号名 备注 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0 从FTB输入 完全透传到输出 包括jalr_target input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_0_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_0_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_0_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_0_hit input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_1_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_1_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_1_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_1_hit input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_2_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_2_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_2_is_jalr RAS 模块使用的信息，透传 input io_in_bits_resp_in_0_s2_full_pred_2_is_call input io_in_bits_resp_in_0_s2_full_pred_2_is_ret input io_in_bits_resp_in_0_s2_full_pred_2_last_may_be_rvi_call input io_in_bits_resp_in_0_s2_full_pred_2_is_br_sharing 从FTB输入 完全透传到输出 包括jalr_target fallThroughErr 表示 FTB项 中记录的 pftAddr 有误 生成方式：比较 pftAddr 代表的预测块结束地址是否大于预测块的起始地址，如果小于，则代表出现错误，此信号置为有效。这种情况可能会发生在 pc 索引到错误的 FTB 项的情况。 FTQ使用这个变量，与ITTAGE无关 input io_in_bits_resp_in_0_s2_full_pred_2_hit input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_3_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_3_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_3_fallThroughErr input io_in_bits_resp_in_0_s2_full_pred_3_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_3_hit input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0 除了 jalr_target 可能被修改，其他都是透传 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_0_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_0_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_0_hit input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_1_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_1_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_1_hit input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_2_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_2_is_jalr input io_in_bits_resp_in_0_s3_full_pred_2_is_call input io_in_bits_resp_in_0_s3_full_pred_2_is_ret input io_in_bits_resp_in_0_s3_full_pred_2_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_2_hit input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_targets_1 input [3:0] io_in_bits_resp_in_0_s3_full_pred_3_offsets_0 input [3:0] io_in_bits_resp_in_0_s3_full_pred_3_offsets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_3_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_3_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_3_hit input io_in_bits_resp_in_0_last_stage_ftb_entry_valid 透传到output，不做修改 来源是FTB input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_offset input [11:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_lower input [1:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_tarStat input io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_sharing input io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_valid input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_offset input [19:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_lower input [1:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_tarStat input io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_sharing input io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_valid input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_pftAddr input io_in_bits_resp_in_0_last_stage_ftb_entry_carry input io_in_bits_resp_in_0_last_stage_ftb_entry_isCall input io_in_bits_resp_in_0_last_stage_ftb_entry_isRet input io_in_bits_resp_in_0_last_stage_ftb_entry_isJalr input io_in_bits_resp_in_0_last_stage_ftb_entry_last_may_be_rvi_call input io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_0 input io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_1 output io_out_s2_full_pred_0_br_taken_mask_0 完全透传传入值 prefix: io_in_bits_resp_in_ output io_out_s2_full_pred_0_br_taken_mask_1 output io_out_s2_full_pred_0_slot_valids_0 output io_out_s2_full_pred_0_slot_valids_1 output [40:0] io_out_s2_full_pred_0_targets_0 output [40:0] io_out_s2_full_pred_0_targets_1 output [40:0] io_out_s2_full_pred_0_jalr_target output [3:0] io_out_s2_full_pred_0_offsets_0 output [3:0] io_out_s2_full_pred_0_offsets_1 output [40:0] io_out_s2_full_pred_0_fallThroughAddr output io_out_s2_full_pred_0_is_br_sharing output io_out_s2_full_pred_0_hit output io_out_s2_full_pred_1_br_taken_mask_0 output io_out_s2_full_pred_1_br_taken_mask_1 output io_out_s2_full_pred_1_slot_valids_0 output io_out_s2_full_pred_1_slot_valids_1 output [40:0] io_out_s2_full_pred_1_targets_0 output [40:0] io_out_s2_full_pred_1_targets_1 output [40:0] io_out_s2_full_pred_1_jalr_target output [3:0] io_out_s2_full_pred_1_offsets_0 output [3:0] io_out_s2_full_pred_1_offsets_1 output [40:0] io_out_s2_full_pred_1_fallThroughAddr output io_out_s2_full_pred_1_is_br_sharing output io_out_s2_full_pred_1_hit output io_out_s2_full_pred_2_br_taken_mask_0 output io_out_s2_full_pred_2_br_taken_mask_1 output io_out_s2_full_pred_2_slot_valids_0 output io_out_s2_full_pred_2_slot_valids_1 output [40:0] io_out_s2_full_pred_2_targets_0 output [40:0] io_out_s2_full_pred_2_targets_1 output [40:0] io_out_s2_full_pred_2_jalr_target output [3:0] io_out_s2_full_pred_2_offsets_0 output [3:0] io_out_s2_full_pred_2_offsets_1 output [40:0] io_out_s2_full_pred_2_fallThroughAddr output io_out_s2_full_pred_2_is_jalr output io_out_s2_full_pred_2_is_call output io_out_s2_full_pred_2_is_ret output io_out_s2_full_pred_2_last_may_be_rvi_call output io_out_s2_full_pred_2_is_br_sharing output io_out_s2_full_pred_2_hit output io_out_s2_full_pred_3_br_taken_mask_0 output io_out_s2_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_3_slot_valids_0 output io_out_s2_full_pred_3_slot_valids_1 output [40:0] io_out_s2_full_pred_3_targets_0 output [40:0] io_out_s2_full_pred_3_targets_1 output [40:0] io_out_s2_full_pred_3_jalr_target output [3:0] io_out_s2_full_pred_3_offsets_0 output [3:0] io_out_s2_full_pred_3_offsets_1 output [40:0] io_out_s2_full_pred_3_fallThroughAddr output io_out_s2_full_pred_3_fallThroughErr output io_out_s2_full_pred_3_is_br_sharing output io_out_s2_full_pred_3_hit output io_out_s3_full_pred_0_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_0_br_taken_mask_1 output io_out_s3_full_pred_0_slot_valids_0 output io_out_s3_full_pred_0_slot_valids_1 output [40:0] io_out_s3_full_pred_0_targets_0 output [40:0] io_out_s3_full_pred_0_targets_1 output [40:0] io_out_s3_full_pred_0_fallThroughAddr output io_out_s3_full_pred_0_fallThroughErr output io_out_s3_full_pred_0_is_br_sharing output io_out_s3_full_pred_0_hit output io_out_s3_full_pred_1_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_1_br_taken_mask_1 output io_out_s3_full_pred_1_slot_valids_0 output io_out_s3_full_pred_1_slot_valids_1 output [40:0] io_out_s3_full_pred_1_targets_0 output [40:0] io_out_s3_full_pred_1_targets_1 output [40:0] io_out_s3_full_pred_1_fallThroughAddr output io_out_s3_full_pred_1_fallThroughErr output io_out_s3_full_pred_1_is_br_sharing output io_out_s3_full_pred_1_hit output io_out_s3_full_pred_2_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_2_br_taken_mask_1 output io_out_s3_full_pred_2_slot_valids_0 output io_out_s3_full_pred_2_slot_valids_1 output [40:0] io_out_s3_full_pred_2_targets_0 output [40:0] io_out_s3_full_pred_2_targets_1 output [40:0] io_out_s3_full_pred_2_fallThroughAddr output io_out_s3_full_pred_2_fallThroughErr output io_out_s3_full_pred_2_is_jalr output io_out_s3_full_pred_2_is_call output io_out_s3_full_pred_2_is_ret output io_out_s3_full_pred_2_is_br_sharing output io_out_s3_full_pred_2_hit output io_out_s3_full_pred_3_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_3_br_taken_mask_1 output io_out_s3_full_pred_3_slot_valids_0 output io_out_s3_full_pred_3_slot_valids_1 output [40:0] io_out_s3_full_pred_3_targets_0 output [40:0] io_out_s3_full_pred_3_targets_1 output [3:0] io_out_s3_full_pred_3_offsets_0 output [3:0] io_out_s3_full_pred_3_offsets_1 output [40:0] io_out_s3_full_pred_3_fallThroughAddr output io_out_s3_full_pred_3_fallThroughErr output io_out_s3_full_pred_3_is_br_sharing output io_out_s3_full_pred_3_hit output io_out_last_stage_ftb_entry_valid 完全透传传入的值 output [3:0] io_out_last_stage_ftb_entry_brSlots_0_offset output [11:0] io_out_last_stage_ftb_entry_brSlots_0_lower output [1:0] io_out_last_stage_ftb_entry_brSlots_0_tarStat output io_out_last_stage_ftb_entry_brSlots_0_sharing output io_out_last_stage_ftb_entry_brSlots_0_valid output [3:0] io_out_last_stage_ftb_entry_tailSlot_offset output [19:0] io_out_last_stage_ftb_entry_tailSlot_lower output [1:0] io_out_last_stage_ftb_entry_tailSlot_tarStat output io_out_last_stage_ftb_entry_tailSlot_sharing output io_out_last_stage_ftb_entry_tailSlot_valid output [3:0] io_out_last_stage_ftb_entry_pftAddr output io_out_last_stage_ftb_entry_carry output io_out_last_stage_ftb_entry_isCall output io_out_last_stage_ftb_entry_isRet output io_out_last_stage_ftb_entry_isJalr output io_out_last_stage_ftb_entry_last_may_be_rvi_call output io_out_last_stage_ftb_entry_always_taken_0 output io_out_last_stage_ftb_entry_always_taken_1 Other Meta information can be found in the corresponding sub-predictor documentation _ubtb_io_out_last_stage_meta\n_tage_io_out_last_stage_meta\n_ftb_io_out_last_stage_meta\nittage_io_out_last_stage_meta[100:0] 位宽 信号名 备注 100 s3_provided 是否有结果 [99:97] s3_provider 提供结果的表项 96 s3_altProvided 是否有替代预测表项 [95:93] s3_altProvider 提供结果的替代预测表项 92 resp_meta_altDiffers 替代预测是否是弱信心的（FTB不算） 91 s3_providerU 主预测的useful bit [90:89] s3_providerCtr 主预测给出的置信度 [88:87] s3_altProviderCtr 替代预测给出的置信度 86 resp_meta_allocate_valid_r 有空余的表项可供申请 [85:83] resp_meta_allocate_bits_r 申请哪个表中的表项 82 s3_tageTaken_dup_3 在不使用FTB的情况下始为true，使用FTB也为true [81:41] s3_providerTarget 主预测给出的跳转地址 [40:0] s3_altProviderTarget 替代预测给出的跳转地址 ","categories":["Xiangshan BPU Submodule"],"description":"","excerpt":"Function Introduction For general conditional branch instructions, …","ref":"/xs-bpu/en/docs/modules/04_ittage/","tags":["Subpredictor","ITTAGE"],"title":"ITTAGE Branch Predictor"},{"body":"Feature List s2 ITTAGE determines whether to generate a prediction result s3 ITTAGE reads the predicted target for a branch s3 ITTAGE outputs meta information ITTAGE performs update training Check for new table entry requests Globally reset useful Prediction direction ","categories":["Feature List"],"description":"","excerpt":"Feature List s2 ITTAGE determines whether to generate a prediction …","ref":"/xs-bpu/en/docs/feature/04_ittagefeature/","tags":["ITTAGE","Feature List"],"title":"ITTAGE Feature List"},{"body":"功能介绍 对于一般条件分支指令，只需要预测跳转（taken），或者不跳转（no taken），但是对于间接跳转，如call/jump等指令，需要预测跳转到哪里去（Target）。为了让TAGE支持预测跳转地址，ITTAGE（Indirect Target TAGE）应运而生。\nITTAGE 与 TAGE 的主要区别在于，在T0和Tn表中，多了 Target PC 数据。预测时，ITTAGE选择匹配到的、历史最长的表项中的 Target 作为预测结果，并利用 2bit 饱和计数器决定是否输出该结果，或选用替代预测结果。TAGE 预测器说明，请参考 TAGE-SC分支预测器。\n昆明湖 ITTAGE 分支预测器 由于在昆明湖的BPU设计中，采用多预测器级联的方式进行预测，因此在子预测器实现上，会与原始预测器有所不同，其中多数表现在默认预测结果上。\n基本功能 ITTAGE基本功能类似 TAGE 分支预测器，但存在以下不同点：\n在表项中增加了 Target 作为跳转的目标地址项，用于预测跳转目标地址。 饱和计数器ctr也不再提供预测方向，而改为决定是否输出结果（只是预测信息）。 由于每个分支预测块中只有一个间接跳转指令，所以ITTAGE也只考虑一条指令。 流水线 ITTAGE 内含三个流水级，第一级计算索引，第二级通过索引读出SRAM表中的结果\n第0拍，s0：第一个流水级的输入，一般是pc和折叠历史。 第一流水级运行：计算index。通过寄存器输出到s1\n第1拍，s1：第二个流水级的输入，为第一级阶段计算好的index等数据。 第二流水级运行：访存SRAM，读取预测用信息。通过寄存器输出到s2\n第2拍，s2：第三个流水级的输入，为第二级从SRAM中读出的原始预测信息。 **第三流水级运行：**处理原始预测信息，决定是否输出预测结果。\n第3拍，s3：预测结果就绪，此时已经可以使用预测结果。 数据结构 在昆明湖的实现中，T0与Tn的表结构如下所示：\n预测器 作用 表项构成 项数 基准预测器T0 用于在其他预测器的预测结果都无效时输出预测结果 虚表，不存在。 直接将上级预测器FTB 的预测结果作为表项结果 虚表，不存在。 直接将上级预测器FTB结果作为索引到的结果 预测表T1-T2 对每个预测块的输入，所有Tn表都进行预测，在所有预测有效的结果中，选择历史记录最长的结果作为 原始预测信息。历史记录长度由输入的H决定 target: 41 bits; valid: 1bit; tag: 9bits; ctr: 2bits; us: 1bit（usefulness计数器） 256项 预测表T3-T5 512项 T0，Tn表的检索方法 检索方法与 TAGE 分支预测器一致，只是各表的配置选项不同。\n表名称 FH长度 FH1长度 FH2长度 最近历史长度（用到GH中的位数） T1 4比特 4比特 4比特 低4位，即把最新4位历史，折叠成FH、FH1、FH2 T2 8比特 8比特 8比特 低8位，即把最新8位历史，折叠成FH、FH1、FH2 T3 9比特 9比特 8比特 低13位，即把最新13位历史，折叠成FH、FH1、FH2 T4 9比特 9比特 8比特 低16位，即把最新16位历史，折叠成FH、FH1、FH2 T5 9比特 9比特 8比特 低32位，即把最新32位历史，折叠成FH、FH1、FH2 其他过程（计算方式 及 计算公式）类似 TAGE-SC分支预测器\n替代预测器 当Tn表给出的预测结果，“预测信心”不足时，需要对预测结果进行跳转，成为“替代预测器”。该过程与 TAGE 类似，具体请先阅读TAGE的对应部分。与 TAGE 不同，ITTAGE 的 ctr 并不给出预测方向，只决定是否输出该结果（预测信心）。当 ctr 为 2b00 的时候，即认定为弱信心。选择替代预测结果：\n如果命中多个个表，输出长度第二长的表中表项的 Target 否则输出 T0 Target（FTB Target） 预测过程 预测过程与 TAGE 类似，但与 TAGE 不同的是ITTAGE 多了一步根据 ctr 决定是否输出预测结果。具体过程如下：\n当 ITTAGE 表项的 ctr 不为 2b00 时，输出 Target 当 ITTAGE 表项的 ctr 为 2b00 时，输出替代预测结果 如果有次长的历史（第二个表也命中），则输出 次长的 Target 否则输出 FTB Target 当 ITTAGE 表项没命中时，输出 T0 Target（FTB Target） 训练过程 该过程与 TAGE 类似，具体不同点如下：\n表项更新（原始预测数据）： ctr： 若预测地址与实际一致，则将对应provider（提供原始预测数据的表）表项的ctr计数器自增1 若预测地址与实际不一致，则将对应provider表项的ctr计数器自减1 ITTAGE中，会根据ctr判断是否采取这个预测的跳转目标结果。 如果多个表均命中，且历史最长表的ctr为0，则采取替代预测逻辑（历史次长的表 或 T0）。 更新时始终更新历史最长的表，如果采用了替代预测，也同时更新替代预测表。 target： 当待更新表项在进行本次预测时的ctr为0时，直接将实际的最终跳转结果存入target，覆盖 如果是在申请新表项，直接将实际的最终跳转结果存入target 否则不修改target usefulness： 当provider预测正确而替代预测错误时provider的usefulness置1 如果替代预测是弱信心，并且预测正确，则provider的usefulness置1。 如果替代预测是弱信心，并且预测错误，则provider的usefulness置0。 新表项： 每次由于历史最长表有信心的预测出错时（不是由于使用替代预测出错），会尝试随机在历史更长表中申请一个表项。能申请到的条件是对应项的useful为0 如果更长的useful均不为0，则分配失败 重置useful位： 每次预测出错时会申请新表项，如果分配失败则tickCtr（用于重置全部useful的8bit饱和计数器） +1，成功则 -1 当tickCtr为最大值时，设置ITTAGE中的所有useful为0，并设置tickCtr为0。 接口列表 接口类型 位宽 信号名 备注 input clock input reset input [40:0] io_in_bits_s0_pc_3 用于预测的PC input [7:0] io_in_bits_folded_hist_3_hist_14_folded_hist T2 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_13_folded_hist T3 折叠历史 input [3:0] io_in_bits_folded_hist_3_hist_12_folded_hist T1 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_10_folded_hist T5 折叠历史 input [8:0] io_in_bits_folded_hist_3_hist_6_folded_hist T4 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_4_folded_hist T3 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_3_folded_hist T5 折叠历史 input [7:0] io_in_bits_folded_hist_3_hist_2_folded_hist T4 折叠历史 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_jalr_target FTB项输入，提供替代预测 output [40:0] io_out_s3_full_pred_0_jalr_target output [40:0] io_out_s3_full_pred_1_jalr_target output [40:0] io_out_s3_full_pred_2_jalr_target output [40:0] io_out_s3_full_pred_3_jalr_target output [222:0] io_out_last_stage_meta [100:0] 有效，是ITTAGE的Meta信息 input io_s0_fire_3 s0阶段使能信号 input io_s1_fire_3 s1阶段使能信号 input io_s2_fire_0 s2阶段使能信号，相同 input io_s2_fire_1 input io_s2_fire_2 input io_s2_fire_3 input io_update_valid 是否进行更新 input [40:0] io_update_bits_pc 待更新的预测块pc索引 input [7:0] io_update_bits_spec_info_folded_hist_hist_14_folded_hist T2 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_13_folded_hist T3 更新时传入的历史 input [3:0] io_update_bits_spec_info_folded_hist_hist_12_folded_hist T1 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_10_folded_hist T5 更新时传入的历史 input [8:0] io_update_bits_spec_info_folded_hist_hist_6_folded_hist T4 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_4_folded_hist T3 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_3_folded_hist T5 更新时传入的历史 input [7:0] io_update_bits_spec_info_folded_hist_hist_2_folded_hist T4 更新时传入的历史 input [3:0] io_update_bits_ftb_entry_tailSlot_offset 待更新的FTB项offset input io_update_bits_ftb_entry_tailSlot_sharing 待更新的FTB项是否是有条件跳转 input io_update_bits_ftb_entry_tailSlot_valid 待更新的tailSlot是否启用 input io_update_bits_ftb_entry_isRet tailSlot是否是Ret指令 input io_update_bits_ftb_entry_isJalr tailSlot是否是Jalr指令 input io_update_bits_cfi_idx_valid 控制流指令在预测块中的索引.valid信号 input [3:0] io_update_bits_cfi_idx_bits 控制流指令在预测块中的索引 input io_update_bits_jmp_taken 预测块内无条件跳转指令被触发 input io_update_bits_mispred_mask_2 是否预测错误 input [222:0] io_update_bits_meta 预测时传出 meta 信息的[222:25] 即{25h0, _ubtb_io_out_last_stage_meta[5:0] ,_tage_io_out_last_stage_meta[87:0] ,_ftb_io_out_last_stage_meta[2:0], _ittage_io_out_last_stage_meta[100:0]} input [40:0] io_update_bits_full_target 预测块的跳转目标（下一个预测块的起始地址） 不产生影响的透传信号 这里的信号不产生影响，不重要 接口类型 位宽 信号名 备注 input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_0 从FTB输入 完全透传到输出 包括jalr_target input io_in_bits_resp_in_0_s2_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_0_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_0_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_0_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_0_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_0_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_0_hit input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_1_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_1_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_1_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_1_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_1_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_1_hit input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_2_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_targets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_jalr_target input [3:0] io_in_bits_resp_in_0_s2_full_pred_2_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_2_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_2_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_2_is_jalr RAS 模块使用的信息，透传 input io_in_bits_resp_in_0_s2_full_pred_2_is_call input io_in_bits_resp_in_0_s2_full_pred_2_is_ret input io_in_bits_resp_in_0_s2_full_pred_2_last_may_be_rvi_call input io_in_bits_resp_in_0_s2_full_pred_2_is_br_sharing 从FTB输入 完全透传到输出 包括jalr_target fallThroughErr 表示 FTB项 中记录的 pftAddr 有误 生成方式：比较 pftAddr 代表的预测块结束地址是否大于预测块的起始地址，如果小于，则代表出现错误，此信号置为有效。这种情况可能会发生在 pc 索引到错误的 FTB 项的情况。 FTQ使用这个变量，与ITTAGE无关 input io_in_bits_resp_in_0_s2_full_pred_2_hit input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_0 input io_in_bits_resp_in_0_s2_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_0 input io_in_bits_resp_in_0_s2_full_pred_3_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_targets_0 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_targets_1 input [3:0] io_in_bits_resp_in_0_s2_full_pred_3_offsets_0 input [3:0] io_in_bits_resp_in_0_s2_full_pred_3_offsets_1 input [40:0] io_in_bits_resp_in_0_s2_full_pred_3_fallThroughAddr input io_in_bits_resp_in_0_s2_full_pred_3_fallThroughErr input io_in_bits_resp_in_0_s2_full_pred_3_is_br_sharing input io_in_bits_resp_in_0_s2_full_pred_3_hit input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_0 除了 jalr_target 可能被修改，其他都是透传 input io_in_bits_resp_in_0_s3_full_pred_0_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_0_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_0_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_0_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_0_hit input [40:0] io_in_bits_resp_in_0_s3_full_pred_0_jalr_target input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_1_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_1_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_1_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_1_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_1_hit input [40:0] io_in_bits_resp_in_0_s3_full_pred_1_jalr_target input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_2_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_2_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_targets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_2_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_2_is_jalr input io_in_bits_resp_in_0_s3_full_pred_2_is_call input io_in_bits_resp_in_0_s3_full_pred_2_is_ret input io_in_bits_resp_in_0_s3_full_pred_2_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_2_hit input [40:0] io_in_bits_resp_in_0_s3_full_pred_2_jalr_target input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_0 同上 input io_in_bits_resp_in_0_s3_full_pred_3_br_taken_mask_1 input io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_0 input io_in_bits_resp_in_0_s3_full_pred_3_slot_valids_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_targets_0 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_targets_1 input [3:0] io_in_bits_resp_in_0_s3_full_pred_3_offsets_0 input [3:0] io_in_bits_resp_in_0_s3_full_pred_3_offsets_1 input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_fallThroughAddr input io_in_bits_resp_in_0_s3_full_pred_3_fallThroughErr input io_in_bits_resp_in_0_s3_full_pred_3_is_br_sharing input io_in_bits_resp_in_0_s3_full_pred_3_hit input [40:0] io_in_bits_resp_in_0_s3_full_pred_3_jalr_target input io_in_bits_resp_in_0_last_stage_ftb_entry_valid 透传到output，不做修改 来源是FTB input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_offset input [11:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_lower input [1:0] io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_tarStat input io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_sharing input io_in_bits_resp_in_0_last_stage_ftb_entry_brSlots_0_valid input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_offset input [19:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_lower input [1:0] io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_tarStat input io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_sharing input io_in_bits_resp_in_0_last_stage_ftb_entry_tailSlot_valid input [3:0] io_in_bits_resp_in_0_last_stage_ftb_entry_pftAddr input io_in_bits_resp_in_0_last_stage_ftb_entry_carry input io_in_bits_resp_in_0_last_stage_ftb_entry_isCall input io_in_bits_resp_in_0_last_stage_ftb_entry_isRet input io_in_bits_resp_in_0_last_stage_ftb_entry_isJalr input io_in_bits_resp_in_0_last_stage_ftb_entry_last_may_be_rvi_call input io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_0 input io_in_bits_resp_in_0_last_stage_ftb_entry_always_taken_1 output io_out_s2_full_pred_0_br_taken_mask_0 完全透传传入值 prefix: io_in_bits_resp_in_ output io_out_s2_full_pred_0_br_taken_mask_1 output io_out_s2_full_pred_0_slot_valids_0 output io_out_s2_full_pred_0_slot_valids_1 output [40:0] io_out_s2_full_pred_0_targets_0 output [40:0] io_out_s2_full_pred_0_targets_1 output [40:0] io_out_s2_full_pred_0_jalr_target output [3:0] io_out_s2_full_pred_0_offsets_0 output [3:0] io_out_s2_full_pred_0_offsets_1 output [40:0] io_out_s2_full_pred_0_fallThroughAddr output io_out_s2_full_pred_0_is_br_sharing output io_out_s2_full_pred_0_hit output io_out_s2_full_pred_1_br_taken_mask_0 output io_out_s2_full_pred_1_br_taken_mask_1 output io_out_s2_full_pred_1_slot_valids_0 output io_out_s2_full_pred_1_slot_valids_1 output [40:0] io_out_s2_full_pred_1_targets_0 output [40:0] io_out_s2_full_pred_1_targets_1 output [40:0] io_out_s2_full_pred_1_jalr_target output [3:0] io_out_s2_full_pred_1_offsets_0 output [3:0] io_out_s2_full_pred_1_offsets_1 output [40:0] io_out_s2_full_pred_1_fallThroughAddr output io_out_s2_full_pred_1_is_br_sharing output io_out_s2_full_pred_1_hit output io_out_s2_full_pred_2_br_taken_mask_0 output io_out_s2_full_pred_2_br_taken_mask_1 output io_out_s2_full_pred_2_slot_valids_0 output io_out_s2_full_pred_2_slot_valids_1 output [40:0] io_out_s2_full_pred_2_targets_0 output [40:0] io_out_s2_full_pred_2_targets_1 output [40:0] io_out_s2_full_pred_2_jalr_target output [3:0] io_out_s2_full_pred_2_offsets_0 output [3:0] io_out_s2_full_pred_2_offsets_1 output [40:0] io_out_s2_full_pred_2_fallThroughAddr output io_out_s2_full_pred_2_is_jalr output io_out_s2_full_pred_2_is_call output io_out_s2_full_pred_2_is_ret output io_out_s2_full_pred_2_last_may_be_rvi_call output io_out_s2_full_pred_2_is_br_sharing output io_out_s2_full_pred_2_hit output io_out_s2_full_pred_3_br_taken_mask_0 output io_out_s2_full_pred_3_br_taken_mask_1 output io_out_s2_full_pred_3_slot_valids_0 output io_out_s2_full_pred_3_slot_valids_1 output [40:0] io_out_s2_full_pred_3_targets_0 output [40:0] io_out_s2_full_pred_3_targets_1 output [40:0] io_out_s2_full_pred_3_jalr_target output [3:0] io_out_s2_full_pred_3_offsets_0 output [3:0] io_out_s2_full_pred_3_offsets_1 output [40:0] io_out_s2_full_pred_3_fallThroughAddr output io_out_s2_full_pred_3_fallThroughErr output io_out_s2_full_pred_3_is_br_sharing output io_out_s2_full_pred_3_hit output io_out_s3_full_pred_0_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_0_br_taken_mask_1 output io_out_s3_full_pred_0_slot_valids_0 output io_out_s3_full_pred_0_slot_valids_1 output [40:0] io_out_s3_full_pred_0_targets_0 output [40:0] io_out_s3_full_pred_0_targets_1 output [40:0] io_out_s3_full_pred_0_fallThroughAddr output io_out_s3_full_pred_0_fallThroughErr output io_out_s3_full_pred_0_is_br_sharing output io_out_s3_full_pred_0_hit output io_out_s3_full_pred_1_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_1_br_taken_mask_1 output io_out_s3_full_pred_1_slot_valids_0 output io_out_s3_full_pred_1_slot_valids_1 output [40:0] io_out_s3_full_pred_1_targets_0 output [40:0] io_out_s3_full_pred_1_targets_1 output [40:0] io_out_s3_full_pred_1_fallThroughAddr output io_out_s3_full_pred_1_fallThroughErr output io_out_s3_full_pred_1_is_br_sharing output io_out_s3_full_pred_1_hit output io_out_s3_full_pred_2_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_2_br_taken_mask_1 output io_out_s3_full_pred_2_slot_valids_0 output io_out_s3_full_pred_2_slot_valids_1 output [40:0] io_out_s3_full_pred_2_targets_0 output [40:0] io_out_s3_full_pred_2_targets_1 output [40:0] io_out_s3_full_pred_2_fallThroughAddr output io_out_s3_full_pred_2_fallThroughErr output io_out_s3_full_pred_2_is_jalr output io_out_s3_full_pred_2_is_call output io_out_s3_full_pred_2_is_ret output io_out_s3_full_pred_2_is_br_sharing output io_out_s3_full_pred_2_hit output io_out_s3_full_pred_3_br_taken_mask_0 见对应prefix的输入 output io_out_s3_full_pred_3_br_taken_mask_1 output io_out_s3_full_pred_3_slot_valids_0 output io_out_s3_full_pred_3_slot_valids_1 output [40:0] io_out_s3_full_pred_3_targets_0 output [40:0] io_out_s3_full_pred_3_targets_1 output [3:0] io_out_s3_full_pred_3_offsets_0 output [3:0] io_out_s3_full_pred_3_offsets_1 output [40:0] io_out_s3_full_pred_3_fallThroughAddr output io_out_s3_full_pred_3_fallThroughErr output io_out_s3_full_pred_3_is_br_sharing output io_out_s3_full_pred_3_hit output io_out_last_stage_ftb_entry_valid 完全透传传入的值 output [3:0] io_out_last_stage_ftb_entry_brSlots_0_offset output [11:0] io_out_last_stage_ftb_entry_brSlots_0_lower output [1:0] io_out_last_stage_ftb_entry_brSlots_0_tarStat output io_out_last_stage_ftb_entry_brSlots_0_sharing output io_out_last_stage_ftb_entry_brSlots_0_valid output [3:0] io_out_last_stage_ftb_entry_tailSlot_offset output [19:0] io_out_last_stage_ftb_entry_tailSlot_lower output [1:0] io_out_last_stage_ftb_entry_tailSlot_tarStat output io_out_last_stage_ftb_entry_tailSlot_sharing output io_out_last_stage_ftb_entry_tailSlot_valid output [3:0] io_out_last_stage_ftb_entry_pftAddr output io_out_last_stage_ftb_entry_carry output io_out_last_stage_ftb_entry_isCall output io_out_last_stage_ftb_entry_isRet output io_out_last_stage_ftb_entry_isJalr output io_out_last_stage_ftb_entry_last_may_be_rvi_call output io_out_last_stage_ftb_entry_always_taken_0 output io_out_last_stage_ftb_entry_always_taken_1 其他Meta见对应子预测其文档 _ubtb_io_out_last_stage_meta\n_tage_io_out_last_stage_meta\n_ftb_io_out_last_stage_meta\nittage_io_out_last_stage_meta[100:0] 位宽 信号名 备注 100 s3_provided 是否有结果 [99:97] s3_provider 提供结果的表项 96 s3_altProvided 是否有替代预测表项 [95:93] s3_altProvider 提供结果的替代预测表项 92 resp_meta_altDiffers 替代预测的结果是否和主预测的结果不同 91 s3_providerU 主预测的useful bit [90:89] s3_providerCtr 主预测给出的置信度 [88:87] s3_altProviderCtr 替代预测给出的置信度 86 resp_meta_allocate_valid_r 有空余的表项可供申请 [85:83] resp_meta_allocate_bits_r 申请哪个表中的表项 82 s3_tageTaken_dup_3 在不使用FTB的情况下始为true，使用FTB也为true [81:41] s3_providerTarget 主预测给出的跳转地址 [40:0] s3_altProviderTarget 替代预测给出的跳转地址 ","categories":["香山 BPU 子模块"],"description":"","excerpt":"功能介绍 对于一般条件分支指令，只需要预测跳转（taken），或者不跳转（no taken），但是对于间接跳转，如call/jump等指令， …","ref":"/xs-bpu/docs/modules/04_ittage/","tags":["子预测器","ITTAGE"],"title":"ITTAGE 分支预测器"},{"body":"功能列表（Feature List） s2 ITTAGE 决定是否产生预测结果 s3 ITTAGE 读取预测的跳转目标 s3 ITTAGE 输出meta信息 ITTAGE 进行更新训练 检查申请新表项 全局重置useful 预测方向 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） s2 ITTAGE 决定是否产生预测结果 s3 ITTAGE 读取预测的跳转目标 s3 ITTAGE  …","ref":"/xs-bpu/docs/feature/04_ittagefeature/","tags":["ITTAGE","Feature List"],"title":"ITTAGE 功能列表"},{"body":"RAS介绍 RAS stands for “Return Address Stack.” It helps determine branch behavior in programs by tracking return addresses. As previously mentioned, there are many branches in a program: if/else, switch/case, while/for loop, iteration, call/return, etc. The RAS branch predictor specifically targets call/return types.\nfunction _add(a, b){ return (a \u003e 0 ? a : 0) + (b \u003e 0? b : 0); } function add(a, b){ return a + b; } function sub(a, b){ return a - b; } function main(){ a = 1; b = 2; c = add(a, b); d = sub(a, b); } As shown above, the main function calls add and sub, and add calls the function _add. In this process, each call’s jump address and return address are fixed, and the return address can be obtained at the time of the call. The function call process is a “stack push and pop” process, so branch prediction can be performed using a “stack” structure: each time a call instruction is encountered, the current PC+4 (compressed instructions and regular instructions have different offsets) is pushed onto the stack; when a return instruction is encountered, a pop operation is performed, and the address obtained is the target jump address. In the block-based BPU, RAS cannot know whether the current block is a call or ret, so it relies on other predictors, using the results of previous predictors for RAS operations.\nSpecifically, in Xiangshan’s RAS predictor, at the s2 stage, it needs to determine whether the previous stage’s s2 output predicts a call or ret (i.e., the input signal io.s2_full_pred.hit_taken_on_call/ret is valid). If it’s a call, it pushes the subsequent instruction address onto the stack; if it’s a ret, it pops the address from the stack as the prediction result. Because in the BPU predictor, the result obtained at the s3 stage is assumed to be better than the s2 stage, the RAS predictor needs to check at the s3 stage. If the previous stage’s s3 prediction result is inconsistent with s2, the s3 result is taken, and it needs to determine whether to cancel or complete the stack operations of the previous s2 stage as needed. For example, if the s2 stage predicted a call instruction and performed a push operation, but s3 predicted a regular branch instruction with no need for any operation, the push must be canceled; if s2 predicted a regular branch instruction and s3 predicted a call, a push operation must be performed to complete.\nRAS Stack Operations In RAS design, function return addresses are predicted using a stack. Ideally, this section assumes that RAS can be backed up at any time, with the stack top pointer represented by sp and the predicted address represented by paddr. The basic RAS operations are as follows:\nPUSH Since predictions can be wrong, the current stack state needs to be backed up (often referred to as a “snapshot” in software; this term is also used in subsequent content). When encountering a call instruction, get the return address of the call instruction addr = current pc + 4 (if it’s a compressed instruction, addr = pc+2), then push onto the stack: sp = addr; sp += 1.\nPOP For the same reason, take a snapshot of the current stack, marked as s. When encountering a ret instruction, the predicted jump address is paddr = sp, then pop: sp = sp - 1. Take a snapshot of the current stack, marked as s.\nRedirect Operation Since the BPU predicts program branches, there are “correct predictions” and “wrong predictions.” When the CPU backend detects a branch prediction error, it performs a redirect operation, informing the BPU where the prediction was wrong and what the correct result is. During redirection, the RAS module receives the correct branch and the RAS stack information at the time of prediction. Depending on the type of correct branch instruction, the following snapshot recovery situations arise:\n(1) The previously predicted instruction is actually a call instruction, and the push operation is executed based on the addr address provided in the redirect. (2) The previously predicted instruction is actually a ret instruction, and the pop operation is executed.\nCommit Operation The commit operation is when the backend informs the frontend that the previous prediction was correct. Ideally, the RAS predictor doesn’t need to perform any operations at this time.\nImplementation of RAS in Kunming Lake In actual circuit design, an infinitely large stack is impossible, and constant backups are not feasible. Therefore, in Kunming Lake’s RAS implementation, the problems and solutions are as follows:\nHow to obtain RAS stack snapshots for each prediction? To achieve the function of taking snapshots of the RAS stack, Kunming Lake adopts a linked representation based on a circular array. The design is as follows:\nAs shown above, a circular array is used for data management. The circular array has a starting address marked as BOS and a tail pointer marked as TOSW. The data between them are valid, and the data outside are free. Within the valid data, a linked structure represents the “RAS stack,” where each stack element records the number of its previous data. When performing stack operations, the corresponding previous element can be obtained through this number. The RAS stack’s bottom pointer shares the BOS. In the initial state S in the figure, the RAS stack elements are 0, 1, 3, 5. Element 5 records the position of its previous element 3, and element 3 records the position of its previous element 1. When a push operation is needed, the RAS stack top pointer TOSR = TOSW, the new element is stored at the new TOSR position 7, and the position of its previous element 5 is recorded in the new element, then TOSW is moved back (TOSW = TOSW+1). When a pop operation is performed, the RAS stack top pointer TOSR moves to the previous element’s position 3 based on the index saved in the stack top element. Therefore, under the condition that the stack does not overflow, the above RAS stack always allocates new data on the array through TOSW during normal Push/Pop operations, so all process states and intermediate data are saved. So, to restore to the state S, it only needs to reset the corresponding stack pointers. Therefore, in each prediction, the corresponding stack pointers (BOS, TOSR, TOSW) also need to be saved in the prediction result for later restoration in case of redirection. The advantage of this structure is that it can save complete process data, but frequent push operations can lead to large space resource consumption.\nChain RA storage space waste? Since the prediction result is correct after commit, the stack will not roll back. When the RAS predictor receives the commit message of the prediction block “P,” it will no longer receive the redirect message of block P, so the snapshot taken during the push operation for block P will not be used again. Therefore, the RAS stack elements can be categorized, with uncommitted elements stored in a “linked” structure and committed elements stored in a regular stack structure (the original RAS stack is split into two parts: the uncommitted part stored in a snapshot-saving linked structure and the committed part stored in a regular stack structure). The optimized RAS structure is shown below:\nAs shown above, based on the normal call/ret predictions and commit call/ret, the original RAS stack can be split into two independent stacks, called the “speculative stack” (spec_stack, linked structure) and the “commit stack” (commit_stack, regular structure). Due to the change in stack structure, the specific Pop/Push operations are as follows:\nEncounter normal call and ret: (1) The call predicted by the prediction block is correct, and a push operation is performed on the spec_stack, specifically the linked stack Push process mentioned above. (2) The ret predicted by the prediction block is correct, and the stack top of the spec_stack is used as the prediction value, then a pop operation is performed. If the spec_stack is empty, the stack top element of the commit_stack is used as the prediction result (no pop operation is performed).\nCommit operation: (1) The FTQ execution result is call correct, and a regular push operation is performed on the commit_stack. (2) The FTQ execution result is ret correct, and a regular pop operation is performed on the commit_stack.\nRedirect operation: (1) Obtain the stack pointers (BOS, TOSR, TOSW, ssp) from the redirect message during the previous prediction and cover the current pointer values to complete the speculative stack rollback. (2) This operation does not affect the commit stack.\nHow to handle when the S3 prediction result is inconsistent with S2 at the input end? Since the S3 result is assumed to be better than S2, the RAS stack needs to be repaired again in case of inconsistency. The specific inconsistency and corresponding repair operations are shown in the table below:\nS2 Pred. Res. S3 Pred. Res. Repair Operation push keep pop keep pop pop pop keep push keep push push S2 and S3 operations do not exist, pop/push or push/pop scenarios (Why not exist?)\nOther Optimizations Each element in the RAS stack has a counter, which is used to save repeated values (for recursive calls). For example, when the address pushed for the first time is 0xff00, and the address pushed for the second time is also 0xff00, only the counter of the top element of the stack needs to be incremented, and there is no need to actually push the address onto the stack. Interface Description In the RAS predictor, the core component is the RASStack, with the following interface:\n接口名称 功能描述 接口名称 功能描述 in.spec_push_valid 预测有Call指令，Spec压栈 in.s2_fire s2信号有效 in.spec_pop_valid 预测有Ret指令，Spec出栈 in.s3_fire s3信号有效 in.spec_push_addr Ret地址 in.s3_cancel s3和s2的预测结果不一样，需要撤销s2的操作 out.spec_pop_addr RAS的栈顶数据 in.s3_meta s3需要的s2时的现场信息 out.ssp commit栈顶指针 in.s3_missed_pop s3判断需要再次进行pop out.sctr commit栈顶重复元素计数器 in.s3_missed_push s3判断需要再次进行push out.nsp commit栈顶，会被ssp覆盖 in.s3_pushAddr 需要再次push时的地址 out.TOSR spec栈栈顶指针 in.redirect_valid 需要重定向 out.TOSW spec栈数据分配指针 in.redirect_isCall 真实执行情况是Call out.BOS spec栈栈低指针 in.redirect_isRet 真实执行情况是Return in.commit_push_valid push操作正确 in.redirect_meta_ssp 之前预测时的现场信息ssp in.commit_pop_valid FTQ执行结果为Call正确 in.redirect_meta_sctr 之前预测时的现场信息sctr in.commit_push_addr 更新信息中的Ret地址 in.redirect_meta_TOSW 之前预测时的现场信息TOSW in.commit_meta_TOSW 更新信息中的TOSW in.redirect_meta_TOSR 之前预测时的现场信息TOSR in.commit_meta_TOSR 更新信息中的TOSR in.redirect_meta_NOS 之前预测时的现场信息NOS in.commit_meta_ssp 更新信息中的现场信息SSP in.redirect_callAddr 重定向地址 in.commit_meta_sctr 更新信息中的现场信息SCTR The relationship between the RASStack module and the BasePredictor interface is as follows:\nstack接口 转换过程 描述 s.spec_push_valid io.s2_fire(2) \u0026\u0026 s2_full_pred.hit_taken_on_call \u0026\u0026 !io.s3_redirect(2) s2输入有效，且上级预测为call跳转 s.spec_pop_valid io.s2_fire(2) \u0026\u0026 s2_full_pred.hit_taken_on_ret \u0026\u0026 !io.s3_redirect(2) s2输入有效，且上级预测为ret跳转 s.spec_push_addr s2_full_pred.fallThroughAddr + Mux(s2_full_pred.last_may_be_rvi_call, 2.U, 0.U) 上级预测器s2预测的fallThroughAddr（即PC+2），判断是否压缩指令是否需要 +2 s.redirect_isCall redirect.bits.level === 0.U \u0026\u0026 recover_cfi.pd.isCall s.redirect_isRet redirect.bits.level === 0.U \u0026\u0026 recover_cfi.pd.isRet s.redirect_meta_* redirect.bits.cfiUpdate.* s.commit_push_valid io.update.is_call_taken call指令预测正确 s.commit_push_valid io.update.is_ret_taken ret指令预测正确 s.commit_push_addr update.ftb_entry.getFallThrough(update.pc) + Mux(update.ftb_entry.last_may_be_rvi_call, 2.U, 0.U) 根据是否为压缩指令，进行地址+2或者+0 s.commit_meta_* io.update.bits.meta.asTypeOf(new RASMeta) io.out.last_stage_spec_info.* s3_meta.* s3_meta = RegEnable(s2_meta, io.s2_fire(2))由s2_meta延迟一怕得到 io.out.last_stage_meta s3_meta io.out.s2.full_pred.*.jalr_target :=stack.spec_pop_addr 预测地址（栈顶地址，只预测ret） io.out.s3.full_pred.*.jalr_target :=RegEnable(stack.spec_pop_addr, io.s2_fire(2)) 由s2延迟一拍得到 io.out.s2/3.full_pred.targets.last :=Mux(s2/3_is_jalr, s2/3_jalr_target, s2/3_last_target_in) 如果时call执行，更新targets.last的结果 Timing Description In RAS, there are only 2 stages, S2 and S3.\nMain tasks in S2\nBased on the S2 prediction result of the previous predictor, complete the prediction through the push/pop process and obtain the result spec_pop_addr. Perform update operations based on commit signal. Main tasks in S3\nBased on the prediction result in the previous predictor S3 and the operations in S2, determine whether to undo the Pop/Push operation. The predictor assumes that the S3 prediction result is better than S2. If S2 and S3 predictions are inconsistent, the RAS predictor accepts the S3 result for stack operation.\nThe prediction process in S3 is the same as in S2, but the data is different.\nPerform redirection operations (redirection information obtained from the previous cycle).\nSince the RASStack appears to complete its tasks within a cycle, data bypassing is needed inside the stack to cache data for processing.\n","categories":["Xiangshan BPU Submodules"],"description":"","excerpt":"RAS介绍 RAS stands for “Return Address Stack.” It helps determine branch …","ref":"/xs-bpu/en/docs/modules/05_ras/","tags":["Sub-Predictor","RAS"],"title":"RAS Branch Predictor"},{"body":"Feature List Supports enabling and disabling of the predictor Supports s3 prediction result overriding s2 prediction result Supports push and pop operations of the RAS stack Supports redirect operation of the RAS stack Supports update operation of the RAS stack Supports base predictor interface Conforms to the standard RAS predictor prediction process ","categories":["Feature List"],"description":"","excerpt":"Feature List Supports enabling and disabling of the predictor Supports …","ref":"/xs-bpu/en/docs/feature/05_rasfeature/","tags":["RAS","Feature List"],"title":"RAS Feature List"},{"body":"RAS介绍 RAS 指的是 “Return Address Stack”，即返回地址栈。它通过跟踪程序的返回地址，帮助确定程序中的分支行为。由前所述，在程序中存在很多分支：if/else、 switch/case、while/for loop、iteration、call/return\n等。RAS分支预测器则专门针对 call/return类型。\nfunction _add(a, b){ return (a \u003e 0 ? a : 0) + (b \u003e 0? b : 0); } function add(a, b){ return a + b; } function sub(a, b){ return a - b; } function main(){ a = 1; b = 2; c = add(a, b); d = sub(a, b); } 如上图所示，在main函数中调用了add和sub，add又调用了函数_add。在该过程中，每次调用（call）的跳转地址和返回的地址固定，且在call的时候，就可以同时得到返回地址。函数的调用过程是一个“栈的出入”过程，因此可以通过“栈”结构进行分支预测：每遇到call指令，把当前PC+4（压缩指令和普通指令的偏移不同）进行压栈push操作；遇到return指令，则进行进行pop操作，得到的地址即为目标跳转地址。在基于“预测块”的BPU中，RAS无法知道当前块是否是call或者ret，因此需要依赖其他预测器，利用前级预测器的结果进行RAS操作。\n具体的，在香山的RAS预测器中，其s2阶段，需要判断上一级s2的输出是否预测为call或者ret（即输入信号io.s2_full_pred.hit_taken_on_call/ret有效 ），如果是call则push其后续指令地址入栈，是ret则从栈中pop出地址作为预测结果。因为在BPU的预测器中，人为假定s3阶段得到的结果比s2阶段好，所以RAS预测器在s3阶段需要进行检查，如果上一级的s3预测结果与s2不一致，则采信s3的结果，并按需求判断是否需要撤销或者补齐之前s2阶段的栈操作。例如s2阶段预测为call指令，进行了push操作，而s3为普通分支指令，不需要进行任何操作，此时就需要撤销push；如果s2预测为普通分支指令，s3预测为call，则需要进行push操作补齐。\nRAS的栈操作 在普通的RAS设计中，通过栈进行预测函数返回地址。在理想情况下，本节假定RAS可进行随时备份，栈顶指针用sp表示，预测地址用paddr表示。RAS的基本操作有以下4种：\nPUSH 由于预测可能会出错，出错时需要回到原始状态，因此在push时需要对当前栈的状态进行备份（在软件领域，通常称为“快照”，本文在后继内容中也用快照进行称呼），标记为s。当遇到call指令时，获取call指令的返回地址 addr = 当前pc + 4（如果是压缩指令则addr = pc+2），然后压栈：sp = addr；sp += 1。\nPOP 原因同上，对当前栈进行快照，标记为s。当遇到ret指令时，预测的跳转地址 paddr = sp，然后进行出栈， sp = sp - 1。对当前栈进行快照，标记为s。\n重定向操作 由于BPU是对程序的分支进行预测，因此就有“预测对”和“预测错”两种情况，当CPU后端发现分支预测错误，就会进行重定向（redirect）操作，告诉BPU哪个地方预测错误，正确结果是多少。重定向时RAS模块会收到正确的分支和当时预测时RAS栈信息。根据正确分支指令类型不同，首先恢复快照s有如下情形：\n（1）之前被预测的指令，实际是 call指令，根据redirect中给的addr地址，执行push操作；\n（2）之前被预测的指令，实际是 ret指令，执行pop操作；\n提交操作 提交（Commit）操作即后端告诉前端，之前预测的结果正确。理想情况下，RAS预测器此时不需要进行任何操作。\n昆明湖中RAS的实现 由于处理器在执行过程中可能会进行错误的猜测，导致数据污染，RAS需要通过恢复机制来确保数据的准确性。特别是在昆明湖的实际电路设计中，不可能有无限大的栈，也无法随时备份数据。因此，昆明湖的RAS实现中，使用了基于持久化栈的返回地址预测器。具体来说，将RAS分为提交栈（commit stack）和推测栈（spec queue）两个部分，推测栈利用了香山BPU的预测结果来进行预测，然后根据后端返回的信息将数据压入提交栈中。下面详细介绍昆明湖的RAS设计：\n每次预测时，RAS栈的快照如何获得？ 为了实现对RAS栈进行快照的功能，昆明湖的RAS推测栈采用了基于循环数组的 “链式表示“ 。设计具体如下：\n在这个设计中，数据管理是通过一个循环数组来实现的。循环数组有一个起始地址（BOS）和一个尾指针（TOSW），两者之间的数据是有效数据，其他部分为空闲数据。使用链式结构表示“Sepc栈”，每个栈元素记录其上一个数据的编号，进行栈操作时可以通过该编号获取上一个元素。RAS栈的栈底指针与BOS共用。\n如图所示的初始状态S，RAS栈的元素为0、1、4、5。元素5记录了其上一个元素的位置4，元素4记录了其上一个元素的位置1。在状态S下进行push操作时，RAS栈顶指针TOSR等于TOSW，在新的TOSR位置7存入新元素，并记录其上一个元素的位置5，然后TOSW后移（TOSW = TOSW + 1）。进行pop操作时，RAS栈顶指针TOSR根据栈顶元素保存的索引移动到上一个元素的位置4，但不会修改内存分配指针TOSW，因此Spec栈的元素不一定是连续的。\n在栈不溢出的情况下，RAS栈每次通过TOSW在数组上分配新数据，因此在正常的Push/Pop操作中，所有过程状态和中间数据都被保存。当需要恢复到状态S时，只需复位相应的栈指针。因此，在每次RAS预测时，需要将操作前的栈指针（BOS、TOSR、TOSW）保存于预测结果中，以便用于commit栈操作和在重定向时恢复状态。该结构的优点是可以保存完整的过程数据，但频繁的push操作会导致较大的空间资源消耗。\n链式RAS浪费存储空间？ 当预测结果被确认（commit）后，表示预测是正确的，不会再进行“栈”回滚。也就是说，RAS预测器在收到预测块“P”的commit消息后，不会再收到P块的重定向消息，因此在对P块进行push操作时的快照将不会被用到。\n为了优化，RAS栈中的元素被分类处理：未commit的元素存储在链式结构的推测栈中，已commit的元素存储在基于循环数组的提交栈中。这样，原始的RAS栈被拆分成两部分：未commit的部分用链式结构保存，以便需要时可以恢复快照；已commit的部分用普通栈结构保存，因为它们不需要快速恢复。优化后的RAS结构如下所示：\n如上图所示，根据普通预测的call/ret和commit提交的call/ret，把原始的RAS栈可以拆分成两个独立的栈，分别称为“预测栈”（ spec_stack，链式结构）和“提交栈”（commit_stack，普通结构），由于RASStack对外表现为当拍内就能完成任务，因此在Stack内需要进行数据旁路，对数据进行缓存，由于栈结构发生了变化，具体的Pop/Push操作如下：\n1、遇到普通call和ret：\n（1）预测块为call正确，在spec_stack上进行push操作，具体为上述链式栈Push过程，并将产生的快照信息送到FTQ存储；\n（2）预测块为ret正确，利用spec_stack的栈顶作为预测值，然后进行pop操作，并将产生的快照信息送到FTQ存储，如果spec_stack为空，则使用commit_stack的栈顶元素作为预测结果（不进行pop操作）。\n2、Commit操作：\n（1）FTQ执行结果为call正确，通过FTQ传回来的快照信息对commit_stack进行push操作，同时更新spec栈的BOS指针，将此次commit的数据及其之前的数据设置为无效数据。\n（2）FTQ执行结果为ret正确，通过FTQ传回来的快照信息对commit_stack上进行普通pop操作，注意到在commit pop之前会先对spec栈进行pop操作，所以commit栈弹出来的地址不需要使用。\n3、Redirect操作：\n（1）从redirect消息中获取之前预测时的栈指针（BOS、TOSR、TOSW，ssp），覆盖当前值指针值完成预测栈状态回滚，若此时为Call指令或者Ret指令，则再对Spec栈进行对应的操作。\n（2）该操作不影响提交栈。\n输入端，S3的预测结果和S2不一致时该如何处理？ 由于昆明湖 BPU 的假定S3的结果比S2好，因此发生不一致时，需要再次对RAS栈进行修复。具体不一致情况和对应的修复操作如下表所示：\nS2预测结果 S3预测结果 修复操作 push keep pop keep pop pop pop keep push keep push push 因为SC预测器的特性，S2和S3的操作不存在pop/push或者push/pop情况\n其他优化 每个RAS的spec栈和commit栈的元素都有一个counter计数器，用于节约重复值（递归调用）。例如：当第一次push地址为0xff00，第二次push的也为0xff00时，则只需要把栈顶元素的 counter 进行+1，不需真的进行push操作。 接口说明 在RAS预测器中，核心组件为 RASStack ，其接口说明如下:\n接口名称 功能描述 接口名称 功能描述 in.spec_push_valid 预测有Call指令，Spec压栈 in.s2_fire s2信号有效 in.spec_pop_valid 预测有Ret指令，Spec出栈 in.s3_fire s3信号有效 in.spec_push_addr Ret地址 in.s3_cancel s3和s2的预测结果不一样，需要撤销s2的操作 out.spec_pop_addr RAS的栈顶数据 in.s3_meta s3需要的s2时的现场信息 out.ssp commit栈顶指针 in.s3_missed_pop s3判断需要再次进行pop out.sctr commit栈顶重复元素计数器 in.s3_missed_push s3判断需要再次进行push out.nsp commit栈顶，会被ssp覆盖 in.s3_pushAddr 需要再次push时的地址 out.TOSR spec栈栈顶指针 in.redirect_valid 需要重定向 out.TOSW spec栈数据分配指针 in.redirect_isCall 真实执行情况是Call out.BOS spec栈栈底指针 in.redirect_isRet 真实执行情况是Return in.commit_push_valid FTQ执行结果为Call正确 in.redirect_meta_ssp 之前预测时的现场信息ssp in.commit_pop_valid FTQ执行结果为Ret正确 in.redirect_meta_sctr 之前预测时的现场信息sctr in.commit_push_addr 更新信息中的Ret地址 in.redirect_meta_TOSW 之前预测时的现场信息TOSW in.commit_meta_TOSW 更新信息中的TOSW in.redirect_meta_TOSR 之前预测时的现场信息TOSR in.commit_meta_TOSR 更新信息中的TOSR in.redirect_meta_NOS 之前预测时的现场信息NOS in.commit_meta_ssp 更新信息中的现场信息SSP in.redirect_callAddr 重定向地址 in.commit_meta_sctr 更新信息中的现场信息SCTR RASStack模块与BasePredictor的接口关系如下：\nstack接口 转换过程 描述 s.spec_push_valid io.s2_fire(2) \u0026\u0026 s2_full_pred.hit_taken_on_call \u0026\u0026 !io.s3_redirect(2) s2输入有效，且上级预测为call跳转 s.spec_pop_valid io.s2_fire(2) \u0026\u0026 s2_full_pred.hit_taken_on_ret \u0026\u0026 !io.s3_redirect(2) s2输入有效，且上级预测为ret跳转 s.spec_push_addr s2_full_pred.fallThroughAddr + Mux(s2_full_pred.last_may_be_rvi_call, 2.U, 0.U) 上级预测器s2预测的fallThroughAddr（即PC+2），判断是否压缩指令是否需要 +2 s.redirect_isCall redirect.bits.level === 0.U \u0026\u0026 recover_cfi.pd.isCall s.redirect_isRet redirect.bits.level === 0.U \u0026\u0026 recover_cfi.pd.isRet s.redirect_meta_* redirect.bits.cfiUpdate.* s.commit_push_valid io.update.is_call_taken call指令预测正确 s.commit_push_valid io.update.is_ret_taken ret指令预测正确 s.commit_push_addr update.ftb_entry.getFallThrough(update.pc) + Mux(update.ftb_entry.last_may_be_rvi_call, 2.U, 0.U) 根据是否为压缩指令，进行地址+2或者+0 s.commit_meta_* io.update.bits.meta.asTypeOf(new RASMeta) io.out.last_stage_spec_info.* s3_meta.* s3_meta = RegEnable(s2_meta, io.s2_fire(2))由s2_meta延迟一怕得到 io.out.last_stage_meta s3_meta io.out.s2.full_pred.*.jalr_target :=stack.spec_pop_addr 预测地址（栈顶地址，只预测ret） io.out.s3.full_pred.*.jalr_target :=RegEnable(stack.spec_pop_addr, io.s2_fire(2)) 由s2延迟一拍得到 io.out.s2/3.full_pred.targets.last :=Mux(s2/3_is_jalr, s2/3_jalr_target, s2/3_last_target_in) 如果时call执行，更新targets.last的结果 时序说明 在RAS中，只涉及到2拍，S2和S3。\nS2中的主要工作：\n1、根据上一个预测器的S2预测结果，完成通过Push/PoP过程完成预测，并得到结果sepc_pop_addr。\n2、根据commit信号执行更新操作\nS3中的主要工作：\n1、根据上一个预测器S3中的预测结果，以及S2进行的操作，判断是否需要进行Pop/Push的撤销操作。预测器假定S3的预测结果比S2好，如果发生S2和S3预测不一致时，RAS预测器采信S3结果进行栈操作。\n2、S3和S2的预测过程一致，只是数据不同。\n3、执行重定向（重定向信息由前一拍获得）操作\n","categories":["香山 BPU 子模块"],"description":"","excerpt":"RAS介绍 RAS 指的是 “Return Address Stack”，即返回地址栈。它通过跟踪程序的返回地址，帮助确定程序中的分支行为。 …","ref":"/xs-bpu/docs/modules/05_ras/","tags":["子预测器","RAS"],"title":"RAS 分支预测器"},{"body":"功能列表（Feature List） 支持预测器的开启关闭操作 支持s3预测结果覆盖s2预测结果 支持ras栈的push、pop操作 支持ras栈的redirect操作 支持ras栈的update操作 支持base predictor接口 符合标准RAS预测器预测流程 ","categories":["Feature List"],"description":"","excerpt":"功能列表（Feature List） 支持预测器的开启关闭操作 支持s3预测结果覆盖s2预测结果 支持ras栈的push、pop操作 支 …","ref":"/xs-bpu/docs/feature/05_rasfeature/","tags":["RAS","Feature List"],"title":"RAS 功能列表"},{"body":" In processor microarchitecture, what is branch prediction? Why is it important? How can it be functionally and performance verified? Through this issue of Open Validation Learning, you will have a deep understanding of the BPU (Branch Prediction Unit) section in the high-performance Kunming Lake microarchitecture.\n","categories":"","description":"","excerpt":" In processor microarchitecture, what is branch prediction? Why is it …","ref":"/xs-bpu/en/docs/","tags":"","title":"Branch Predictor and Kunming Lake Microarchitecture Implementation"},{"body":" 在处理器微架构中，什么是分支预测？它为何重要？如何对其进行功能、性能验证？通过本期的开放验证学习，你将对高性能昆明湖微架构中的BPU（Branch Prediction Unit）部分有深入的了解\n","categories":"","description":"","excerpt":" 在处理器微架构中，什么是分支预测？它为何重要？如何对其进行功能、性能验证？通过本期的开放验证学习，你将对高性能昆明湖微架构中 …","ref":"/xs-bpu/docs/","tags":"","title":"分支预测器与昆明湖微架构实现"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/bpu/","tags":"","title":"BPU"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/bpu/","tags":"","title":"BPU"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/bpu-top-level/","tags":"","title":"BPU Top Level"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/bpu-%E9%A1%B6%E5%B1%82/","tags":"","title":"BPU 顶层"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/branch-folding-history/","tags":"","title":"Branch Folding History"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/branch-prediction/","tags":"","title":"Branch Prediction"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/branch-prediction-block/","tags":"","title":"Branch Prediction Block"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/branch-prediction-unit-bpu/","tags":"","title":"Branch Prediction Unit (BPU)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/example/","tags":"","title":"Example"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/feature-list/","tags":"","title":"Feature List"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/feature-list/","tags":"","title":"Feature List"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/feature-list/","tags":"","title":"Feature List"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/feature-list/","tags":"","title":"Feature List"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/fetch-target-queue-ftq/","tags":"","title":"Fetch Target Queue (FTQ)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/ftb/","tags":"","title":"FTB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/ftb/","tags":"","title":"FTB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/ftb-%E9%A1%B9/","tags":"","title":"FTB 项"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/global-branch-history/","tags":"","title":"Global Branch History"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/ittage/","tags":"","title":"ITTAGE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/ittage/","tags":"","title":"ITTAGE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/pipeline-control/","tags":"","title":"Pipeline Control"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/prediction-result-redirection/","tags":"","title":"Prediction Result Redirection"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/xs-bpu/en/","tags":"","title":"Project Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/ras/","tags":"","title":"RAS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/ras/","tags":"","title":"RAS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/redirect-request/","tags":"","title":"Redirect Request"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/redirection-request/","tags":"","title":"Redirection Request"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/shanshan/","tags":"","title":"Shanshan"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/shanshan-bpu-basic-design/","tags":"","title":"Shanshan BPU Basic Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/sub-predictor/","tags":"","title":"Sub-Predictor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/sub-predictors/","tags":"","title":"Sub-Predictors"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/subpredictor/","tags":"","title":"Subpredictor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/tage-sc/","tags":"","title":"TAGE-SC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/tage-sc/","tags":"","title":"TAGE-SC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/tutorial/","tags":"","title":"Tutorial"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/uftb/","tags":"","title":"UFTB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/uftb/","tags":"","title":"UFTB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/update-request/","tags":"","title":"Update Request"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/xiangshan/","tags":"","title":"Xiangshan"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/xiangshan-bpu-basic-design/","tags":"","title":"Xiangshan BPU Basic Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/xiangshan-bpu-interface/","tags":"","title":"Xiangshan BPU Interface"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/xiangshan-bpu-interfaces/","tags":"","title":"Xiangshan BPU Interfaces"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/xiangshan-bpu-structure/","tags":"","title":"Xiangshan BPU Structure"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/xiangshan-bpu-submodule/","tags":"","title":"Xiangshan BPU Submodule"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/categories/xiangshan-bpu-submodules/","tags":"","title":"Xiangshan BPU Submodules"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/en/tags/xiangshan-bpu-timing/","tags":"","title":"Xiangshan BPU Timing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B/","tags":"","title":"分支预测"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E5%8D%95%E5%85%83bpu/","tags":"","title":"分支预测单元（BPU）"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E5%9D%97/","tags":"","title":"分支预测块"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%88%86%E6%94%AF%E6%8A%98%E5%8F%A0%E5%8E%86%E5%8F%B2/","tags":"","title":"分支折叠历史"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E6%9B%B4%E6%96%B0%E8%AF%B7%E6%B1%82/","tags":"","title":"更新请求"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E6%95%99%E7%A8%8B/","tags":"","title":"教程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%8E%A7%E5%88%B6/","tags":"","title":"流水线控制"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%8F%96%E6%8C%87%E7%9B%AE%E6%A0%87%E9%98%9F%E5%88%97ftq/","tags":"","title":"取指目标队列（FTQ）"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%85%A8%E5%B1%80%E5%88%86%E6%94%AF%E5%8E%86%E5%8F%B2/","tags":"","title":"全局分支历史"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E7%A4%BA%E4%BE%8B/","tags":"","title":"示例"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/search/","tags":"","title":"搜索结果"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E9%A6%99%E5%B1%B1/","tags":"","title":"香山"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E9%A6%99%E5%B1%B1-bpu-%E5%9F%BA%E7%A1%80%E8%AE%BE%E8%AE%A1/","tags":"","title":"香山 BPU 基础设计"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E9%A6%99%E5%B1%B1-bpu-%E6%8E%A5%E5%8F%A3/","tags":"","title":"香山 BPU 接口"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E9%A6%99%E5%B1%B1-bpu-%E7%BB%93%E6%9E%84/","tags":"","title":"香山 BPU 结构"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E9%A6%99%E5%B1%B1-bpu-%E6%97%B6%E5%BA%8F/","tags":"","title":"香山 BPU 时序"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/categories/%E9%A6%99%E5%B1%B1-bpu-%E5%AD%90%E6%A8%A1%E5%9D%97/","tags":"","title":"香山 BPU 子模块"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/xs-bpu/","tags":"","title":"项目文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E9%87%8D%E5%AE%9A%E5%90%91/","tags":"","title":"预测结果重定向"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E9%87%8D%E5%AE%9A%E5%90%91%E8%AF%B7%E6%B1%82/","tags":"","title":"重定向请求"},{"body":"","categories":"","description":"","excerpt":"","ref":"/xs-bpu/tags/%E5%AD%90%E9%A2%84%E6%B5%8B%E5%99%A8/","tags":"","title":"子预测器"}]